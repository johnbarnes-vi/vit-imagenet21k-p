{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure you follow the preprocessing instructions in the README.md file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Lets see the directory structure of imagenet1k\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        jpeg_files = [f for f in files if f.endswith('.JPEG')]\n",
    "        if jpeg_files:  # if the list is not empty\n",
    "            print('{}Number of JPEG files: {}'.format(subindent, len(jpeg_files)))\n",
    "        for f in files:\n",
    "            if f.endswith('.txt'):\n",
    "                print('{}{}'.format(subindent, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet1k_resized/ILSVRC2012_img_train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet1k_resized/ILSVRC2012_img_val/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet21k_resized_new/imagenet21k_train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet21k_resized_new/imagenet21k_val/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is clear from the output of the above cells that preprocessing worked!\n",
    "\n",
    "We are looking to see if the validation and training sets are organized in the same manner and that they are ordered the same.\n",
    "\n",
    "This makes input into the `torchvision.datasets.ImageFolder` class work without a hitch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries to unzip `tiny-imagenet-200.zip`\n",
    "import zipfile\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "# Importing pytorch libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing custom VisionTransformer Model\n",
    "\n",
    "from models.vit import VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cuda')\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 144\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "patch_size_ = 16     # to be changed\n",
    "D_ = 1280            # to be changed\n",
    "num_layers_ = 12     # to be changed\n",
    "num_classes_ = 1000\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "model = VisionTransformer(patch_size=patch_size_, D=D_, num_layers=num_layers_, num_classes=num_classes_)\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "\n",
    "# Define a transform for training data\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Pad(4),  # Pad the image by 4 pixels\n",
    "    transforms.RandomCrop(224),  # Randomly crop a 224x224 region from the padded image\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Define a transform for validation data\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to range [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CPU cores: 24\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of available CPU cores:\", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageNet-1k has 1,281,168 training images and 50,112 validation images!\n"
     ]
    }
   ],
   "source": [
    "# Load ImageNet1k dataset and make DataLoaders\n",
    "train_dataset1k = datasets.ImageFolder(root='/mnt/imagenet1k_resized/ILSVRC2012_img_train', transform=train_transform)\n",
    "val_dataset1k = datasets.ImageFolder(root='/mnt/imagenet1k_resized/ILSVRC2012_img_val', transform=val_transform)\n",
    "\n",
    "train_loader1k = DataLoader(dataset=train_dataset1k, batch_size=batch_size, shuffle=True, num_workers=20, pin_memory=True)\n",
    "val_loader1k = DataLoader(dataset=val_dataset1k, batch_size=batch_size, shuffle=False, num_workers=20, pin_memory=True)\n",
    "\n",
    "#Calculate total steps\n",
    "total_steps = len(train_loader1k) * num_epochs\n",
    "\n",
    "# StepLR that decays the learning rate every 30 epochs\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.9)\n",
    "\n",
    "print(f\"ImageNet-1k has {len(train_loader1k)*batch_size:,} training images and {len(val_loader1k)*batch_size:,} validation images!\")\n",
    "\n",
    "# Load ImageNet21k dataset and make DataLoaders\n",
    "#train_dataset21k = datasets.ImageFolder(root='/mnt/imagenet21k_resized_new/imagenet21k_train', transform=train_transform)\n",
    "#val_dataset21k = datasets.ImageFolder(root='/mnt/imagenet21k_resized_new/imagenet21k_val', transform=val_transform)\n",
    "\n",
    "#train_loader21k = DataLoader(dataset=train_dataset21k, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "#val_loader21k = DataLoader(dataset=val_dataset21k, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "#print(f\"ImageNet-21k has {len(train_loader21k)*batch_size:,} training images and {len(val_loader21k)*batch_size:,} validation images!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8897"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images batch shape: torch.Size([144, 3, 224, 224])\n",
      "Train labels batch shape: torch.Size([144])\n",
      "Train images data type: torch.float32\n",
      "Train labels data type: torch.int64\n",
      "Validation images batch shape: torch.Size([144, 3, 224, 224])\n",
      "Validation labels batch shape: torch.Size([144])\n",
      "Validation images data type: torch.float32\n",
      "Validation labels data type: torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Inspect a batch from train_loader1k\n",
    "train_images, train_labels = next(iter(train_loader1k))\n",
    "train_images, train_labels = train_images.to(device), train_labels.to(device)\n",
    "\n",
    "print(\"Train images batch shape:\", train_images.shape)\n",
    "print(\"Train labels batch shape:\", train_labels.shape)\n",
    "print(\"Train images data type:\", train_images.dtype)\n",
    "print(\"Train labels data type:\", train_labels.dtype)\n",
    "\n",
    "# Inspect a batch from val_loader1k\n",
    "val_images, val_labels = next(iter(val_loader1k))\n",
    "\n",
    "print(\"Validation images batch shape:\", val_images.shape)\n",
    "print(\"Validation labels batch shape:\", val_labels.shape)\n",
    "print(\"Validation images data type:\", val_images.dtype)\n",
    "print(\"Validation labels data type:\", val_labels.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING COMPONENTS OF vit.py IN IPYNB BEFORE MOVING TO .PY FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Class for Image Preprocessing\n",
    "class ImagePreprocessor(nn.Module):\n",
    "    def __init__(self, patch_size):\n",
    "        super(ImagePreprocessor, self).__init__()\n",
    "        self.patch_size = patch_size  # Size of each patch\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Dynamically get the batch size and channel dimensions\n",
    "        batch_size, channel, _, _ = x.size()\n",
    "\n",
    "        # Using unfold to create patches\n",
    "        x_p = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "\n",
    "        # Reshape into the desired shape\n",
    "        x_p = x_p.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "        x_p = x_p.view(batch_size, -1, self.patch_size * self.patch_size * channel)\n",
    "\n",
    "        # Now x_p should have shape [batch_size, (Height * Width) / (patch_size * patch_size), (patch_size * patch_size * channel)]\n",
    "        \n",
    "        return x_p\n",
    "\n",
    "# Class for Patch Embedding\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_dim, D):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.D = D  # Dimension to project to\n",
    "        self.linear = nn.Linear(patch_dim, D)  # Linear projection layer\n",
    "\n",
    "    def forward(self, x_p):\n",
    "        # Project patches to D dimensions\n",
    "        x_emb = self.linear(x_p)\n",
    "        return x_emb\n",
    "\n",
    "# Class for adding a Class Token\n",
    "class ClassToken(nn.Module):\n",
    "    def __init__(self, D):\n",
    "        super(ClassToken, self).__init__()\n",
    "        self.class_token_embedding = nn.Parameter(torch.randn(1, 1, D))  # Learnable class token\n",
    "\n",
    "    def forward(self, x_emb):\n",
    "        # Prepend class token to patch embeddings\n",
    "        batch_size = x_emb.size(0)\n",
    "        class_token = self.class_token_embedding.repeat(batch_size, 1, 1)\n",
    "        x_class = torch.cat([class_token, x_emb], dim=1)\n",
    "        return x_class\n",
    "\n",
    "# Class for Position Embeddings\n",
    "class PositionEmbedding(nn.Module):\n",
    "    def __init__(self, seq_len, D):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(1, seq_len, D))  # Learnable position embeddings\n",
    "\n",
    "    def forward(self, x_class):\n",
    "        # Add position embeddings\n",
    "        x_pos = x_class + self.position_embeddings\n",
    "        return x_pos\n",
    "\n",
    "# Class for Transformer Encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, D, num_layers):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_norm = nn.LayerNorm(D)\n",
    "        self.multihead_attention = nn.MultiheadAttention(D, num_heads=4, batch_first=True)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(D, D),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(D, D)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_pos):\n",
    "        # Transformer Encoder Logic\n",
    "        for _ in range(self.num_layers):\n",
    "            x_norm = self.layer_norm(x_pos)\n",
    "            x_att, _ = self.multihead_attention(x_norm, x_norm, x_norm)\n",
    "            x_pos = x_pos + x_att\n",
    "            x_pos = x_pos + self.mlp(self.layer_norm(x_pos))\n",
    "        return x_pos\n",
    "\n",
    "# Class for Classification Head\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, D, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.linear = nn.Linear(D, num_classes)  # Linear layer for classification\n",
    "\n",
    "    def forward(self, x_transformed):\n",
    "        # Take the class token and perform classification\n",
    "        x_class_token = x_transformed[:, 0, :]\n",
    "        print(\"x_class_token.shape: \", x_class_token.shape)\n",
    "        output = self.linear(x_class_token)\n",
    "        return output\n",
    "\n",
    "# Main Vision Transformer Class\n",
    "class VisionTransformerTest(nn.Module):\n",
    "    def __init__(self, patch_size, D, num_layers, num_classes):\n",
    "        super(VisionTransformerTest, self).__init__()\n",
    "        self.image_preprocessor = ImagePreprocessor(patch_size)\n",
    "        self.patch_embedding = PatchEmbedding(patch_size * patch_size * 3, D)  # 3 channels, patch_size x patch_size patches\n",
    "        self.class_token = ClassToken(D)\n",
    "        self.position_embedding = PositionEmbedding(197, D)  # 196 patches + 1 class token\n",
    "        self.transformer_encoder = TransformerEncoder(D, num_layers)\n",
    "        self.classification_head = ClassificationHead(D, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"x.shape\",x.shape)\n",
    "        \n",
    "        # Preprocess the image into patches\n",
    "        x_p = self.image_preprocessor(x)\n",
    "        print(\"x_p.shape: \", x_p.shape)\n",
    "\n",
    "        # Generate patch embeddings\n",
    "        x_emb = self.patch_embedding(x_p)\n",
    "        print(\"x_emb.shape: \", x_emb.shape)\n",
    "\n",
    "        # Prepend the class token\n",
    "        x_class = self.class_token(x_emb)\n",
    "        print(\"x_class.shape: \", x_class.shape)\n",
    "\n",
    "        # Add position embeddings\n",
    "        x_pos = self.position_embedding(x_class)\n",
    "        print(\"x_pos.shape: \", x_pos.shape)\n",
    "\n",
    "        # Pass through the Transformer Encoder\n",
    "        x_transformed = self.transformer_encoder(x_pos)\n",
    "        print(\"x_transformed.shape: \", x_transformed.shape)\n",
    "    \n",
    "        # Perform classification\n",
    "        output = self.classification_head(x_transformed)\n",
    "        print(\"output.shape: \", output.shape)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = VisionTransformerTest(patch_size=patch_size_, D=D_, num_layers=num_layers_, num_classes=num_classes_)\n",
    "model_test.to(device)\n",
    "output = model_test(train_images)\n",
    "print(\"Above was the transformation path of the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING COMPONENTS OF vit.py IN IPYNB BEFORE MOVING TO .PY FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1/8897], Loss: 8.9146\n",
      "Epoch [1/1], Step [2/8897], Loss: 8.5165\n",
      "Epoch [1/1], Step [3/8897], Loss: 8.5579\n",
      "Epoch [1/1], Step [4/8897], Loss: 8.3703\n",
      "Epoch [1/1], Step [5/8897], Loss: 8.2106\n",
      "Epoch [1/1], Step [6/8897], Loss: 8.2161\n",
      "Epoch [1/1], Step [7/8897], Loss: 7.9085\n",
      "Epoch [1/1], Step [8/8897], Loss: 7.7888\n",
      "Epoch [1/1], Step [9/8897], Loss: 7.5731\n",
      "Epoch [1/1], Step [10/8897], Loss: 7.5243\n",
      "Epoch [1/1], Step [11/8897], Loss: 7.4088\n",
      "Epoch [1/1], Step [12/8897], Loss: 7.5002\n",
      "Epoch [1/1], Step [13/8897], Loss: 7.4136\n",
      "Epoch [1/1], Step [14/8897], Loss: 7.4367\n",
      "Epoch [1/1], Step [15/8897], Loss: 7.3578\n",
      "Epoch [1/1], Step [16/8897], Loss: 7.2442\n",
      "Epoch [1/1], Step [17/8897], Loss: 7.6125\n",
      "Epoch [1/1], Step [18/8897], Loss: 7.5075\n",
      "Epoch [1/1], Step [19/8897], Loss: 7.4366\n",
      "Epoch [1/1], Step [20/8897], Loss: 7.2725\n",
      "Epoch [1/1], Step [21/8897], Loss: 7.4335\n",
      "Epoch [1/1], Step [22/8897], Loss: 7.4321\n",
      "Epoch [1/1], Step [23/8897], Loss: 7.3891\n",
      "Epoch [1/1], Step [24/8897], Loss: 7.4236\n",
      "Epoch [1/1], Step [25/8897], Loss: 7.3339\n",
      "Epoch [1/1], Step [26/8897], Loss: 7.3859\n",
      "Epoch [1/1], Step [27/8897], Loss: 7.4322\n",
      "Epoch [1/1], Step [28/8897], Loss: 7.2629\n",
      "Epoch [1/1], Step [29/8897], Loss: 7.2528\n",
      "Epoch [1/1], Step [30/8897], Loss: 7.1444\n",
      "Epoch [1/1], Step [31/8897], Loss: 7.3100\n",
      "Epoch [1/1], Step [32/8897], Loss: 7.2656\n",
      "Epoch [1/1], Step [33/8897], Loss: 7.1450\n",
      "Epoch [1/1], Step [34/8897], Loss: 7.3017\n",
      "Epoch [1/1], Step [35/8897], Loss: 7.1863\n",
      "Epoch [1/1], Step [36/8897], Loss: 7.3376\n",
      "Epoch [1/1], Step [37/8897], Loss: 7.2879\n",
      "Epoch [1/1], Step [38/8897], Loss: 7.1696\n",
      "Epoch [1/1], Step [39/8897], Loss: 7.1598\n",
      "Epoch [1/1], Step [40/8897], Loss: 7.1724\n",
      "Epoch [1/1], Step [41/8897], Loss: 7.3479\n",
      "Epoch [1/1], Step [42/8897], Loss: 7.2518\n",
      "Epoch [1/1], Step [43/8897], Loss: 7.2798\n",
      "Epoch [1/1], Step [44/8897], Loss: 7.2613\n",
      "Epoch [1/1], Step [45/8897], Loss: 7.2246\n",
      "Epoch [1/1], Step [46/8897], Loss: 7.2805\n",
      "Epoch [1/1], Step [47/8897], Loss: 7.1707\n",
      "Epoch [1/1], Step [48/8897], Loss: 7.1649\n",
      "Epoch [1/1], Step [49/8897], Loss: 7.0931\n",
      "Epoch [1/1], Step [50/8897], Loss: 7.2490\n",
      "Epoch [1/1], Step [51/8897], Loss: 7.2664\n",
      "Epoch [1/1], Step [52/8897], Loss: 7.3194\n",
      "Epoch [1/1], Step [53/8897], Loss: 7.2971\n",
      "Epoch [1/1], Step [54/8897], Loss: 7.2627\n",
      "Epoch [1/1], Step [55/8897], Loss: 7.2361\n",
      "Epoch [1/1], Step [56/8897], Loss: 7.2234\n",
      "Epoch [1/1], Step [57/8897], Loss: 7.1001\n",
      "Epoch [1/1], Step [58/8897], Loss: 7.2084\n",
      "Epoch [1/1], Step [59/8897], Loss: 7.0201\n",
      "Epoch [1/1], Step [60/8897], Loss: 7.1153\n",
      "Epoch [1/1], Step [61/8897], Loss: 7.2682\n",
      "Epoch [1/1], Step [62/8897], Loss: 7.1621\n",
      "Epoch [1/1], Step [63/8897], Loss: 7.0692\n",
      "Epoch [1/1], Step [64/8897], Loss: 7.1561\n",
      "Epoch [1/1], Step [65/8897], Loss: 7.1713\n",
      "Epoch [1/1], Step [66/8897], Loss: 7.2442\n",
      "Epoch [1/1], Step [67/8897], Loss: 7.1984\n",
      "Epoch [1/1], Step [68/8897], Loss: 6.9643\n",
      "Epoch [1/1], Step [69/8897], Loss: 7.1385\n",
      "Epoch [1/1], Step [70/8897], Loss: 7.1612\n",
      "Epoch [1/1], Step [71/8897], Loss: 7.2961\n",
      "Epoch [1/1], Step [72/8897], Loss: 7.2401\n",
      "Epoch [1/1], Step [73/8897], Loss: 7.1934\n",
      "Epoch [1/1], Step [74/8897], Loss: 7.1162\n",
      "Epoch [1/1], Step [75/8897], Loss: 7.1855\n",
      "Epoch [1/1], Step [76/8897], Loss: 7.0785\n",
      "Epoch [1/1], Step [77/8897], Loss: 7.1365\n",
      "Epoch [1/1], Step [78/8897], Loss: 7.0881\n",
      "Epoch [1/1], Step [79/8897], Loss: 7.2505\n",
      "Epoch [1/1], Step [80/8897], Loss: 7.1900\n",
      "Epoch [1/1], Step [81/8897], Loss: 7.2042\n",
      "Epoch [1/1], Step [82/8897], Loss: 7.0773\n",
      "Epoch [1/1], Step [83/8897], Loss: 7.1541\n",
      "Epoch [1/1], Step [84/8897], Loss: 7.1741\n",
      "Epoch [1/1], Step [85/8897], Loss: 7.1053\n",
      "Epoch [1/1], Step [86/8897], Loss: 7.1541\n",
      "Epoch [1/1], Step [87/8897], Loss: 7.3304\n",
      "Epoch [1/1], Step [88/8897], Loss: 7.2535\n",
      "Epoch [1/1], Step [89/8897], Loss: 7.2359\n",
      "Epoch [1/1], Step [90/8897], Loss: 7.0327\n",
      "Epoch [1/1], Step [91/8897], Loss: 7.0218\n",
      "Epoch [1/1], Step [92/8897], Loss: 7.0577\n",
      "Epoch [1/1], Step [93/8897], Loss: 7.1864\n",
      "Epoch [1/1], Step [94/8897], Loss: 7.0553\n",
      "Epoch [1/1], Step [95/8897], Loss: 7.0934\n",
      "Epoch [1/1], Step [96/8897], Loss: 7.0878\n",
      "Epoch [1/1], Step [97/8897], Loss: 6.9084\n",
      "Epoch [1/1], Step [98/8897], Loss: 7.1317\n",
      "Epoch [1/1], Step [99/8897], Loss: 6.9905\n",
      "Epoch [1/1], Step [100/8897], Loss: 7.1550\n",
      "Epoch [1/1], Step [101/8897], Loss: 7.0474\n",
      "Epoch [1/1], Step [102/8897], Loss: 7.0059\n",
      "Epoch [1/1], Step [103/8897], Loss: 6.9608\n",
      "Epoch [1/1], Step [104/8897], Loss: 7.1042\n",
      "Epoch [1/1], Step [105/8897], Loss: 7.1724\n",
      "Epoch [1/1], Step [106/8897], Loss: 7.2015\n",
      "Epoch [1/1], Step [107/8897], Loss: 7.1713\n",
      "Epoch [1/1], Step [108/8897], Loss: 7.1609\n",
      "Epoch [1/1], Step [109/8897], Loss: 7.1258\n",
      "Epoch [1/1], Step [110/8897], Loss: 7.0892\n",
      "Epoch [1/1], Step [111/8897], Loss: 7.0981\n",
      "Epoch [1/1], Step [112/8897], Loss: 7.1854\n",
      "Epoch [1/1], Step [113/8897], Loss: 7.1108\n",
      "Epoch [1/1], Step [114/8897], Loss: 7.1049\n",
      "Epoch [1/1], Step [115/8897], Loss: 7.1481\n",
      "Epoch [1/1], Step [116/8897], Loss: 7.0661\n",
      "Epoch [1/1], Step [117/8897], Loss: 6.9043\n",
      "Epoch [1/1], Step [118/8897], Loss: 7.1542\n",
      "Epoch [1/1], Step [119/8897], Loss: 7.0848\n",
      "Epoch [1/1], Step [120/8897], Loss: 7.1788\n",
      "Epoch [1/1], Step [121/8897], Loss: 7.0806\n",
      "Epoch [1/1], Step [122/8897], Loss: 7.0921\n",
      "Epoch [1/1], Step [123/8897], Loss: 7.1277\n",
      "Epoch [1/1], Step [124/8897], Loss: 7.1274\n",
      "Epoch [1/1], Step [125/8897], Loss: 6.9903\n",
      "Epoch [1/1], Step [126/8897], Loss: 7.0799\n",
      "Epoch [1/1], Step [127/8897], Loss: 7.0496\n",
      "Epoch [1/1], Step [128/8897], Loss: 7.1879\n",
      "Epoch [1/1], Step [129/8897], Loss: 7.1775\n",
      "Epoch [1/1], Step [130/8897], Loss: 7.1721\n",
      "Epoch [1/1], Step [131/8897], Loss: 7.1167\n",
      "Epoch [1/1], Step [132/8897], Loss: 7.0137\n",
      "Epoch [1/1], Step [133/8897], Loss: 6.9050\n",
      "Epoch [1/1], Step [134/8897], Loss: 7.0106\n",
      "Epoch [1/1], Step [135/8897], Loss: 7.1083\n",
      "Epoch [1/1], Step [136/8897], Loss: 7.1744\n",
      "Epoch [1/1], Step [137/8897], Loss: 7.1952\n",
      "Epoch [1/1], Step [138/8897], Loss: 7.0130\n",
      "Epoch [1/1], Step [139/8897], Loss: 7.0788\n",
      "Epoch [1/1], Step [140/8897], Loss: 7.0613\n",
      "Epoch [1/1], Step [141/8897], Loss: 7.1627\n",
      "Epoch [1/1], Step [142/8897], Loss: 7.0196\n",
      "Epoch [1/1], Step [143/8897], Loss: 7.0041\n",
      "Epoch [1/1], Step [144/8897], Loss: 6.9956\n",
      "Epoch [1/1], Step [145/8897], Loss: 7.0828\n",
      "Epoch [1/1], Step [146/8897], Loss: 6.9794\n",
      "Epoch [1/1], Step [147/8897], Loss: 7.0416\n",
      "Epoch [1/1], Step [148/8897], Loss: 7.0680\n",
      "Epoch [1/1], Step [149/8897], Loss: 7.1083\n",
      "Epoch [1/1], Step [150/8897], Loss: 6.9311\n",
      "Epoch [1/1], Step [151/8897], Loss: 6.9329\n",
      "Epoch [1/1], Step [152/8897], Loss: 7.0668\n",
      "Epoch [1/1], Step [153/8897], Loss: 7.1678\n",
      "Epoch [1/1], Step [154/8897], Loss: 6.9855\n",
      "Epoch [1/1], Step [155/8897], Loss: 7.0197\n",
      "Epoch [1/1], Step [156/8897], Loss: 6.8886\n",
      "Epoch [1/1], Step [157/8897], Loss: 6.8249\n",
      "Epoch [1/1], Step [158/8897], Loss: 7.0336\n",
      "Epoch [1/1], Step [159/8897], Loss: 7.0006\n",
      "Epoch [1/1], Step [160/8897], Loss: 7.0605\n",
      "Epoch [1/1], Step [161/8897], Loss: 7.0731\n",
      "Epoch [1/1], Step [162/8897], Loss: 6.9096\n",
      "Epoch [1/1], Step [163/8897], Loss: 7.0870\n",
      "Epoch [1/1], Step [164/8897], Loss: 6.9815\n",
      "Epoch [1/1], Step [165/8897], Loss: 7.0608\n",
      "Epoch [1/1], Step [166/8897], Loss: 6.8973\n",
      "Epoch [1/1], Step [167/8897], Loss: 6.9624\n",
      "Epoch [1/1], Step [168/8897], Loss: 7.0517\n",
      "Epoch [1/1], Step [169/8897], Loss: 7.0654\n",
      "Epoch [1/1], Step [170/8897], Loss: 7.0226\n",
      "Epoch [1/1], Step [171/8897], Loss: 7.0374\n",
      "Epoch [1/1], Step [172/8897], Loss: 7.0079\n",
      "Epoch [1/1], Step [173/8897], Loss: 7.0148\n",
      "Epoch [1/1], Step [174/8897], Loss: 7.0427\n",
      "Epoch [1/1], Step [175/8897], Loss: 6.9250\n",
      "Epoch [1/1], Step [176/8897], Loss: 7.0612\n",
      "Epoch [1/1], Step [177/8897], Loss: 7.0481\n",
      "Epoch [1/1], Step [178/8897], Loss: 7.1583\n",
      "Epoch [1/1], Step [179/8897], Loss: 6.9854\n",
      "Epoch [1/1], Step [180/8897], Loss: 7.0712\n",
      "Epoch [1/1], Step [181/8897], Loss: 7.0362\n",
      "Epoch [1/1], Step [182/8897], Loss: 6.9566\n",
      "Epoch [1/1], Step [183/8897], Loss: 7.0118\n",
      "Epoch [1/1], Step [184/8897], Loss: 6.9331\n",
      "Epoch [1/1], Step [185/8897], Loss: 7.0454\n",
      "Epoch [1/1], Step [186/8897], Loss: 7.0624\n",
      "Epoch [1/1], Step [187/8897], Loss: 7.0215\n",
      "Epoch [1/1], Step [188/8897], Loss: 6.9372\n",
      "Epoch [1/1], Step [189/8897], Loss: 6.9876\n",
      "Epoch [1/1], Step [190/8897], Loss: 6.8768\n",
      "Epoch [1/1], Step [191/8897], Loss: 6.9034\n",
      "Epoch [1/1], Step [192/8897], Loss: 6.9589\n",
      "Epoch [1/1], Step [193/8897], Loss: 7.0276\n",
      "Epoch [1/1], Step [194/8897], Loss: 6.9789\n",
      "Epoch [1/1], Step [195/8897], Loss: 6.8225\n",
      "Epoch [1/1], Step [196/8897], Loss: 6.9542\n",
      "Epoch [1/1], Step [197/8897], Loss: 7.0186\n",
      "Epoch [1/1], Step [198/8897], Loss: 7.0024\n",
      "Epoch [1/1], Step [199/8897], Loss: 6.9576\n",
      "Epoch [1/1], Step [200/8897], Loss: 6.9518\n",
      "Epoch [1/1], Step [201/8897], Loss: 6.9520\n",
      "Epoch [1/1], Step [202/8897], Loss: 6.9276\n",
      "Epoch [1/1], Step [203/8897], Loss: 7.0540\n",
      "Epoch [1/1], Step [204/8897], Loss: 7.0600\n",
      "Epoch [1/1], Step [205/8897], Loss: 7.1338\n",
      "Epoch [1/1], Step [206/8897], Loss: 6.8954\n",
      "Epoch [1/1], Step [207/8897], Loss: 7.0307\n",
      "Epoch [1/1], Step [208/8897], Loss: 7.0336\n",
      "Epoch [1/1], Step [209/8897], Loss: 6.9555\n",
      "Epoch [1/1], Step [210/8897], Loss: 6.9887\n",
      "Epoch [1/1], Step [211/8897], Loss: 6.9317\n",
      "Epoch [1/1], Step [212/8897], Loss: 6.9787\n",
      "Epoch [1/1], Step [213/8897], Loss: 6.8856\n",
      "Epoch [1/1], Step [214/8897], Loss: 6.9115\n",
      "Epoch [1/1], Step [215/8897], Loss: 6.9672\n",
      "Epoch [1/1], Step [216/8897], Loss: 6.9832\n",
      "Epoch [1/1], Step [217/8897], Loss: 6.9696\n",
      "Epoch [1/1], Step [218/8897], Loss: 6.9108\n",
      "Epoch [1/1], Step [219/8897], Loss: 7.0363\n",
      "Epoch [1/1], Step [220/8897], Loss: 6.8531\n",
      "Epoch [1/1], Step [221/8897], Loss: 6.9011\n",
      "Epoch [1/1], Step [222/8897], Loss: 7.0367\n",
      "Epoch [1/1], Step [223/8897], Loss: 6.9161\n",
      "Epoch [1/1], Step [224/8897], Loss: 6.9392\n",
      "Epoch [1/1], Step [225/8897], Loss: 7.0011\n",
      "Epoch [1/1], Step [226/8897], Loss: 6.8821\n",
      "Epoch [1/1], Step [227/8897], Loss: 7.0643\n",
      "Epoch [1/1], Step [228/8897], Loss: 6.8384\n",
      "Epoch [1/1], Step [229/8897], Loss: 6.9221\n",
      "Epoch [1/1], Step [230/8897], Loss: 6.8945\n",
      "Epoch [1/1], Step [231/8897], Loss: 7.0316\n",
      "Epoch [1/1], Step [232/8897], Loss: 7.0013\n",
      "Epoch [1/1], Step [233/8897], Loss: 6.8102\n",
      "Epoch [1/1], Step [234/8897], Loss: 6.8664\n",
      "Epoch [1/1], Step [235/8897], Loss: 6.7886\n",
      "Epoch [1/1], Step [236/8897], Loss: 6.8715\n",
      "Epoch [1/1], Step [237/8897], Loss: 6.8421\n",
      "Epoch [1/1], Step [238/8897], Loss: 7.0357\n",
      "Epoch [1/1], Step [239/8897], Loss: 7.0623\n",
      "Epoch [1/1], Step [240/8897], Loss: 6.9692\n",
      "Epoch [1/1], Step [241/8897], Loss: 6.9037\n",
      "Epoch [1/1], Step [242/8897], Loss: 6.9468\n",
      "Epoch [1/1], Step [243/8897], Loss: 6.8143\n",
      "Epoch [1/1], Step [244/8897], Loss: 6.9057\n",
      "Epoch [1/1], Step [245/8897], Loss: 6.8999\n",
      "Epoch [1/1], Step [246/8897], Loss: 6.8908\n",
      "Epoch [1/1], Step [247/8897], Loss: 6.9238\n",
      "Epoch [1/1], Step [248/8897], Loss: 6.8954\n",
      "Epoch [1/1], Step [249/8897], Loss: 6.9048\n",
      "Epoch [1/1], Step [250/8897], Loss: 6.8506\n",
      "Epoch [1/1], Step [251/8897], Loss: 6.9630\n",
      "Epoch [1/1], Step [252/8897], Loss: 6.7382\n",
      "Epoch [1/1], Step [253/8897], Loss: 6.9359\n",
      "Epoch [1/1], Step [254/8897], Loss: 6.9100\n",
      "Epoch [1/1], Step [255/8897], Loss: 6.8102\n",
      "Epoch [1/1], Step [256/8897], Loss: 7.0368\n",
      "Epoch [1/1], Step [257/8897], Loss: 6.8643\n",
      "Epoch [1/1], Step [258/8897], Loss: 7.0083\n",
      "Epoch [1/1], Step [259/8897], Loss: 6.8236\n",
      "Epoch [1/1], Step [260/8897], Loss: 6.8999\n",
      "Epoch [1/1], Step [261/8897], Loss: 6.8326\n",
      "Epoch [1/1], Step [262/8897], Loss: 6.9608\n",
      "Epoch [1/1], Step [263/8897], Loss: 7.0480\n",
      "Epoch [1/1], Step [264/8897], Loss: 6.8792\n",
      "Epoch [1/1], Step [265/8897], Loss: 6.9403\n",
      "Epoch [1/1], Step [266/8897], Loss: 6.9195\n",
      "Epoch [1/1], Step [267/8897], Loss: 6.9797\n",
      "Epoch [1/1], Step [268/8897], Loss: 6.9079\n",
      "Epoch [1/1], Step [269/8897], Loss: 6.9764\n",
      "Epoch [1/1], Step [270/8897], Loss: 6.8903\n",
      "Epoch [1/1], Step [271/8897], Loss: 6.9112\n",
      "Epoch [1/1], Step [272/8897], Loss: 7.0179\n",
      "Epoch [1/1], Step [273/8897], Loss: 6.8366\n",
      "Epoch [1/1], Step [274/8897], Loss: 6.9121\n",
      "Epoch [1/1], Step [275/8897], Loss: 6.8729\n",
      "Epoch [1/1], Step [276/8897], Loss: 6.9202\n",
      "Epoch [1/1], Step [277/8897], Loss: 6.8858\n",
      "Epoch [1/1], Step [278/8897], Loss: 6.8059\n",
      "Epoch [1/1], Step [279/8897], Loss: 6.9052\n",
      "Epoch [1/1], Step [280/8897], Loss: 6.8120\n",
      "Epoch [1/1], Step [281/8897], Loss: 6.9091\n",
      "Epoch [1/1], Step [282/8897], Loss: 6.9015\n",
      "Epoch [1/1], Step [283/8897], Loss: 7.0837\n",
      "Epoch [1/1], Step [284/8897], Loss: 6.8486\n",
      "Epoch [1/1], Step [285/8897], Loss: 7.0041\n",
      "Epoch [1/1], Step [286/8897], Loss: 6.8794\n",
      "Epoch [1/1], Step [287/8897], Loss: 6.8473\n",
      "Epoch [1/1], Step [288/8897], Loss: 6.8065\n",
      "Epoch [1/1], Step [289/8897], Loss: 6.7380\n",
      "Epoch [1/1], Step [290/8897], Loss: 6.9703\n",
      "Epoch [1/1], Step [291/8897], Loss: 6.8961\n",
      "Epoch [1/1], Step [292/8897], Loss: 6.8375\n",
      "Epoch [1/1], Step [293/8897], Loss: 7.0050\n",
      "Epoch [1/1], Step [294/8897], Loss: 6.8086\n",
      "Epoch [1/1], Step [295/8897], Loss: 6.8907\n",
      "Epoch [1/1], Step [296/8897], Loss: 7.0056\n",
      "Epoch [1/1], Step [297/8897], Loss: 6.8343\n",
      "Epoch [1/1], Step [298/8897], Loss: 6.8622\n",
      "Epoch [1/1], Step [299/8897], Loss: 7.0232\n",
      "Epoch [1/1], Step [300/8897], Loss: 6.7866\n",
      "Epoch [1/1], Step [301/8897], Loss: 6.8138\n",
      "Epoch [1/1], Step [302/8897], Loss: 6.8313\n",
      "Epoch [1/1], Step [303/8897], Loss: 7.0963\n",
      "Epoch [1/1], Step [304/8897], Loss: 6.9124\n",
      "Epoch [1/1], Step [305/8897], Loss: 6.6674\n",
      "Epoch [1/1], Step [306/8897], Loss: 6.9763\n",
      "Epoch [1/1], Step [307/8897], Loss: 6.8176\n",
      "Epoch [1/1], Step [308/8897], Loss: 6.7710\n",
      "Epoch [1/1], Step [309/8897], Loss: 6.8799\n",
      "Epoch [1/1], Step [310/8897], Loss: 6.9947\n",
      "Epoch [1/1], Step [311/8897], Loss: 6.8503\n",
      "Epoch [1/1], Step [312/8897], Loss: 6.8173\n",
      "Epoch [1/1], Step [313/8897], Loss: 6.8765\n",
      "Epoch [1/1], Step [314/8897], Loss: 6.9080\n",
      "Epoch [1/1], Step [315/8897], Loss: 6.9186\n",
      "Epoch [1/1], Step [316/8897], Loss: 6.8923\n",
      "Epoch [1/1], Step [317/8897], Loss: 6.9781\n",
      "Epoch [1/1], Step [318/8897], Loss: 6.8021\n",
      "Epoch [1/1], Step [319/8897], Loss: 6.9760\n",
      "Epoch [1/1], Step [320/8897], Loss: 6.9377\n",
      "Epoch [1/1], Step [321/8897], Loss: 6.8973\n",
      "Epoch [1/1], Step [322/8897], Loss: 6.9331\n",
      "Epoch [1/1], Step [323/8897], Loss: 6.9505\n",
      "Epoch [1/1], Step [324/8897], Loss: 6.8142\n",
      "Epoch [1/1], Step [325/8897], Loss: 6.8350\n",
      "Epoch [1/1], Step [326/8897], Loss: 6.7994\n",
      "Epoch [1/1], Step [327/8897], Loss: 6.9050\n",
      "Epoch [1/1], Step [328/8897], Loss: 6.8465\n",
      "Epoch [1/1], Step [329/8897], Loss: 6.8983\n",
      "Epoch [1/1], Step [330/8897], Loss: 6.7928\n",
      "Epoch [1/1], Step [331/8897], Loss: 6.8807\n",
      "Epoch [1/1], Step [332/8897], Loss: 7.0094\n",
      "Epoch [1/1], Step [333/8897], Loss: 7.0213\n",
      "Epoch [1/1], Step [334/8897], Loss: 6.8269\n",
      "Epoch [1/1], Step [335/8897], Loss: 7.0023\n",
      "Epoch [1/1], Step [336/8897], Loss: 6.8476\n",
      "Epoch [1/1], Step [337/8897], Loss: 6.8841\n",
      "Epoch [1/1], Step [338/8897], Loss: 6.9148\n",
      "Epoch [1/1], Step [339/8897], Loss: 6.7091\n",
      "Epoch [1/1], Step [340/8897], Loss: 6.9815\n",
      "Epoch [1/1], Step [341/8897], Loss: 6.8784\n",
      "Epoch [1/1], Step [342/8897], Loss: 6.8171\n",
      "Epoch [1/1], Step [343/8897], Loss: 6.8704\n",
      "Epoch [1/1], Step [344/8897], Loss: 6.9216\n",
      "Epoch [1/1], Step [345/8897], Loss: 6.7775\n",
      "Epoch [1/1], Step [346/8897], Loss: 6.8374\n",
      "Epoch [1/1], Step [347/8897], Loss: 6.8760\n",
      "Epoch [1/1], Step [348/8897], Loss: 6.7916\n",
      "Epoch [1/1], Step [349/8897], Loss: 6.8147\n",
      "Epoch [1/1], Step [350/8897], Loss: 6.9011\n",
      "Epoch [1/1], Step [351/8897], Loss: 6.7726\n",
      "Epoch [1/1], Step [352/8897], Loss: 6.9887\n",
      "Epoch [1/1], Step [353/8897], Loss: 6.7739\n",
      "Epoch [1/1], Step [354/8897], Loss: 6.7759\n",
      "Epoch [1/1], Step [355/8897], Loss: 6.9065\n",
      "Epoch [1/1], Step [356/8897], Loss: 7.0020\n",
      "Epoch [1/1], Step [357/8897], Loss: 6.8738\n",
      "Epoch [1/1], Step [358/8897], Loss: 6.7596\n",
      "Epoch [1/1], Step [359/8897], Loss: 6.8122\n",
      "Epoch [1/1], Step [360/8897], Loss: 6.8077\n",
      "Epoch [1/1], Step [361/8897], Loss: 6.8484\n",
      "Epoch [1/1], Step [362/8897], Loss: 6.8539\n",
      "Epoch [1/1], Step [363/8897], Loss: 6.6955\n",
      "Epoch [1/1], Step [364/8897], Loss: 6.7822\n",
      "Epoch [1/1], Step [365/8897], Loss: 6.7221\n",
      "Epoch [1/1], Step [366/8897], Loss: 6.7965\n",
      "Epoch [1/1], Step [367/8897], Loss: 6.9108\n",
      "Epoch [1/1], Step [368/8897], Loss: 6.8737\n",
      "Epoch [1/1], Step [369/8897], Loss: 6.9373\n",
      "Epoch [1/1], Step [370/8897], Loss: 6.8655\n",
      "Epoch [1/1], Step [371/8897], Loss: 6.8386\n",
      "Epoch [1/1], Step [372/8897], Loss: 6.7418\n",
      "Epoch [1/1], Step [373/8897], Loss: 6.9022\n",
      "Epoch [1/1], Step [374/8897], Loss: 6.9392\n",
      "Epoch [1/1], Step [375/8897], Loss: 6.8246\n",
      "Epoch [1/1], Step [376/8897], Loss: 6.9797\n",
      "Epoch [1/1], Step [377/8897], Loss: 6.9256\n",
      "Epoch [1/1], Step [378/8897], Loss: 6.8148\n",
      "Epoch [1/1], Step [379/8897], Loss: 6.8897\n",
      "Epoch [1/1], Step [380/8897], Loss: 6.7871\n",
      "Epoch [1/1], Step [381/8897], Loss: 6.8032\n",
      "Epoch [1/1], Step [382/8897], Loss: 6.9312\n",
      "Epoch [1/1], Step [383/8897], Loss: 6.8040\n",
      "Epoch [1/1], Step [384/8897], Loss: 6.8211\n",
      "Epoch [1/1], Step [385/8897], Loss: 6.7143\n",
      "Epoch [1/1], Step [386/8897], Loss: 6.8466\n",
      "Epoch [1/1], Step [387/8897], Loss: 6.7946\n",
      "Epoch [1/1], Step [388/8897], Loss: 6.9624\n",
      "Epoch [1/1], Step [389/8897], Loss: 6.8512\n",
      "Epoch [1/1], Step [390/8897], Loss: 6.8524\n",
      "Epoch [1/1], Step [391/8897], Loss: 6.8956\n",
      "Epoch [1/1], Step [392/8897], Loss: 6.8824\n",
      "Epoch [1/1], Step [393/8897], Loss: 6.9596\n",
      "Epoch [1/1], Step [394/8897], Loss: 6.9211\n",
      "Epoch [1/1], Step [395/8897], Loss: 6.7162\n",
      "Epoch [1/1], Step [396/8897], Loss: 6.8893\n",
      "Epoch [1/1], Step [397/8897], Loss: 6.8526\n",
      "Epoch [1/1], Step [398/8897], Loss: 6.8219\n",
      "Epoch [1/1], Step [399/8897], Loss: 6.8740\n",
      "Epoch [1/1], Step [400/8897], Loss: 6.8472\n",
      "Epoch [1/1], Step [401/8897], Loss: 6.6879\n",
      "Epoch [1/1], Step [402/8897], Loss: 6.6067\n",
      "Epoch [1/1], Step [403/8897], Loss: 6.5749\n",
      "Epoch [1/1], Step [404/8897], Loss: 6.8721\n",
      "Epoch [1/1], Step [405/8897], Loss: 6.7204\n",
      "Epoch [1/1], Step [406/8897], Loss: 6.8439\n",
      "Epoch [1/1], Step [407/8897], Loss: 6.9244\n",
      "Epoch [1/1], Step [408/8897], Loss: 6.9184\n",
      "Epoch [1/1], Step [409/8897], Loss: 6.8493\n",
      "Epoch [1/1], Step [410/8897], Loss: 6.8955\n",
      "Epoch [1/1], Step [411/8897], Loss: 6.7063\n",
      "Epoch [1/1], Step [412/8897], Loss: 6.8847\n",
      "Epoch [1/1], Step [413/8897], Loss: 6.7497\n",
      "Epoch [1/1], Step [414/8897], Loss: 6.7168\n",
      "Epoch [1/1], Step [415/8897], Loss: 6.8909\n",
      "Epoch [1/1], Step [416/8897], Loss: 6.7858\n",
      "Epoch [1/1], Step [417/8897], Loss: 6.9107\n",
      "Epoch [1/1], Step [418/8897], Loss: 6.8582\n",
      "Epoch [1/1], Step [419/8897], Loss: 6.7465\n",
      "Epoch [1/1], Step [420/8897], Loss: 6.8935\n",
      "Epoch [1/1], Step [421/8897], Loss: 6.8556\n",
      "Epoch [1/1], Step [422/8897], Loss: 6.5932\n",
      "Epoch [1/1], Step [423/8897], Loss: 6.7897\n",
      "Epoch [1/1], Step [424/8897], Loss: 6.8695\n",
      "Epoch [1/1], Step [425/8897], Loss: 6.9393\n",
      "Epoch [1/1], Step [426/8897], Loss: 6.8944\n",
      "Epoch [1/1], Step [427/8897], Loss: 6.8850\n",
      "Epoch [1/1], Step [428/8897], Loss: 6.6642\n",
      "Epoch [1/1], Step [429/8897], Loss: 6.7881\n",
      "Epoch [1/1], Step [430/8897], Loss: 6.8469\n",
      "Epoch [1/1], Step [431/8897], Loss: 6.6615\n",
      "Epoch [1/1], Step [432/8897], Loss: 6.7103\n",
      "Epoch [1/1], Step [433/8897], Loss: 6.7780\n",
      "Epoch [1/1], Step [434/8897], Loss: 6.7893\n",
      "Epoch [1/1], Step [435/8897], Loss: 6.8263\n",
      "Epoch [1/1], Step [436/8897], Loss: 6.7955\n",
      "Epoch [1/1], Step [437/8897], Loss: 6.8380\n",
      "Epoch [1/1], Step [438/8897], Loss: 6.7851\n",
      "Epoch [1/1], Step [439/8897], Loss: 6.9308\n",
      "Epoch [1/1], Step [440/8897], Loss: 6.7760\n",
      "Epoch [1/1], Step [441/8897], Loss: 6.6761\n",
      "Epoch [1/1], Step [442/8897], Loss: 6.7771\n",
      "Epoch [1/1], Step [443/8897], Loss: 6.7345\n",
      "Epoch [1/1], Step [444/8897], Loss: 6.9796\n",
      "Epoch [1/1], Step [445/8897], Loss: 6.9052\n",
      "Epoch [1/1], Step [446/8897], Loss: 6.7051\n",
      "Epoch [1/1], Step [447/8897], Loss: 6.7391\n",
      "Epoch [1/1], Step [448/8897], Loss: 6.8319\n",
      "Epoch [1/1], Step [449/8897], Loss: 6.8576\n",
      "Epoch [1/1], Step [450/8897], Loss: 6.8717\n",
      "Epoch [1/1], Step [451/8897], Loss: 6.8858\n",
      "Epoch [1/1], Step [452/8897], Loss: 6.7863\n",
      "Epoch [1/1], Step [453/8897], Loss: 6.9057\n",
      "Epoch [1/1], Step [454/8897], Loss: 6.8252\n",
      "Epoch [1/1], Step [455/8897], Loss: 6.7582\n",
      "Epoch [1/1], Step [456/8897], Loss: 6.6609\n",
      "Epoch [1/1], Step [457/8897], Loss: 6.7886\n",
      "Epoch [1/1], Step [458/8897], Loss: 6.9558\n",
      "Epoch [1/1], Step [459/8897], Loss: 6.8285\n",
      "Epoch [1/1], Step [460/8897], Loss: 6.9090\n",
      "Epoch [1/1], Step [461/8897], Loss: 6.8907\n",
      "Epoch [1/1], Step [462/8897], Loss: 6.9288\n",
      "Epoch [1/1], Step [463/8897], Loss: 6.7359\n",
      "Epoch [1/1], Step [464/8897], Loss: 6.7872\n",
      "Epoch [1/1], Step [465/8897], Loss: 6.7930\n",
      "Epoch [1/1], Step [466/8897], Loss: 6.7084\n",
      "Epoch [1/1], Step [467/8897], Loss: 6.8769\n",
      "Epoch [1/1], Step [468/8897], Loss: 6.7374\n",
      "Epoch [1/1], Step [469/8897], Loss: 6.6788\n",
      "Epoch [1/1], Step [470/8897], Loss: 6.8553\n",
      "Epoch [1/1], Step [471/8897], Loss: 6.7112\n",
      "Epoch [1/1], Step [472/8897], Loss: 6.7935\n",
      "Epoch [1/1], Step [473/8897], Loss: 6.7752\n",
      "Epoch [1/1], Step [474/8897], Loss: 6.8356\n",
      "Epoch [1/1], Step [475/8897], Loss: 6.6937\n",
      "Epoch [1/1], Step [476/8897], Loss: 6.8170\n",
      "Epoch [1/1], Step [477/8897], Loss: 6.8885\n",
      "Epoch [1/1], Step [478/8897], Loss: 6.7276\n",
      "Epoch [1/1], Step [479/8897], Loss: 6.8193\n",
      "Epoch [1/1], Step [480/8897], Loss: 6.7248\n",
      "Epoch [1/1], Step [481/8897], Loss: 6.7869\n",
      "Epoch [1/1], Step [482/8897], Loss: 6.8511\n",
      "Epoch [1/1], Step [483/8897], Loss: 6.7474\n",
      "Epoch [1/1], Step [484/8897], Loss: 6.9373\n",
      "Epoch [1/1], Step [485/8897], Loss: 6.8499\n",
      "Epoch [1/1], Step [486/8897], Loss: 6.7411\n",
      "Epoch [1/1], Step [487/8897], Loss: 6.8086\n",
      "Epoch [1/1], Step [488/8897], Loss: 6.8397\n",
      "Epoch [1/1], Step [489/8897], Loss: 6.8501\n",
      "Epoch [1/1], Step [490/8897], Loss: 6.8539\n",
      "Epoch [1/1], Step [491/8897], Loss: 6.7437\n",
      "Epoch [1/1], Step [492/8897], Loss: 6.8555\n",
      "Epoch [1/1], Step [493/8897], Loss: 6.9349\n",
      "Epoch [1/1], Step [494/8897], Loss: 6.7652\n",
      "Epoch [1/1], Step [495/8897], Loss: 6.8431\n",
      "Epoch [1/1], Step [496/8897], Loss: 6.7729\n",
      "Epoch [1/1], Step [497/8897], Loss: 6.7911\n",
      "Epoch [1/1], Step [498/8897], Loss: 6.8309\n",
      "Epoch [1/1], Step [499/8897], Loss: 6.7846\n",
      "Epoch [1/1], Step [500/8897], Loss: 6.7888\n",
      "Epoch [1/1], Step [501/8897], Loss: 6.7333\n",
      "Epoch [1/1], Step [502/8897], Loss: 6.7619\n",
      "Epoch [1/1], Step [503/8897], Loss: 6.8527\n",
      "Epoch [1/1], Step [504/8897], Loss: 6.8468\n",
      "Epoch [1/1], Step [505/8897], Loss: 6.7494\n",
      "Epoch [1/1], Step [506/8897], Loss: 6.7470\n",
      "Epoch [1/1], Step [507/8897], Loss: 6.8660\n",
      "Epoch [1/1], Step [508/8897], Loss: 6.6488\n",
      "Epoch [1/1], Step [509/8897], Loss: 6.8986\n",
      "Epoch [1/1], Step [510/8897], Loss: 6.7927\n",
      "Epoch [1/1], Step [511/8897], Loss: 6.9060\n",
      "Epoch [1/1], Step [512/8897], Loss: 6.7824\n",
      "Epoch [1/1], Step [513/8897], Loss: 6.8439\n",
      "Epoch [1/1], Step [514/8897], Loss: 6.8691\n",
      "Epoch [1/1], Step [515/8897], Loss: 6.7015\n",
      "Epoch [1/1], Step [516/8897], Loss: 6.6344\n",
      "Epoch [1/1], Step [517/8897], Loss: 6.7225\n",
      "Epoch [1/1], Step [518/8897], Loss: 6.5945\n",
      "Epoch [1/1], Step [519/8897], Loss: 6.9292\n",
      "Epoch [1/1], Step [520/8897], Loss: 6.8622\n",
      "Epoch [1/1], Step [521/8897], Loss: 6.8014\n",
      "Epoch [1/1], Step [522/8897], Loss: 6.9693\n",
      "Epoch [1/1], Step [523/8897], Loss: 6.7329\n",
      "Epoch [1/1], Step [524/8897], Loss: 6.6450\n",
      "Epoch [1/1], Step [525/8897], Loss: 6.6078\n",
      "Epoch [1/1], Step [526/8897], Loss: 6.7477\n",
      "Epoch [1/1], Step [527/8897], Loss: 6.7631\n",
      "Epoch [1/1], Step [528/8897], Loss: 6.7480\n",
      "Epoch [1/1], Step [529/8897], Loss: 6.6401\n",
      "Epoch [1/1], Step [530/8897], Loss: 6.7200\n",
      "Epoch [1/1], Step [531/8897], Loss: 6.8416\n",
      "Epoch [1/1], Step [532/8897], Loss: 6.8203\n",
      "Epoch [1/1], Step [533/8897], Loss: 6.5985\n",
      "Epoch [1/1], Step [534/8897], Loss: 6.7174\n",
      "Epoch [1/1], Step [535/8897], Loss: 6.7532\n",
      "Epoch [1/1], Step [536/8897], Loss: 6.6071\n",
      "Epoch [1/1], Step [537/8897], Loss: 6.8178\n",
      "Epoch [1/1], Step [538/8897], Loss: 6.8515\n",
      "Epoch [1/1], Step [539/8897], Loss: 6.7224\n",
      "Epoch [1/1], Step [540/8897], Loss: 6.8413\n",
      "Epoch [1/1], Step [541/8897], Loss: 6.7263\n",
      "Epoch [1/1], Step [542/8897], Loss: 6.8336\n",
      "Epoch [1/1], Step [543/8897], Loss: 6.7742\n",
      "Epoch [1/1], Step [544/8897], Loss: 6.7857\n",
      "Epoch [1/1], Step [545/8897], Loss: 6.8379\n",
      "Epoch [1/1], Step [546/8897], Loss: 6.6888\n",
      "Epoch [1/1], Step [547/8897], Loss: 6.6608\n",
      "Epoch [1/1], Step [548/8897], Loss: 6.6027\n",
      "Epoch [1/1], Step [549/8897], Loss: 6.9048\n",
      "Epoch [1/1], Step [550/8897], Loss: 6.7804\n",
      "Epoch [1/1], Step [551/8897], Loss: 6.9202\n",
      "Epoch [1/1], Step [552/8897], Loss: 6.6120\n",
      "Epoch [1/1], Step [553/8897], Loss: 6.7323\n",
      "Epoch [1/1], Step [554/8897], Loss: 6.7803\n",
      "Epoch [1/1], Step [555/8897], Loss: 6.7888\n",
      "Epoch [1/1], Step [556/8897], Loss: 6.6577\n",
      "Epoch [1/1], Step [557/8897], Loss: 6.7372\n",
      "Epoch [1/1], Step [558/8897], Loss: 6.7242\n",
      "Epoch [1/1], Step [559/8897], Loss: 6.8352\n",
      "Epoch [1/1], Step [560/8897], Loss: 6.6928\n",
      "Epoch [1/1], Step [561/8897], Loss: 6.8582\n",
      "Epoch [1/1], Step [562/8897], Loss: 6.6775\n",
      "Epoch [1/1], Step [563/8897], Loss: 6.6965\n",
      "Epoch [1/1], Step [564/8897], Loss: 6.5951\n",
      "Epoch [1/1], Step [565/8897], Loss: 6.6549\n",
      "Epoch [1/1], Step [566/8897], Loss: 6.7852\n",
      "Epoch [1/1], Step [567/8897], Loss: 6.6401\n",
      "Epoch [1/1], Step [568/8897], Loss: 6.6149\n",
      "Epoch [1/1], Step [569/8897], Loss: 6.8152\n",
      "Epoch [1/1], Step [570/8897], Loss: 6.7031\n",
      "Epoch [1/1], Step [571/8897], Loss: 6.8739\n",
      "Epoch [1/1], Step [572/8897], Loss: 6.8424\n",
      "Epoch [1/1], Step [573/8897], Loss: 6.9130\n",
      "Epoch [1/1], Step [574/8897], Loss: 6.8759\n",
      "Epoch [1/1], Step [575/8897], Loss: 6.8639\n",
      "Epoch [1/1], Step [576/8897], Loss: 6.7684\n",
      "Epoch [1/1], Step [577/8897], Loss: 6.6631\n",
      "Epoch [1/1], Step [578/8897], Loss: 6.7443\n",
      "Epoch [1/1], Step [579/8897], Loss: 6.9322\n",
      "Epoch [1/1], Step [580/8897], Loss: 6.7070\n",
      "Epoch [1/1], Step [581/8897], Loss: 6.7770\n",
      "Epoch [1/1], Step [582/8897], Loss: 6.9231\n",
      "Epoch [1/1], Step [583/8897], Loss: 6.8408\n",
      "Epoch [1/1], Step [584/8897], Loss: 6.8237\n",
      "Epoch [1/1], Step [585/8897], Loss: 6.9437\n",
      "Epoch [1/1], Step [586/8897], Loss: 6.7442\n",
      "Epoch [1/1], Step [587/8897], Loss: 6.7432\n",
      "Epoch [1/1], Step [588/8897], Loss: 6.8750\n",
      "Epoch [1/1], Step [589/8897], Loss: 6.6864\n",
      "Epoch [1/1], Step [590/8897], Loss: 6.8104\n",
      "Epoch [1/1], Step [591/8897], Loss: 6.6885\n",
      "Epoch [1/1], Step [592/8897], Loss: 6.6592\n",
      "Epoch [1/1], Step [593/8897], Loss: 6.7947\n",
      "Epoch [1/1], Step [594/8897], Loss: 6.6837\n",
      "Epoch [1/1], Step [595/8897], Loss: 6.8910\n",
      "Epoch [1/1], Step [596/8897], Loss: 6.6825\n",
      "Epoch [1/1], Step [597/8897], Loss: 6.6365\n",
      "Epoch [1/1], Step [598/8897], Loss: 6.8617\n",
      "Epoch [1/1], Step [599/8897], Loss: 6.9052\n",
      "Epoch [1/1], Step [600/8897], Loss: 6.6278\n",
      "Epoch [1/1], Step [601/8897], Loss: 6.7414\n",
      "Epoch [1/1], Step [602/8897], Loss: 6.7827\n",
      "Epoch [1/1], Step [603/8897], Loss: 6.7613\n",
      "Epoch [1/1], Step [604/8897], Loss: 6.8395\n",
      "Epoch [1/1], Step [605/8897], Loss: 6.7810\n",
      "Epoch [1/1], Step [606/8897], Loss: 6.9033\n",
      "Epoch [1/1], Step [607/8897], Loss: 6.8342\n",
      "Epoch [1/1], Step [608/8897], Loss: 6.6960\n",
      "Epoch [1/1], Step [609/8897], Loss: 6.8025\n",
      "Epoch [1/1], Step [610/8897], Loss: 6.6665\n",
      "Epoch [1/1], Step [611/8897], Loss: 6.6603\n",
      "Epoch [1/1], Step [612/8897], Loss: 6.6160\n",
      "Epoch [1/1], Step [613/8897], Loss: 6.7763\n",
      "Epoch [1/1], Step [614/8897], Loss: 6.7973\n",
      "Epoch [1/1], Step [615/8897], Loss: 6.8038\n",
      "Epoch [1/1], Step [616/8897], Loss: 6.7341\n",
      "Epoch [1/1], Step [617/8897], Loss: 6.7500\n",
      "Epoch [1/1], Step [618/8897], Loss: 6.7259\n",
      "Epoch [1/1], Step [619/8897], Loss: 6.8902\n",
      "Epoch [1/1], Step [620/8897], Loss: 6.6203\n",
      "Epoch [1/1], Step [621/8897], Loss: 6.7314\n",
      "Epoch [1/1], Step [622/8897], Loss: 6.6660\n",
      "Epoch [1/1], Step [623/8897], Loss: 6.8048\n",
      "Epoch [1/1], Step [624/8897], Loss: 6.7864\n",
      "Epoch [1/1], Step [625/8897], Loss: 6.6354\n",
      "Epoch [1/1], Step [626/8897], Loss: 6.7543\n",
      "Epoch [1/1], Step [627/8897], Loss: 6.6718\n",
      "Epoch [1/1], Step [628/8897], Loss: 6.8037\n",
      "Epoch [1/1], Step [629/8897], Loss: 6.6948\n",
      "Epoch [1/1], Step [630/8897], Loss: 6.7988\n",
      "Epoch [1/1], Step [631/8897], Loss: 6.6394\n",
      "Epoch [1/1], Step [632/8897], Loss: 6.6488\n",
      "Epoch [1/1], Step [633/8897], Loss: 6.5873\n",
      "Epoch [1/1], Step [634/8897], Loss: 6.7504\n",
      "Epoch [1/1], Step [635/8897], Loss: 6.7465\n",
      "Epoch [1/1], Step [636/8897], Loss: 6.7311\n",
      "Epoch [1/1], Step [637/8897], Loss: 6.7765\n",
      "Epoch [1/1], Step [638/8897], Loss: 6.7491\n",
      "Epoch [1/1], Step [639/8897], Loss: 6.7067\n",
      "Epoch [1/1], Step [640/8897], Loss: 6.7065\n",
      "Epoch [1/1], Step [641/8897], Loss: 6.6765\n",
      "Epoch [1/1], Step [642/8897], Loss: 6.7596\n",
      "Epoch [1/1], Step [643/8897], Loss: 6.7907\n",
      "Epoch [1/1], Step [644/8897], Loss: 6.7762\n",
      "Epoch [1/1], Step [645/8897], Loss: 6.9180\n",
      "Epoch [1/1], Step [646/8897], Loss: 6.8980\n",
      "Epoch [1/1], Step [647/8897], Loss: 6.7816\n",
      "Epoch [1/1], Step [648/8897], Loss: 6.8025\n",
      "Epoch [1/1], Step [649/8897], Loss: 6.8279\n",
      "Epoch [1/1], Step [650/8897], Loss: 6.6435\n",
      "Epoch [1/1], Step [651/8897], Loss: 6.6692\n",
      "Epoch [1/1], Step [652/8897], Loss: 6.6800\n",
      "Epoch [1/1], Step [653/8897], Loss: 6.6358\n",
      "Epoch [1/1], Step [654/8897], Loss: 6.7012\n",
      "Epoch [1/1], Step [655/8897], Loss: 6.8214\n",
      "Epoch [1/1], Step [656/8897], Loss: 6.5693\n",
      "Epoch [1/1], Step [657/8897], Loss: 6.6632\n",
      "Epoch [1/1], Step [658/8897], Loss: 6.7191\n",
      "Epoch [1/1], Step [659/8897], Loss: 6.6797\n",
      "Epoch [1/1], Step [660/8897], Loss: 6.6923\n",
      "Epoch [1/1], Step [661/8897], Loss: 6.5705\n",
      "Epoch [1/1], Step [662/8897], Loss: 6.6467\n",
      "Epoch [1/1], Step [663/8897], Loss: 6.7547\n",
      "Epoch [1/1], Step [664/8897], Loss: 6.7164\n",
      "Epoch [1/1], Step [665/8897], Loss: 6.7304\n",
      "Epoch [1/1], Step [666/8897], Loss: 6.6956\n",
      "Epoch [1/1], Step [667/8897], Loss: 6.8383\n",
      "Epoch [1/1], Step [668/8897], Loss: 6.7622\n",
      "Epoch [1/1], Step [669/8897], Loss: 6.7690\n",
      "Epoch [1/1], Step [670/8897], Loss: 6.7055\n",
      "Epoch [1/1], Step [671/8897], Loss: 6.6222\n",
      "Epoch [1/1], Step [672/8897], Loss: 6.8044\n",
      "Epoch [1/1], Step [673/8897], Loss: 6.8549\n",
      "Epoch [1/1], Step [674/8897], Loss: 6.8397\n",
      "Epoch [1/1], Step [675/8897], Loss: 6.7891\n",
      "Epoch [1/1], Step [676/8897], Loss: 6.6490\n",
      "Epoch [1/1], Step [677/8897], Loss: 6.6562\n",
      "Epoch [1/1], Step [678/8897], Loss: 6.5649\n",
      "Epoch [1/1], Step [679/8897], Loss: 6.6004\n",
      "Epoch [1/1], Step [680/8897], Loss: 6.6496\n",
      "Epoch [1/1], Step [681/8897], Loss: 6.7043\n",
      "Epoch [1/1], Step [682/8897], Loss: 6.6468\n",
      "Epoch [1/1], Step [683/8897], Loss: 6.8042\n",
      "Epoch [1/1], Step [684/8897], Loss: 6.6614\n",
      "Epoch [1/1], Step [685/8897], Loss: 6.7652\n",
      "Epoch [1/1], Step [686/8897], Loss: 6.8857\n",
      "Epoch [1/1], Step [687/8897], Loss: 6.8429\n",
      "Epoch [1/1], Step [688/8897], Loss: 6.7740\n",
      "Epoch [1/1], Step [689/8897], Loss: 6.7359\n",
      "Epoch [1/1], Step [690/8897], Loss: 6.6496\n",
      "Epoch [1/1], Step [691/8897], Loss: 6.7986\n",
      "Epoch [1/1], Step [692/8897], Loss: 6.6883\n",
      "Epoch [1/1], Step [693/8897], Loss: 6.8251\n",
      "Epoch [1/1], Step [694/8897], Loss: 6.7464\n",
      "Epoch [1/1], Step [695/8897], Loss: 6.6293\n",
      "Epoch [1/1], Step [696/8897], Loss: 6.5428\n",
      "Epoch [1/1], Step [697/8897], Loss: 6.7323\n",
      "Epoch [1/1], Step [698/8897], Loss: 6.4619\n",
      "Epoch [1/1], Step [699/8897], Loss: 6.7468\n",
      "Epoch [1/1], Step [700/8897], Loss: 6.7600\n",
      "Epoch [1/1], Step [701/8897], Loss: 6.6697\n",
      "Epoch [1/1], Step [702/8897], Loss: 6.7445\n",
      "Epoch [1/1], Step [703/8897], Loss: 6.7265\n",
      "Epoch [1/1], Step [704/8897], Loss: 6.6683\n",
      "Epoch [1/1], Step [705/8897], Loss: 6.7048\n",
      "Epoch [1/1], Step [706/8897], Loss: 6.9881\n",
      "Epoch [1/1], Step [707/8897], Loss: 6.7234\n",
      "Epoch [1/1], Step [708/8897], Loss: 6.5997\n",
      "Epoch [1/1], Step [709/8897], Loss: 6.7715\n",
      "Epoch [1/1], Step [710/8897], Loss: 6.6074\n",
      "Epoch [1/1], Step [711/8897], Loss: 6.3863\n",
      "Epoch [1/1], Step [712/8897], Loss: 6.5734\n",
      "Epoch [1/1], Step [713/8897], Loss: 6.7408\n",
      "Epoch [1/1], Step [714/8897], Loss: 6.8284\n",
      "Epoch [1/1], Step [715/8897], Loss: 6.5278\n",
      "Epoch [1/1], Step [716/8897], Loss: 6.6141\n",
      "Epoch [1/1], Step [717/8897], Loss: 6.6467\n",
      "Epoch [1/1], Step [718/8897], Loss: 6.5946\n",
      "Epoch [1/1], Step [719/8897], Loss: 6.4926\n",
      "Epoch [1/1], Step [720/8897], Loss: 6.6560\n",
      "Epoch [1/1], Step [721/8897], Loss: 6.7259\n",
      "Epoch [1/1], Step [722/8897], Loss: 6.6488\n",
      "Epoch [1/1], Step [723/8897], Loss: 6.6420\n",
      "Epoch [1/1], Step [724/8897], Loss: 6.6785\n",
      "Epoch [1/1], Step [725/8897], Loss: 6.6253\n",
      "Epoch [1/1], Step [726/8897], Loss: 6.5994\n",
      "Epoch [1/1], Step [727/8897], Loss: 6.6722\n",
      "Epoch [1/1], Step [728/8897], Loss: 6.6531\n",
      "Epoch [1/1], Step [729/8897], Loss: 6.5785\n",
      "Epoch [1/1], Step [730/8897], Loss: 6.5777\n",
      "Epoch [1/1], Step [731/8897], Loss: 6.6168\n",
      "Epoch [1/1], Step [732/8897], Loss: 6.7225\n",
      "Epoch [1/1], Step [733/8897], Loss: 6.5489\n",
      "Epoch [1/1], Step [734/8897], Loss: 6.6488\n",
      "Epoch [1/1], Step [735/8897], Loss: 6.5650\n",
      "Epoch [1/1], Step [736/8897], Loss: 6.7034\n",
      "Epoch [1/1], Step [737/8897], Loss: 6.6743\n",
      "Epoch [1/1], Step [738/8897], Loss: 6.6650\n",
      "Epoch [1/1], Step [739/8897], Loss: 6.8253\n",
      "Epoch [1/1], Step [740/8897], Loss: 6.5332\n",
      "Epoch [1/1], Step [741/8897], Loss: 6.6975\n",
      "Epoch [1/1], Step [742/8897], Loss: 6.7289\n",
      "Epoch [1/1], Step [743/8897], Loss: 6.5538\n",
      "Epoch [1/1], Step [744/8897], Loss: 6.5865\n",
      "Epoch [1/1], Step [745/8897], Loss: 6.5591\n",
      "Epoch [1/1], Step [746/8897], Loss: 6.6744\n",
      "Epoch [1/1], Step [747/8897], Loss: 6.7513\n",
      "Epoch [1/1], Step [748/8897], Loss: 6.4563\n",
      "Epoch [1/1], Step [749/8897], Loss: 6.6589\n",
      "Epoch [1/1], Step [750/8897], Loss: 6.7470\n",
      "Epoch [1/1], Step [751/8897], Loss: 6.7537\n",
      "Epoch [1/1], Step [752/8897], Loss: 6.8812\n",
      "Epoch [1/1], Step [753/8897], Loss: 6.6152\n",
      "Epoch [1/1], Step [754/8897], Loss: 6.7503\n",
      "Epoch [1/1], Step [755/8897], Loss: 6.6746\n",
      "Epoch [1/1], Step [756/8897], Loss: 6.7781\n",
      "Epoch [1/1], Step [757/8897], Loss: 6.6998\n",
      "Epoch [1/1], Step [758/8897], Loss: 6.7088\n",
      "Epoch [1/1], Step [759/8897], Loss: 6.6267\n",
      "Epoch [1/1], Step [760/8897], Loss: 6.8554\n",
      "Epoch [1/1], Step [761/8897], Loss: 6.5081\n",
      "Epoch [1/1], Step [762/8897], Loss: 6.5965\n",
      "Epoch [1/1], Step [763/8897], Loss: 6.6595\n",
      "Epoch [1/1], Step [764/8897], Loss: 6.6565\n",
      "Epoch [1/1], Step [765/8897], Loss: 6.7407\n",
      "Epoch [1/1], Step [766/8897], Loss: 6.5220\n",
      "Epoch [1/1], Step [767/8897], Loss: 6.7331\n",
      "Epoch [1/1], Step [768/8897], Loss: 6.4866\n",
      "Epoch [1/1], Step [769/8897], Loss: 6.7207\n",
      "Epoch [1/1], Step [770/8897], Loss: 6.6445\n",
      "Epoch [1/1], Step [771/8897], Loss: 6.6511\n",
      "Epoch [1/1], Step [772/8897], Loss: 6.5635\n",
      "Epoch [1/1], Step [773/8897], Loss: 6.7351\n",
      "Epoch [1/1], Step [774/8897], Loss: 6.6048\n",
      "Epoch [1/1], Step [775/8897], Loss: 6.6150\n",
      "Epoch [1/1], Step [776/8897], Loss: 6.7170\n",
      "Epoch [1/1], Step [777/8897], Loss: 6.6533\n",
      "Epoch [1/1], Step [778/8897], Loss: 6.7206\n",
      "Epoch [1/1], Step [779/8897], Loss: 6.6692\n",
      "Epoch [1/1], Step [780/8897], Loss: 6.7796\n",
      "Epoch [1/1], Step [781/8897], Loss: 6.7717\n",
      "Epoch [1/1], Step [782/8897], Loss: 6.6840\n",
      "Epoch [1/1], Step [783/8897], Loss: 6.5586\n",
      "Epoch [1/1], Step [784/8897], Loss: 6.7407\n",
      "Epoch [1/1], Step [785/8897], Loss: 6.5589\n",
      "Epoch [1/1], Step [786/8897], Loss: 6.6694\n",
      "Epoch [1/1], Step [787/8897], Loss: 6.8161\n",
      "Epoch [1/1], Step [788/8897], Loss: 6.7488\n",
      "Epoch [1/1], Step [789/8897], Loss: 6.6239\n",
      "Epoch [1/1], Step [790/8897], Loss: 6.5418\n",
      "Epoch [1/1], Step [791/8897], Loss: 6.6565\n",
      "Epoch [1/1], Step [792/8897], Loss: 6.6474\n",
      "Epoch [1/1], Step [793/8897], Loss: 6.6831\n",
      "Epoch [1/1], Step [794/8897], Loss: 6.7360\n",
      "Epoch [1/1], Step [795/8897], Loss: 6.7814\n",
      "Epoch [1/1], Step [796/8897], Loss: 6.6907\n",
      "Epoch [1/1], Step [797/8897], Loss: 6.7407\n",
      "Epoch [1/1], Step [798/8897], Loss: 6.5520\n",
      "Epoch [1/1], Step [799/8897], Loss: 6.7119\n",
      "Epoch [1/1], Step [800/8897], Loss: 6.7446\n",
      "Epoch [1/1], Step [801/8897], Loss: 6.6147\n",
      "Epoch [1/1], Step [802/8897], Loss: 6.5210\n",
      "Epoch [1/1], Step [803/8897], Loss: 6.6812\n",
      "Epoch [1/1], Step [804/8897], Loss: 6.8601\n",
      "Epoch [1/1], Step [805/8897], Loss: 6.8239\n",
      "Epoch [1/1], Step [806/8897], Loss: 6.5178\n",
      "Epoch [1/1], Step [807/8897], Loss: 6.6412\n",
      "Epoch [1/1], Step [808/8897], Loss: 6.6596\n",
      "Epoch [1/1], Step [809/8897], Loss: 6.5284\n",
      "Epoch [1/1], Step [810/8897], Loss: 6.7457\n",
      "Epoch [1/1], Step [811/8897], Loss: 6.6818\n",
      "Epoch [1/1], Step [812/8897], Loss: 6.7599\n",
      "Epoch [1/1], Step [813/8897], Loss: 6.6235\n",
      "Epoch [1/1], Step [814/8897], Loss: 6.5573\n",
      "Epoch [1/1], Step [815/8897], Loss: 6.6805\n",
      "Epoch [1/1], Step [816/8897], Loss: 6.6245\n",
      "Epoch [1/1], Step [817/8897], Loss: 6.8398\n",
      "Epoch [1/1], Step [818/8897], Loss: 6.6698\n",
      "Epoch [1/1], Step [819/8897], Loss: 6.6820\n",
      "Epoch [1/1], Step [820/8897], Loss: 6.6928\n",
      "Epoch [1/1], Step [821/8897], Loss: 6.7264\n",
      "Epoch [1/1], Step [822/8897], Loss: 6.7236\n",
      "Epoch [1/1], Step [823/8897], Loss: 6.5115\n",
      "Epoch [1/1], Step [824/8897], Loss: 6.6080\n",
      "Epoch [1/1], Step [825/8897], Loss: 6.6660\n",
      "Epoch [1/1], Step [826/8897], Loss: 6.6109\n",
      "Epoch [1/1], Step [827/8897], Loss: 6.5579\n",
      "Epoch [1/1], Step [828/8897], Loss: 6.6163\n",
      "Epoch [1/1], Step [829/8897], Loss: 6.5960\n",
      "Epoch [1/1], Step [830/8897], Loss: 6.7857\n",
      "Epoch [1/1], Step [831/8897], Loss: 6.7347\n",
      "Epoch [1/1], Step [832/8897], Loss: 6.7659\n",
      "Epoch [1/1], Step [833/8897], Loss: 6.7199\n",
      "Epoch [1/1], Step [834/8897], Loss: 6.8215\n",
      "Epoch [1/1], Step [835/8897], Loss: 6.6165\n",
      "Epoch [1/1], Step [836/8897], Loss: 6.5645\n",
      "Epoch [1/1], Step [837/8897], Loss: 6.6923\n",
      "Epoch [1/1], Step [838/8897], Loss: 6.6866\n",
      "Epoch [1/1], Step [839/8897], Loss: 6.7360\n",
      "Epoch [1/1], Step [840/8897], Loss: 6.6532\n",
      "Epoch [1/1], Step [841/8897], Loss: 6.7843\n",
      "Epoch [1/1], Step [842/8897], Loss: 6.7951\n",
      "Epoch [1/1], Step [843/8897], Loss: 6.7744\n",
      "Epoch [1/1], Step [844/8897], Loss: 6.6579\n",
      "Epoch [1/1], Step [845/8897], Loss: 6.5535\n",
      "Epoch [1/1], Step [846/8897], Loss: 6.7449\n",
      "Epoch [1/1], Step [847/8897], Loss: 6.5225\n",
      "Epoch [1/1], Step [848/8897], Loss: 6.5877\n",
      "Epoch [1/1], Step [849/8897], Loss: 6.5440\n",
      "Epoch [1/1], Step [850/8897], Loss: 6.7278\n",
      "Epoch [1/1], Step [851/8897], Loss: 6.7527\n",
      "Epoch [1/1], Step [852/8897], Loss: 6.6201\n",
      "Epoch [1/1], Step [853/8897], Loss: 6.6652\n",
      "Epoch [1/1], Step [854/8897], Loss: 6.4888\n",
      "Epoch [1/1], Step [855/8897], Loss: 6.5159\n",
      "Epoch [1/1], Step [856/8897], Loss: 6.5945\n",
      "Epoch [1/1], Step [857/8897], Loss: 6.6366\n",
      "Epoch [1/1], Step [858/8897], Loss: 6.5516\n",
      "Epoch [1/1], Step [859/8897], Loss: 6.6167\n",
      "Epoch [1/1], Step [860/8897], Loss: 6.6828\n",
      "Epoch [1/1], Step [861/8897], Loss: 6.7547\n",
      "Epoch [1/1], Step [862/8897], Loss: 6.6115\n",
      "Epoch [1/1], Step [863/8897], Loss: 6.5962\n",
      "Epoch [1/1], Step [864/8897], Loss: 6.5414\n",
      "Epoch [1/1], Step [865/8897], Loss: 6.7235\n",
      "Epoch [1/1], Step [866/8897], Loss: 6.6849\n",
      "Epoch [1/1], Step [867/8897], Loss: 6.7214\n",
      "Epoch [1/1], Step [868/8897], Loss: 6.6738\n",
      "Epoch [1/1], Step [869/8897], Loss: 6.5165\n",
      "Epoch [1/1], Step [870/8897], Loss: 6.6605\n",
      "Epoch [1/1], Step [871/8897], Loss: 6.7570\n",
      "Epoch [1/1], Step [872/8897], Loss: 6.7308\n",
      "Epoch [1/1], Step [873/8897], Loss: 6.4272\n",
      "Epoch [1/1], Step [874/8897], Loss: 6.7499\n",
      "Epoch [1/1], Step [875/8897], Loss: 6.6278\n",
      "Epoch [1/1], Step [876/8897], Loss: 6.6743\n",
      "Epoch [1/1], Step [877/8897], Loss: 6.6134\n",
      "Epoch [1/1], Step [878/8897], Loss: 6.7362\n",
      "Epoch [1/1], Step [879/8897], Loss: 6.6386\n",
      "Epoch [1/1], Step [880/8897], Loss: 6.5143\n",
      "Epoch [1/1], Step [881/8897], Loss: 6.5663\n",
      "Epoch [1/1], Step [882/8897], Loss: 6.5605\n",
      "Epoch [1/1], Step [883/8897], Loss: 6.6011\n",
      "Epoch [1/1], Step [884/8897], Loss: 6.6264\n",
      "Epoch [1/1], Step [885/8897], Loss: 6.6770\n",
      "Epoch [1/1], Step [886/8897], Loss: 6.8043\n",
      "Epoch [1/1], Step [887/8897], Loss: 6.5733\n",
      "Epoch [1/1], Step [888/8897], Loss: 6.7256\n",
      "Epoch [1/1], Step [889/8897], Loss: 6.5552\n",
      "Epoch [1/1], Step [890/8897], Loss: 6.5077\n",
      "Epoch [1/1], Step [891/8897], Loss: 6.5695\n",
      "Epoch [1/1], Step [892/8897], Loss: 6.6646\n",
      "Epoch [1/1], Step [893/8897], Loss: 6.6611\n",
      "Epoch [1/1], Step [894/8897], Loss: 6.6448\n",
      "Epoch [1/1], Step [895/8897], Loss: 6.5863\n",
      "Epoch [1/1], Step [896/8897], Loss: 6.7009\n",
      "Epoch [1/1], Step [897/8897], Loss: 6.6799\n",
      "Epoch [1/1], Step [898/8897], Loss: 6.6202\n",
      "Epoch [1/1], Step [899/8897], Loss: 6.7074\n",
      "Epoch [1/1], Step [900/8897], Loss: 6.5812\n",
      "Epoch [1/1], Step [901/8897], Loss: 6.7283\n",
      "Epoch [1/1], Step [902/8897], Loss: 6.6503\n",
      "Epoch [1/1], Step [903/8897], Loss: 6.6230\n",
      "Epoch [1/1], Step [904/8897], Loss: 6.5722\n",
      "Epoch [1/1], Step [905/8897], Loss: 6.6440\n",
      "Epoch [1/1], Step [906/8897], Loss: 6.5948\n",
      "Epoch [1/1], Step [907/8897], Loss: 6.5454\n",
      "Epoch [1/1], Step [908/8897], Loss: 6.5526\n",
      "Epoch [1/1], Step [909/8897], Loss: 6.7700\n",
      "Epoch [1/1], Step [910/8897], Loss: 6.7581\n",
      "Epoch [1/1], Step [911/8897], Loss: 6.6054\n",
      "Epoch [1/1], Step [912/8897], Loss: 6.6794\n",
      "Epoch [1/1], Step [913/8897], Loss: 6.7246\n",
      "Epoch [1/1], Step [914/8897], Loss: 6.7435\n",
      "Epoch [1/1], Step [915/8897], Loss: 6.5930\n",
      "Epoch [1/1], Step [916/8897], Loss: 6.6074\n",
      "Epoch [1/1], Step [917/8897], Loss: 6.6868\n",
      "Epoch [1/1], Step [918/8897], Loss: 6.6613\n",
      "Epoch [1/1], Step [919/8897], Loss: 6.6044\n",
      "Epoch [1/1], Step [920/8897], Loss: 6.5595\n",
      "Epoch [1/1], Step [921/8897], Loss: 6.6436\n",
      "Epoch [1/1], Step [922/8897], Loss: 6.5934\n",
      "Epoch [1/1], Step [923/8897], Loss: 6.5726\n",
      "Epoch [1/1], Step [924/8897], Loss: 6.7828\n",
      "Epoch [1/1], Step [925/8897], Loss: 6.4852\n",
      "Epoch [1/1], Step [926/8897], Loss: 6.7387\n",
      "Epoch [1/1], Step [927/8897], Loss: 6.4584\n",
      "Epoch [1/1], Step [928/8897], Loss: 6.6745\n",
      "Epoch [1/1], Step [929/8897], Loss: 6.6762\n",
      "Epoch [1/1], Step [930/8897], Loss: 6.5975\n",
      "Epoch [1/1], Step [931/8897], Loss: 6.6749\n",
      "Epoch [1/1], Step [932/8897], Loss: 6.7513\n",
      "Epoch [1/1], Step [933/8897], Loss: 6.7099\n",
      "Epoch [1/1], Step [934/8897], Loss: 6.6014\n",
      "Epoch [1/1], Step [935/8897], Loss: 6.6642\n",
      "Epoch [1/1], Step [936/8897], Loss: 6.6880\n",
      "Epoch [1/1], Step [937/8897], Loss: 6.7000\n",
      "Epoch [1/1], Step [938/8897], Loss: 6.5561\n",
      "Epoch [1/1], Step [939/8897], Loss: 6.5190\n",
      "Epoch [1/1], Step [940/8897], Loss: 6.7444\n",
      "Epoch [1/1], Step [941/8897], Loss: 6.4166\n",
      "Epoch [1/1], Step [942/8897], Loss: 6.6569\n",
      "Epoch [1/1], Step [943/8897], Loss: 6.6674\n",
      "Epoch [1/1], Step [944/8897], Loss: 6.5440\n",
      "Epoch [1/1], Step [945/8897], Loss: 6.6188\n",
      "Epoch [1/1], Step [946/8897], Loss: 6.5900\n",
      "Epoch [1/1], Step [947/8897], Loss: 6.7403\n",
      "Epoch [1/1], Step [948/8897], Loss: 6.6656\n",
      "Epoch [1/1], Step [949/8897], Loss: 6.5960\n",
      "Epoch [1/1], Step [950/8897], Loss: 6.6202\n",
      "Epoch [1/1], Step [951/8897], Loss: 6.5349\n",
      "Epoch [1/1], Step [952/8897], Loss: 6.7236\n",
      "Epoch [1/1], Step [953/8897], Loss: 6.4342\n",
      "Epoch [1/1], Step [954/8897], Loss: 6.5240\n",
      "Epoch [1/1], Step [955/8897], Loss: 6.5935\n",
      "Epoch [1/1], Step [956/8897], Loss: 6.6706\n",
      "Epoch [1/1], Step [957/8897], Loss: 6.7453\n",
      "Epoch [1/1], Step [958/8897], Loss: 6.6803\n",
      "Epoch [1/1], Step [959/8897], Loss: 6.6062\n",
      "Epoch [1/1], Step [960/8897], Loss: 6.6047\n",
      "Epoch [1/1], Step [961/8897], Loss: 6.6114\n",
      "Epoch [1/1], Step [962/8897], Loss: 6.4931\n",
      "Epoch [1/1], Step [963/8897], Loss: 6.5493\n",
      "Epoch [1/1], Step [964/8897], Loss: 6.6037\n",
      "Epoch [1/1], Step [965/8897], Loss: 6.7133\n",
      "Epoch [1/1], Step [966/8897], Loss: 6.4894\n",
      "Epoch [1/1], Step [967/8897], Loss: 6.6555\n",
      "Epoch [1/1], Step [968/8897], Loss: 6.4626\n",
      "Epoch [1/1], Step [969/8897], Loss: 6.5268\n",
      "Epoch [1/1], Step [970/8897], Loss: 6.6941\n",
      "Epoch [1/1], Step [971/8897], Loss: 6.7951\n",
      "Epoch [1/1], Step [972/8897], Loss: 6.7628\n",
      "Epoch [1/1], Step [973/8897], Loss: 6.5772\n",
      "Epoch [1/1], Step [974/8897], Loss: 6.5827\n",
      "Epoch [1/1], Step [975/8897], Loss: 6.7617\n",
      "Epoch [1/1], Step [976/8897], Loss: 6.8266\n",
      "Epoch [1/1], Step [977/8897], Loss: 6.5622\n",
      "Epoch [1/1], Step [978/8897], Loss: 6.6352\n",
      "Epoch [1/1], Step [979/8897], Loss: 6.6578\n",
      "Epoch [1/1], Step [980/8897], Loss: 6.5550\n",
      "Epoch [1/1], Step [981/8897], Loss: 6.6015\n",
      "Epoch [1/1], Step [982/8897], Loss: 6.6260\n",
      "Epoch [1/1], Step [983/8897], Loss: 6.7104\n",
      "Epoch [1/1], Step [984/8897], Loss: 6.6199\n",
      "Epoch [1/1], Step [985/8897], Loss: 6.5289\n",
      "Epoch [1/1], Step [986/8897], Loss: 6.7249\n",
      "Epoch [1/1], Step [987/8897], Loss: 6.6645\n",
      "Epoch [1/1], Step [988/8897], Loss: 6.6202\n",
      "Epoch [1/1], Step [989/8897], Loss: 6.6135\n",
      "Epoch [1/1], Step [990/8897], Loss: 6.5950\n",
      "Epoch [1/1], Step [991/8897], Loss: 6.5954\n",
      "Epoch [1/1], Step [992/8897], Loss: 6.5976\n",
      "Epoch [1/1], Step [993/8897], Loss: 6.7176\n",
      "Epoch [1/1], Step [994/8897], Loss: 6.6460\n",
      "Epoch [1/1], Step [995/8897], Loss: 6.6976\n",
      "Epoch [1/1], Step [996/8897], Loss: 6.6497\n",
      "Epoch [1/1], Step [997/8897], Loss: 6.5530\n",
      "Epoch [1/1], Step [998/8897], Loss: 6.5259\n",
      "Epoch [1/1], Step [999/8897], Loss: 6.5914\n",
      "Epoch [1/1], Step [1000/8897], Loss: 6.5024\n",
      "Epoch [1/1], Step [1001/8897], Loss: 6.5435\n",
      "Epoch [1/1], Step [1002/8897], Loss: 6.6304\n",
      "Epoch [1/1], Step [1003/8897], Loss: 6.6210\n",
      "Epoch [1/1], Step [1004/8897], Loss: 6.5188\n",
      "Epoch [1/1], Step [1005/8897], Loss: 6.5401\n",
      "Epoch [1/1], Step [1006/8897], Loss: 6.5182\n",
      "Epoch [1/1], Step [1007/8897], Loss: 6.5385\n",
      "Epoch [1/1], Step [1008/8897], Loss: 6.6109\n",
      "Epoch [1/1], Step [1009/8897], Loss: 6.5514\n",
      "Epoch [1/1], Step [1010/8897], Loss: 6.6890\n",
      "Epoch [1/1], Step [1011/8897], Loss: 6.4767\n",
      "Epoch [1/1], Step [1012/8897], Loss: 6.4756\n",
      "Epoch [1/1], Step [1013/8897], Loss: 6.6062\n",
      "Epoch [1/1], Step [1014/8897], Loss: 6.5656\n",
      "Epoch [1/1], Step [1015/8897], Loss: 6.4822\n",
      "Epoch [1/1], Step [1016/8897], Loss: 6.5715\n",
      "Epoch [1/1], Step [1017/8897], Loss: 6.6536\n",
      "Epoch [1/1], Step [1018/8897], Loss: 6.4434\n",
      "Epoch [1/1], Step [1019/8897], Loss: 6.4687\n",
      "Epoch [1/1], Step [1020/8897], Loss: 6.5241\n",
      "Epoch [1/1], Step [1021/8897], Loss: 6.5086\n",
      "Epoch [1/1], Step [1022/8897], Loss: 6.5611\n",
      "Epoch [1/1], Step [1023/8897], Loss: 6.8030\n",
      "Epoch [1/1], Step [1024/8897], Loss: 6.6283\n",
      "Epoch [1/1], Step [1025/8897], Loss: 6.6467\n",
      "Epoch [1/1], Step [1026/8897], Loss: 6.4096\n",
      "Epoch [1/1], Step [1027/8897], Loss: 6.5702\n",
      "Epoch [1/1], Step [1028/8897], Loss: 6.5642\n",
      "Epoch [1/1], Step [1029/8897], Loss: 6.4796\n",
      "Epoch [1/1], Step [1030/8897], Loss: 6.6124\n",
      "Epoch [1/1], Step [1031/8897], Loss: 6.5215\n",
      "Epoch [1/1], Step [1032/8897], Loss: 6.6976\n",
      "Epoch [1/1], Step [1033/8897], Loss: 6.5960\n",
      "Epoch [1/1], Step [1034/8897], Loss: 6.6293\n",
      "Epoch [1/1], Step [1035/8897], Loss: 6.5432\n",
      "Epoch [1/1], Step [1036/8897], Loss: 6.7091\n",
      "Epoch [1/1], Step [1037/8897], Loss: 6.5190\n",
      "Epoch [1/1], Step [1038/8897], Loss: 6.7463\n",
      "Epoch [1/1], Step [1039/8897], Loss: 6.6633\n",
      "Epoch [1/1], Step [1040/8897], Loss: 6.6516\n",
      "Epoch [1/1], Step [1041/8897], Loss: 6.5777\n",
      "Epoch [1/1], Step [1042/8897], Loss: 6.7676\n",
      "Epoch [1/1], Step [1043/8897], Loss: 6.6630\n",
      "Epoch [1/1], Step [1044/8897], Loss: 6.4660\n",
      "Epoch [1/1], Step [1045/8897], Loss: 6.6788\n",
      "Epoch [1/1], Step [1046/8897], Loss: 6.4408\n",
      "Epoch [1/1], Step [1047/8897], Loss: 6.6624\n",
      "Epoch [1/1], Step [1048/8897], Loss: 6.5824\n",
      "Epoch [1/1], Step [1049/8897], Loss: 6.5834\n",
      "Epoch [1/1], Step [1050/8897], Loss: 6.5071\n",
      "Epoch [1/1], Step [1051/8897], Loss: 6.5143\n",
      "Epoch [1/1], Step [1052/8897], Loss: 6.5042\n",
      "Epoch [1/1], Step [1053/8897], Loss: 6.5149\n",
      "Epoch [1/1], Step [1054/8897], Loss: 6.5975\n",
      "Epoch [1/1], Step [1055/8897], Loss: 6.4843\n",
      "Epoch [1/1], Step [1056/8897], Loss: 6.5321\n",
      "Epoch [1/1], Step [1057/8897], Loss: 6.6603\n",
      "Epoch [1/1], Step [1058/8897], Loss: 6.5568\n",
      "Epoch [1/1], Step [1059/8897], Loss: 6.5738\n",
      "Epoch [1/1], Step [1060/8897], Loss: 6.5969\n",
      "Epoch [1/1], Step [1061/8897], Loss: 6.5515\n",
      "Epoch [1/1], Step [1062/8897], Loss: 6.6704\n",
      "Epoch [1/1], Step [1063/8897], Loss: 6.5596\n",
      "Epoch [1/1], Step [1064/8897], Loss: 6.4977\n",
      "Epoch [1/1], Step [1065/8897], Loss: 6.5702\n",
      "Epoch [1/1], Step [1066/8897], Loss: 6.5963\n",
      "Epoch [1/1], Step [1067/8897], Loss: 6.5017\n",
      "Epoch [1/1], Step [1068/8897], Loss: 6.4510\n",
      "Epoch [1/1], Step [1069/8897], Loss: 6.5484\n",
      "Epoch [1/1], Step [1070/8897], Loss: 6.5070\n",
      "Epoch [1/1], Step [1071/8897], Loss: 6.6028\n",
      "Epoch [1/1], Step [1072/8897], Loss: 6.5901\n",
      "Epoch [1/1], Step [1073/8897], Loss: 6.6406\n",
      "Epoch [1/1], Step [1074/8897], Loss: 6.4811\n",
      "Epoch [1/1], Step [1075/8897], Loss: 6.5306\n",
      "Epoch [1/1], Step [1076/8897], Loss: 6.4400\n",
      "Epoch [1/1], Step [1077/8897], Loss: 6.4275\n",
      "Epoch [1/1], Step [1078/8897], Loss: 6.4670\n",
      "Epoch [1/1], Step [1079/8897], Loss: 6.4947\n",
      "Epoch [1/1], Step [1080/8897], Loss: 6.3948\n",
      "Epoch [1/1], Step [1081/8897], Loss: 6.5178\n",
      "Epoch [1/1], Step [1082/8897], Loss: 6.4678\n",
      "Epoch [1/1], Step [1083/8897], Loss: 6.6992\n",
      "Epoch [1/1], Step [1084/8897], Loss: 6.5055\n",
      "Epoch [1/1], Step [1085/8897], Loss: 6.5752\n",
      "Epoch [1/1], Step [1086/8897], Loss: 6.4193\n",
      "Epoch [1/1], Step [1087/8897], Loss: 6.4661\n",
      "Epoch [1/1], Step [1088/8897], Loss: 6.5966\n",
      "Epoch [1/1], Step [1089/8897], Loss: 6.4238\n",
      "Epoch [1/1], Step [1090/8897], Loss: 6.4523\n",
      "Epoch [1/1], Step [1091/8897], Loss: 6.2905\n",
      "Epoch [1/1], Step [1092/8897], Loss: 6.6268\n",
      "Epoch [1/1], Step [1093/8897], Loss: 6.4512\n",
      "Epoch [1/1], Step [1094/8897], Loss: 6.4992\n",
      "Epoch [1/1], Step [1095/8897], Loss: 6.5184\n",
      "Epoch [1/1], Step [1096/8897], Loss: 6.8097\n",
      "Epoch [1/1], Step [1097/8897], Loss: 6.5737\n",
      "Epoch [1/1], Step [1098/8897], Loss: 6.4635\n",
      "Epoch [1/1], Step [1099/8897], Loss: 6.5498\n",
      "Epoch [1/1], Step [1100/8897], Loss: 6.7252\n",
      "Epoch [1/1], Step [1101/8897], Loss: 6.6122\n",
      "Epoch [1/1], Step [1102/8897], Loss: 6.5593\n",
      "Epoch [1/1], Step [1103/8897], Loss: 6.4466\n",
      "Epoch [1/1], Step [1104/8897], Loss: 6.6107\n",
      "Epoch [1/1], Step [1105/8897], Loss: 6.5609\n",
      "Epoch [1/1], Step [1106/8897], Loss: 6.4175\n",
      "Epoch [1/1], Step [1107/8897], Loss: 6.5157\n",
      "Epoch [1/1], Step [1108/8897], Loss: 6.5030\n",
      "Epoch [1/1], Step [1109/8897], Loss: 6.7223\n",
      "Epoch [1/1], Step [1110/8897], Loss: 6.5441\n",
      "Epoch [1/1], Step [1111/8897], Loss: 6.6187\n",
      "Epoch [1/1], Step [1112/8897], Loss: 6.4843\n",
      "Epoch [1/1], Step [1113/8897], Loss: 6.5199\n",
      "Epoch [1/1], Step [1114/8897], Loss: 6.4714\n",
      "Epoch [1/1], Step [1115/8897], Loss: 6.5039\n",
      "Epoch [1/1], Step [1116/8897], Loss: 6.6015\n",
      "Epoch [1/1], Step [1117/8897], Loss: 6.4863\n",
      "Epoch [1/1], Step [1118/8897], Loss: 6.5979\n",
      "Epoch [1/1], Step [1119/8897], Loss: 6.6378\n",
      "Epoch [1/1], Step [1120/8897], Loss: 6.5402\n",
      "Epoch [1/1], Step [1121/8897], Loss: 6.4866\n",
      "Epoch [1/1], Step [1122/8897], Loss: 6.5120\n",
      "Epoch [1/1], Step [1123/8897], Loss: 6.3782\n",
      "Epoch [1/1], Step [1124/8897], Loss: 6.6049\n",
      "Epoch [1/1], Step [1125/8897], Loss: 6.5254\n",
      "Epoch [1/1], Step [1126/8897], Loss: 6.4637\n",
      "Epoch [1/1], Step [1127/8897], Loss: 6.5813\n",
      "Epoch [1/1], Step [1128/8897], Loss: 6.6828\n",
      "Epoch [1/1], Step [1129/8897], Loss: 6.5038\n",
      "Epoch [1/1], Step [1130/8897], Loss: 6.6839\n",
      "Epoch [1/1], Step [1131/8897], Loss: 6.6160\n",
      "Epoch [1/1], Step [1132/8897], Loss: 6.4122\n",
      "Epoch [1/1], Step [1133/8897], Loss: 6.4035\n",
      "Epoch [1/1], Step [1134/8897], Loss: 6.5410\n",
      "Epoch [1/1], Step [1135/8897], Loss: 6.4161\n",
      "Epoch [1/1], Step [1136/8897], Loss: 6.4190\n",
      "Epoch [1/1], Step [1137/8897], Loss: 6.4954\n",
      "Epoch [1/1], Step [1138/8897], Loss: 6.4173\n",
      "Epoch [1/1], Step [1139/8897], Loss: 6.4470\n",
      "Epoch [1/1], Step [1140/8897], Loss: 6.5454\n",
      "Epoch [1/1], Step [1141/8897], Loss: 6.6350\n",
      "Epoch [1/1], Step [1142/8897], Loss: 6.5910\n",
      "Epoch [1/1], Step [1143/8897], Loss: 6.5648\n",
      "Epoch [1/1], Step [1144/8897], Loss: 6.6266\n",
      "Epoch [1/1], Step [1145/8897], Loss: 6.5486\n",
      "Epoch [1/1], Step [1146/8897], Loss: 6.5722\n",
      "Epoch [1/1], Step [1147/8897], Loss: 6.5015\n",
      "Epoch [1/1], Step [1148/8897], Loss: 6.4899\n",
      "Epoch [1/1], Step [1149/8897], Loss: 6.4508\n",
      "Epoch [1/1], Step [1150/8897], Loss: 6.3395\n",
      "Epoch [1/1], Step [1151/8897], Loss: 6.5464\n",
      "Epoch [1/1], Step [1152/8897], Loss: 6.3334\n",
      "Epoch [1/1], Step [1153/8897], Loss: 6.5088\n",
      "Epoch [1/1], Step [1154/8897], Loss: 6.6288\n",
      "Epoch [1/1], Step [1155/8897], Loss: 6.5497\n",
      "Epoch [1/1], Step [1156/8897], Loss: 6.4067\n",
      "Epoch [1/1], Step [1157/8897], Loss: 6.5094\n",
      "Epoch [1/1], Step [1158/8897], Loss: 6.5562\n",
      "Epoch [1/1], Step [1159/8897], Loss: 6.7036\n",
      "Epoch [1/1], Step [1160/8897], Loss: 6.4959\n",
      "Epoch [1/1], Step [1161/8897], Loss: 6.5948\n",
      "Epoch [1/1], Step [1162/8897], Loss: 6.5512\n",
      "Epoch [1/1], Step [1163/8897], Loss: 6.6179\n",
      "Epoch [1/1], Step [1164/8897], Loss: 6.3907\n",
      "Epoch [1/1], Step [1165/8897], Loss: 6.4625\n",
      "Epoch [1/1], Step [1166/8897], Loss: 6.5913\n",
      "Epoch [1/1], Step [1167/8897], Loss: 6.3455\n",
      "Epoch [1/1], Step [1168/8897], Loss: 6.5323\n",
      "Epoch [1/1], Step [1169/8897], Loss: 6.4680\n",
      "Epoch [1/1], Step [1170/8897], Loss: 6.6027\n",
      "Epoch [1/1], Step [1171/8897], Loss: 6.5598\n",
      "Epoch [1/1], Step [1172/8897], Loss: 6.5878\n",
      "Epoch [1/1], Step [1173/8897], Loss: 6.4910\n",
      "Epoch [1/1], Step [1174/8897], Loss: 6.5827\n",
      "Epoch [1/1], Step [1175/8897], Loss: 6.4439\n",
      "Epoch [1/1], Step [1176/8897], Loss: 6.6418\n",
      "Epoch [1/1], Step [1177/8897], Loss: 6.3858\n",
      "Epoch [1/1], Step [1178/8897], Loss: 6.4204\n",
      "Epoch [1/1], Step [1179/8897], Loss: 6.5631\n",
      "Epoch [1/1], Step [1180/8897], Loss: 6.6649\n",
      "Epoch [1/1], Step [1181/8897], Loss: 6.7206\n",
      "Epoch [1/1], Step [1182/8897], Loss: 6.5188\n",
      "Epoch [1/1], Step [1183/8897], Loss: 6.5183\n",
      "Epoch [1/1], Step [1184/8897], Loss: 6.4406\n",
      "Epoch [1/1], Step [1185/8897], Loss: 6.7061\n",
      "Epoch [1/1], Step [1186/8897], Loss: 6.6449\n",
      "Epoch [1/1], Step [1187/8897], Loss: 6.5055\n",
      "Epoch [1/1], Step [1188/8897], Loss: 6.6679\n",
      "Epoch [1/1], Step [1189/8897], Loss: 6.5931\n",
      "Epoch [1/1], Step [1190/8897], Loss: 6.4792\n",
      "Epoch [1/1], Step [1191/8897], Loss: 6.4483\n",
      "Epoch [1/1], Step [1192/8897], Loss: 6.5725\n",
      "Epoch [1/1], Step [1193/8897], Loss: 6.5721\n",
      "Epoch [1/1], Step [1194/8897], Loss: 6.5097\n",
      "Epoch [1/1], Step [1195/8897], Loss: 6.4113\n",
      "Epoch [1/1], Step [1196/8897], Loss: 6.5094\n",
      "Epoch [1/1], Step [1197/8897], Loss: 6.6324\n",
      "Epoch [1/1], Step [1198/8897], Loss: 6.5110\n",
      "Epoch [1/1], Step [1199/8897], Loss: 6.4872\n",
      "Epoch [1/1], Step [1200/8897], Loss: 6.4747\n",
      "Epoch [1/1], Step [1201/8897], Loss: 6.5323\n",
      "Epoch [1/1], Step [1202/8897], Loss: 6.5279\n",
      "Epoch [1/1], Step [1203/8897], Loss: 6.6655\n",
      "Epoch [1/1], Step [1204/8897], Loss: 6.6099\n",
      "Epoch [1/1], Step [1205/8897], Loss: 6.4455\n",
      "Epoch [1/1], Step [1206/8897], Loss: 6.5029\n",
      "Epoch [1/1], Step [1207/8897], Loss: 6.5081\n",
      "Epoch [1/1], Step [1208/8897], Loss: 6.4228\n",
      "Epoch [1/1], Step [1209/8897], Loss: 6.4523\n",
      "Epoch [1/1], Step [1210/8897], Loss: 6.4859\n",
      "Epoch [1/1], Step [1211/8897], Loss: 6.5593\n",
      "Epoch [1/1], Step [1212/8897], Loss: 6.3807\n",
      "Epoch [1/1], Step [1213/8897], Loss: 6.5925\n",
      "Epoch [1/1], Step [1214/8897], Loss: 6.3950\n",
      "Epoch [1/1], Step [1215/8897], Loss: 6.5442\n",
      "Epoch [1/1], Step [1216/8897], Loss: 6.3993\n",
      "Epoch [1/1], Step [1217/8897], Loss: 6.5006\n",
      "Epoch [1/1], Step [1218/8897], Loss: 6.4871\n",
      "Epoch [1/1], Step [1219/8897], Loss: 6.3938\n",
      "Epoch [1/1], Step [1220/8897], Loss: 6.4052\n",
      "Epoch [1/1], Step [1221/8897], Loss: 6.5185\n",
      "Epoch [1/1], Step [1222/8897], Loss: 6.5540\n",
      "Epoch [1/1], Step [1223/8897], Loss: 6.4460\n",
      "Epoch [1/1], Step [1224/8897], Loss: 6.6396\n",
      "Epoch [1/1], Step [1225/8897], Loss: 6.5196\n",
      "Epoch [1/1], Step [1226/8897], Loss: 6.3896\n",
      "Epoch [1/1], Step [1227/8897], Loss: 6.6258\n",
      "Epoch [1/1], Step [1228/8897], Loss: 6.4348\n",
      "Epoch [1/1], Step [1229/8897], Loss: 6.5940\n",
      "Epoch [1/1], Step [1230/8897], Loss: 6.5338\n",
      "Epoch [1/1], Step [1231/8897], Loss: 6.6465\n",
      "Epoch [1/1], Step [1232/8897], Loss: 6.5917\n",
      "Epoch [1/1], Step [1233/8897], Loss: 6.5641\n",
      "Epoch [1/1], Step [1234/8897], Loss: 6.4987\n",
      "Epoch [1/1], Step [1235/8897], Loss: 6.4855\n",
      "Epoch [1/1], Step [1236/8897], Loss: 6.5604\n",
      "Epoch [1/1], Step [1237/8897], Loss: 6.4808\n",
      "Epoch [1/1], Step [1238/8897], Loss: 6.4694\n",
      "Epoch [1/1], Step [1239/8897], Loss: 6.3884\n",
      "Epoch [1/1], Step [1240/8897], Loss: 6.3153\n",
      "Epoch [1/1], Step [1241/8897], Loss: 6.5123\n",
      "Epoch [1/1], Step [1242/8897], Loss: 6.5309\n",
      "Epoch [1/1], Step [1243/8897], Loss: 6.5630\n",
      "Epoch [1/1], Step [1244/8897], Loss: 6.3821\n",
      "Epoch [1/1], Step [1245/8897], Loss: 6.4427\n",
      "Epoch [1/1], Step [1246/8897], Loss: 6.4017\n",
      "Epoch [1/1], Step [1247/8897], Loss: 6.4448\n",
      "Epoch [1/1], Step [1248/8897], Loss: 6.4531\n",
      "Epoch [1/1], Step [1249/8897], Loss: 6.5486\n",
      "Epoch [1/1], Step [1250/8897], Loss: 6.5984\n",
      "Epoch [1/1], Step [1251/8897], Loss: 6.4812\n",
      "Epoch [1/1], Step [1252/8897], Loss: 6.4829\n",
      "Epoch [1/1], Step [1253/8897], Loss: 6.4388\n",
      "Epoch [1/1], Step [1254/8897], Loss: 6.4763\n",
      "Epoch [1/1], Step [1255/8897], Loss: 6.5225\n",
      "Epoch [1/1], Step [1256/8897], Loss: 6.3195\n",
      "Epoch [1/1], Step [1257/8897], Loss: 6.5458\n",
      "Epoch [1/1], Step [1258/8897], Loss: 6.4217\n",
      "Epoch [1/1], Step [1259/8897], Loss: 6.5806\n",
      "Epoch [1/1], Step [1260/8897], Loss: 6.3533\n",
      "Epoch [1/1], Step [1261/8897], Loss: 6.5853\n",
      "Epoch [1/1], Step [1262/8897], Loss: 6.4062\n",
      "Epoch [1/1], Step [1263/8897], Loss: 6.5215\n",
      "Epoch [1/1], Step [1264/8897], Loss: 6.3610\n",
      "Epoch [1/1], Step [1265/8897], Loss: 6.4252\n",
      "Epoch [1/1], Step [1266/8897], Loss: 6.5594\n",
      "Epoch [1/1], Step [1267/8897], Loss: 6.5722\n",
      "Epoch [1/1], Step [1268/8897], Loss: 6.5603\n",
      "Epoch [1/1], Step [1269/8897], Loss: 6.5433\n",
      "Epoch [1/1], Step [1270/8897], Loss: 6.4537\n",
      "Epoch [1/1], Step [1271/8897], Loss: 6.5191\n",
      "Epoch [1/1], Step [1272/8897], Loss: 6.2795\n",
      "Epoch [1/1], Step [1273/8897], Loss: 6.4564\n",
      "Epoch [1/1], Step [1274/8897], Loss: 6.3461\n",
      "Epoch [1/1], Step [1275/8897], Loss: 6.6604\n",
      "Epoch [1/1], Step [1276/8897], Loss: 6.4372\n",
      "Epoch [1/1], Step [1277/8897], Loss: 6.5116\n",
      "Epoch [1/1], Step [1278/8897], Loss: 6.5140\n",
      "Epoch [1/1], Step [1279/8897], Loss: 6.5669\n",
      "Epoch [1/1], Step [1280/8897], Loss: 6.4266\n",
      "Epoch [1/1], Step [1281/8897], Loss: 6.4845\n",
      "Epoch [1/1], Step [1282/8897], Loss: 6.5839\n",
      "Epoch [1/1], Step [1283/8897], Loss: 6.4273\n",
      "Epoch [1/1], Step [1284/8897], Loss: 6.4582\n",
      "Epoch [1/1], Step [1285/8897], Loss: 6.5757\n",
      "Epoch [1/1], Step [1286/8897], Loss: 6.6741\n",
      "Epoch [1/1], Step [1287/8897], Loss: 6.4473\n",
      "Epoch [1/1], Step [1288/8897], Loss: 6.4564\n",
      "Epoch [1/1], Step [1289/8897], Loss: 6.4902\n",
      "Epoch [1/1], Step [1290/8897], Loss: 6.5624\n",
      "Epoch [1/1], Step [1291/8897], Loss: 6.3870\n",
      "Epoch [1/1], Step [1292/8897], Loss: 6.4252\n",
      "Epoch [1/1], Step [1293/8897], Loss: 6.4890\n",
      "Epoch [1/1], Step [1294/8897], Loss: 6.3300\n",
      "Epoch [1/1], Step [1295/8897], Loss: 6.4367\n",
      "Epoch [1/1], Step [1296/8897], Loss: 6.4705\n",
      "Epoch [1/1], Step [1297/8897], Loss: 6.4773\n",
      "Epoch [1/1], Step [1298/8897], Loss: 6.4480\n",
      "Epoch [1/1], Step [1299/8897], Loss: 6.5351\n",
      "Epoch [1/1], Step [1300/8897], Loss: 6.5500\n",
      "Epoch [1/1], Step [1301/8897], Loss: 6.4638\n",
      "Epoch [1/1], Step [1302/8897], Loss: 6.5738\n",
      "Epoch [1/1], Step [1303/8897], Loss: 6.4171\n",
      "Epoch [1/1], Step [1304/8897], Loss: 6.4260\n",
      "Epoch [1/1], Step [1305/8897], Loss: 6.2844\n",
      "Epoch [1/1], Step [1306/8897], Loss: 6.5433\n",
      "Epoch [1/1], Step [1307/8897], Loss: 6.3416\n",
      "Epoch [1/1], Step [1308/8897], Loss: 6.5424\n",
      "Epoch [1/1], Step [1309/8897], Loss: 6.4286\n",
      "Epoch [1/1], Step [1310/8897], Loss: 6.5063\n",
      "Epoch [1/1], Step [1311/8897], Loss: 6.4441\n",
      "Epoch [1/1], Step [1312/8897], Loss: 6.4598\n",
      "Epoch [1/1], Step [1313/8897], Loss: 6.4552\n",
      "Epoch [1/1], Step [1314/8897], Loss: 6.5806\n",
      "Epoch [1/1], Step [1315/8897], Loss: 6.4160\n",
      "Epoch [1/1], Step [1316/8897], Loss: 6.3787\n",
      "Epoch [1/1], Step [1317/8897], Loss: 6.4494\n",
      "Epoch [1/1], Step [1318/8897], Loss: 6.3938\n",
      "Epoch [1/1], Step [1319/8897], Loss: 6.3170\n",
      "Epoch [1/1], Step [1320/8897], Loss: 6.4247\n",
      "Epoch [1/1], Step [1321/8897], Loss: 6.4673\n",
      "Epoch [1/1], Step [1322/8897], Loss: 6.2789\n",
      "Epoch [1/1], Step [1323/8897], Loss: 6.5346\n",
      "Epoch [1/1], Step [1324/8897], Loss: 6.4747\n",
      "Epoch [1/1], Step [1325/8897], Loss: 6.5447\n",
      "Epoch [1/1], Step [1326/8897], Loss: 6.4686\n",
      "Epoch [1/1], Step [1327/8897], Loss: 6.4237\n",
      "Epoch [1/1], Step [1328/8897], Loss: 6.4787\n",
      "Epoch [1/1], Step [1329/8897], Loss: 6.4773\n",
      "Epoch [1/1], Step [1330/8897], Loss: 6.3217\n",
      "Epoch [1/1], Step [1331/8897], Loss: 6.5157\n",
      "Epoch [1/1], Step [1332/8897], Loss: 6.3870\n",
      "Epoch [1/1], Step [1333/8897], Loss: 6.3188\n",
      "Epoch [1/1], Step [1334/8897], Loss: 6.6088\n",
      "Epoch [1/1], Step [1335/8897], Loss: 6.4750\n",
      "Epoch [1/1], Step [1336/8897], Loss: 6.4190\n",
      "Epoch [1/1], Step [1337/8897], Loss: 6.4692\n",
      "Epoch [1/1], Step [1338/8897], Loss: 6.6097\n",
      "Epoch [1/1], Step [1339/8897], Loss: 6.5474\n",
      "Epoch [1/1], Step [1340/8897], Loss: 6.4171\n",
      "Epoch [1/1], Step [1341/8897], Loss: 6.4237\n",
      "Epoch [1/1], Step [1342/8897], Loss: 6.5534\n",
      "Epoch [1/1], Step [1343/8897], Loss: 6.3493\n",
      "Epoch [1/1], Step [1344/8897], Loss: 6.5044\n",
      "Epoch [1/1], Step [1345/8897], Loss: 6.4301\n",
      "Epoch [1/1], Step [1346/8897], Loss: 6.4124\n",
      "Epoch [1/1], Step [1347/8897], Loss: 6.5623\n",
      "Epoch [1/1], Step [1348/8897], Loss: 6.3949\n",
      "Epoch [1/1], Step [1349/8897], Loss: 6.4105\n",
      "Epoch [1/1], Step [1350/8897], Loss: 6.5374\n",
      "Epoch [1/1], Step [1351/8897], Loss: 6.5452\n",
      "Epoch [1/1], Step [1352/8897], Loss: 6.5277\n",
      "Epoch [1/1], Step [1353/8897], Loss: 6.4378\n",
      "Epoch [1/1], Step [1354/8897], Loss: 6.4356\n",
      "Epoch [1/1], Step [1355/8897], Loss: 6.4386\n",
      "Epoch [1/1], Step [1356/8897], Loss: 6.5247\n",
      "Epoch [1/1], Step [1357/8897], Loss: 6.5118\n",
      "Epoch [1/1], Step [1358/8897], Loss: 6.4398\n",
      "Epoch [1/1], Step [1359/8897], Loss: 6.5319\n",
      "Epoch [1/1], Step [1360/8897], Loss: 6.6074\n",
      "Epoch [1/1], Step [1361/8897], Loss: 6.3565\n",
      "Epoch [1/1], Step [1362/8897], Loss: 6.3672\n",
      "Epoch [1/1], Step [1363/8897], Loss: 6.6040\n",
      "Epoch [1/1], Step [1364/8897], Loss: 6.4126\n",
      "Epoch [1/1], Step [1365/8897], Loss: 6.4160\n",
      "Epoch [1/1], Step [1366/8897], Loss: 6.4642\n",
      "Epoch [1/1], Step [1367/8897], Loss: 6.6075\n",
      "Epoch [1/1], Step [1368/8897], Loss: 6.4791\n",
      "Epoch [1/1], Step [1369/8897], Loss: 6.4915\n",
      "Epoch [1/1], Step [1370/8897], Loss: 6.3936\n",
      "Epoch [1/1], Step [1371/8897], Loss: 6.4581\n",
      "Epoch [1/1], Step [1372/8897], Loss: 6.3369\n",
      "Epoch [1/1], Step [1373/8897], Loss: 6.4805\n",
      "Epoch [1/1], Step [1374/8897], Loss: 6.3843\n",
      "Epoch [1/1], Step [1375/8897], Loss: 6.6749\n",
      "Epoch [1/1], Step [1376/8897], Loss: 6.4292\n",
      "Epoch [1/1], Step [1377/8897], Loss: 6.4760\n",
      "Epoch [1/1], Step [1378/8897], Loss: 6.3260\n",
      "Epoch [1/1], Step [1379/8897], Loss: 6.3051\n",
      "Epoch [1/1], Step [1380/8897], Loss: 6.3015\n",
      "Epoch [1/1], Step [1381/8897], Loss: 6.6121\n",
      "Epoch [1/1], Step [1382/8897], Loss: 6.3395\n",
      "Epoch [1/1], Step [1383/8897], Loss: 6.2885\n",
      "Epoch [1/1], Step [1384/8897], Loss: 6.3309\n",
      "Epoch [1/1], Step [1385/8897], Loss: 6.4371\n",
      "Epoch [1/1], Step [1386/8897], Loss: 6.4547\n",
      "Epoch [1/1], Step [1387/8897], Loss: 6.4456\n",
      "Epoch [1/1], Step [1388/8897], Loss: 6.2282\n",
      "Epoch [1/1], Step [1389/8897], Loss: 6.5139\n",
      "Epoch [1/1], Step [1390/8897], Loss: 6.3870\n",
      "Epoch [1/1], Step [1391/8897], Loss: 6.4821\n",
      "Epoch [1/1], Step [1392/8897], Loss: 6.4416\n",
      "Epoch [1/1], Step [1393/8897], Loss: 6.4828\n",
      "Epoch [1/1], Step [1394/8897], Loss: 6.4241\n",
      "Epoch [1/1], Step [1395/8897], Loss: 6.2778\n",
      "Epoch [1/1], Step [1396/8897], Loss: 6.5535\n",
      "Epoch [1/1], Step [1397/8897], Loss: 6.4003\n",
      "Epoch [1/1], Step [1398/8897], Loss: 6.3642\n",
      "Epoch [1/1], Step [1399/8897], Loss: 6.4924\n",
      "Epoch [1/1], Step [1400/8897], Loss: 6.4617\n",
      "Epoch [1/1], Step [1401/8897], Loss: 6.4585\n",
      "Epoch [1/1], Step [1402/8897], Loss: 6.3972\n",
      "Epoch [1/1], Step [1403/8897], Loss: 6.4479\n",
      "Epoch [1/1], Step [1404/8897], Loss: 6.2999\n",
      "Epoch [1/1], Step [1405/8897], Loss: 6.2253\n",
      "Epoch [1/1], Step [1406/8897], Loss: 6.5860\n",
      "Epoch [1/1], Step [1407/8897], Loss: 6.4156\n",
      "Epoch [1/1], Step [1408/8897], Loss: 6.3179\n",
      "Epoch [1/1], Step [1409/8897], Loss: 6.4145\n",
      "Epoch [1/1], Step [1410/8897], Loss: 6.5272\n",
      "Epoch [1/1], Step [1411/8897], Loss: 6.3979\n",
      "Epoch [1/1], Step [1412/8897], Loss: 6.5029\n",
      "Epoch [1/1], Step [1413/8897], Loss: 6.6119\n",
      "Epoch [1/1], Step [1414/8897], Loss: 6.5701\n",
      "Epoch [1/1], Step [1415/8897], Loss: 6.3999\n",
      "Epoch [1/1], Step [1416/8897], Loss: 6.3409\n",
      "Epoch [1/1], Step [1417/8897], Loss: 6.4076\n",
      "Epoch [1/1], Step [1418/8897], Loss: 6.3616\n",
      "Epoch [1/1], Step [1419/8897], Loss: 6.4814\n",
      "Epoch [1/1], Step [1420/8897], Loss: 6.5143\n",
      "Epoch [1/1], Step [1421/8897], Loss: 6.4328\n",
      "Epoch [1/1], Step [1422/8897], Loss: 6.5653\n",
      "Epoch [1/1], Step [1423/8897], Loss: 6.3488\n",
      "Epoch [1/1], Step [1424/8897], Loss: 6.4131\n",
      "Epoch [1/1], Step [1425/8897], Loss: 6.4840\n",
      "Epoch [1/1], Step [1426/8897], Loss: 6.4396\n",
      "Epoch [1/1], Step [1427/8897], Loss: 6.2312\n",
      "Epoch [1/1], Step [1428/8897], Loss: 6.4929\n",
      "Epoch [1/1], Step [1429/8897], Loss: 6.2932\n",
      "Epoch [1/1], Step [1430/8897], Loss: 6.3960\n",
      "Epoch [1/1], Step [1431/8897], Loss: 6.6327\n",
      "Epoch [1/1], Step [1432/8897], Loss: 6.3505\n",
      "Epoch [1/1], Step [1433/8897], Loss: 6.2901\n",
      "Epoch [1/1], Step [1434/8897], Loss: 6.5204\n",
      "Epoch [1/1], Step [1435/8897], Loss: 6.5514\n",
      "Epoch [1/1], Step [1436/8897], Loss: 6.5272\n",
      "Epoch [1/1], Step [1437/8897], Loss: 6.4601\n",
      "Epoch [1/1], Step [1438/8897], Loss: 6.5540\n",
      "Epoch [1/1], Step [1439/8897], Loss: 6.5147\n",
      "Epoch [1/1], Step [1440/8897], Loss: 6.5083\n",
      "Epoch [1/1], Step [1441/8897], Loss: 6.4896\n",
      "Epoch [1/1], Step [1442/8897], Loss: 6.5547\n",
      "Epoch [1/1], Step [1443/8897], Loss: 6.3756\n",
      "Epoch [1/1], Step [1444/8897], Loss: 6.3197\n",
      "Epoch [1/1], Step [1445/8897], Loss: 6.4315\n",
      "Epoch [1/1], Step [1446/8897], Loss: 6.4457\n",
      "Epoch [1/1], Step [1447/8897], Loss: 6.5437\n",
      "Epoch [1/1], Step [1448/8897], Loss: 6.3961\n",
      "Epoch [1/1], Step [1449/8897], Loss: 6.1560\n",
      "Epoch [1/1], Step [1450/8897], Loss: 6.4123\n",
      "Epoch [1/1], Step [1451/8897], Loss: 6.4159\n",
      "Epoch [1/1], Step [1452/8897], Loss: 6.5270\n",
      "Epoch [1/1], Step [1453/8897], Loss: 6.4210\n",
      "Epoch [1/1], Step [1454/8897], Loss: 6.5135\n",
      "Epoch [1/1], Step [1455/8897], Loss: 6.5188\n",
      "Epoch [1/1], Step [1456/8897], Loss: 6.3254\n",
      "Epoch [1/1], Step [1457/8897], Loss: 6.3807\n",
      "Epoch [1/1], Step [1458/8897], Loss: 6.3822\n",
      "Epoch [1/1], Step [1459/8897], Loss: 6.2208\n",
      "Epoch [1/1], Step [1460/8897], Loss: 6.5339\n",
      "Epoch [1/1], Step [1461/8897], Loss: 6.4750\n",
      "Epoch [1/1], Step [1462/8897], Loss: 6.3874\n",
      "Epoch [1/1], Step [1463/8897], Loss: 6.3583\n",
      "Epoch [1/1], Step [1464/8897], Loss: 6.3970\n",
      "Epoch [1/1], Step [1465/8897], Loss: 6.5046\n",
      "Epoch [1/1], Step [1466/8897], Loss: 6.5140\n",
      "Epoch [1/1], Step [1467/8897], Loss: 6.3993\n",
      "Epoch [1/1], Step [1468/8897], Loss: 6.3863\n",
      "Epoch [1/1], Step [1469/8897], Loss: 6.3239\n",
      "Epoch [1/1], Step [1470/8897], Loss: 6.2955\n",
      "Epoch [1/1], Step [1471/8897], Loss: 6.4229\n",
      "Epoch [1/1], Step [1472/8897], Loss: 6.2584\n",
      "Epoch [1/1], Step [1473/8897], Loss: 6.3947\n",
      "Epoch [1/1], Step [1474/8897], Loss: 6.6188\n",
      "Epoch [1/1], Step [1475/8897], Loss: 6.4811\n",
      "Epoch [1/1], Step [1476/8897], Loss: 6.4120\n",
      "Epoch [1/1], Step [1477/8897], Loss: 6.4428\n",
      "Epoch [1/1], Step [1478/8897], Loss: 6.3555\n",
      "Epoch [1/1], Step [1479/8897], Loss: 6.4261\n",
      "Epoch [1/1], Step [1480/8897], Loss: 6.3780\n",
      "Epoch [1/1], Step [1481/8897], Loss: 6.3883\n",
      "Epoch [1/1], Step [1482/8897], Loss: 6.4052\n",
      "Epoch [1/1], Step [1483/8897], Loss: 6.3716\n",
      "Epoch [1/1], Step [1484/8897], Loss: 6.3332\n",
      "Epoch [1/1], Step [1485/8897], Loss: 6.3574\n",
      "Epoch [1/1], Step [1486/8897], Loss: 6.3580\n",
      "Epoch [1/1], Step [1487/8897], Loss: 6.4311\n",
      "Epoch [1/1], Step [1488/8897], Loss: 6.5323\n",
      "Epoch [1/1], Step [1489/8897], Loss: 6.5869\n",
      "Epoch [1/1], Step [1490/8897], Loss: 6.4964\n",
      "Epoch [1/1], Step [1491/8897], Loss: 6.4064\n",
      "Epoch [1/1], Step [1492/8897], Loss: 6.3422\n",
      "Epoch [1/1], Step [1493/8897], Loss: 6.4889\n",
      "Epoch [1/1], Step [1494/8897], Loss: 6.4161\n",
      "Epoch [1/1], Step [1495/8897], Loss: 6.3587\n",
      "Epoch [1/1], Step [1496/8897], Loss: 6.4646\n",
      "Epoch [1/1], Step [1497/8897], Loss: 6.4287\n",
      "Epoch [1/1], Step [1498/8897], Loss: 6.4173\n",
      "Epoch [1/1], Step [1499/8897], Loss: 6.3221\n",
      "Epoch [1/1], Step [1500/8897], Loss: 6.4139\n",
      "Epoch [1/1], Step [1501/8897], Loss: 6.3642\n",
      "Epoch [1/1], Step [1502/8897], Loss: 6.4277\n",
      "Epoch [1/1], Step [1503/8897], Loss: 6.4037\n",
      "Epoch [1/1], Step [1504/8897], Loss: 6.4449\n",
      "Epoch [1/1], Step [1505/8897], Loss: 6.4447\n",
      "Epoch [1/1], Step [1506/8897], Loss: 6.5078\n",
      "Epoch [1/1], Step [1507/8897], Loss: 6.5970\n",
      "Epoch [1/1], Step [1508/8897], Loss: 6.4693\n",
      "Epoch [1/1], Step [1509/8897], Loss: 6.3710\n",
      "Epoch [1/1], Step [1510/8897], Loss: 6.1537\n",
      "Epoch [1/1], Step [1511/8897], Loss: 6.3366\n",
      "Epoch [1/1], Step [1512/8897], Loss: 6.4866\n",
      "Epoch [1/1], Step [1513/8897], Loss: 6.3013\n",
      "Epoch [1/1], Step [1514/8897], Loss: 6.4559\n",
      "Epoch [1/1], Step [1515/8897], Loss: 6.4901\n",
      "Epoch [1/1], Step [1516/8897], Loss: 6.2519\n",
      "Epoch [1/1], Step [1517/8897], Loss: 6.3729\n",
      "Epoch [1/1], Step [1518/8897], Loss: 6.2944\n",
      "Epoch [1/1], Step [1519/8897], Loss: 6.4370\n",
      "Epoch [1/1], Step [1520/8897], Loss: 6.2589\n",
      "Epoch [1/1], Step [1521/8897], Loss: 6.3282\n",
      "Epoch [1/1], Step [1522/8897], Loss: 6.2661\n",
      "Epoch [1/1], Step [1523/8897], Loss: 6.4711\n",
      "Epoch [1/1], Step [1524/8897], Loss: 6.3732\n",
      "Epoch [1/1], Step [1525/8897], Loss: 6.2124\n",
      "Epoch [1/1], Step [1526/8897], Loss: 6.2983\n",
      "Epoch [1/1], Step [1527/8897], Loss: 6.2865\n",
      "Epoch [1/1], Step [1528/8897], Loss: 6.5430\n",
      "Epoch [1/1], Step [1529/8897], Loss: 6.4413\n",
      "Epoch [1/1], Step [1530/8897], Loss: 6.3681\n",
      "Epoch [1/1], Step [1531/8897], Loss: 6.1622\n",
      "Epoch [1/1], Step [1532/8897], Loss: 6.4477\n",
      "Epoch [1/1], Step [1533/8897], Loss: 6.4659\n",
      "Epoch [1/1], Step [1534/8897], Loss: 6.4710\n",
      "Epoch [1/1], Step [1535/8897], Loss: 6.3695\n",
      "Epoch [1/1], Step [1536/8897], Loss: 6.4768\n",
      "Epoch [1/1], Step [1537/8897], Loss: 6.3409\n",
      "Epoch [1/1], Step [1538/8897], Loss: 6.1802\n",
      "Epoch [1/1], Step [1539/8897], Loss: 6.3060\n",
      "Epoch [1/1], Step [1540/8897], Loss: 6.2051\n",
      "Epoch [1/1], Step [1541/8897], Loss: 6.2488\n",
      "Epoch [1/1], Step [1542/8897], Loss: 6.4466\n",
      "Epoch [1/1], Step [1543/8897], Loss: 6.3133\n",
      "Epoch [1/1], Step [1544/8897], Loss: 6.4133\n",
      "Epoch [1/1], Step [1545/8897], Loss: 6.4644\n",
      "Epoch [1/1], Step [1546/8897], Loss: 6.2230\n",
      "Epoch [1/1], Step [1547/8897], Loss: 6.2728\n",
      "Epoch [1/1], Step [1548/8897], Loss: 6.3099\n",
      "Epoch [1/1], Step [1549/8897], Loss: 6.5774\n",
      "Epoch [1/1], Step [1550/8897], Loss: 6.2573\n",
      "Epoch [1/1], Step [1551/8897], Loss: 6.3966\n",
      "Epoch [1/1], Step [1552/8897], Loss: 6.3711\n",
      "Epoch [1/1], Step [1553/8897], Loss: 6.3372\n",
      "Epoch [1/1], Step [1554/8897], Loss: 6.2179\n",
      "Epoch [1/1], Step [1555/8897], Loss: 6.4143\n",
      "Epoch [1/1], Step [1556/8897], Loss: 6.2645\n",
      "Epoch [1/1], Step [1557/8897], Loss: 6.2407\n",
      "Epoch [1/1], Step [1558/8897], Loss: 6.2855\n",
      "Epoch [1/1], Step [1559/8897], Loss: 6.4674\n",
      "Epoch [1/1], Step [1560/8897], Loss: 6.3400\n",
      "Epoch [1/1], Step [1561/8897], Loss: 6.3342\n",
      "Epoch [1/1], Step [1562/8897], Loss: 6.3185\n",
      "Epoch [1/1], Step [1563/8897], Loss: 6.4024\n",
      "Epoch [1/1], Step [1564/8897], Loss: 6.2715\n",
      "Epoch [1/1], Step [1565/8897], Loss: 6.3223\n",
      "Epoch [1/1], Step [1566/8897], Loss: 6.2744\n",
      "Epoch [1/1], Step [1567/8897], Loss: 6.3331\n",
      "Epoch [1/1], Step [1568/8897], Loss: 6.4602\n",
      "Epoch [1/1], Step [1569/8897], Loss: 6.3603\n",
      "Epoch [1/1], Step [1570/8897], Loss: 6.2086\n",
      "Epoch [1/1], Step [1571/8897], Loss: 6.4548\n",
      "Epoch [1/1], Step [1572/8897], Loss: 6.4347\n",
      "Epoch [1/1], Step [1573/8897], Loss: 6.3974\n",
      "Epoch [1/1], Step [1574/8897], Loss: 6.2548\n",
      "Epoch [1/1], Step [1575/8897], Loss: 6.3815\n",
      "Epoch [1/1], Step [1576/8897], Loss: 6.3242\n",
      "Epoch [1/1], Step [1577/8897], Loss: 6.4188\n",
      "Epoch [1/1], Step [1578/8897], Loss: 6.2569\n",
      "Epoch [1/1], Step [1579/8897], Loss: 6.6142\n",
      "Epoch [1/1], Step [1580/8897], Loss: 6.5655\n",
      "Epoch [1/1], Step [1581/8897], Loss: 6.2781\n",
      "Epoch [1/1], Step [1582/8897], Loss: 6.3781\n",
      "Epoch [1/1], Step [1583/8897], Loss: 6.2759\n",
      "Epoch [1/1], Step [1584/8897], Loss: 6.5152\n",
      "Epoch [1/1], Step [1585/8897], Loss: 6.4637\n",
      "Epoch [1/1], Step [1586/8897], Loss: 6.2545\n",
      "Epoch [1/1], Step [1587/8897], Loss: 6.2501\n",
      "Epoch [1/1], Step [1588/8897], Loss: 6.4717\n",
      "Epoch [1/1], Step [1589/8897], Loss: 6.4280\n",
      "Epoch [1/1], Step [1590/8897], Loss: 6.3554\n",
      "Epoch [1/1], Step [1591/8897], Loss: 6.4020\n",
      "Epoch [1/1], Step [1592/8897], Loss: 6.4095\n",
      "Epoch [1/1], Step [1593/8897], Loss: 6.4549\n",
      "Epoch [1/1], Step [1594/8897], Loss: 6.3826\n",
      "Epoch [1/1], Step [1595/8897], Loss: 6.6291\n",
      "Epoch [1/1], Step [1596/8897], Loss: 6.4000\n",
      "Epoch [1/1], Step [1597/8897], Loss: 6.3937\n",
      "Epoch [1/1], Step [1598/8897], Loss: 6.3931\n",
      "Epoch [1/1], Step [1599/8897], Loss: 6.3586\n",
      "Epoch [1/1], Step [1600/8897], Loss: 6.3302\n",
      "Epoch [1/1], Step [1601/8897], Loss: 6.2988\n",
      "Epoch [1/1], Step [1602/8897], Loss: 6.2759\n",
      "Epoch [1/1], Step [1603/8897], Loss: 6.3055\n",
      "Epoch [1/1], Step [1604/8897], Loss: 6.3639\n",
      "Epoch [1/1], Step [1605/8897], Loss: 6.3007\n",
      "Epoch [1/1], Step [1606/8897], Loss: 6.3636\n",
      "Epoch [1/1], Step [1607/8897], Loss: 6.2481\n",
      "Epoch [1/1], Step [1608/8897], Loss: 6.3234\n",
      "Epoch [1/1], Step [1609/8897], Loss: 6.3114\n",
      "Epoch [1/1], Step [1610/8897], Loss: 6.5225\n",
      "Epoch [1/1], Step [1611/8897], Loss: 6.3307\n",
      "Epoch [1/1], Step [1612/8897], Loss: 6.6240\n",
      "Epoch [1/1], Step [1613/8897], Loss: 6.3266\n",
      "Epoch [1/1], Step [1614/8897], Loss: 6.3750\n",
      "Epoch [1/1], Step [1615/8897], Loss: 6.5473\n",
      "Epoch [1/1], Step [1616/8897], Loss: 6.2984\n",
      "Epoch [1/1], Step [1617/8897], Loss: 6.4477\n",
      "Epoch [1/1], Step [1618/8897], Loss: 6.2402\n",
      "Epoch [1/1], Step [1619/8897], Loss: 6.3719\n",
      "Epoch [1/1], Step [1620/8897], Loss: 6.3254\n",
      "Epoch [1/1], Step [1621/8897], Loss: 6.4563\n",
      "Epoch [1/1], Step [1622/8897], Loss: 6.1021\n",
      "Epoch [1/1], Step [1623/8897], Loss: 6.0278\n",
      "Epoch [1/1], Step [1624/8897], Loss: 6.3020\n",
      "Epoch [1/1], Step [1625/8897], Loss: 6.4610\n",
      "Epoch [1/1], Step [1626/8897], Loss: 6.5374\n",
      "Epoch [1/1], Step [1627/8897], Loss: 6.3062\n",
      "Epoch [1/1], Step [1628/8897], Loss: 6.0469\n",
      "Epoch [1/1], Step [1629/8897], Loss: 6.2707\n",
      "Epoch [1/1], Step [1630/8897], Loss: 6.2780\n",
      "Epoch [1/1], Step [1631/8897], Loss: 6.1186\n",
      "Epoch [1/1], Step [1632/8897], Loss: 6.2576\n",
      "Epoch [1/1], Step [1633/8897], Loss: 6.2693\n",
      "Epoch [1/1], Step [1634/8897], Loss: 6.4351\n",
      "Epoch [1/1], Step [1635/8897], Loss: 6.3673\n",
      "Epoch [1/1], Step [1636/8897], Loss: 6.2773\n",
      "Epoch [1/1], Step [1637/8897], Loss: 6.3659\n",
      "Epoch [1/1], Step [1638/8897], Loss: 6.3064\n",
      "Epoch [1/1], Step [1639/8897], Loss: 6.5125\n",
      "Epoch [1/1], Step [1640/8897], Loss: 6.3590\n",
      "Epoch [1/1], Step [1641/8897], Loss: 6.4157\n",
      "Epoch [1/1], Step [1642/8897], Loss: 6.2757\n",
      "Epoch [1/1], Step [1643/8897], Loss: 6.5375\n",
      "Epoch [1/1], Step [1644/8897], Loss: 6.3942\n",
      "Epoch [1/1], Step [1645/8897], Loss: 6.3460\n",
      "Epoch [1/1], Step [1646/8897], Loss: 6.2170\n",
      "Epoch [1/1], Step [1647/8897], Loss: 6.3646\n",
      "Epoch [1/1], Step [1648/8897], Loss: 6.2610\n",
      "Epoch [1/1], Step [1649/8897], Loss: 6.3348\n",
      "Epoch [1/1], Step [1650/8897], Loss: 6.2277\n",
      "Epoch [1/1], Step [1651/8897], Loss: 6.4037\n",
      "Epoch [1/1], Step [1652/8897], Loss: 6.4191\n",
      "Epoch [1/1], Step [1653/8897], Loss: 6.3886\n",
      "Epoch [1/1], Step [1654/8897], Loss: 6.4523\n",
      "Epoch [1/1], Step [1655/8897], Loss: 6.3412\n",
      "Epoch [1/1], Step [1656/8897], Loss: 6.2301\n",
      "Epoch [1/1], Step [1657/8897], Loss: 6.4749\n",
      "Epoch [1/1], Step [1658/8897], Loss: 6.2446\n",
      "Epoch [1/1], Step [1659/8897], Loss: 6.1733\n",
      "Epoch [1/1], Step [1660/8897], Loss: 6.2145\n",
      "Epoch [1/1], Step [1661/8897], Loss: 6.2242\n",
      "Epoch [1/1], Step [1662/8897], Loss: 6.1590\n",
      "Epoch [1/1], Step [1663/8897], Loss: 6.2682\n",
      "Epoch [1/1], Step [1664/8897], Loss: 6.3802\n",
      "Epoch [1/1], Step [1665/8897], Loss: 6.2832\n",
      "Epoch [1/1], Step [1666/8897], Loss: 6.4295\n",
      "Epoch [1/1], Step [1667/8897], Loss: 6.4535\n",
      "Epoch [1/1], Step [1668/8897], Loss: 6.5447\n",
      "Epoch [1/1], Step [1669/8897], Loss: 6.4434\n",
      "Epoch [1/1], Step [1670/8897], Loss: 6.2901\n",
      "Epoch [1/1], Step [1671/8897], Loss: 6.4246\n",
      "Epoch [1/1], Step [1672/8897], Loss: 6.4041\n",
      "Epoch [1/1], Step [1673/8897], Loss: 6.3223\n",
      "Epoch [1/1], Step [1674/8897], Loss: 6.5166\n",
      "Epoch [1/1], Step [1675/8897], Loss: 6.3811\n",
      "Epoch [1/1], Step [1676/8897], Loss: 6.4056\n",
      "Epoch [1/1], Step [1677/8897], Loss: 6.3692\n",
      "Epoch [1/1], Step [1678/8897], Loss: 6.4711\n",
      "Epoch [1/1], Step [1679/8897], Loss: 6.3645\n",
      "Epoch [1/1], Step [1680/8897], Loss: 6.3724\n",
      "Epoch [1/1], Step [1681/8897], Loss: 6.2244\n",
      "Epoch [1/1], Step [1682/8897], Loss: 6.2237\n",
      "Epoch [1/1], Step [1683/8897], Loss: 6.3128\n",
      "Epoch [1/1], Step [1684/8897], Loss: 6.4799\n",
      "Epoch [1/1], Step [1685/8897], Loss: 6.3803\n",
      "Epoch [1/1], Step [1686/8897], Loss: 6.2919\n",
      "Epoch [1/1], Step [1687/8897], Loss: 6.4173\n",
      "Epoch [1/1], Step [1688/8897], Loss: 6.3481\n",
      "Epoch [1/1], Step [1689/8897], Loss: 6.3746\n",
      "Epoch [1/1], Step [1690/8897], Loss: 6.2917\n",
      "Epoch [1/1], Step [1691/8897], Loss: 6.3021\n",
      "Epoch [1/1], Step [1692/8897], Loss: 6.1097\n",
      "Epoch [1/1], Step [1693/8897], Loss: 6.3370\n",
      "Epoch [1/1], Step [1694/8897], Loss: 6.2995\n",
      "Epoch [1/1], Step [1695/8897], Loss: 6.3797\n",
      "Epoch [1/1], Step [1696/8897], Loss: 6.2561\n",
      "Epoch [1/1], Step [1697/8897], Loss: 6.0670\n",
      "Epoch [1/1], Step [1698/8897], Loss: 6.4560\n",
      "Epoch [1/1], Step [1699/8897], Loss: 6.3445\n",
      "Epoch [1/1], Step [1700/8897], Loss: 6.2428\n",
      "Epoch [1/1], Step [1701/8897], Loss: 6.3169\n",
      "Epoch [1/1], Step [1702/8897], Loss: 6.3747\n",
      "Epoch [1/1], Step [1703/8897], Loss: 6.4117\n",
      "Epoch [1/1], Step [1704/8897], Loss: 6.5985\n",
      "Epoch [1/1], Step [1705/8897], Loss: 6.5038\n",
      "Epoch [1/1], Step [1706/8897], Loss: 6.2349\n",
      "Epoch [1/1], Step [1707/8897], Loss: 6.2832\n",
      "Epoch [1/1], Step [1708/8897], Loss: 6.3931\n",
      "Epoch [1/1], Step [1709/8897], Loss: 6.2586\n",
      "Epoch [1/1], Step [1710/8897], Loss: 6.1785\n",
      "Epoch [1/1], Step [1711/8897], Loss: 6.4187\n",
      "Epoch [1/1], Step [1712/8897], Loss: 6.4328\n",
      "Epoch [1/1], Step [1713/8897], Loss: 6.2763\n",
      "Epoch [1/1], Step [1714/8897], Loss: 6.3265\n",
      "Epoch [1/1], Step [1715/8897], Loss: 6.3558\n",
      "Epoch [1/1], Step [1716/8897], Loss: 6.4205\n",
      "Epoch [1/1], Step [1717/8897], Loss: 6.3317\n",
      "Epoch [1/1], Step [1718/8897], Loss: 6.3702\n",
      "Epoch [1/1], Step [1719/8897], Loss: 6.2651\n",
      "Epoch [1/1], Step [1720/8897], Loss: 6.3553\n",
      "Epoch [1/1], Step [1721/8897], Loss: 6.2355\n",
      "Epoch [1/1], Step [1722/8897], Loss: 6.0992\n",
      "Epoch [1/1], Step [1723/8897], Loss: 6.2766\n",
      "Epoch [1/1], Step [1724/8897], Loss: 6.0692\n",
      "Epoch [1/1], Step [1725/8897], Loss: 6.2967\n",
      "Epoch [1/1], Step [1726/8897], Loss: 6.3003\n",
      "Epoch [1/1], Step [1727/8897], Loss: 6.3501\n",
      "Epoch [1/1], Step [1728/8897], Loss: 6.5900\n",
      "Epoch [1/1], Step [1729/8897], Loss: 6.5796\n",
      "Epoch [1/1], Step [1730/8897], Loss: 6.3559\n",
      "Epoch [1/1], Step [1731/8897], Loss: 6.3423\n",
      "Epoch [1/1], Step [1732/8897], Loss: 6.4754\n",
      "Epoch [1/1], Step [1733/8897], Loss: 6.1906\n",
      "Epoch [1/1], Step [1734/8897], Loss: 6.2904\n",
      "Epoch [1/1], Step [1735/8897], Loss: 6.4230\n",
      "Epoch [1/1], Step [1736/8897], Loss: 6.4609\n",
      "Epoch [1/1], Step [1737/8897], Loss: 6.4505\n",
      "Epoch [1/1], Step [1738/8897], Loss: 6.3678\n",
      "Epoch [1/1], Step [1739/8897], Loss: 6.3830\n",
      "Epoch [1/1], Step [1740/8897], Loss: 6.4834\n",
      "Epoch [1/1], Step [1741/8897], Loss: 6.3707\n",
      "Epoch [1/1], Step [1742/8897], Loss: 6.2439\n",
      "Epoch [1/1], Step [1743/8897], Loss: 6.3825\n",
      "Epoch [1/1], Step [1744/8897], Loss: 6.2867\n",
      "Epoch [1/1], Step [1745/8897], Loss: 6.4939\n",
      "Epoch [1/1], Step [1746/8897], Loss: 6.1589\n",
      "Epoch [1/1], Step [1747/8897], Loss: 6.3539\n",
      "Epoch [1/1], Step [1748/8897], Loss: 6.3666\n",
      "Epoch [1/1], Step [1749/8897], Loss: 6.3388\n",
      "Epoch [1/1], Step [1750/8897], Loss: 6.3165\n",
      "Epoch [1/1], Step [1751/8897], Loss: 6.3436\n",
      "Epoch [1/1], Step [1752/8897], Loss: 6.5195\n",
      "Epoch [1/1], Step [1753/8897], Loss: 6.3691\n",
      "Epoch [1/1], Step [1754/8897], Loss: 6.1128\n",
      "Epoch [1/1], Step [1755/8897], Loss: 6.2699\n",
      "Epoch [1/1], Step [1756/8897], Loss: 6.2583\n",
      "Epoch [1/1], Step [1757/8897], Loss: 6.2340\n",
      "Epoch [1/1], Step [1758/8897], Loss: 6.1497\n",
      "Epoch [1/1], Step [1759/8897], Loss: 6.3918\n",
      "Epoch [1/1], Step [1760/8897], Loss: 6.1510\n",
      "Epoch [1/1], Step [1761/8897], Loss: 6.2848\n",
      "Epoch [1/1], Step [1762/8897], Loss: 6.3081\n",
      "Epoch [1/1], Step [1763/8897], Loss: 6.3291\n",
      "Epoch [1/1], Step [1764/8897], Loss: 6.3863\n",
      "Epoch [1/1], Step [1765/8897], Loss: 6.3701\n",
      "Epoch [1/1], Step [1766/8897], Loss: 6.3224\n",
      "Epoch [1/1], Step [1767/8897], Loss: 6.3534\n",
      "Epoch [1/1], Step [1768/8897], Loss: 6.1673\n",
      "Epoch [1/1], Step [1769/8897], Loss: 6.1393\n",
      "Epoch [1/1], Step [1770/8897], Loss: 6.3520\n",
      "Epoch [1/1], Step [1771/8897], Loss: 6.3714\n",
      "Epoch [1/1], Step [1772/8897], Loss: 6.0802\n",
      "Epoch [1/1], Step [1773/8897], Loss: 6.4620\n",
      "Epoch [1/1], Step [1774/8897], Loss: 6.0883\n",
      "Epoch [1/1], Step [1775/8897], Loss: 6.2685\n",
      "Epoch [1/1], Step [1776/8897], Loss: 6.2760\n",
      "Epoch [1/1], Step [1777/8897], Loss: 6.2566\n",
      "Epoch [1/1], Step [1778/8897], Loss: 6.3295\n",
      "Epoch [1/1], Step [1779/8897], Loss: 6.3290\n",
      "Epoch [1/1], Step [1780/8897], Loss: 6.3937\n",
      "Epoch [1/1], Step [1781/8897], Loss: 6.1481\n",
      "Epoch [1/1], Step [1782/8897], Loss: 6.3375\n",
      "Epoch [1/1], Step [1783/8897], Loss: 6.3810\n",
      "Epoch [1/1], Step [1784/8897], Loss: 6.3507\n",
      "Epoch [1/1], Step [1785/8897], Loss: 6.2036\n",
      "Epoch [1/1], Step [1786/8897], Loss: 6.4270\n",
      "Epoch [1/1], Step [1787/8897], Loss: 6.4753\n",
      "Epoch [1/1], Step [1788/8897], Loss: 6.3014\n",
      "Epoch [1/1], Step [1789/8897], Loss: 6.2349\n",
      "Epoch [1/1], Step [1790/8897], Loss: 6.2595\n",
      "Epoch [1/1], Step [1791/8897], Loss: 6.1892\n",
      "Epoch [1/1], Step [1792/8897], Loss: 6.3565\n",
      "Epoch [1/1], Step [1793/8897], Loss: 6.4177\n",
      "Epoch [1/1], Step [1794/8897], Loss: 6.4140\n",
      "Epoch [1/1], Step [1795/8897], Loss: 6.3281\n",
      "Epoch [1/1], Step [1796/8897], Loss: 6.1874\n",
      "Epoch [1/1], Step [1797/8897], Loss: 6.3258\n",
      "Epoch [1/1], Step [1798/8897], Loss: 6.3726\n",
      "Epoch [1/1], Step [1799/8897], Loss: 6.1175\n",
      "Epoch [1/1], Step [1800/8897], Loss: 6.4935\n",
      "Epoch [1/1], Step [1801/8897], Loss: 6.3860\n",
      "Epoch [1/1], Step [1802/8897], Loss: 6.2055\n",
      "Epoch [1/1], Step [1803/8897], Loss: 6.4360\n",
      "Epoch [1/1], Step [1804/8897], Loss: 6.3000\n",
      "Epoch [1/1], Step [1805/8897], Loss: 6.1989\n",
      "Epoch [1/1], Step [1806/8897], Loss: 6.1059\n",
      "Epoch [1/1], Step [1807/8897], Loss: 6.3219\n",
      "Epoch [1/1], Step [1808/8897], Loss: 6.2516\n",
      "Epoch [1/1], Step [1809/8897], Loss: 6.2327\n",
      "Epoch [1/1], Step [1810/8897], Loss: 6.2554\n",
      "Epoch [1/1], Step [1811/8897], Loss: 6.2963\n",
      "Epoch [1/1], Step [1812/8897], Loss: 6.2246\n",
      "Epoch [1/1], Step [1813/8897], Loss: 6.5230\n",
      "Epoch [1/1], Step [1814/8897], Loss: 6.3240\n",
      "Epoch [1/1], Step [1815/8897], Loss: 6.2179\n",
      "Epoch [1/1], Step [1816/8897], Loss: 6.1442\n",
      "Epoch [1/1], Step [1817/8897], Loss: 6.4569\n",
      "Epoch [1/1], Step [1818/8897], Loss: 6.3915\n",
      "Epoch [1/1], Step [1819/8897], Loss: 6.1111\n",
      "Epoch [1/1], Step [1820/8897], Loss: 6.2680\n",
      "Epoch [1/1], Step [1821/8897], Loss: 6.2141\n",
      "Epoch [1/1], Step [1822/8897], Loss: 6.4516\n",
      "Epoch [1/1], Step [1823/8897], Loss: 6.2511\n",
      "Epoch [1/1], Step [1824/8897], Loss: 6.2657\n",
      "Epoch [1/1], Step [1825/8897], Loss: 6.4323\n",
      "Epoch [1/1], Step [1826/8897], Loss: 6.2641\n",
      "Epoch [1/1], Step [1827/8897], Loss: 6.3952\n",
      "Epoch [1/1], Step [1828/8897], Loss: 6.2026\n",
      "Epoch [1/1], Step [1829/8897], Loss: 6.3912\n",
      "Epoch [1/1], Step [1830/8897], Loss: 6.2696\n",
      "Epoch [1/1], Step [1831/8897], Loss: 6.4794\n",
      "Epoch [1/1], Step [1832/8897], Loss: 6.2006\n",
      "Epoch [1/1], Step [1833/8897], Loss: 6.3393\n",
      "Epoch [1/1], Step [1834/8897], Loss: 6.4605\n",
      "Epoch [1/1], Step [1835/8897], Loss: 6.3736\n",
      "Epoch [1/1], Step [1836/8897], Loss: 6.2560\n",
      "Epoch [1/1], Step [1837/8897], Loss: 6.4491\n",
      "Epoch [1/1], Step [1838/8897], Loss: 6.4186\n",
      "Epoch [1/1], Step [1839/8897], Loss: 6.2765\n",
      "Epoch [1/1], Step [1840/8897], Loss: 6.3761\n",
      "Epoch [1/1], Step [1841/8897], Loss: 6.1408\n",
      "Epoch [1/1], Step [1842/8897], Loss: 6.2493\n",
      "Epoch [1/1], Step [1843/8897], Loss: 6.4070\n",
      "Epoch [1/1], Step [1844/8897], Loss: 6.4026\n",
      "Epoch [1/1], Step [1845/8897], Loss: 6.3349\n",
      "Epoch [1/1], Step [1846/8897], Loss: 6.2052\n",
      "Epoch [1/1], Step [1847/8897], Loss: 6.2583\n",
      "Epoch [1/1], Step [1848/8897], Loss: 6.1348\n",
      "Epoch [1/1], Step [1849/8897], Loss: 6.2542\n",
      "Epoch [1/1], Step [1850/8897], Loss: 6.1408\n",
      "Epoch [1/1], Step [1851/8897], Loss: 6.1979\n",
      "Epoch [1/1], Step [1852/8897], Loss: 6.4153\n",
      "Epoch [1/1], Step [1853/8897], Loss: 6.1899\n",
      "Epoch [1/1], Step [1854/8897], Loss: 6.2216\n",
      "Epoch [1/1], Step [1855/8897], Loss: 6.3509\n",
      "Epoch [1/1], Step [1856/8897], Loss: 6.2521\n",
      "Epoch [1/1], Step [1857/8897], Loss: 6.3084\n",
      "Epoch [1/1], Step [1858/8897], Loss: 6.3166\n",
      "Epoch [1/1], Step [1859/8897], Loss: 6.2569\n",
      "Epoch [1/1], Step [1860/8897], Loss: 6.2644\n",
      "Epoch [1/1], Step [1861/8897], Loss: 6.4739\n",
      "Epoch [1/1], Step [1862/8897], Loss: 6.3061\n",
      "Epoch [1/1], Step [1863/8897], Loss: 6.3759\n",
      "Epoch [1/1], Step [1864/8897], Loss: 6.3267\n",
      "Epoch [1/1], Step [1865/8897], Loss: 6.3217\n",
      "Epoch [1/1], Step [1866/8897], Loss: 6.2798\n",
      "Epoch [1/1], Step [1867/8897], Loss: 6.3991\n",
      "Epoch [1/1], Step [1868/8897], Loss: 6.3282\n",
      "Epoch [1/1], Step [1869/8897], Loss: 6.2767\n",
      "Epoch [1/1], Step [1870/8897], Loss: 6.1678\n",
      "Epoch [1/1], Step [1871/8897], Loss: 6.1934\n",
      "Epoch [1/1], Step [1872/8897], Loss: 6.2811\n",
      "Epoch [1/1], Step [1873/8897], Loss: 6.2232\n",
      "Epoch [1/1], Step [1874/8897], Loss: 6.2958\n",
      "Epoch [1/1], Step [1875/8897], Loss: 6.2453\n",
      "Epoch [1/1], Step [1876/8897], Loss: 6.3112\n",
      "Epoch [1/1], Step [1877/8897], Loss: 6.3195\n",
      "Epoch [1/1], Step [1878/8897], Loss: 6.0827\n",
      "Epoch [1/1], Step [1879/8897], Loss: 6.3231\n",
      "Epoch [1/1], Step [1880/8897], Loss: 6.2415\n",
      "Epoch [1/1], Step [1881/8897], Loss: 6.3141\n",
      "Epoch [1/1], Step [1882/8897], Loss: 6.3497\n",
      "Epoch [1/1], Step [1883/8897], Loss: 6.4285\n",
      "Epoch [1/1], Step [1884/8897], Loss: 6.2761\n",
      "Epoch [1/1], Step [1885/8897], Loss: 6.1773\n",
      "Epoch [1/1], Step [1886/8897], Loss: 6.2821\n",
      "Epoch [1/1], Step [1887/8897], Loss: 6.4127\n",
      "Epoch [1/1], Step [1888/8897], Loss: 6.4604\n",
      "Epoch [1/1], Step [1889/8897], Loss: 6.2984\n",
      "Epoch [1/1], Step [1890/8897], Loss: 6.3518\n",
      "Epoch [1/1], Step [1891/8897], Loss: 6.4037\n",
      "Epoch [1/1], Step [1892/8897], Loss: 6.4242\n",
      "Epoch [1/1], Step [1893/8897], Loss: 6.3745\n",
      "Epoch [1/1], Step [1894/8897], Loss: 6.2640\n",
      "Epoch [1/1], Step [1895/8897], Loss: 6.3152\n",
      "Epoch [1/1], Step [1896/8897], Loss: 6.2112\n",
      "Epoch [1/1], Step [1897/8897], Loss: 6.2754\n",
      "Epoch [1/1], Step [1898/8897], Loss: 6.3407\n",
      "Epoch [1/1], Step [1899/8897], Loss: 6.2121\n",
      "Epoch [1/1], Step [1900/8897], Loss: 6.1666\n",
      "Epoch [1/1], Step [1901/8897], Loss: 6.4208\n",
      "Epoch [1/1], Step [1902/8897], Loss: 6.3741\n",
      "Epoch [1/1], Step [1903/8897], Loss: 6.2932\n",
      "Epoch [1/1], Step [1904/8897], Loss: 6.3186\n",
      "Epoch [1/1], Step [1905/8897], Loss: 6.2046\n",
      "Epoch [1/1], Step [1906/8897], Loss: 6.1498\n",
      "Epoch [1/1], Step [1907/8897], Loss: 6.1571\n",
      "Epoch [1/1], Step [1908/8897], Loss: 6.3338\n",
      "Epoch [1/1], Step [1909/8897], Loss: 6.3573\n",
      "Epoch [1/1], Step [1910/8897], Loss: 6.4883\n",
      "Epoch [1/1], Step [1911/8897], Loss: 6.4994\n",
      "Epoch [1/1], Step [1912/8897], Loss: 6.1844\n",
      "Epoch [1/1], Step [1913/8897], Loss: 6.3686\n",
      "Epoch [1/1], Step [1914/8897], Loss: 6.0233\n",
      "Epoch [1/1], Step [1915/8897], Loss: 6.3283\n",
      "Epoch [1/1], Step [1916/8897], Loss: 6.3582\n",
      "Epoch [1/1], Step [1917/8897], Loss: 6.4895\n",
      "Epoch [1/1], Step [1918/8897], Loss: 6.4166\n",
      "Epoch [1/1], Step [1919/8897], Loss: 6.2309\n",
      "Epoch [1/1], Step [1920/8897], Loss: 6.3972\n",
      "Epoch [1/1], Step [1921/8897], Loss: 6.1948\n",
      "Epoch [1/1], Step [1922/8897], Loss: 6.2428\n",
      "Epoch [1/1], Step [1923/8897], Loss: 6.2486\n",
      "Epoch [1/1], Step [1924/8897], Loss: 6.2245\n",
      "Epoch [1/1], Step [1925/8897], Loss: 6.2293\n",
      "Epoch [1/1], Step [1926/8897], Loss: 6.3285\n",
      "Epoch [1/1], Step [1927/8897], Loss: 6.3505\n",
      "Epoch [1/1], Step [1928/8897], Loss: 6.3208\n",
      "Epoch [1/1], Step [1929/8897], Loss: 6.2806\n",
      "Epoch [1/1], Step [1930/8897], Loss: 5.9888\n",
      "Epoch [1/1], Step [1931/8897], Loss: 6.3815\n",
      "Epoch [1/1], Step [1932/8897], Loss: 6.3074\n",
      "Epoch [1/1], Step [1933/8897], Loss: 6.2915\n",
      "Epoch [1/1], Step [1934/8897], Loss: 6.2391\n",
      "Epoch [1/1], Step [1935/8897], Loss: 6.3889\n",
      "Epoch [1/1], Step [1936/8897], Loss: 6.2739\n",
      "Epoch [1/1], Step [1937/8897], Loss: 6.3252\n",
      "Epoch [1/1], Step [1938/8897], Loss: 6.1612\n",
      "Epoch [1/1], Step [1939/8897], Loss: 6.3777\n",
      "Epoch [1/1], Step [1940/8897], Loss: 6.2442\n",
      "Epoch [1/1], Step [1941/8897], Loss: 6.3086\n",
      "Epoch [1/1], Step [1942/8897], Loss: 6.3013\n",
      "Epoch [1/1], Step [1943/8897], Loss: 6.4062\n",
      "Epoch [1/1], Step [1944/8897], Loss: 6.0236\n",
      "Epoch [1/1], Step [1945/8897], Loss: 6.3412\n",
      "Epoch [1/1], Step [1946/8897], Loss: 6.2208\n",
      "Epoch [1/1], Step [1947/8897], Loss: 6.4432\n",
      "Epoch [1/1], Step [1948/8897], Loss: 6.4318\n",
      "Epoch [1/1], Step [1949/8897], Loss: 6.3142\n",
      "Epoch [1/1], Step [1950/8897], Loss: 6.4855\n",
      "Epoch [1/1], Step [1951/8897], Loss: 6.3016\n",
      "Epoch [1/1], Step [1952/8897], Loss: 6.2799\n",
      "Epoch [1/1], Step [1953/8897], Loss: 6.0229\n",
      "Epoch [1/1], Step [1954/8897], Loss: 6.3383\n",
      "Epoch [1/1], Step [1955/8897], Loss: 6.1034\n",
      "Epoch [1/1], Step [1956/8897], Loss: 6.4016\n",
      "Epoch [1/1], Step [1957/8897], Loss: 6.0612\n",
      "Epoch [1/1], Step [1958/8897], Loss: 6.2553\n",
      "Epoch [1/1], Step [1959/8897], Loss: 6.2045\n",
      "Epoch [1/1], Step [1960/8897], Loss: 6.3073\n",
      "Epoch [1/1], Step [1961/8897], Loss: 6.2414\n",
      "Epoch [1/1], Step [1962/8897], Loss: 6.1972\n",
      "Epoch [1/1], Step [1963/8897], Loss: 6.3398\n",
      "Epoch [1/1], Step [1964/8897], Loss: 6.2329\n",
      "Epoch [1/1], Step [1965/8897], Loss: 6.2542\n",
      "Epoch [1/1], Step [1966/8897], Loss: 6.5352\n",
      "Epoch [1/1], Step [1967/8897], Loss: 6.4335\n",
      "Epoch [1/1], Step [1968/8897], Loss: 6.3803\n",
      "Epoch [1/1], Step [1969/8897], Loss: 6.0712\n",
      "Epoch [1/1], Step [1970/8897], Loss: 6.5097\n",
      "Epoch [1/1], Step [1971/8897], Loss: 6.2601\n",
      "Epoch [1/1], Step [1972/8897], Loss: 6.1800\n",
      "Epoch [1/1], Step [1973/8897], Loss: 6.1904\n",
      "Epoch [1/1], Step [1974/8897], Loss: 6.1977\n",
      "Epoch [1/1], Step [1975/8897], Loss: 6.2092\n",
      "Epoch [1/1], Step [1976/8897], Loss: 6.3229\n",
      "Epoch [1/1], Step [1977/8897], Loss: 6.2411\n",
      "Epoch [1/1], Step [1978/8897], Loss: 6.2459\n",
      "Epoch [1/1], Step [1979/8897], Loss: 6.3696\n",
      "Epoch [1/1], Step [1980/8897], Loss: 6.2911\n",
      "Epoch [1/1], Step [1981/8897], Loss: 6.3315\n",
      "Epoch [1/1], Step [1982/8897], Loss: 6.1345\n",
      "Epoch [1/1], Step [1983/8897], Loss: 6.5005\n",
      "Epoch [1/1], Step [1984/8897], Loss: 6.1452\n",
      "Epoch [1/1], Step [1985/8897], Loss: 6.1838\n",
      "Epoch [1/1], Step [1986/8897], Loss: 6.1520\n",
      "Epoch [1/1], Step [1987/8897], Loss: 6.4624\n",
      "Epoch [1/1], Step [1988/8897], Loss: 6.1989\n",
      "Epoch [1/1], Step [1989/8897], Loss: 6.2941\n",
      "Epoch [1/1], Step [1990/8897], Loss: 6.1854\n",
      "Epoch [1/1], Step [1991/8897], Loss: 5.9866\n",
      "Epoch [1/1], Step [1992/8897], Loss: 6.5175\n",
      "Epoch [1/1], Step [1993/8897], Loss: 6.1620\n",
      "Epoch [1/1], Step [1994/8897], Loss: 6.3094\n",
      "Epoch [1/1], Step [1995/8897], Loss: 6.0475\n",
      "Epoch [1/1], Step [1996/8897], Loss: 6.2472\n",
      "Epoch [1/1], Step [1997/8897], Loss: 6.0923\n",
      "Epoch [1/1], Step [1998/8897], Loss: 6.2848\n",
      "Epoch [1/1], Step [1999/8897], Loss: 6.2863\n",
      "Epoch [1/1], Step [2000/8897], Loss: 6.1030\n",
      "Epoch [1/1], Step [2001/8897], Loss: 6.2287\n",
      "Epoch [1/1], Step [2002/8897], Loss: 6.1718\n",
      "Epoch [1/1], Step [2003/8897], Loss: 6.4349\n",
      "Epoch [1/1], Step [2004/8897], Loss: 6.3716\n",
      "Epoch [1/1], Step [2005/8897], Loss: 6.2497\n",
      "Epoch [1/1], Step [2006/8897], Loss: 6.1529\n",
      "Epoch [1/1], Step [2007/8897], Loss: 6.2784\n",
      "Epoch [1/1], Step [2008/8897], Loss: 6.2689\n",
      "Epoch [1/1], Step [2009/8897], Loss: 6.3021\n",
      "Epoch [1/1], Step [2010/8897], Loss: 6.3912\n",
      "Epoch [1/1], Step [2011/8897], Loss: 6.2835\n",
      "Epoch [1/1], Step [2012/8897], Loss: 6.1494\n",
      "Epoch [1/1], Step [2013/8897], Loss: 6.1887\n",
      "Epoch [1/1], Step [2014/8897], Loss: 6.1581\n",
      "Epoch [1/1], Step [2015/8897], Loss: 6.2413\n",
      "Epoch [1/1], Step [2016/8897], Loss: 6.2734\n",
      "Epoch [1/1], Step [2017/8897], Loss: 6.2646\n",
      "Epoch [1/1], Step [2018/8897], Loss: 6.3178\n",
      "Epoch [1/1], Step [2019/8897], Loss: 6.5893\n",
      "Epoch [1/1], Step [2020/8897], Loss: 6.2546\n",
      "Epoch [1/1], Step [2021/8897], Loss: 6.0760\n",
      "Epoch [1/1], Step [2022/8897], Loss: 6.3761\n",
      "Epoch [1/1], Step [2023/8897], Loss: 6.4594\n",
      "Epoch [1/1], Step [2024/8897], Loss: 6.1165\n",
      "Epoch [1/1], Step [2025/8897], Loss: 6.1559\n",
      "Epoch [1/1], Step [2026/8897], Loss: 6.1838\n",
      "Epoch [1/1], Step [2027/8897], Loss: 6.2781\n",
      "Epoch [1/1], Step [2028/8897], Loss: 6.3985\n",
      "Epoch [1/1], Step [2029/8897], Loss: 6.3599\n",
      "Epoch [1/1], Step [2030/8897], Loss: 6.3395\n",
      "Epoch [1/1], Step [2031/8897], Loss: 6.1829\n",
      "Epoch [1/1], Step [2032/8897], Loss: 6.2542\n",
      "Epoch [1/1], Step [2033/8897], Loss: 6.1662\n",
      "Epoch [1/1], Step [2034/8897], Loss: 6.2334\n",
      "Epoch [1/1], Step [2035/8897], Loss: 6.2432\n",
      "Epoch [1/1], Step [2036/8897], Loss: 6.2121\n",
      "Epoch [1/1], Step [2037/8897], Loss: 6.1412\n",
      "Epoch [1/1], Step [2038/8897], Loss: 6.1191\n",
      "Epoch [1/1], Step [2039/8897], Loss: 6.1713\n",
      "Epoch [1/1], Step [2040/8897], Loss: 6.2123\n",
      "Epoch [1/1], Step [2041/8897], Loss: 6.3791\n",
      "Epoch [1/1], Step [2042/8897], Loss: 6.1763\n",
      "Epoch [1/1], Step [2043/8897], Loss: 6.2454\n",
      "Epoch [1/1], Step [2044/8897], Loss: 6.5536\n",
      "Epoch [1/1], Step [2045/8897], Loss: 6.2529\n",
      "Epoch [1/1], Step [2046/8897], Loss: 6.2750\n",
      "Epoch [1/1], Step [2047/8897], Loss: 6.0849\n",
      "Epoch [1/1], Step [2048/8897], Loss: 6.1736\n",
      "Epoch [1/1], Step [2049/8897], Loss: 6.3824\n",
      "Epoch [1/1], Step [2050/8897], Loss: 6.5120\n",
      "Epoch [1/1], Step [2051/8897], Loss: 6.3280\n",
      "Epoch [1/1], Step [2052/8897], Loss: 6.3311\n",
      "Epoch [1/1], Step [2053/8897], Loss: 6.2072\n",
      "Epoch [1/1], Step [2054/8897], Loss: 6.3667\n",
      "Epoch [1/1], Step [2055/8897], Loss: 6.2720\n",
      "Epoch [1/1], Step [2056/8897], Loss: 6.1659\n",
      "Epoch [1/1], Step [2057/8897], Loss: 6.1916\n",
      "Epoch [1/1], Step [2058/8897], Loss: 6.2600\n",
      "Epoch [1/1], Step [2059/8897], Loss: 6.3059\n",
      "Epoch [1/1], Step [2060/8897], Loss: 6.1298\n",
      "Epoch [1/1], Step [2061/8897], Loss: 6.2961\n",
      "Epoch [1/1], Step [2062/8897], Loss: 6.4339\n",
      "Epoch [1/1], Step [2063/8897], Loss: 6.2522\n",
      "Epoch [1/1], Step [2064/8897], Loss: 6.3971\n",
      "Epoch [1/1], Step [2065/8897], Loss: 6.3180\n",
      "Epoch [1/1], Step [2066/8897], Loss: 6.2966\n",
      "Epoch [1/1], Step [2067/8897], Loss: 6.2152\n",
      "Epoch [1/1], Step [2068/8897], Loss: 6.0871\n",
      "Epoch [1/1], Step [2069/8897], Loss: 6.2431\n",
      "Epoch [1/1], Step [2070/8897], Loss: 6.3333\n",
      "Epoch [1/1], Step [2071/8897], Loss: 6.1325\n",
      "Epoch [1/1], Step [2072/8897], Loss: 6.1730\n",
      "Epoch [1/1], Step [2073/8897], Loss: 6.0605\n",
      "Epoch [1/1], Step [2074/8897], Loss: 5.9261\n",
      "Epoch [1/1], Step [2075/8897], Loss: 6.1230\n",
      "Epoch [1/1], Step [2076/8897], Loss: 6.0439\n",
      "Epoch [1/1], Step [2077/8897], Loss: 6.4165\n",
      "Epoch [1/1], Step [2078/8897], Loss: 6.2168\n",
      "Epoch [1/1], Step [2079/8897], Loss: 6.1145\n",
      "Epoch [1/1], Step [2080/8897], Loss: 6.3829\n",
      "Epoch [1/1], Step [2081/8897], Loss: 6.2093\n",
      "Epoch [1/1], Step [2082/8897], Loss: 6.2125\n",
      "Epoch [1/1], Step [2083/8897], Loss: 6.1122\n",
      "Epoch [1/1], Step [2084/8897], Loss: 6.3251\n",
      "Epoch [1/1], Step [2085/8897], Loss: 6.1657\n",
      "Epoch [1/1], Step [2086/8897], Loss: 6.1934\n",
      "Epoch [1/1], Step [2087/8897], Loss: 6.2731\n",
      "Epoch [1/1], Step [2088/8897], Loss: 6.2004\n",
      "Epoch [1/1], Step [2089/8897], Loss: 6.1300\n",
      "Epoch [1/1], Step [2090/8897], Loss: 6.3212\n",
      "Epoch [1/1], Step [2091/8897], Loss: 6.4678\n",
      "Epoch [1/1], Step [2092/8897], Loss: 6.2828\n",
      "Epoch [1/1], Step [2093/8897], Loss: 6.2381\n",
      "Epoch [1/1], Step [2094/8897], Loss: 6.3633\n",
      "Epoch [1/1], Step [2095/8897], Loss: 6.3694\n",
      "Epoch [1/1], Step [2096/8897], Loss: 6.2700\n",
      "Epoch [1/1], Step [2097/8897], Loss: 6.2176\n",
      "Epoch [1/1], Step [2098/8897], Loss: 6.2982\n",
      "Epoch [1/1], Step [2099/8897], Loss: 6.3324\n",
      "Epoch [1/1], Step [2100/8897], Loss: 6.2235\n",
      "Epoch [1/1], Step [2101/8897], Loss: 6.0680\n",
      "Epoch [1/1], Step [2102/8897], Loss: 6.2444\n",
      "Epoch [1/1], Step [2103/8897], Loss: 6.2115\n",
      "Epoch [1/1], Step [2104/8897], Loss: 6.2381\n",
      "Epoch [1/1], Step [2105/8897], Loss: 6.1874\n",
      "Epoch [1/1], Step [2106/8897], Loss: 6.2443\n",
      "Epoch [1/1], Step [2107/8897], Loss: 6.3100\n",
      "Epoch [1/1], Step [2108/8897], Loss: 6.2080\n",
      "Epoch [1/1], Step [2109/8897], Loss: 6.1921\n",
      "Epoch [1/1], Step [2110/8897], Loss: 6.1722\n",
      "Epoch [1/1], Step [2111/8897], Loss: 6.4271\n",
      "Epoch [1/1], Step [2112/8897], Loss: 6.0624\n",
      "Epoch [1/1], Step [2113/8897], Loss: 6.3145\n",
      "Epoch [1/1], Step [2114/8897], Loss: 6.1600\n",
      "Epoch [1/1], Step [2115/8897], Loss: 6.2115\n",
      "Epoch [1/1], Step [2116/8897], Loss: 6.3576\n",
      "Epoch [1/1], Step [2117/8897], Loss: 6.1783\n",
      "Epoch [1/1], Step [2118/8897], Loss: 6.3403\n",
      "Epoch [1/1], Step [2119/8897], Loss: 6.0005\n",
      "Epoch [1/1], Step [2120/8897], Loss: 6.0738\n",
      "Epoch [1/1], Step [2121/8897], Loss: 6.3665\n",
      "Epoch [1/1], Step [2122/8897], Loss: 6.1828\n",
      "Epoch [1/1], Step [2123/8897], Loss: 6.3271\n",
      "Epoch [1/1], Step [2124/8897], Loss: 6.2142\n",
      "Epoch [1/1], Step [2125/8897], Loss: 6.1708\n",
      "Epoch [1/1], Step [2126/8897], Loss: 6.2267\n",
      "Epoch [1/1], Step [2127/8897], Loss: 6.1580\n",
      "Epoch [1/1], Step [2128/8897], Loss: 6.2224\n",
      "Epoch [1/1], Step [2129/8897], Loss: 6.0454\n",
      "Epoch [1/1], Step [2130/8897], Loss: 6.2947\n",
      "Epoch [1/1], Step [2131/8897], Loss: 6.1998\n",
      "Epoch [1/1], Step [2132/8897], Loss: 6.1395\n",
      "Epoch [1/1], Step [2133/8897], Loss: 6.3637\n",
      "Epoch [1/1], Step [2134/8897], Loss: 6.0752\n",
      "Epoch [1/1], Step [2135/8897], Loss: 6.1467\n",
      "Epoch [1/1], Step [2136/8897], Loss: 6.2034\n",
      "Epoch [1/1], Step [2137/8897], Loss: 6.1954\n",
      "Epoch [1/1], Step [2138/8897], Loss: 6.3244\n",
      "Epoch [1/1], Step [2139/8897], Loss: 6.2795\n",
      "Epoch [1/1], Step [2140/8897], Loss: 6.1827\n",
      "Epoch [1/1], Step [2141/8897], Loss: 6.1699\n",
      "Epoch [1/1], Step [2142/8897], Loss: 6.3048\n",
      "Epoch [1/1], Step [2143/8897], Loss: 6.3488\n",
      "Epoch [1/1], Step [2144/8897], Loss: 6.2025\n",
      "Epoch [1/1], Step [2145/8897], Loss: 6.2694\n",
      "Epoch [1/1], Step [2146/8897], Loss: 6.2300\n",
      "Epoch [1/1], Step [2147/8897], Loss: 6.0224\n",
      "Epoch [1/1], Step [2148/8897], Loss: 6.4524\n",
      "Epoch [1/1], Step [2149/8897], Loss: 6.3468\n",
      "Epoch [1/1], Step [2150/8897], Loss: 6.1656\n",
      "Epoch [1/1], Step [2151/8897], Loss: 6.4327\n",
      "Epoch [1/1], Step [2152/8897], Loss: 6.2944\n",
      "Epoch [1/1], Step [2153/8897], Loss: 6.2026\n",
      "Epoch [1/1], Step [2154/8897], Loss: 6.1257\n",
      "Epoch [1/1], Step [2155/8897], Loss: 6.2080\n",
      "Epoch [1/1], Step [2156/8897], Loss: 6.0242\n",
      "Epoch [1/1], Step [2157/8897], Loss: 6.1564\n",
      "Epoch [1/1], Step [2158/8897], Loss: 6.2066\n",
      "Epoch [1/1], Step [2159/8897], Loss: 6.1322\n",
      "Epoch [1/1], Step [2160/8897], Loss: 6.0477\n",
      "Epoch [1/1], Step [2161/8897], Loss: 6.2653\n",
      "Epoch [1/1], Step [2162/8897], Loss: 6.2299\n",
      "Epoch [1/1], Step [2163/8897], Loss: 6.2137\n",
      "Epoch [1/1], Step [2164/8897], Loss: 6.3031\n",
      "Epoch [1/1], Step [2165/8897], Loss: 6.0878\n",
      "Epoch [1/1], Step [2166/8897], Loss: 6.1622\n",
      "Epoch [1/1], Step [2167/8897], Loss: 6.1394\n",
      "Epoch [1/1], Step [2168/8897], Loss: 6.2247\n",
      "Epoch [1/1], Step [2169/8897], Loss: 6.3171\n",
      "Epoch [1/1], Step [2170/8897], Loss: 6.2673\n",
      "Epoch [1/1], Step [2171/8897], Loss: 6.2259\n",
      "Epoch [1/1], Step [2172/8897], Loss: 6.2789\n",
      "Epoch [1/1], Step [2173/8897], Loss: 6.1420\n",
      "Epoch [1/1], Step [2174/8897], Loss: 6.2087\n",
      "Epoch [1/1], Step [2175/8897], Loss: 6.3756\n",
      "Epoch [1/1], Step [2176/8897], Loss: 6.0519\n",
      "Epoch [1/1], Step [2177/8897], Loss: 6.1400\n",
      "Epoch [1/1], Step [2178/8897], Loss: 6.1855\n",
      "Epoch [1/1], Step [2179/8897], Loss: 6.2836\n",
      "Epoch [1/1], Step [2180/8897], Loss: 6.1247\n",
      "Epoch [1/1], Step [2181/8897], Loss: 6.2107\n",
      "Epoch [1/1], Step [2182/8897], Loss: 6.2725\n",
      "Epoch [1/1], Step [2183/8897], Loss: 6.1909\n",
      "Epoch [1/1], Step [2184/8897], Loss: 6.2671\n",
      "Epoch [1/1], Step [2185/8897], Loss: 6.1051\n",
      "Epoch [1/1], Step [2186/8897], Loss: 6.0813\n",
      "Epoch [1/1], Step [2187/8897], Loss: 6.1344\n",
      "Epoch [1/1], Step [2188/8897], Loss: 6.1623\n",
      "Epoch [1/1], Step [2189/8897], Loss: 6.0319\n",
      "Epoch [1/1], Step [2190/8897], Loss: 6.1542\n",
      "Epoch [1/1], Step [2191/8897], Loss: 6.3912\n",
      "Epoch [1/1], Step [2192/8897], Loss: 6.2124\n",
      "Epoch [1/1], Step [2193/8897], Loss: 6.2624\n",
      "Epoch [1/1], Step [2194/8897], Loss: 6.2497\n",
      "Epoch [1/1], Step [2195/8897], Loss: 6.1177\n",
      "Epoch [1/1], Step [2196/8897], Loss: 6.3507\n",
      "Epoch [1/1], Step [2197/8897], Loss: 6.0391\n",
      "Epoch [1/1], Step [2198/8897], Loss: 6.1099\n",
      "Epoch [1/1], Step [2199/8897], Loss: 6.2811\n",
      "Epoch [1/1], Step [2200/8897], Loss: 6.0910\n",
      "Epoch [1/1], Step [2201/8897], Loss: 6.2880\n",
      "Epoch [1/1], Step [2202/8897], Loss: 6.0824\n",
      "Epoch [1/1], Step [2203/8897], Loss: 6.1368\n",
      "Epoch [1/1], Step [2204/8897], Loss: 6.0995\n",
      "Epoch [1/1], Step [2205/8897], Loss: 6.1166\n",
      "Epoch [1/1], Step [2206/8897], Loss: 6.3266\n",
      "Epoch [1/1], Step [2207/8897], Loss: 6.2851\n",
      "Epoch [1/1], Step [2208/8897], Loss: 6.3614\n",
      "Epoch [1/1], Step [2209/8897], Loss: 6.0452\n",
      "Epoch [1/1], Step [2210/8897], Loss: 6.0754\n",
      "Epoch [1/1], Step [2211/8897], Loss: 5.9787\n",
      "Epoch [1/1], Step [2212/8897], Loss: 6.1714\n",
      "Epoch [1/1], Step [2213/8897], Loss: 6.4204\n",
      "Epoch [1/1], Step [2214/8897], Loss: 6.3545\n",
      "Epoch [1/1], Step [2215/8897], Loss: 6.3384\n",
      "Epoch [1/1], Step [2216/8897], Loss: 6.1806\n",
      "Epoch [1/1], Step [2217/8897], Loss: 6.0338\n",
      "Epoch [1/1], Step [2218/8897], Loss: 6.3963\n",
      "Epoch [1/1], Step [2219/8897], Loss: 6.2877\n",
      "Epoch [1/1], Step [2220/8897], Loss: 6.1708\n",
      "Epoch [1/1], Step [2221/8897], Loss: 6.2980\n",
      "Epoch [1/1], Step [2222/8897], Loss: 6.1977\n",
      "Epoch [1/1], Step [2223/8897], Loss: 6.0319\n",
      "Epoch [1/1], Step [2224/8897], Loss: 6.4387\n",
      "Epoch [1/1], Step [2225/8897], Loss: 6.1822\n",
      "Epoch [1/1], Step [2226/8897], Loss: 6.2249\n",
      "Epoch [1/1], Step [2227/8897], Loss: 5.9775\n",
      "Epoch [1/1], Step [2228/8897], Loss: 6.1034\n",
      "Epoch [1/1], Step [2229/8897], Loss: 6.0578\n",
      "Epoch [1/1], Step [2230/8897], Loss: 6.2278\n",
      "Epoch [1/1], Step [2231/8897], Loss: 6.3565\n",
      "Epoch [1/1], Step [2232/8897], Loss: 6.0987\n",
      "Epoch [1/1], Step [2233/8897], Loss: 6.2462\n",
      "Epoch [1/1], Step [2234/8897], Loss: 6.3524\n",
      "Epoch [1/1], Step [2235/8897], Loss: 6.2955\n",
      "Epoch [1/1], Step [2236/8897], Loss: 6.2546\n",
      "Epoch [1/1], Step [2237/8897], Loss: 5.9782\n",
      "Epoch [1/1], Step [2238/8897], Loss: 6.2196\n",
      "Epoch [1/1], Step [2239/8897], Loss: 6.2148\n",
      "Epoch [1/1], Step [2240/8897], Loss: 6.0539\n",
      "Epoch [1/1], Step [2241/8897], Loss: 6.3212\n",
      "Epoch [1/1], Step [2242/8897], Loss: 6.0994\n",
      "Epoch [1/1], Step [2243/8897], Loss: 6.2709\n",
      "Epoch [1/1], Step [2244/8897], Loss: 6.2607\n",
      "Epoch [1/1], Step [2245/8897], Loss: 6.0750\n",
      "Epoch [1/1], Step [2246/8897], Loss: 6.0810\n",
      "Epoch [1/1], Step [2247/8897], Loss: 6.4878\n",
      "Epoch [1/1], Step [2248/8897], Loss: 5.9605\n",
      "Epoch [1/1], Step [2249/8897], Loss: 6.1864\n",
      "Epoch [1/1], Step [2250/8897], Loss: 6.3615\n",
      "Epoch [1/1], Step [2251/8897], Loss: 6.2232\n",
      "Epoch [1/1], Step [2252/8897], Loss: 6.2381\n",
      "Epoch [1/1], Step [2253/8897], Loss: 6.2362\n",
      "Epoch [1/1], Step [2254/8897], Loss: 6.1070\n",
      "Epoch [1/1], Step [2255/8897], Loss: 6.2367\n",
      "Epoch [1/1], Step [2256/8897], Loss: 6.3317\n",
      "Epoch [1/1], Step [2257/8897], Loss: 6.1869\n",
      "Epoch [1/1], Step [2258/8897], Loss: 6.1189\n",
      "Epoch [1/1], Step [2259/8897], Loss: 6.1492\n",
      "Epoch [1/1], Step [2260/8897], Loss: 6.1538\n",
      "Epoch [1/1], Step [2261/8897], Loss: 6.1314\n",
      "Epoch [1/1], Step [2262/8897], Loss: 6.1725\n",
      "Epoch [1/1], Step [2263/8897], Loss: 6.1080\n",
      "Epoch [1/1], Step [2264/8897], Loss: 6.4729\n",
      "Epoch [1/1], Step [2265/8897], Loss: 6.1481\n",
      "Epoch [1/1], Step [2266/8897], Loss: 6.1670\n",
      "Epoch [1/1], Step [2267/8897], Loss: 6.1431\n",
      "Epoch [1/1], Step [2268/8897], Loss: 6.0447\n",
      "Epoch [1/1], Step [2269/8897], Loss: 6.2289\n",
      "Epoch [1/1], Step [2270/8897], Loss: 6.1173\n",
      "Epoch [1/1], Step [2271/8897], Loss: 6.1311\n",
      "Epoch [1/1], Step [2272/8897], Loss: 6.0998\n",
      "Epoch [1/1], Step [2273/8897], Loss: 6.1694\n",
      "Epoch [1/1], Step [2274/8897], Loss: 6.0891\n",
      "Epoch [1/1], Step [2275/8897], Loss: 6.1968\n",
      "Epoch [1/1], Step [2276/8897], Loss: 6.3350\n",
      "Epoch [1/1], Step [2277/8897], Loss: 6.2153\n",
      "Epoch [1/1], Step [2278/8897], Loss: 6.1590\n",
      "Epoch [1/1], Step [2279/8897], Loss: 6.2222\n",
      "Epoch [1/1], Step [2280/8897], Loss: 6.2309\n",
      "Epoch [1/1], Step [2281/8897], Loss: 6.0436\n",
      "Epoch [1/1], Step [2282/8897], Loss: 6.0807\n",
      "Epoch [1/1], Step [2283/8897], Loss: 6.3095\n",
      "Epoch [1/1], Step [2284/8897], Loss: 6.1930\n",
      "Epoch [1/1], Step [2285/8897], Loss: 5.9625\n",
      "Epoch [1/1], Step [2286/8897], Loss: 6.2186\n",
      "Epoch [1/1], Step [2287/8897], Loss: 6.1695\n",
      "Epoch [1/1], Step [2288/8897], Loss: 6.2437\n",
      "Epoch [1/1], Step [2289/8897], Loss: 6.0477\n",
      "Epoch [1/1], Step [2290/8897], Loss: 6.0070\n",
      "Epoch [1/1], Step [2291/8897], Loss: 6.2070\n",
      "Epoch [1/1], Step [2292/8897], Loss: 6.2802\n",
      "Epoch [1/1], Step [2293/8897], Loss: 6.2557\n",
      "Epoch [1/1], Step [2294/8897], Loss: 6.1590\n",
      "Epoch [1/1], Step [2295/8897], Loss: 6.1104\n",
      "Epoch [1/1], Step [2296/8897], Loss: 6.1014\n",
      "Epoch [1/1], Step [2297/8897], Loss: 6.1506\n",
      "Epoch [1/1], Step [2298/8897], Loss: 6.2066\n",
      "Epoch [1/1], Step [2299/8897], Loss: 5.9912\n",
      "Epoch [1/1], Step [2300/8897], Loss: 6.3445\n",
      "Epoch [1/1], Step [2301/8897], Loss: 6.2779\n",
      "Epoch [1/1], Step [2302/8897], Loss: 6.2265\n",
      "Epoch [1/1], Step [2303/8897], Loss: 6.2723\n",
      "Epoch [1/1], Step [2304/8897], Loss: 5.8588\n",
      "Epoch [1/1], Step [2305/8897], Loss: 6.3564\n",
      "Epoch [1/1], Step [2306/8897], Loss: 6.2052\n",
      "Epoch [1/1], Step [2307/8897], Loss: 6.0320\n",
      "Epoch [1/1], Step [2308/8897], Loss: 6.3406\n",
      "Epoch [1/1], Step [2309/8897], Loss: 6.2634\n",
      "Epoch [1/1], Step [2310/8897], Loss: 6.1933\n",
      "Epoch [1/1], Step [2311/8897], Loss: 6.3476\n",
      "Epoch [1/1], Step [2312/8897], Loss: 6.1956\n",
      "Epoch [1/1], Step [2313/8897], Loss: 6.1904\n",
      "Epoch [1/1], Step [2314/8897], Loss: 6.1162\n",
      "Epoch [1/1], Step [2315/8897], Loss: 6.1278\n",
      "Epoch [1/1], Step [2316/8897], Loss: 6.0087\n",
      "Epoch [1/1], Step [2317/8897], Loss: 6.4123\n",
      "Epoch [1/1], Step [2318/8897], Loss: 6.2319\n",
      "Epoch [1/1], Step [2319/8897], Loss: 6.1511\n",
      "Epoch [1/1], Step [2320/8897], Loss: 6.2917\n",
      "Epoch [1/1], Step [2321/8897], Loss: 6.1873\n",
      "Epoch [1/1], Step [2322/8897], Loss: 5.9243\n",
      "Epoch [1/1], Step [2323/8897], Loss: 6.1700\n",
      "Epoch [1/1], Step [2324/8897], Loss: 6.2592\n",
      "Epoch [1/1], Step [2325/8897], Loss: 6.2955\n",
      "Epoch [1/1], Step [2326/8897], Loss: 6.3082\n",
      "Epoch [1/1], Step [2327/8897], Loss: 6.1414\n",
      "Epoch [1/1], Step [2328/8897], Loss: 6.1055\n",
      "Epoch [1/1], Step [2329/8897], Loss: 6.2414\n",
      "Epoch [1/1], Step [2330/8897], Loss: 6.3387\n",
      "Epoch [1/1], Step [2331/8897], Loss: 6.1177\n",
      "Epoch [1/1], Step [2332/8897], Loss: 6.2212\n",
      "Epoch [1/1], Step [2333/8897], Loss: 6.3213\n",
      "Epoch [1/1], Step [2334/8897], Loss: 6.1119\n",
      "Epoch [1/1], Step [2335/8897], Loss: 6.1729\n",
      "Epoch [1/1], Step [2336/8897], Loss: 6.3039\n",
      "Epoch [1/1], Step [2337/8897], Loss: 6.1284\n",
      "Epoch [1/1], Step [2338/8897], Loss: 6.0524\n",
      "Epoch [1/1], Step [2339/8897], Loss: 6.1686\n",
      "Epoch [1/1], Step [2340/8897], Loss: 6.2798\n",
      "Epoch [1/1], Step [2341/8897], Loss: 6.0909\n",
      "Epoch [1/1], Step [2342/8897], Loss: 6.1175\n",
      "Epoch [1/1], Step [2343/8897], Loss: 6.1831\n",
      "Epoch [1/1], Step [2344/8897], Loss: 5.9988\n",
      "Epoch [1/1], Step [2345/8897], Loss: 6.2096\n",
      "Epoch [1/1], Step [2346/8897], Loss: 6.2175\n",
      "Epoch [1/1], Step [2347/8897], Loss: 6.2873\n",
      "Epoch [1/1], Step [2348/8897], Loss: 6.2259\n",
      "Epoch [1/1], Step [2349/8897], Loss: 6.2849\n",
      "Epoch [1/1], Step [2350/8897], Loss: 6.2905\n",
      "Epoch [1/1], Step [2351/8897], Loss: 6.0352\n",
      "Epoch [1/1], Step [2352/8897], Loss: 6.2867\n",
      "Epoch [1/1], Step [2353/8897], Loss: 6.1112\n",
      "Epoch [1/1], Step [2354/8897], Loss: 6.2399\n",
      "Epoch [1/1], Step [2355/8897], Loss: 5.9882\n",
      "Epoch [1/1], Step [2356/8897], Loss: 6.2447\n",
      "Epoch [1/1], Step [2357/8897], Loss: 6.2488\n",
      "Epoch [1/1], Step [2358/8897], Loss: 6.0736\n",
      "Epoch [1/1], Step [2359/8897], Loss: 5.9940\n",
      "Epoch [1/1], Step [2360/8897], Loss: 6.2741\n",
      "Epoch [1/1], Step [2361/8897], Loss: 6.2183\n",
      "Epoch [1/1], Step [2362/8897], Loss: 6.2962\n",
      "Epoch [1/1], Step [2363/8897], Loss: 6.2827\n",
      "Epoch [1/1], Step [2364/8897], Loss: 6.1300\n",
      "Epoch [1/1], Step [2365/8897], Loss: 6.1087\n",
      "Epoch [1/1], Step [2366/8897], Loss: 6.3274\n",
      "Epoch [1/1], Step [2367/8897], Loss: 6.1734\n",
      "Epoch [1/1], Step [2368/8897], Loss: 6.1634\n",
      "Epoch [1/1], Step [2369/8897], Loss: 6.1468\n",
      "Epoch [1/1], Step [2370/8897], Loss: 6.0987\n",
      "Epoch [1/1], Step [2371/8897], Loss: 6.0566\n",
      "Epoch [1/1], Step [2372/8897], Loss: 6.2534\n",
      "Epoch [1/1], Step [2373/8897], Loss: 6.1889\n",
      "Epoch [1/1], Step [2374/8897], Loss: 6.0769\n",
      "Epoch [1/1], Step [2375/8897], Loss: 6.2581\n",
      "Epoch [1/1], Step [2376/8897], Loss: 6.1008\n",
      "Epoch [1/1], Step [2377/8897], Loss: 6.0503\n",
      "Epoch [1/1], Step [2378/8897], Loss: 6.1956\n",
      "Epoch [1/1], Step [2379/8897], Loss: 6.1319\n",
      "Epoch [1/1], Step [2380/8897], Loss: 6.3198\n",
      "Epoch [1/1], Step [2381/8897], Loss: 6.1104\n",
      "Epoch [1/1], Step [2382/8897], Loss: 6.1328\n",
      "Epoch [1/1], Step [2383/8897], Loss: 6.1603\n",
      "Epoch [1/1], Step [2384/8897], Loss: 6.1479\n",
      "Epoch [1/1], Step [2385/8897], Loss: 6.0270\n",
      "Epoch [1/1], Step [2386/8897], Loss: 6.1731\n",
      "Epoch [1/1], Step [2387/8897], Loss: 6.0019\n",
      "Epoch [1/1], Step [2388/8897], Loss: 6.2766\n",
      "Epoch [1/1], Step [2389/8897], Loss: 5.9978\n",
      "Epoch [1/1], Step [2390/8897], Loss: 6.2127\n",
      "Epoch [1/1], Step [2391/8897], Loss: 6.2078\n",
      "Epoch [1/1], Step [2392/8897], Loss: 6.1208\n",
      "Epoch [1/1], Step [2393/8897], Loss: 6.1422\n",
      "Epoch [1/1], Step [2394/8897], Loss: 6.2342\n",
      "Epoch [1/1], Step [2395/8897], Loss: 6.0740\n",
      "Epoch [1/1], Step [2396/8897], Loss: 6.0873\n",
      "Epoch [1/1], Step [2397/8897], Loss: 6.2368\n",
      "Epoch [1/1], Step [2398/8897], Loss: 6.1174\n",
      "Epoch [1/1], Step [2399/8897], Loss: 6.0705\n",
      "Epoch [1/1], Step [2400/8897], Loss: 6.1526\n",
      "Epoch [1/1], Step [2401/8897], Loss: 6.3192\n",
      "Epoch [1/1], Step [2402/8897], Loss: 6.0930\n",
      "Epoch [1/1], Step [2403/8897], Loss: 6.0954\n",
      "Epoch [1/1], Step [2404/8897], Loss: 5.9517\n",
      "Epoch [1/1], Step [2405/8897], Loss: 6.1781\n",
      "Epoch [1/1], Step [2406/8897], Loss: 6.1601\n",
      "Epoch [1/1], Step [2407/8897], Loss: 6.0687\n",
      "Epoch [1/1], Step [2408/8897], Loss: 6.1851\n",
      "Epoch [1/1], Step [2409/8897], Loss: 6.5358\n",
      "Epoch [1/1], Step [2410/8897], Loss: 6.0806\n",
      "Epoch [1/1], Step [2411/8897], Loss: 6.0651\n",
      "Epoch [1/1], Step [2412/8897], Loss: 6.0824\n",
      "Epoch [1/1], Step [2413/8897], Loss: 6.2261\n",
      "Epoch [1/1], Step [2414/8897], Loss: 5.9896\n",
      "Epoch [1/1], Step [2415/8897], Loss: 6.1949\n",
      "Epoch [1/1], Step [2416/8897], Loss: 6.1332\n",
      "Epoch [1/1], Step [2417/8897], Loss: 5.8731\n",
      "Epoch [1/1], Step [2418/8897], Loss: 6.2856\n",
      "Epoch [1/1], Step [2419/8897], Loss: 6.1697\n",
      "Epoch [1/1], Step [2420/8897], Loss: 6.0787\n",
      "Epoch [1/1], Step [2421/8897], Loss: 6.1810\n",
      "Epoch [1/1], Step [2422/8897], Loss: 6.0876\n",
      "Epoch [1/1], Step [2423/8897], Loss: 6.0538\n",
      "Epoch [1/1], Step [2424/8897], Loss: 5.9218\n",
      "Epoch [1/1], Step [2425/8897], Loss: 6.0444\n",
      "Epoch [1/1], Step [2426/8897], Loss: 6.0763\n",
      "Epoch [1/1], Step [2427/8897], Loss: 6.2414\n",
      "Epoch [1/1], Step [2428/8897], Loss: 6.3154\n",
      "Epoch [1/1], Step [2429/8897], Loss: 5.9507\n",
      "Epoch [1/1], Step [2430/8897], Loss: 6.3241\n",
      "Epoch [1/1], Step [2431/8897], Loss: 6.0220\n",
      "Epoch [1/1], Step [2432/8897], Loss: 5.8463\n",
      "Epoch [1/1], Step [2433/8897], Loss: 6.1803\n",
      "Epoch [1/1], Step [2434/8897], Loss: 6.1176\n",
      "Epoch [1/1], Step [2435/8897], Loss: 6.0107\n",
      "Epoch [1/1], Step [2436/8897], Loss: 6.0454\n",
      "Epoch [1/1], Step [2437/8897], Loss: 6.2206\n",
      "Epoch [1/1], Step [2438/8897], Loss: 6.3046\n",
      "Epoch [1/1], Step [2439/8897], Loss: 6.2028\n",
      "Epoch [1/1], Step [2440/8897], Loss: 6.1601\n",
      "Epoch [1/1], Step [2441/8897], Loss: 6.2365\n",
      "Epoch [1/1], Step [2442/8897], Loss: 5.9194\n",
      "Epoch [1/1], Step [2443/8897], Loss: 5.9188\n",
      "Epoch [1/1], Step [2444/8897], Loss: 6.0276\n",
      "Epoch [1/1], Step [2445/8897], Loss: 6.0359\n",
      "Epoch [1/1], Step [2446/8897], Loss: 6.0972\n",
      "Epoch [1/1], Step [2447/8897], Loss: 6.2317\n",
      "Epoch [1/1], Step [2448/8897], Loss: 6.1880\n",
      "Epoch [1/1], Step [2449/8897], Loss: 6.2286\n",
      "Epoch [1/1], Step [2450/8897], Loss: 6.3578\n",
      "Epoch [1/1], Step [2451/8897], Loss: 6.2464\n",
      "Epoch [1/1], Step [2452/8897], Loss: 6.3267\n",
      "Epoch [1/1], Step [2453/8897], Loss: 6.2121\n",
      "Epoch [1/1], Step [2454/8897], Loss: 6.2396\n",
      "Epoch [1/1], Step [2455/8897], Loss: 6.1341\n",
      "Epoch [1/1], Step [2456/8897], Loss: 6.2614\n",
      "Epoch [1/1], Step [2457/8897], Loss: 6.1258\n",
      "Epoch [1/1], Step [2458/8897], Loss: 6.1410\n",
      "Epoch [1/1], Step [2459/8897], Loss: 6.2042\n",
      "Epoch [1/1], Step [2460/8897], Loss: 6.1928\n",
      "Epoch [1/1], Step [2461/8897], Loss: 6.2931\n",
      "Epoch [1/1], Step [2462/8897], Loss: 6.2336\n",
      "Epoch [1/1], Step [2463/8897], Loss: 6.1895\n",
      "Epoch [1/1], Step [2464/8897], Loss: 6.1608\n",
      "Epoch [1/1], Step [2465/8897], Loss: 6.1983\n",
      "Epoch [1/1], Step [2466/8897], Loss: 6.2030\n",
      "Epoch [1/1], Step [2467/8897], Loss: 6.2685\n",
      "Epoch [1/1], Step [2468/8897], Loss: 6.2794\n",
      "Epoch [1/1], Step [2469/8897], Loss: 6.1981\n",
      "Epoch [1/1], Step [2470/8897], Loss: 6.2251\n",
      "Epoch [1/1], Step [2471/8897], Loss: 6.0912\n",
      "Epoch [1/1], Step [2472/8897], Loss: 6.2263\n",
      "Epoch [1/1], Step [2473/8897], Loss: 6.1054\n",
      "Epoch [1/1], Step [2474/8897], Loss: 6.1244\n",
      "Epoch [1/1], Step [2475/8897], Loss: 6.1636\n",
      "Epoch [1/1], Step [2476/8897], Loss: 5.9691\n",
      "Epoch [1/1], Step [2477/8897], Loss: 6.0451\n",
      "Epoch [1/1], Step [2478/8897], Loss: 6.1659\n",
      "Epoch [1/1], Step [2479/8897], Loss: 6.0543\n",
      "Epoch [1/1], Step [2480/8897], Loss: 6.0022\n",
      "Epoch [1/1], Step [2481/8897], Loss: 6.2400\n",
      "Epoch [1/1], Step [2482/8897], Loss: 6.3064\n",
      "Epoch [1/1], Step [2483/8897], Loss: 6.0326\n",
      "Epoch [1/1], Step [2484/8897], Loss: 6.0746\n",
      "Epoch [1/1], Step [2485/8897], Loss: 6.3351\n",
      "Epoch [1/1], Step [2486/8897], Loss: 6.2237\n",
      "Epoch [1/1], Step [2487/8897], Loss: 6.1669\n",
      "Epoch [1/1], Step [2488/8897], Loss: 6.1485\n",
      "Epoch [1/1], Step [2489/8897], Loss: 6.2662\n",
      "Epoch [1/1], Step [2490/8897], Loss: 6.1556\n",
      "Epoch [1/1], Step [2491/8897], Loss: 6.0336\n",
      "Epoch [1/1], Step [2492/8897], Loss: 6.1461\n",
      "Epoch [1/1], Step [2493/8897], Loss: 6.2372\n",
      "Epoch [1/1], Step [2494/8897], Loss: 6.2308\n",
      "Epoch [1/1], Step [2495/8897], Loss: 6.1007\n",
      "Epoch [1/1], Step [2496/8897], Loss: 6.1650\n",
      "Epoch [1/1], Step [2497/8897], Loss: 6.0808\n",
      "Epoch [1/1], Step [2498/8897], Loss: 6.2607\n",
      "Epoch [1/1], Step [2499/8897], Loss: 6.3294\n",
      "Epoch [1/1], Step [2500/8897], Loss: 6.1207\n",
      "Epoch [1/1], Step [2501/8897], Loss: 6.3428\n",
      "Epoch [1/1], Step [2502/8897], Loss: 6.1833\n",
      "Epoch [1/1], Step [2503/8897], Loss: 6.2795\n",
      "Epoch [1/1], Step [2504/8897], Loss: 6.1017\n",
      "Epoch [1/1], Step [2505/8897], Loss: 6.0745\n",
      "Epoch [1/1], Step [2506/8897], Loss: 5.9914\n",
      "Epoch [1/1], Step [2507/8897], Loss: 6.1880\n",
      "Epoch [1/1], Step [2508/8897], Loss: 6.0386\n",
      "Epoch [1/1], Step [2509/8897], Loss: 6.1703\n",
      "Epoch [1/1], Step [2510/8897], Loss: 6.0408\n",
      "Epoch [1/1], Step [2511/8897], Loss: 6.1294\n",
      "Epoch [1/1], Step [2512/8897], Loss: 6.1105\n",
      "Epoch [1/1], Step [2513/8897], Loss: 6.1978\n",
      "Epoch [1/1], Step [2514/8897], Loss: 6.2129\n",
      "Epoch [1/1], Step [2515/8897], Loss: 6.2239\n",
      "Epoch [1/1], Step [2516/8897], Loss: 6.0993\n",
      "Epoch [1/1], Step [2517/8897], Loss: 6.3205\n",
      "Epoch [1/1], Step [2518/8897], Loss: 6.1376\n",
      "Epoch [1/1], Step [2519/8897], Loss: 5.9283\n",
      "Epoch [1/1], Step [2520/8897], Loss: 5.9282\n",
      "Epoch [1/1], Step [2521/8897], Loss: 6.1025\n",
      "Epoch [1/1], Step [2522/8897], Loss: 6.2148\n",
      "Epoch [1/1], Step [2523/8897], Loss: 6.1887\n",
      "Epoch [1/1], Step [2524/8897], Loss: 6.2115\n",
      "Epoch [1/1], Step [2525/8897], Loss: 6.1216\n",
      "Epoch [1/1], Step [2526/8897], Loss: 6.0761\n",
      "Epoch [1/1], Step [2527/8897], Loss: 6.2655\n",
      "Epoch [1/1], Step [2528/8897], Loss: 6.2496\n",
      "Epoch [1/1], Step [2529/8897], Loss: 6.1323\n",
      "Epoch [1/1], Step [2530/8897], Loss: 6.2731\n",
      "Epoch [1/1], Step [2531/8897], Loss: 6.2344\n",
      "Epoch [1/1], Step [2532/8897], Loss: 6.1483\n",
      "Epoch [1/1], Step [2533/8897], Loss: 6.1656\n",
      "Epoch [1/1], Step [2534/8897], Loss: 6.1735\n",
      "Epoch [1/1], Step [2535/8897], Loss: 6.2376\n",
      "Epoch [1/1], Step [2536/8897], Loss: 6.1957\n",
      "Epoch [1/1], Step [2537/8897], Loss: 6.0645\n",
      "Epoch [1/1], Step [2538/8897], Loss: 6.4158\n",
      "Epoch [1/1], Step [2539/8897], Loss: 6.3083\n",
      "Epoch [1/1], Step [2540/8897], Loss: 6.2356\n",
      "Epoch [1/1], Step [2541/8897], Loss: 6.1520\n",
      "Epoch [1/1], Step [2542/8897], Loss: 5.9640\n",
      "Epoch [1/1], Step [2543/8897], Loss: 6.0930\n",
      "Epoch [1/1], Step [2544/8897], Loss: 5.8266\n",
      "Epoch [1/1], Step [2545/8897], Loss: 6.1789\n",
      "Epoch [1/1], Step [2546/8897], Loss: 6.0987\n",
      "Epoch [1/1], Step [2547/8897], Loss: 6.2026\n",
      "Epoch [1/1], Step [2548/8897], Loss: 6.0798\n",
      "Epoch [1/1], Step [2549/8897], Loss: 5.9613\n",
      "Epoch [1/1], Step [2550/8897], Loss: 6.1408\n",
      "Epoch [1/1], Step [2551/8897], Loss: 6.2112\n",
      "Epoch [1/1], Step [2552/8897], Loss: 6.1523\n",
      "Epoch [1/1], Step [2553/8897], Loss: 6.0256\n",
      "Epoch [1/1], Step [2554/8897], Loss: 6.1750\n",
      "Epoch [1/1], Step [2555/8897], Loss: 6.3684\n",
      "Epoch [1/1], Step [2556/8897], Loss: 6.1485\n",
      "Epoch [1/1], Step [2557/8897], Loss: 5.9458\n",
      "Epoch [1/1], Step [2558/8897], Loss: 5.9979\n",
      "Epoch [1/1], Step [2559/8897], Loss: 6.2702\n",
      "Epoch [1/1], Step [2560/8897], Loss: 6.0555\n",
      "Epoch [1/1], Step [2561/8897], Loss: 6.0699\n",
      "Epoch [1/1], Step [2562/8897], Loss: 6.1439\n",
      "Epoch [1/1], Step [2563/8897], Loss: 6.0804\n",
      "Epoch [1/1], Step [2564/8897], Loss: 6.1981\n",
      "Epoch [1/1], Step [2565/8897], Loss: 5.9421\n",
      "Epoch [1/1], Step [2566/8897], Loss: 6.1184\n",
      "Epoch [1/1], Step [2567/8897], Loss: 6.2097\n",
      "Epoch [1/1], Step [2568/8897], Loss: 6.3294\n",
      "Epoch [1/1], Step [2569/8897], Loss: 5.9200\n",
      "Epoch [1/1], Step [2570/8897], Loss: 5.9838\n",
      "Epoch [1/1], Step [2571/8897], Loss: 6.1058\n",
      "Epoch [1/1], Step [2572/8897], Loss: 6.1504\n",
      "Epoch [1/1], Step [2573/8897], Loss: 6.1612\n",
      "Epoch [1/1], Step [2574/8897], Loss: 6.1882\n",
      "Epoch [1/1], Step [2575/8897], Loss: 5.9576\n",
      "Epoch [1/1], Step [2576/8897], Loss: 6.0494\n",
      "Epoch [1/1], Step [2577/8897], Loss: 6.3680\n",
      "Epoch [1/1], Step [2578/8897], Loss: 6.0262\n",
      "Epoch [1/1], Step [2579/8897], Loss: 6.0742\n",
      "Epoch [1/1], Step [2580/8897], Loss: 6.1396\n",
      "Epoch [1/1], Step [2581/8897], Loss: 6.1965\n",
      "Epoch [1/1], Step [2582/8897], Loss: 5.9459\n",
      "Epoch [1/1], Step [2583/8897], Loss: 6.2113\n",
      "Epoch [1/1], Step [2584/8897], Loss: 6.3404\n",
      "Epoch [1/1], Step [2585/8897], Loss: 6.1378\n",
      "Epoch [1/1], Step [2586/8897], Loss: 5.9645\n",
      "Epoch [1/1], Step [2587/8897], Loss: 6.2385\n",
      "Epoch [1/1], Step [2588/8897], Loss: 6.2214\n",
      "Epoch [1/1], Step [2589/8897], Loss: 6.1426\n",
      "Epoch [1/1], Step [2590/8897], Loss: 6.1848\n",
      "Epoch [1/1], Step [2591/8897], Loss: 5.9461\n",
      "Epoch [1/1], Step [2592/8897], Loss: 5.9912\n",
      "Epoch [1/1], Step [2593/8897], Loss: 6.0469\n",
      "Epoch [1/1], Step [2594/8897], Loss: 6.0722\n",
      "Epoch [1/1], Step [2595/8897], Loss: 6.1526\n",
      "Epoch [1/1], Step [2596/8897], Loss: 6.0884\n",
      "Epoch [1/1], Step [2597/8897], Loss: 5.9551\n",
      "Epoch [1/1], Step [2598/8897], Loss: 6.1106\n",
      "Epoch [1/1], Step [2599/8897], Loss: 6.1650\n",
      "Epoch [1/1], Step [2600/8897], Loss: 5.9872\n",
      "Epoch [1/1], Step [2601/8897], Loss: 6.1441\n",
      "Epoch [1/1], Step [2602/8897], Loss: 6.3123\n",
      "Epoch [1/1], Step [2603/8897], Loss: 6.0278\n",
      "Epoch [1/1], Step [2604/8897], Loss: 6.0947\n",
      "Epoch [1/1], Step [2605/8897], Loss: 6.2224\n",
      "Epoch [1/1], Step [2606/8897], Loss: 6.1949\n",
      "Epoch [1/1], Step [2607/8897], Loss: 5.9308\n",
      "Epoch [1/1], Step [2608/8897], Loss: 5.9474\n",
      "Epoch [1/1], Step [2609/8897], Loss: 6.1919\n",
      "Epoch [1/1], Step [2610/8897], Loss: 6.0854\n",
      "Epoch [1/1], Step [2611/8897], Loss: 6.0728\n",
      "Epoch [1/1], Step [2612/8897], Loss: 6.1195\n",
      "Epoch [1/1], Step [2613/8897], Loss: 6.1295\n",
      "Epoch [1/1], Step [2614/8897], Loss: 6.0086\n",
      "Epoch [1/1], Step [2615/8897], Loss: 6.2021\n",
      "Epoch [1/1], Step [2616/8897], Loss: 6.2872\n",
      "Epoch [1/1], Step [2617/8897], Loss: 6.1394\n",
      "Epoch [1/1], Step [2618/8897], Loss: 6.0977\n",
      "Epoch [1/1], Step [2619/8897], Loss: 6.1210\n",
      "Epoch [1/1], Step [2620/8897], Loss: 6.0635\n",
      "Epoch [1/1], Step [2621/8897], Loss: 6.1921\n",
      "Epoch [1/1], Step [2622/8897], Loss: 6.1342\n",
      "Epoch [1/1], Step [2623/8897], Loss: 6.1966\n",
      "Epoch [1/1], Step [2624/8897], Loss: 6.2041\n",
      "Epoch [1/1], Step [2625/8897], Loss: 6.0448\n",
      "Epoch [1/1], Step [2626/8897], Loss: 5.9798\n",
      "Epoch [1/1], Step [2627/8897], Loss: 6.2116\n",
      "Epoch [1/1], Step [2628/8897], Loss: 6.0374\n",
      "Epoch [1/1], Step [2629/8897], Loss: 6.1228\n",
      "Epoch [1/1], Step [2630/8897], Loss: 5.9308\n",
      "Epoch [1/1], Step [2631/8897], Loss: 6.0740\n",
      "Epoch [1/1], Step [2632/8897], Loss: 5.6784\n",
      "Epoch [1/1], Step [2633/8897], Loss: 6.0716\n",
      "Epoch [1/1], Step [2634/8897], Loss: 6.2361\n",
      "Epoch [1/1], Step [2635/8897], Loss: 6.0800\n",
      "Epoch [1/1], Step [2636/8897], Loss: 6.2859\n",
      "Epoch [1/1], Step [2637/8897], Loss: 6.2809\n",
      "Epoch [1/1], Step [2638/8897], Loss: 6.1857\n",
      "Epoch [1/1], Step [2639/8897], Loss: 6.3967\n",
      "Epoch [1/1], Step [2640/8897], Loss: 6.1814\n",
      "Epoch [1/1], Step [2641/8897], Loss: 6.3337\n",
      "Epoch [1/1], Step [2642/8897], Loss: 6.0965\n",
      "Epoch [1/1], Step [2643/8897], Loss: 5.9962\n",
      "Epoch [1/1], Step [2644/8897], Loss: 6.1470\n",
      "Epoch [1/1], Step [2645/8897], Loss: 6.2517\n",
      "Epoch [1/1], Step [2646/8897], Loss: 6.1189\n",
      "Epoch [1/1], Step [2647/8897], Loss: 6.0381\n",
      "Epoch [1/1], Step [2648/8897], Loss: 5.9894\n",
      "Epoch [1/1], Step [2649/8897], Loss: 5.9770\n",
      "Epoch [1/1], Step [2650/8897], Loss: 5.9975\n",
      "Epoch [1/1], Step [2651/8897], Loss: 6.1190\n",
      "Epoch [1/1], Step [2652/8897], Loss: 6.0526\n",
      "Epoch [1/1], Step [2653/8897], Loss: 6.1008\n",
      "Epoch [1/1], Step [2654/8897], Loss: 5.9735\n",
      "Epoch [1/1], Step [2655/8897], Loss: 6.1493\n",
      "Epoch [1/1], Step [2656/8897], Loss: 6.0812\n",
      "Epoch [1/1], Step [2657/8897], Loss: 6.1413\n",
      "Epoch [1/1], Step [2658/8897], Loss: 6.1772\n",
      "Epoch [1/1], Step [2659/8897], Loss: 6.0962\n",
      "Epoch [1/1], Step [2660/8897], Loss: 6.0740\n",
      "Epoch [1/1], Step [2661/8897], Loss: 6.0017\n",
      "Epoch [1/1], Step [2662/8897], Loss: 6.0043\n",
      "Epoch [1/1], Step [2663/8897], Loss: 5.9712\n",
      "Epoch [1/1], Step [2664/8897], Loss: 6.0125\n",
      "Epoch [1/1], Step [2665/8897], Loss: 6.1051\n",
      "Epoch [1/1], Step [2666/8897], Loss: 6.0688\n",
      "Epoch [1/1], Step [2667/8897], Loss: 6.1205\n",
      "Epoch [1/1], Step [2668/8897], Loss: 6.1638\n",
      "Epoch [1/1], Step [2669/8897], Loss: 6.1429\n",
      "Epoch [1/1], Step [2670/8897], Loss: 6.4481\n",
      "Epoch [1/1], Step [2671/8897], Loss: 5.9732\n",
      "Epoch [1/1], Step [2672/8897], Loss: 6.3322\n",
      "Epoch [1/1], Step [2673/8897], Loss: 6.0072\n",
      "Epoch [1/1], Step [2674/8897], Loss: 5.9529\n",
      "Epoch [1/1], Step [2675/8897], Loss: 5.9187\n",
      "Epoch [1/1], Step [2676/8897], Loss: 5.9069\n",
      "Epoch [1/1], Step [2677/8897], Loss: 6.0856\n",
      "Epoch [1/1], Step [2678/8897], Loss: 6.1461\n",
      "Epoch [1/1], Step [2679/8897], Loss: 6.1904\n",
      "Epoch [1/1], Step [2680/8897], Loss: 5.9481\n",
      "Epoch [1/1], Step [2681/8897], Loss: 5.7806\n",
      "Epoch [1/1], Step [2682/8897], Loss: 5.9638\n",
      "Epoch [1/1], Step [2683/8897], Loss: 6.0956\n",
      "Epoch [1/1], Step [2684/8897], Loss: 5.8348\n",
      "Epoch [1/1], Step [2685/8897], Loss: 5.9732\n",
      "Epoch [1/1], Step [2686/8897], Loss: 6.0982\n",
      "Epoch [1/1], Step [2687/8897], Loss: 6.2625\n",
      "Epoch [1/1], Step [2688/8897], Loss: 5.9324\n",
      "Epoch [1/1], Step [2689/8897], Loss: 6.1986\n",
      "Epoch [1/1], Step [2690/8897], Loss: 6.2783\n",
      "Epoch [1/1], Step [2691/8897], Loss: 5.9529\n",
      "Epoch [1/1], Step [2692/8897], Loss: 6.0772\n",
      "Epoch [1/1], Step [2693/8897], Loss: 6.0845\n",
      "Epoch [1/1], Step [2694/8897], Loss: 6.0463\n",
      "Epoch [1/1], Step [2695/8897], Loss: 5.8393\n",
      "Epoch [1/1], Step [2696/8897], Loss: 5.7967\n",
      "Epoch [1/1], Step [2697/8897], Loss: 5.8719\n",
      "Epoch [1/1], Step [2698/8897], Loss: 6.2094\n",
      "Epoch [1/1], Step [2699/8897], Loss: 6.2119\n",
      "Epoch [1/1], Step [2700/8897], Loss: 6.0351\n",
      "Epoch [1/1], Step [2701/8897], Loss: 5.9884\n",
      "Epoch [1/1], Step [2702/8897], Loss: 6.1347\n",
      "Epoch [1/1], Step [2703/8897], Loss: 5.9499\n",
      "Epoch [1/1], Step [2704/8897], Loss: 6.2305\n",
      "Epoch [1/1], Step [2705/8897], Loss: 6.1584\n",
      "Epoch [1/1], Step [2706/8897], Loss: 6.0833\n",
      "Epoch [1/1], Step [2707/8897], Loss: 6.1745\n",
      "Epoch [1/1], Step [2708/8897], Loss: 6.0812\n",
      "Epoch [1/1], Step [2709/8897], Loss: 6.0303\n",
      "Epoch [1/1], Step [2710/8897], Loss: 6.0443\n",
      "Epoch [1/1], Step [2711/8897], Loss: 6.1598\n",
      "Epoch [1/1], Step [2712/8897], Loss: 5.9765\n",
      "Epoch [1/1], Step [2713/8897], Loss: 6.0556\n",
      "Epoch [1/1], Step [2714/8897], Loss: 6.0104\n",
      "Epoch [1/1], Step [2715/8897], Loss: 6.0375\n",
      "Epoch [1/1], Step [2716/8897], Loss: 6.1474\n",
      "Epoch [1/1], Step [2717/8897], Loss: 6.0886\n",
      "Epoch [1/1], Step [2718/8897], Loss: 6.1337\n",
      "Epoch [1/1], Step [2719/8897], Loss: 6.2317\n",
      "Epoch [1/1], Step [2720/8897], Loss: 6.1618\n",
      "Epoch [1/1], Step [2721/8897], Loss: 6.1632\n",
      "Epoch [1/1], Step [2722/8897], Loss: 6.2810\n",
      "Epoch [1/1], Step [2723/8897], Loss: 6.0442\n",
      "Epoch [1/1], Step [2724/8897], Loss: 6.1181\n",
      "Epoch [1/1], Step [2725/8897], Loss: 6.3168\n",
      "Epoch [1/1], Step [2726/8897], Loss: 5.9345\n",
      "Epoch [1/1], Step [2727/8897], Loss: 6.1147\n",
      "Epoch [1/1], Step [2728/8897], Loss: 5.9743\n",
      "Epoch [1/1], Step [2729/8897], Loss: 6.0083\n",
      "Epoch [1/1], Step [2730/8897], Loss: 6.0489\n",
      "Epoch [1/1], Step [2731/8897], Loss: 6.0863\n",
      "Epoch [1/1], Step [2732/8897], Loss: 5.9739\n",
      "Epoch [1/1], Step [2733/8897], Loss: 6.2605\n",
      "Epoch [1/1], Step [2734/8897], Loss: 6.1163\n",
      "Epoch [1/1], Step [2735/8897], Loss: 5.9837\n",
      "Epoch [1/1], Step [2736/8897], Loss: 6.2213\n",
      "Epoch [1/1], Step [2737/8897], Loss: 6.0939\n",
      "Epoch [1/1], Step [2738/8897], Loss: 6.2132\n",
      "Epoch [1/1], Step [2739/8897], Loss: 6.0523\n",
      "Epoch [1/1], Step [2740/8897], Loss: 6.1818\n",
      "Epoch [1/1], Step [2741/8897], Loss: 6.2676\n",
      "Epoch [1/1], Step [2742/8897], Loss: 6.1189\n",
      "Epoch [1/1], Step [2743/8897], Loss: 5.9471\n",
      "Epoch [1/1], Step [2744/8897], Loss: 6.0686\n",
      "Epoch [1/1], Step [2745/8897], Loss: 6.0410\n",
      "Epoch [1/1], Step [2746/8897], Loss: 6.0665\n",
      "Epoch [1/1], Step [2747/8897], Loss: 6.2490\n",
      "Epoch [1/1], Step [2748/8897], Loss: 6.0837\n",
      "Epoch [1/1], Step [2749/8897], Loss: 6.1475\n",
      "Epoch [1/1], Step [2750/8897], Loss: 6.0237\n",
      "Epoch [1/1], Step [2751/8897], Loss: 6.0304\n",
      "Epoch [1/1], Step [2752/8897], Loss: 6.0506\n",
      "Epoch [1/1], Step [2753/8897], Loss: 5.8563\n",
      "Epoch [1/1], Step [2754/8897], Loss: 6.0571\n",
      "Epoch [1/1], Step [2755/8897], Loss: 6.1978\n",
      "Epoch [1/1], Step [2756/8897], Loss: 5.9964\n",
      "Epoch [1/1], Step [2757/8897], Loss: 6.1771\n",
      "Epoch [1/1], Step [2758/8897], Loss: 5.9591\n",
      "Epoch [1/1], Step [2759/8897], Loss: 5.9384\n",
      "Epoch [1/1], Step [2760/8897], Loss: 6.1685\n",
      "Epoch [1/1], Step [2761/8897], Loss: 5.9199\n",
      "Epoch [1/1], Step [2762/8897], Loss: 5.8530\n",
      "Epoch [1/1], Step [2763/8897], Loss: 5.9892\n",
      "Epoch [1/1], Step [2764/8897], Loss: 5.9464\n",
      "Epoch [1/1], Step [2765/8897], Loss: 6.0799\n",
      "Epoch [1/1], Step [2766/8897], Loss: 5.9581\n",
      "Epoch [1/1], Step [2767/8897], Loss: 6.2959\n",
      "Epoch [1/1], Step [2768/8897], Loss: 5.7713\n",
      "Epoch [1/1], Step [2769/8897], Loss: 6.1895\n",
      "Epoch [1/1], Step [2770/8897], Loss: 5.9830\n",
      "Epoch [1/1], Step [2771/8897], Loss: 6.1226\n",
      "Epoch [1/1], Step [2772/8897], Loss: 5.9924\n",
      "Epoch [1/1], Step [2773/8897], Loss: 6.0458\n",
      "Epoch [1/1], Step [2774/8897], Loss: 5.8472\n",
      "Epoch [1/1], Step [2775/8897], Loss: 6.2559\n",
      "Epoch [1/1], Step [2776/8897], Loss: 6.1131\n",
      "Epoch [1/1], Step [2777/8897], Loss: 6.2907\n",
      "Epoch [1/1], Step [2778/8897], Loss: 6.0591\n",
      "Epoch [1/1], Step [2779/8897], Loss: 6.4491\n",
      "Epoch [1/1], Step [2780/8897], Loss: 5.8709\n",
      "Epoch [1/1], Step [2781/8897], Loss: 5.9742\n",
      "Epoch [1/1], Step [2782/8897], Loss: 6.2668\n",
      "Epoch [1/1], Step [2783/8897], Loss: 6.1187\n",
      "Epoch [1/1], Step [2784/8897], Loss: 6.0524\n",
      "Epoch [1/1], Step [2785/8897], Loss: 6.1107\n",
      "Epoch [1/1], Step [2786/8897], Loss: 6.2599\n",
      "Epoch [1/1], Step [2787/8897], Loss: 6.0457\n",
      "Epoch [1/1], Step [2788/8897], Loss: 6.3234\n",
      "Epoch [1/1], Step [2789/8897], Loss: 6.0747\n",
      "Epoch [1/1], Step [2790/8897], Loss: 6.0408\n",
      "Epoch [1/1], Step [2791/8897], Loss: 6.1777\n",
      "Epoch [1/1], Step [2792/8897], Loss: 5.9281\n",
      "Epoch [1/1], Step [2793/8897], Loss: 6.2681\n",
      "Epoch [1/1], Step [2794/8897], Loss: 6.1278\n",
      "Epoch [1/1], Step [2795/8897], Loss: 6.0331\n",
      "Epoch [1/1], Step [2796/8897], Loss: 6.0591\n",
      "Epoch [1/1], Step [2797/8897], Loss: 5.9341\n",
      "Epoch [1/1], Step [2798/8897], Loss: 5.7945\n",
      "Epoch [1/1], Step [2799/8897], Loss: 6.0852\n",
      "Epoch [1/1], Step [2800/8897], Loss: 6.2327\n",
      "Epoch [1/1], Step [2801/8897], Loss: 6.0914\n",
      "Epoch [1/1], Step [2802/8897], Loss: 6.1867\n",
      "Epoch [1/1], Step [2803/8897], Loss: 6.1487\n",
      "Epoch [1/1], Step [2804/8897], Loss: 6.0554\n",
      "Epoch [1/1], Step [2805/8897], Loss: 6.0641\n",
      "Epoch [1/1], Step [2806/8897], Loss: 6.2388\n",
      "Epoch [1/1], Step [2807/8897], Loss: 6.0385\n",
      "Epoch [1/1], Step [2808/8897], Loss: 5.8452\n",
      "Epoch [1/1], Step [2809/8897], Loss: 6.0897\n",
      "Epoch [1/1], Step [2810/8897], Loss: 6.0486\n",
      "Epoch [1/1], Step [2811/8897], Loss: 6.1321\n",
      "Epoch [1/1], Step [2812/8897], Loss: 5.9952\n",
      "Epoch [1/1], Step [2813/8897], Loss: 6.1872\n",
      "Epoch [1/1], Step [2814/8897], Loss: 6.1652\n",
      "Epoch [1/1], Step [2815/8897], Loss: 6.1950\n",
      "Epoch [1/1], Step [2816/8897], Loss: 5.7918\n",
      "Epoch [1/1], Step [2817/8897], Loss: 6.0672\n",
      "Epoch [1/1], Step [2818/8897], Loss: 6.0680\n",
      "Epoch [1/1], Step [2819/8897], Loss: 6.0666\n",
      "Epoch [1/1], Step [2820/8897], Loss: 5.8783\n",
      "Epoch [1/1], Step [2821/8897], Loss: 6.1234\n",
      "Epoch [1/1], Step [2822/8897], Loss: 6.1578\n",
      "Epoch [1/1], Step [2823/8897], Loss: 6.1250\n",
      "Epoch [1/1], Step [2824/8897], Loss: 6.2907\n",
      "Epoch [1/1], Step [2825/8897], Loss: 6.2544\n",
      "Epoch [1/1], Step [2826/8897], Loss: 6.0280\n",
      "Epoch [1/1], Step [2827/8897], Loss: 6.0891\n",
      "Epoch [1/1], Step [2828/8897], Loss: 5.8907\n",
      "Epoch [1/1], Step [2829/8897], Loss: 5.9884\n",
      "Epoch [1/1], Step [2830/8897], Loss: 6.0952\n",
      "Epoch [1/1], Step [2831/8897], Loss: 6.0037\n",
      "Epoch [1/1], Step [2832/8897], Loss: 6.1355\n",
      "Epoch [1/1], Step [2833/8897], Loss: 5.8771\n",
      "Epoch [1/1], Step [2834/8897], Loss: 6.1150\n",
      "Epoch [1/1], Step [2835/8897], Loss: 6.0775\n",
      "Epoch [1/1], Step [2836/8897], Loss: 6.3212\n",
      "Epoch [1/1], Step [2837/8897], Loss: 6.1036\n",
      "Epoch [1/1], Step [2838/8897], Loss: 5.9199\n",
      "Epoch [1/1], Step [2839/8897], Loss: 6.1255\n",
      "Epoch [1/1], Step [2840/8897], Loss: 6.0565\n",
      "Epoch [1/1], Step [2841/8897], Loss: 5.8813\n",
      "Epoch [1/1], Step [2842/8897], Loss: 6.0846\n",
      "Epoch [1/1], Step [2843/8897], Loss: 6.0668\n",
      "Epoch [1/1], Step [2844/8897], Loss: 6.0471\n",
      "Epoch [1/1], Step [2845/8897], Loss: 6.1055\n",
      "Epoch [1/1], Step [2846/8897], Loss: 5.9918\n",
      "Epoch [1/1], Step [2847/8897], Loss: 6.1372\n",
      "Epoch [1/1], Step [2848/8897], Loss: 5.9641\n",
      "Epoch [1/1], Step [2849/8897], Loss: 6.0491\n",
      "Epoch [1/1], Step [2850/8897], Loss: 6.1722\n",
      "Epoch [1/1], Step [2851/8897], Loss: 5.9238\n",
      "Epoch [1/1], Step [2852/8897], Loss: 6.0813\n",
      "Epoch [1/1], Step [2853/8897], Loss: 5.9987\n",
      "Epoch [1/1], Step [2854/8897], Loss: 6.1528\n",
      "Epoch [1/1], Step [2855/8897], Loss: 5.8049\n",
      "Epoch [1/1], Step [2856/8897], Loss: 6.0409\n",
      "Epoch [1/1], Step [2857/8897], Loss: 6.0664\n",
      "Epoch [1/1], Step [2858/8897], Loss: 6.0199\n",
      "Epoch [1/1], Step [2859/8897], Loss: 6.0310\n",
      "Epoch [1/1], Step [2860/8897], Loss: 6.1251\n",
      "Epoch [1/1], Step [2861/8897], Loss: 6.2749\n",
      "Epoch [1/1], Step [2862/8897], Loss: 6.1848\n",
      "Epoch [1/1], Step [2863/8897], Loss: 6.2880\n",
      "Epoch [1/1], Step [2864/8897], Loss: 6.1481\n",
      "Epoch [1/1], Step [2865/8897], Loss: 6.1467\n",
      "Epoch [1/1], Step [2866/8897], Loss: 6.0321\n",
      "Epoch [1/1], Step [2867/8897], Loss: 6.3989\n",
      "Epoch [1/1], Step [2868/8897], Loss: 6.2150\n",
      "Epoch [1/1], Step [2869/8897], Loss: 6.0581\n",
      "Epoch [1/1], Step [2870/8897], Loss: 6.2243\n",
      "Epoch [1/1], Step [2871/8897], Loss: 5.9501\n",
      "Epoch [1/1], Step [2872/8897], Loss: 6.0662\n",
      "Epoch [1/1], Step [2873/8897], Loss: 5.9950\n",
      "Epoch [1/1], Step [2874/8897], Loss: 5.9531\n",
      "Epoch [1/1], Step [2875/8897], Loss: 5.8557\n",
      "Epoch [1/1], Step [2876/8897], Loss: 6.1582\n",
      "Epoch [1/1], Step [2877/8897], Loss: 6.0070\n",
      "Epoch [1/1], Step [2878/8897], Loss: 6.1779\n",
      "Epoch [1/1], Step [2879/8897], Loss: 6.0029\n",
      "Epoch [1/1], Step [2880/8897], Loss: 5.9376\n",
      "Epoch [1/1], Step [2881/8897], Loss: 6.0827\n",
      "Epoch [1/1], Step [2882/8897], Loss: 6.0367\n",
      "Epoch [1/1], Step [2883/8897], Loss: 6.1362\n",
      "Epoch [1/1], Step [2884/8897], Loss: 6.2104\n",
      "Epoch [1/1], Step [2885/8897], Loss: 5.9274\n",
      "Epoch [1/1], Step [2886/8897], Loss: 5.9212\n",
      "Epoch [1/1], Step [2887/8897], Loss: 6.4359\n",
      "Epoch [1/1], Step [2888/8897], Loss: 6.1495\n",
      "Epoch [1/1], Step [2889/8897], Loss: 5.9184\n",
      "Epoch [1/1], Step [2890/8897], Loss: 6.2145\n",
      "Epoch [1/1], Step [2891/8897], Loss: 5.9521\n",
      "Epoch [1/1], Step [2892/8897], Loss: 6.0797\n",
      "Epoch [1/1], Step [2893/8897], Loss: 5.9654\n",
      "Epoch [1/1], Step [2894/8897], Loss: 6.3402\n",
      "Epoch [1/1], Step [2895/8897], Loss: 6.0177\n",
      "Epoch [1/1], Step [2896/8897], Loss: 6.2524\n",
      "Epoch [1/1], Step [2897/8897], Loss: 6.0609\n",
      "Epoch [1/1], Step [2898/8897], Loss: 6.0255\n",
      "Epoch [1/1], Step [2899/8897], Loss: 6.0191\n",
      "Epoch [1/1], Step [2900/8897], Loss: 6.0409\n",
      "Epoch [1/1], Step [2901/8897], Loss: 6.1421\n",
      "Epoch [1/1], Step [2902/8897], Loss: 6.1637\n",
      "Epoch [1/1], Step [2903/8897], Loss: 5.9116\n",
      "Epoch [1/1], Step [2904/8897], Loss: 6.0471\n",
      "Epoch [1/1], Step [2905/8897], Loss: 6.1572\n",
      "Epoch [1/1], Step [2906/8897], Loss: 6.0784\n",
      "Epoch [1/1], Step [2907/8897], Loss: 6.3094\n",
      "Epoch [1/1], Step [2908/8897], Loss: 5.9717\n",
      "Epoch [1/1], Step [2909/8897], Loss: 6.1808\n",
      "Epoch [1/1], Step [2910/8897], Loss: 5.9387\n",
      "Epoch [1/1], Step [2911/8897], Loss: 6.1348\n",
      "Epoch [1/1], Step [2912/8897], Loss: 6.0015\n",
      "Epoch [1/1], Step [2913/8897], Loss: 6.0666\n",
      "Epoch [1/1], Step [2914/8897], Loss: 5.8752\n",
      "Epoch [1/1], Step [2915/8897], Loss: 6.2965\n",
      "Epoch [1/1], Step [2916/8897], Loss: 6.0712\n",
      "Epoch [1/1], Step [2917/8897], Loss: 5.8675\n",
      "Epoch [1/1], Step [2918/8897], Loss: 6.1229\n",
      "Epoch [1/1], Step [2919/8897], Loss: 6.3289\n",
      "Epoch [1/1], Step [2920/8897], Loss: 6.2211\n",
      "Epoch [1/1], Step [2921/8897], Loss: 6.0661\n",
      "Epoch [1/1], Step [2922/8897], Loss: 5.9127\n",
      "Epoch [1/1], Step [2923/8897], Loss: 5.8853\n",
      "Epoch [1/1], Step [2924/8897], Loss: 6.1336\n",
      "Epoch [1/1], Step [2925/8897], Loss: 5.9285\n",
      "Epoch [1/1], Step [2926/8897], Loss: 5.9269\n",
      "Epoch [1/1], Step [2927/8897], Loss: 6.0084\n",
      "Epoch [1/1], Step [2928/8897], Loss: 5.9994\n",
      "Epoch [1/1], Step [2929/8897], Loss: 6.1640\n",
      "Epoch [1/1], Step [2930/8897], Loss: 6.0912\n",
      "Epoch [1/1], Step [2931/8897], Loss: 5.7802\n",
      "Epoch [1/1], Step [2932/8897], Loss: 6.1759\n",
      "Epoch [1/1], Step [2933/8897], Loss: 6.0969\n",
      "Epoch [1/1], Step [2934/8897], Loss: 5.9424\n",
      "Epoch [1/1], Step [2935/8897], Loss: 5.9338\n",
      "Epoch [1/1], Step [2936/8897], Loss: 5.9875\n",
      "Epoch [1/1], Step [2937/8897], Loss: 5.9854\n",
      "Epoch [1/1], Step [2938/8897], Loss: 5.9345\n",
      "Epoch [1/1], Step [2939/8897], Loss: 5.9097\n",
      "Epoch [1/1], Step [2940/8897], Loss: 6.0405\n",
      "Epoch [1/1], Step [2941/8897], Loss: 6.0454\n",
      "Epoch [1/1], Step [2942/8897], Loss: 6.1550\n",
      "Epoch [1/1], Step [2943/8897], Loss: 6.0757\n",
      "Epoch [1/1], Step [2944/8897], Loss: 6.1174\n",
      "Epoch [1/1], Step [2945/8897], Loss: 6.1561\n",
      "Epoch [1/1], Step [2946/8897], Loss: 6.1549\n",
      "Epoch [1/1], Step [2947/8897], Loss: 5.9053\n",
      "Epoch [1/1], Step [2948/8897], Loss: 6.1632\n",
      "Epoch [1/1], Step [2949/8897], Loss: 6.0255\n",
      "Epoch [1/1], Step [2950/8897], Loss: 6.0275\n",
      "Epoch [1/1], Step [2951/8897], Loss: 6.1277\n",
      "Epoch [1/1], Step [2952/8897], Loss: 5.9470\n",
      "Epoch [1/1], Step [2953/8897], Loss: 5.9069\n",
      "Epoch [1/1], Step [2954/8897], Loss: 6.3056\n",
      "Epoch [1/1], Step [2955/8897], Loss: 5.9462\n",
      "Epoch [1/1], Step [2956/8897], Loss: 6.0166\n",
      "Epoch [1/1], Step [2957/8897], Loss: 5.9986\n",
      "Epoch [1/1], Step [2958/8897], Loss: 6.1495\n",
      "Epoch [1/1], Step [2959/8897], Loss: 6.0810\n",
      "Epoch [1/1], Step [2960/8897], Loss: 6.0349\n",
      "Epoch [1/1], Step [2961/8897], Loss: 5.9113\n",
      "Epoch [1/1], Step [2962/8897], Loss: 5.9529\n",
      "Epoch [1/1], Step [2963/8897], Loss: 6.1213\n",
      "Epoch [1/1], Step [2964/8897], Loss: 6.2018\n",
      "Epoch [1/1], Step [2965/8897], Loss: 5.9840\n",
      "Epoch [1/1], Step [2966/8897], Loss: 6.0044\n",
      "Epoch [1/1], Step [2967/8897], Loss: 5.9825\n",
      "Epoch [1/1], Step [2968/8897], Loss: 6.1421\n",
      "Epoch [1/1], Step [2969/8897], Loss: 5.9861\n",
      "Epoch [1/1], Step [2970/8897], Loss: 6.0174\n",
      "Epoch [1/1], Step [2971/8897], Loss: 5.9739\n",
      "Epoch [1/1], Step [2972/8897], Loss: 6.0715\n",
      "Epoch [1/1], Step [2973/8897], Loss: 6.1002\n",
      "Epoch [1/1], Step [2974/8897], Loss: 5.8949\n",
      "Epoch [1/1], Step [2975/8897], Loss: 6.0014\n",
      "Epoch [1/1], Step [2976/8897], Loss: 6.1506\n",
      "Epoch [1/1], Step [2977/8897], Loss: 6.1177\n",
      "Epoch [1/1], Step [2978/8897], Loss: 5.9032\n",
      "Epoch [1/1], Step [2979/8897], Loss: 5.9418\n",
      "Epoch [1/1], Step [2980/8897], Loss: 5.9615\n",
      "Epoch [1/1], Step [2981/8897], Loss: 5.8913\n",
      "Epoch [1/1], Step [2982/8897], Loss: 6.0661\n",
      "Epoch [1/1], Step [2983/8897], Loss: 6.0394\n",
      "Epoch [1/1], Step [2984/8897], Loss: 5.9583\n",
      "Epoch [1/1], Step [2985/8897], Loss: 5.8502\n",
      "Epoch [1/1], Step [2986/8897], Loss: 6.2309\n",
      "Epoch [1/1], Step [2987/8897], Loss: 5.9736\n",
      "Epoch [1/1], Step [2988/8897], Loss: 5.8873\n",
      "Epoch [1/1], Step [2989/8897], Loss: 5.7184\n",
      "Epoch [1/1], Step [2990/8897], Loss: 5.9531\n",
      "Epoch [1/1], Step [2991/8897], Loss: 5.9582\n",
      "Epoch [1/1], Step [2992/8897], Loss: 6.1337\n",
      "Epoch [1/1], Step [2993/8897], Loss: 5.9657\n",
      "Epoch [1/1], Step [2994/8897], Loss: 6.0037\n",
      "Epoch [1/1], Step [2995/8897], Loss: 6.0747\n",
      "Epoch [1/1], Step [2996/8897], Loss: 5.9179\n",
      "Epoch [1/1], Step [2997/8897], Loss: 6.0572\n",
      "Epoch [1/1], Step [2998/8897], Loss: 6.2366\n",
      "Epoch [1/1], Step [2999/8897], Loss: 5.9757\n",
      "Epoch [1/1], Step [3000/8897], Loss: 5.9405\n",
      "Epoch [1/1], Step [3001/8897], Loss: 6.2607\n",
      "Epoch [1/1], Step [3002/8897], Loss: 6.1801\n",
      "Epoch [1/1], Step [3003/8897], Loss: 6.0626\n",
      "Epoch [1/1], Step [3004/8897], Loss: 5.8733\n",
      "Epoch [1/1], Step [3005/8897], Loss: 5.8530\n",
      "Epoch [1/1], Step [3006/8897], Loss: 6.0593\n",
      "Epoch [1/1], Step [3007/8897], Loss: 6.1420\n",
      "Epoch [1/1], Step [3008/8897], Loss: 6.1016\n",
      "Epoch [1/1], Step [3009/8897], Loss: 6.1395\n",
      "Epoch [1/1], Step [3010/8897], Loss: 6.1358\n",
      "Epoch [1/1], Step [3011/8897], Loss: 6.0423\n",
      "Epoch [1/1], Step [3012/8897], Loss: 6.1203\n",
      "Epoch [1/1], Step [3013/8897], Loss: 5.9426\n",
      "Epoch [1/1], Step [3014/8897], Loss: 6.2919\n",
      "Epoch [1/1], Step [3015/8897], Loss: 5.7421\n",
      "Epoch [1/1], Step [3016/8897], Loss: 6.1203\n",
      "Epoch [1/1], Step [3017/8897], Loss: 6.2298\n",
      "Epoch [1/1], Step [3018/8897], Loss: 5.8987\n",
      "Epoch [1/1], Step [3019/8897], Loss: 5.9875\n",
      "Epoch [1/1], Step [3020/8897], Loss: 6.1379\n",
      "Epoch [1/1], Step [3021/8897], Loss: 6.1299\n",
      "Epoch [1/1], Step [3022/8897], Loss: 6.1482\n",
      "Epoch [1/1], Step [3023/8897], Loss: 6.0342\n",
      "Epoch [1/1], Step [3024/8897], Loss: 5.9849\n",
      "Epoch [1/1], Step [3025/8897], Loss: 6.2161\n",
      "Epoch [1/1], Step [3026/8897], Loss: 5.9218\n",
      "Epoch [1/1], Step [3027/8897], Loss: 5.9914\n",
      "Epoch [1/1], Step [3028/8897], Loss: 6.2718\n",
      "Epoch [1/1], Step [3029/8897], Loss: 6.1220\n",
      "Epoch [1/1], Step [3030/8897], Loss: 6.1356\n",
      "Epoch [1/1], Step [3031/8897], Loss: 6.0311\n",
      "Epoch [1/1], Step [3032/8897], Loss: 6.2684\n",
      "Epoch [1/1], Step [3033/8897], Loss: 5.7842\n",
      "Epoch [1/1], Step [3034/8897], Loss: 6.1363\n",
      "Epoch [1/1], Step [3035/8897], Loss: 5.8614\n",
      "Epoch [1/1], Step [3036/8897], Loss: 6.1196\n",
      "Epoch [1/1], Step [3037/8897], Loss: 5.9533\n",
      "Epoch [1/1], Step [3038/8897], Loss: 5.8442\n",
      "Epoch [1/1], Step [3039/8897], Loss: 6.0001\n",
      "Epoch [1/1], Step [3040/8897], Loss: 6.2260\n",
      "Epoch [1/1], Step [3041/8897], Loss: 6.1867\n",
      "Epoch [1/1], Step [3042/8897], Loss: 6.0323\n",
      "Epoch [1/1], Step [3043/8897], Loss: 5.9390\n",
      "Epoch [1/1], Step [3044/8897], Loss: 6.0146\n",
      "Epoch [1/1], Step [3045/8897], Loss: 6.0446\n",
      "Epoch [1/1], Step [3046/8897], Loss: 5.7650\n",
      "Epoch [1/1], Step [3047/8897], Loss: 6.0828\n",
      "Epoch [1/1], Step [3048/8897], Loss: 5.8022\n",
      "Epoch [1/1], Step [3049/8897], Loss: 5.9372\n",
      "Epoch [1/1], Step [3050/8897], Loss: 6.1535\n",
      "Epoch [1/1], Step [3051/8897], Loss: 5.7193\n",
      "Epoch [1/1], Step [3052/8897], Loss: 5.9723\n",
      "Epoch [1/1], Step [3053/8897], Loss: 6.1142\n",
      "Epoch [1/1], Step [3054/8897], Loss: 6.1611\n",
      "Epoch [1/1], Step [3055/8897], Loss: 5.9602\n",
      "Epoch [1/1], Step [3056/8897], Loss: 5.9625\n",
      "Epoch [1/1], Step [3057/8897], Loss: 5.7688\n",
      "Epoch [1/1], Step [3058/8897], Loss: 6.1521\n",
      "Epoch [1/1], Step [3059/8897], Loss: 6.0294\n",
      "Epoch [1/1], Step [3060/8897], Loss: 6.0143\n",
      "Epoch [1/1], Step [3061/8897], Loss: 5.8918\n",
      "Epoch [1/1], Step [3062/8897], Loss: 6.1811\n",
      "Epoch [1/1], Step [3063/8897], Loss: 5.8946\n",
      "Epoch [1/1], Step [3064/8897], Loss: 6.1159\n",
      "Epoch [1/1], Step [3065/8897], Loss: 6.0024\n",
      "Epoch [1/1], Step [3066/8897], Loss: 6.1238\n",
      "Epoch [1/1], Step [3067/8897], Loss: 6.2163\n",
      "Epoch [1/1], Step [3068/8897], Loss: 6.1656\n",
      "Epoch [1/1], Step [3069/8897], Loss: 6.3035\n",
      "Epoch [1/1], Step [3070/8897], Loss: 5.9181\n",
      "Epoch [1/1], Step [3071/8897], Loss: 5.7984\n",
      "Epoch [1/1], Step [3072/8897], Loss: 6.1313\n",
      "Epoch [1/1], Step [3073/8897], Loss: 6.1201\n",
      "Epoch [1/1], Step [3074/8897], Loss: 5.8589\n",
      "Epoch [1/1], Step [3075/8897], Loss: 5.9762\n",
      "Epoch [1/1], Step [3076/8897], Loss: 5.8363\n",
      "Epoch [1/1], Step [3077/8897], Loss: 5.9824\n",
      "Epoch [1/1], Step [3078/8897], Loss: 5.9835\n",
      "Epoch [1/1], Step [3079/8897], Loss: 6.0676\n",
      "Epoch [1/1], Step [3080/8897], Loss: 6.0713\n",
      "Epoch [1/1], Step [3081/8897], Loss: 6.3462\n",
      "Epoch [1/1], Step [3082/8897], Loss: 5.9814\n",
      "Epoch [1/1], Step [3083/8897], Loss: 6.1750\n",
      "Epoch [1/1], Step [3084/8897], Loss: 6.0651\n",
      "Epoch [1/1], Step [3085/8897], Loss: 5.9906\n",
      "Epoch [1/1], Step [3086/8897], Loss: 6.0559\n",
      "Epoch [1/1], Step [3087/8897], Loss: 5.9963\n",
      "Epoch [1/1], Step [3088/8897], Loss: 6.0280\n",
      "Epoch [1/1], Step [3089/8897], Loss: 5.8748\n",
      "Epoch [1/1], Step [3090/8897], Loss: 5.9234\n",
      "Epoch [1/1], Step [3091/8897], Loss: 5.9993\n",
      "Epoch [1/1], Step [3092/8897], Loss: 5.9707\n",
      "Epoch [1/1], Step [3093/8897], Loss: 6.1992\n",
      "Epoch [1/1], Step [3094/8897], Loss: 6.1023\n",
      "Epoch [1/1], Step [3095/8897], Loss: 5.9136\n",
      "Epoch [1/1], Step [3096/8897], Loss: 6.1990\n",
      "Epoch [1/1], Step [3097/8897], Loss: 5.9338\n",
      "Epoch [1/1], Step [3098/8897], Loss: 6.0554\n",
      "Epoch [1/1], Step [3099/8897], Loss: 5.9175\n",
      "Epoch [1/1], Step [3100/8897], Loss: 6.2539\n",
      "Epoch [1/1], Step [3101/8897], Loss: 5.8968\n",
      "Epoch [1/1], Step [3102/8897], Loss: 6.0745\n",
      "Epoch [1/1], Step [3103/8897], Loss: 6.0732\n",
      "Epoch [1/1], Step [3104/8897], Loss: 6.1291\n",
      "Epoch [1/1], Step [3105/8897], Loss: 5.9597\n",
      "Epoch [1/1], Step [3106/8897], Loss: 5.9602\n",
      "Epoch [1/1], Step [3107/8897], Loss: 6.0057\n",
      "Epoch [1/1], Step [3108/8897], Loss: 5.9997\n",
      "Epoch [1/1], Step [3109/8897], Loss: 6.0512\n",
      "Epoch [1/1], Step [3110/8897], Loss: 6.1015\n",
      "Epoch [1/1], Step [3111/8897], Loss: 6.0706\n",
      "Epoch [1/1], Step [3112/8897], Loss: 5.9511\n",
      "Epoch [1/1], Step [3113/8897], Loss: 6.0433\n",
      "Epoch [1/1], Step [3114/8897], Loss: 6.0727\n",
      "Epoch [1/1], Step [3115/8897], Loss: 5.8042\n",
      "Epoch [1/1], Step [3116/8897], Loss: 6.1210\n",
      "Epoch [1/1], Step [3117/8897], Loss: 5.8071\n",
      "Epoch [1/1], Step [3118/8897], Loss: 6.0178\n",
      "Epoch [1/1], Step [3119/8897], Loss: 5.8932\n",
      "Epoch [1/1], Step [3120/8897], Loss: 6.2004\n",
      "Epoch [1/1], Step [3121/8897], Loss: 5.8841\n",
      "Epoch [1/1], Step [3122/8897], Loss: 6.1473\n",
      "Epoch [1/1], Step [3123/8897], Loss: 6.1471\n",
      "Epoch [1/1], Step [3124/8897], Loss: 5.9344\n",
      "Epoch [1/1], Step [3125/8897], Loss: 6.1714\n",
      "Epoch [1/1], Step [3126/8897], Loss: 5.7792\n",
      "Epoch [1/1], Step [3127/8897], Loss: 6.1303\n",
      "Epoch [1/1], Step [3128/8897], Loss: 5.8966\n",
      "Epoch [1/1], Step [3129/8897], Loss: 5.9436\n",
      "Epoch [1/1], Step [3130/8897], Loss: 6.1462\n",
      "Epoch [1/1], Step [3131/8897], Loss: 6.1313\n",
      "Epoch [1/1], Step [3132/8897], Loss: 6.1491\n",
      "Epoch [1/1], Step [3133/8897], Loss: 5.8950\n",
      "Epoch [1/1], Step [3134/8897], Loss: 6.2526\n",
      "Epoch [1/1], Step [3135/8897], Loss: 6.0255\n",
      "Epoch [1/1], Step [3136/8897], Loss: 6.0592\n",
      "Epoch [1/1], Step [3137/8897], Loss: 5.8365\n",
      "Epoch [1/1], Step [3138/8897], Loss: 6.2832\n",
      "Epoch [1/1], Step [3139/8897], Loss: 6.0092\n",
      "Epoch [1/1], Step [3140/8897], Loss: 5.9417\n",
      "Epoch [1/1], Step [3141/8897], Loss: 6.0684\n",
      "Epoch [1/1], Step [3142/8897], Loss: 5.6273\n",
      "Epoch [1/1], Step [3143/8897], Loss: 5.8848\n",
      "Epoch [1/1], Step [3144/8897], Loss: 5.9847\n",
      "Epoch [1/1], Step [3145/8897], Loss: 6.2016\n",
      "Epoch [1/1], Step [3146/8897], Loss: 5.6642\n",
      "Epoch [1/1], Step [3147/8897], Loss: 5.9218\n",
      "Epoch [1/1], Step [3148/8897], Loss: 5.8908\n",
      "Epoch [1/1], Step [3149/8897], Loss: 5.9347\n",
      "Epoch [1/1], Step [3150/8897], Loss: 5.9239\n",
      "Epoch [1/1], Step [3151/8897], Loss: 6.2639\n",
      "Epoch [1/1], Step [3152/8897], Loss: 5.8429\n",
      "Epoch [1/1], Step [3153/8897], Loss: 6.0428\n",
      "Epoch [1/1], Step [3154/8897], Loss: 6.1125\n",
      "Epoch [1/1], Step [3155/8897], Loss: 6.2173\n",
      "Epoch [1/1], Step [3156/8897], Loss: 5.9827\n",
      "Epoch [1/1], Step [3157/8897], Loss: 6.1363\n",
      "Epoch [1/1], Step [3158/8897], Loss: 6.0518\n",
      "Epoch [1/1], Step [3159/8897], Loss: 5.7153\n",
      "Epoch [1/1], Step [3160/8897], Loss: 6.0239\n",
      "Epoch [1/1], Step [3161/8897], Loss: 5.9551\n",
      "Epoch [1/1], Step [3162/8897], Loss: 5.9174\n",
      "Epoch [1/1], Step [3163/8897], Loss: 5.9216\n",
      "Epoch [1/1], Step [3164/8897], Loss: 6.1114\n",
      "Epoch [1/1], Step [3165/8897], Loss: 6.1328\n",
      "Epoch [1/1], Step [3166/8897], Loss: 5.9907\n",
      "Epoch [1/1], Step [3167/8897], Loss: 6.2389\n",
      "Epoch [1/1], Step [3168/8897], Loss: 6.0327\n",
      "Epoch [1/1], Step [3169/8897], Loss: 5.9106\n",
      "Epoch [1/1], Step [3170/8897], Loss: 6.0573\n",
      "Epoch [1/1], Step [3171/8897], Loss: 5.8947\n",
      "Epoch [1/1], Step [3172/8897], Loss: 5.9241\n",
      "Epoch [1/1], Step [3173/8897], Loss: 5.9887\n",
      "Epoch [1/1], Step [3174/8897], Loss: 5.9088\n",
      "Epoch [1/1], Step [3175/8897], Loss: 5.9484\n",
      "Epoch [1/1], Step [3176/8897], Loss: 5.9585\n",
      "Epoch [1/1], Step [3177/8897], Loss: 5.7217\n",
      "Epoch [1/1], Step [3178/8897], Loss: 5.8938\n",
      "Epoch [1/1], Step [3179/8897], Loss: 6.0247\n",
      "Epoch [1/1], Step [3180/8897], Loss: 6.0161\n",
      "Epoch [1/1], Step [3181/8897], Loss: 6.0260\n",
      "Epoch [1/1], Step [3182/8897], Loss: 6.1008\n",
      "Epoch [1/1], Step [3183/8897], Loss: 5.9395\n",
      "Epoch [1/1], Step [3184/8897], Loss: 6.1768\n",
      "Epoch [1/1], Step [3185/8897], Loss: 5.8531\n",
      "Epoch [1/1], Step [3186/8897], Loss: 6.0061\n",
      "Epoch [1/1], Step [3187/8897], Loss: 5.9565\n",
      "Epoch [1/1], Step [3188/8897], Loss: 5.9000\n",
      "Epoch [1/1], Step [3189/8897], Loss: 6.1125\n",
      "Epoch [1/1], Step [3190/8897], Loss: 6.0970\n",
      "Epoch [1/1], Step [3191/8897], Loss: 5.8212\n",
      "Epoch [1/1], Step [3192/8897], Loss: 6.0409\n",
      "Epoch [1/1], Step [3193/8897], Loss: 6.1322\n",
      "Epoch [1/1], Step [3194/8897], Loss: 6.0679\n",
      "Epoch [1/1], Step [3195/8897], Loss: 6.1209\n",
      "Epoch [1/1], Step [3196/8897], Loss: 5.9972\n",
      "Epoch [1/1], Step [3197/8897], Loss: 6.0221\n",
      "Epoch [1/1], Step [3198/8897], Loss: 6.0221\n",
      "Epoch [1/1], Step [3199/8897], Loss: 6.0062\n",
      "Epoch [1/1], Step [3200/8897], Loss: 6.1280\n",
      "Epoch [1/1], Step [3201/8897], Loss: 5.9830\n",
      "Epoch [1/1], Step [3202/8897], Loss: 5.9964\n",
      "Epoch [1/1], Step [3203/8897], Loss: 5.9604\n",
      "Epoch [1/1], Step [3204/8897], Loss: 6.0752\n",
      "Epoch [1/1], Step [3205/8897], Loss: 6.0279\n",
      "Epoch [1/1], Step [3206/8897], Loss: 6.0015\n",
      "Epoch [1/1], Step [3207/8897], Loss: 6.2290\n",
      "Epoch [1/1], Step [3208/8897], Loss: 5.9830\n",
      "Epoch [1/1], Step [3209/8897], Loss: 5.7559\n",
      "Epoch [1/1], Step [3210/8897], Loss: 6.1382\n",
      "Epoch [1/1], Step [3211/8897], Loss: 6.0632\n",
      "Epoch [1/1], Step [3212/8897], Loss: 6.1723\n",
      "Epoch [1/1], Step [3213/8897], Loss: 6.0011\n",
      "Epoch [1/1], Step [3214/8897], Loss: 5.9081\n",
      "Epoch [1/1], Step [3215/8897], Loss: 6.0443\n",
      "Epoch [1/1], Step [3216/8897], Loss: 6.0060\n",
      "Epoch [1/1], Step [3217/8897], Loss: 6.2165\n",
      "Epoch [1/1], Step [3218/8897], Loss: 5.8921\n",
      "Epoch [1/1], Step [3219/8897], Loss: 6.0634\n",
      "Epoch [1/1], Step [3220/8897], Loss: 6.1091\n",
      "Epoch [1/1], Step [3221/8897], Loss: 5.9352\n",
      "Epoch [1/1], Step [3222/8897], Loss: 5.7892\n",
      "Epoch [1/1], Step [3223/8897], Loss: 6.0772\n",
      "Epoch [1/1], Step [3224/8897], Loss: 6.3346\n",
      "Epoch [1/1], Step [3225/8897], Loss: 5.9514\n",
      "Epoch [1/1], Step [3226/8897], Loss: 6.0109\n",
      "Epoch [1/1], Step [3227/8897], Loss: 6.0092\n",
      "Epoch [1/1], Step [3228/8897], Loss: 5.8176\n",
      "Epoch [1/1], Step [3229/8897], Loss: 6.0361\n",
      "Epoch [1/1], Step [3230/8897], Loss: 6.0520\n",
      "Epoch [1/1], Step [3231/8897], Loss: 6.1069\n",
      "Epoch [1/1], Step [3232/8897], Loss: 5.8513\n",
      "Epoch [1/1], Step [3233/8897], Loss: 6.0330\n",
      "Epoch [1/1], Step [3234/8897], Loss: 6.0871\n",
      "Epoch [1/1], Step [3235/8897], Loss: 5.8518\n",
      "Epoch [1/1], Step [3236/8897], Loss: 5.9643\n",
      "Epoch [1/1], Step [3237/8897], Loss: 5.9013\n",
      "Epoch [1/1], Step [3238/8897], Loss: 6.1085\n",
      "Epoch [1/1], Step [3239/8897], Loss: 6.1299\n",
      "Epoch [1/1], Step [3240/8897], Loss: 5.8732\n",
      "Epoch [1/1], Step [3241/8897], Loss: 6.0440\n",
      "Epoch [1/1], Step [3242/8897], Loss: 5.9920\n",
      "Epoch [1/1], Step [3243/8897], Loss: 6.0362\n",
      "Epoch [1/1], Step [3244/8897], Loss: 6.0684\n",
      "Epoch [1/1], Step [3245/8897], Loss: 5.9380\n",
      "Epoch [1/1], Step [3246/8897], Loss: 5.8733\n",
      "Epoch [1/1], Step [3247/8897], Loss: 6.0561\n",
      "Epoch [1/1], Step [3248/8897], Loss: 5.7302\n",
      "Epoch [1/1], Step [3249/8897], Loss: 5.9035\n",
      "Epoch [1/1], Step [3250/8897], Loss: 5.8674\n",
      "Epoch [1/1], Step [3251/8897], Loss: 6.2041\n",
      "Epoch [1/1], Step [3252/8897], Loss: 5.8140\n",
      "Epoch [1/1], Step [3253/8897], Loss: 5.9819\n",
      "Epoch [1/1], Step [3254/8897], Loss: 5.8413\n",
      "Epoch [1/1], Step [3255/8897], Loss: 6.1458\n",
      "Epoch [1/1], Step [3256/8897], Loss: 5.8854\n",
      "Epoch [1/1], Step [3257/8897], Loss: 6.0001\n",
      "Epoch [1/1], Step [3258/8897], Loss: 5.9580\n",
      "Epoch [1/1], Step [3259/8897], Loss: 5.9947\n",
      "Epoch [1/1], Step [3260/8897], Loss: 6.0519\n",
      "Epoch [1/1], Step [3261/8897], Loss: 5.7632\n",
      "Epoch [1/1], Step [3262/8897], Loss: 5.7468\n",
      "Epoch [1/1], Step [3263/8897], Loss: 5.9890\n",
      "Epoch [1/1], Step [3264/8897], Loss: 5.8313\n",
      "Epoch [1/1], Step [3265/8897], Loss: 6.0001\n",
      "Epoch [1/1], Step [3266/8897], Loss: 6.0519\n",
      "Epoch [1/1], Step [3267/8897], Loss: 5.9846\n",
      "Epoch [1/1], Step [3268/8897], Loss: 5.8189\n",
      "Epoch [1/1], Step [3269/8897], Loss: 6.0235\n",
      "Epoch [1/1], Step [3270/8897], Loss: 6.1054\n",
      "Epoch [1/1], Step [3271/8897], Loss: 5.9112\n",
      "Epoch [1/1], Step [3272/8897], Loss: 6.0245\n",
      "Epoch [1/1], Step [3273/8897], Loss: 6.2042\n",
      "Epoch [1/1], Step [3274/8897], Loss: 6.0146\n",
      "Epoch [1/1], Step [3275/8897], Loss: 5.8941\n",
      "Epoch [1/1], Step [3276/8897], Loss: 6.0221\n",
      "Epoch [1/1], Step [3277/8897], Loss: 6.0385\n",
      "Epoch [1/1], Step [3278/8897], Loss: 6.0879\n",
      "Epoch [1/1], Step [3279/8897], Loss: 6.1171\n",
      "Epoch [1/1], Step [3280/8897], Loss: 6.1857\n",
      "Epoch [1/1], Step [3281/8897], Loss: 5.9094\n",
      "Epoch [1/1], Step [3282/8897], Loss: 6.0385\n",
      "Epoch [1/1], Step [3283/8897], Loss: 5.8928\n",
      "Epoch [1/1], Step [3284/8897], Loss: 6.0105\n",
      "Epoch [1/1], Step [3285/8897], Loss: 6.1233\n",
      "Epoch [1/1], Step [3286/8897], Loss: 6.1446\n",
      "Epoch [1/1], Step [3287/8897], Loss: 6.0643\n",
      "Epoch [1/1], Step [3288/8897], Loss: 5.9819\n",
      "Epoch [1/1], Step [3289/8897], Loss: 6.1368\n",
      "Epoch [1/1], Step [3290/8897], Loss: 6.0968\n",
      "Epoch [1/1], Step [3291/8897], Loss: 6.0332\n",
      "Epoch [1/1], Step [3292/8897], Loss: 5.8424\n",
      "Epoch [1/1], Step [3293/8897], Loss: 5.9925\n",
      "Epoch [1/1], Step [3294/8897], Loss: 6.1248\n",
      "Epoch [1/1], Step [3295/8897], Loss: 6.1532\n",
      "Epoch [1/1], Step [3296/8897], Loss: 6.0629\n",
      "Epoch [1/1], Step [3297/8897], Loss: 6.0184\n",
      "Epoch [1/1], Step [3298/8897], Loss: 5.9832\n",
      "Epoch [1/1], Step [3299/8897], Loss: 5.9388\n",
      "Epoch [1/1], Step [3300/8897], Loss: 6.1722\n",
      "Epoch [1/1], Step [3301/8897], Loss: 5.9897\n",
      "Epoch [1/1], Step [3302/8897], Loss: 6.0253\n",
      "Epoch [1/1], Step [3303/8897], Loss: 5.9656\n",
      "Epoch [1/1], Step [3304/8897], Loss: 6.0842\n",
      "Epoch [1/1], Step [3305/8897], Loss: 5.9997\n",
      "Epoch [1/1], Step [3306/8897], Loss: 6.0489\n",
      "Epoch [1/1], Step [3307/8897], Loss: 5.9776\n",
      "Epoch [1/1], Step [3308/8897], Loss: 5.8761\n",
      "Epoch [1/1], Step [3309/8897], Loss: 6.1241\n",
      "Epoch [1/1], Step [3310/8897], Loss: 6.2365\n",
      "Epoch [1/1], Step [3311/8897], Loss: 6.0720\n",
      "Epoch [1/1], Step [3312/8897], Loss: 6.0203\n",
      "Epoch [1/1], Step [3313/8897], Loss: 5.9570\n",
      "Epoch [1/1], Step [3314/8897], Loss: 5.8422\n",
      "Epoch [1/1], Step [3315/8897], Loss: 6.1111\n",
      "Epoch [1/1], Step [3316/8897], Loss: 5.8229\n",
      "Epoch [1/1], Step [3317/8897], Loss: 6.0181\n",
      "Epoch [1/1], Step [3318/8897], Loss: 5.9721\n",
      "Epoch [1/1], Step [3319/8897], Loss: 6.1451\n",
      "Epoch [1/1], Step [3320/8897], Loss: 6.0857\n",
      "Epoch [1/1], Step [3321/8897], Loss: 5.9796\n",
      "Epoch [1/1], Step [3322/8897], Loss: 5.9560\n",
      "Epoch [1/1], Step [3323/8897], Loss: 5.9107\n",
      "Epoch [1/1], Step [3324/8897], Loss: 5.7809\n",
      "Epoch [1/1], Step [3325/8897], Loss: 5.9018\n",
      "Epoch [1/1], Step [3326/8897], Loss: 6.0086\n",
      "Epoch [1/1], Step [3327/8897], Loss: 5.8567\n",
      "Epoch [1/1], Step [3328/8897], Loss: 5.8463\n",
      "Epoch [1/1], Step [3329/8897], Loss: 5.9207\n",
      "Epoch [1/1], Step [3330/8897], Loss: 6.2346\n",
      "Epoch [1/1], Step [3331/8897], Loss: 6.0017\n",
      "Epoch [1/1], Step [3332/8897], Loss: 5.9900\n",
      "Epoch [1/1], Step [3333/8897], Loss: 5.9728\n",
      "Epoch [1/1], Step [3334/8897], Loss: 5.7111\n",
      "Epoch [1/1], Step [3335/8897], Loss: 6.0525\n",
      "Epoch [1/1], Step [3336/8897], Loss: 5.9221\n",
      "Epoch [1/1], Step [3337/8897], Loss: 5.7832\n",
      "Epoch [1/1], Step [3338/8897], Loss: 5.6551\n",
      "Epoch [1/1], Step [3339/8897], Loss: 6.1048\n",
      "Epoch [1/1], Step [3340/8897], Loss: 6.0819\n",
      "Epoch [1/1], Step [3341/8897], Loss: 6.0971\n",
      "Epoch [1/1], Step [3342/8897], Loss: 6.0056\n",
      "Epoch [1/1], Step [3343/8897], Loss: 6.0392\n",
      "Epoch [1/1], Step [3344/8897], Loss: 5.7481\n",
      "Epoch [1/1], Step [3345/8897], Loss: 5.8336\n",
      "Epoch [1/1], Step [3346/8897], Loss: 5.8948\n",
      "Epoch [1/1], Step [3347/8897], Loss: 6.0538\n",
      "Epoch [1/1], Step [3348/8897], Loss: 5.9931\n",
      "Epoch [1/1], Step [3349/8897], Loss: 5.9309\n",
      "Epoch [1/1], Step [3350/8897], Loss: 5.7953\n",
      "Epoch [1/1], Step [3351/8897], Loss: 6.1578\n",
      "Epoch [1/1], Step [3352/8897], Loss: 5.9142\n",
      "Epoch [1/1], Step [3353/8897], Loss: 6.1407\n",
      "Epoch [1/1], Step [3354/8897], Loss: 6.1207\n",
      "Epoch [1/1], Step [3355/8897], Loss: 6.1432\n",
      "Epoch [1/1], Step [3356/8897], Loss: 5.8646\n",
      "Epoch [1/1], Step [3357/8897], Loss: 6.0640\n",
      "Epoch [1/1], Step [3358/8897], Loss: 5.8633\n",
      "Epoch [1/1], Step [3359/8897], Loss: 5.9935\n",
      "Epoch [1/1], Step [3360/8897], Loss: 6.1197\n",
      "Epoch [1/1], Step [3361/8897], Loss: 6.0937\n",
      "Epoch [1/1], Step [3362/8897], Loss: 6.0167\n",
      "Epoch [1/1], Step [3363/8897], Loss: 5.8676\n",
      "Epoch [1/1], Step [3364/8897], Loss: 5.8944\n",
      "Epoch [1/1], Step [3365/8897], Loss: 6.0115\n",
      "Epoch [1/1], Step [3366/8897], Loss: 6.1320\n",
      "Epoch [1/1], Step [3367/8897], Loss: 5.8446\n",
      "Epoch [1/1], Step [3368/8897], Loss: 5.8698\n",
      "Epoch [1/1], Step [3369/8897], Loss: 6.1134\n",
      "Epoch [1/1], Step [3370/8897], Loss: 5.8446\n",
      "Epoch [1/1], Step [3371/8897], Loss: 5.9258\n",
      "Epoch [1/1], Step [3372/8897], Loss: 5.9489\n",
      "Epoch [1/1], Step [3373/8897], Loss: 5.9416\n",
      "Epoch [1/1], Step [3374/8897], Loss: 6.0749\n",
      "Epoch [1/1], Step [3375/8897], Loss: 6.1089\n",
      "Epoch [1/1], Step [3376/8897], Loss: 5.9578\n",
      "Epoch [1/1], Step [3377/8897], Loss: 5.9280\n",
      "Epoch [1/1], Step [3378/8897], Loss: 6.0780\n",
      "Epoch [1/1], Step [3379/8897], Loss: 6.0450\n",
      "Epoch [1/1], Step [3380/8897], Loss: 5.8509\n",
      "Epoch [1/1], Step [3381/8897], Loss: 6.0525\n",
      "Epoch [1/1], Step [3382/8897], Loss: 5.8332\n",
      "Epoch [1/1], Step [3383/8897], Loss: 5.9964\n",
      "Epoch [1/1], Step [3384/8897], Loss: 5.9044\n",
      "Epoch [1/1], Step [3385/8897], Loss: 5.9648\n",
      "Epoch [1/1], Step [3386/8897], Loss: 5.8312\n",
      "Epoch [1/1], Step [3387/8897], Loss: 5.6801\n",
      "Epoch [1/1], Step [3388/8897], Loss: 5.9173\n",
      "Epoch [1/1], Step [3389/8897], Loss: 5.9819\n",
      "Epoch [1/1], Step [3390/8897], Loss: 5.9518\n",
      "Epoch [1/1], Step [3391/8897], Loss: 5.8516\n",
      "Epoch [1/1], Step [3392/8897], Loss: 5.7610\n",
      "Epoch [1/1], Step [3393/8897], Loss: 6.0022\n",
      "Epoch [1/1], Step [3394/8897], Loss: 6.2138\n",
      "Epoch [1/1], Step [3395/8897], Loss: 5.8445\n",
      "Epoch [1/1], Step [3396/8897], Loss: 5.8156\n",
      "Epoch [1/1], Step [3397/8897], Loss: 6.1022\n",
      "Epoch [1/1], Step [3398/8897], Loss: 6.0984\n",
      "Epoch [1/1], Step [3399/8897], Loss: 6.1737\n",
      "Epoch [1/1], Step [3400/8897], Loss: 6.0006\n",
      "Epoch [1/1], Step [3401/8897], Loss: 6.1678\n",
      "Epoch [1/1], Step [3402/8897], Loss: 5.8794\n",
      "Epoch [1/1], Step [3403/8897], Loss: 5.8967\n",
      "Epoch [1/1], Step [3404/8897], Loss: 6.1086\n",
      "Epoch [1/1], Step [3405/8897], Loss: 5.9474\n",
      "Epoch [1/1], Step [3406/8897], Loss: 5.8990\n",
      "Epoch [1/1], Step [3407/8897], Loss: 5.9047\n",
      "Epoch [1/1], Step [3408/8897], Loss: 6.0395\n",
      "Epoch [1/1], Step [3409/8897], Loss: 6.0045\n",
      "Epoch [1/1], Step [3410/8897], Loss: 5.9678\n",
      "Epoch [1/1], Step [3411/8897], Loss: 6.0719\n",
      "Epoch [1/1], Step [3412/8897], Loss: 5.8533\n",
      "Epoch [1/1], Step [3413/8897], Loss: 6.0236\n",
      "Epoch [1/1], Step [3414/8897], Loss: 5.9907\n",
      "Epoch [1/1], Step [3415/8897], Loss: 6.0058\n",
      "Epoch [1/1], Step [3416/8897], Loss: 5.8959\n",
      "Epoch [1/1], Step [3417/8897], Loss: 5.7629\n",
      "Epoch [1/1], Step [3418/8897], Loss: 6.0058\n",
      "Epoch [1/1], Step [3419/8897], Loss: 5.9729\n",
      "Epoch [1/1], Step [3420/8897], Loss: 5.9298\n",
      "Epoch [1/1], Step [3421/8897], Loss: 5.9988\n",
      "Epoch [1/1], Step [3422/8897], Loss: 5.8795\n",
      "Epoch [1/1], Step [3423/8897], Loss: 5.8522\n",
      "Epoch [1/1], Step [3424/8897], Loss: 5.7953\n",
      "Epoch [1/1], Step [3425/8897], Loss: 6.0225\n",
      "Epoch [1/1], Step [3426/8897], Loss: 5.8031\n",
      "Epoch [1/1], Step [3427/8897], Loss: 5.9205\n",
      "Epoch [1/1], Step [3428/8897], Loss: 5.9358\n",
      "Epoch [1/1], Step [3429/8897], Loss: 6.0201\n",
      "Epoch [1/1], Step [3430/8897], Loss: 5.9722\n",
      "Epoch [1/1], Step [3431/8897], Loss: 5.9686\n",
      "Epoch [1/1], Step [3432/8897], Loss: 5.9988\n",
      "Epoch [1/1], Step [3433/8897], Loss: 6.0544\n",
      "Epoch [1/1], Step [3434/8897], Loss: 6.1886\n",
      "Epoch [1/1], Step [3435/8897], Loss: 5.7962\n",
      "Epoch [1/1], Step [3436/8897], Loss: 6.1524\n",
      "Epoch [1/1], Step [3437/8897], Loss: 5.9133\n",
      "Epoch [1/1], Step [3438/8897], Loss: 5.9856\n",
      "Epoch [1/1], Step [3439/8897], Loss: 5.9515\n",
      "Epoch [1/1], Step [3440/8897], Loss: 5.9136\n",
      "Epoch [1/1], Step [3441/8897], Loss: 5.9117\n",
      "Epoch [1/1], Step [3442/8897], Loss: 5.8471\n",
      "Epoch [1/1], Step [3443/8897], Loss: 6.0526\n",
      "Epoch [1/1], Step [3444/8897], Loss: 6.1281\n",
      "Epoch [1/1], Step [3445/8897], Loss: 6.0900\n",
      "Epoch [1/1], Step [3446/8897], Loss: 6.1332\n",
      "Epoch [1/1], Step [3447/8897], Loss: 6.0691\n",
      "Epoch [1/1], Step [3448/8897], Loss: 5.7760\n",
      "Epoch [1/1], Step [3449/8897], Loss: 6.0156\n",
      "Epoch [1/1], Step [3450/8897], Loss: 5.9980\n",
      "Epoch [1/1], Step [3451/8897], Loss: 5.8946\n",
      "Epoch [1/1], Step [3452/8897], Loss: 5.7050\n",
      "Epoch [1/1], Step [3453/8897], Loss: 6.1241\n",
      "Epoch [1/1], Step [3454/8897], Loss: 6.0021\n",
      "Epoch [1/1], Step [3455/8897], Loss: 5.9716\n",
      "Epoch [1/1], Step [3456/8897], Loss: 6.0623\n",
      "Epoch [1/1], Step [3457/8897], Loss: 6.0024\n",
      "Epoch [1/1], Step [3458/8897], Loss: 5.9789\n",
      "Epoch [1/1], Step [3459/8897], Loss: 5.7893\n",
      "Epoch [1/1], Step [3460/8897], Loss: 5.7889\n",
      "Epoch [1/1], Step [3461/8897], Loss: 5.7725\n",
      "Epoch [1/1], Step [3462/8897], Loss: 5.9655\n",
      "Epoch [1/1], Step [3463/8897], Loss: 5.9641\n",
      "Epoch [1/1], Step [3464/8897], Loss: 5.9819\n",
      "Epoch [1/1], Step [3465/8897], Loss: 5.9700\n",
      "Epoch [1/1], Step [3466/8897], Loss: 5.7504\n",
      "Epoch [1/1], Step [3467/8897], Loss: 6.0194\n",
      "Epoch [1/1], Step [3468/8897], Loss: 5.7717\n",
      "Epoch [1/1], Step [3469/8897], Loss: 5.8608\n",
      "Epoch [1/1], Step [3470/8897], Loss: 5.9164\n",
      "Epoch [1/1], Step [3471/8897], Loss: 5.8478\n",
      "Epoch [1/1], Step [3472/8897], Loss: 5.8453\n",
      "Epoch [1/1], Step [3473/8897], Loss: 5.9568\n",
      "Epoch [1/1], Step [3474/8897], Loss: 5.9066\n",
      "Epoch [1/1], Step [3475/8897], Loss: 6.0311\n",
      "Epoch [1/1], Step [3476/8897], Loss: 5.8436\n",
      "Epoch [1/1], Step [3477/8897], Loss: 5.9330\n",
      "Epoch [1/1], Step [3478/8897], Loss: 6.0081\n",
      "Epoch [1/1], Step [3479/8897], Loss: 6.0965\n",
      "Epoch [1/1], Step [3480/8897], Loss: 5.9931\n",
      "Epoch [1/1], Step [3481/8897], Loss: 6.1977\n",
      "Epoch [1/1], Step [3482/8897], Loss: 5.6589\n",
      "Epoch [1/1], Step [3483/8897], Loss: 5.6595\n",
      "Epoch [1/1], Step [3484/8897], Loss: 6.1426\n",
      "Epoch [1/1], Step [3485/8897], Loss: 6.0448\n",
      "Epoch [1/1], Step [3486/8897], Loss: 5.7839\n",
      "Epoch [1/1], Step [3487/8897], Loss: 5.8335\n",
      "Epoch [1/1], Step [3488/8897], Loss: 5.9897\n",
      "Epoch [1/1], Step [3489/8897], Loss: 6.1882\n",
      "Epoch [1/1], Step [3490/8897], Loss: 6.0405\n",
      "Epoch [1/1], Step [3491/8897], Loss: 5.9124\n",
      "Epoch [1/1], Step [3492/8897], Loss: 6.0165\n",
      "Epoch [1/1], Step [3493/8897], Loss: 6.1284\n",
      "Epoch [1/1], Step [3494/8897], Loss: 5.9417\n",
      "Epoch [1/1], Step [3495/8897], Loss: 5.9058\n",
      "Epoch [1/1], Step [3496/8897], Loss: 6.0806\n",
      "Epoch [1/1], Step [3497/8897], Loss: 5.8831\n",
      "Epoch [1/1], Step [3498/8897], Loss: 5.9863\n",
      "Epoch [1/1], Step [3499/8897], Loss: 5.9705\n",
      "Epoch [1/1], Step [3500/8897], Loss: 6.1948\n",
      "Epoch [1/1], Step [3501/8897], Loss: 5.9732\n",
      "Epoch [1/1], Step [3502/8897], Loss: 5.9426\n",
      "Epoch [1/1], Step [3503/8897], Loss: 5.9257\n",
      "Epoch [1/1], Step [3504/8897], Loss: 5.7923\n",
      "Epoch [1/1], Step [3505/8897], Loss: 6.0117\n",
      "Epoch [1/1], Step [3506/8897], Loss: 6.1416\n",
      "Epoch [1/1], Step [3507/8897], Loss: 6.1167\n",
      "Epoch [1/1], Step [3508/8897], Loss: 5.9996\n",
      "Epoch [1/1], Step [3509/8897], Loss: 5.8696\n",
      "Epoch [1/1], Step [3510/8897], Loss: 5.8289\n",
      "Epoch [1/1], Step [3511/8897], Loss: 5.8408\n",
      "Epoch [1/1], Step [3512/8897], Loss: 5.9442\n",
      "Epoch [1/1], Step [3513/8897], Loss: 6.1296\n",
      "Epoch [1/1], Step [3514/8897], Loss: 6.0941\n",
      "Epoch [1/1], Step [3515/8897], Loss: 5.9332\n",
      "Epoch [1/1], Step [3516/8897], Loss: 5.8767\n",
      "Epoch [1/1], Step [3517/8897], Loss: 5.9314\n",
      "Epoch [1/1], Step [3518/8897], Loss: 5.8858\n",
      "Epoch [1/1], Step [3519/8897], Loss: 6.0043\n",
      "Epoch [1/1], Step [3520/8897], Loss: 5.8978\n",
      "Epoch [1/1], Step [3521/8897], Loss: 6.0214\n",
      "Epoch [1/1], Step [3522/8897], Loss: 5.8816\n",
      "Epoch [1/1], Step [3523/8897], Loss: 5.9683\n",
      "Epoch [1/1], Step [3524/8897], Loss: 6.0302\n",
      "Epoch [1/1], Step [3525/8897], Loss: 6.0750\n",
      "Epoch [1/1], Step [3526/8897], Loss: 6.0973\n",
      "Epoch [1/1], Step [3527/8897], Loss: 5.8646\n",
      "Epoch [1/1], Step [3528/8897], Loss: 5.9777\n",
      "Epoch [1/1], Step [3529/8897], Loss: 5.9626\n",
      "Epoch [1/1], Step [3530/8897], Loss: 5.8175\n",
      "Epoch [1/1], Step [3531/8897], Loss: 5.9584\n",
      "Epoch [1/1], Step [3532/8897], Loss: 5.7762\n",
      "Epoch [1/1], Step [3533/8897], Loss: 5.9849\n",
      "Epoch [1/1], Step [3534/8897], Loss: 5.8716\n",
      "Epoch [1/1], Step [3535/8897], Loss: 6.0735\n",
      "Epoch [1/1], Step [3536/8897], Loss: 5.8555\n",
      "Epoch [1/1], Step [3537/8897], Loss: 5.8463\n",
      "Epoch [1/1], Step [3538/8897], Loss: 6.0976\n",
      "Epoch [1/1], Step [3539/8897], Loss: 5.7925\n",
      "Epoch [1/1], Step [3540/8897], Loss: 6.0502\n",
      "Epoch [1/1], Step [3541/8897], Loss: 6.0573\n",
      "Epoch [1/1], Step [3542/8897], Loss: 5.9818\n",
      "Epoch [1/1], Step [3543/8897], Loss: 6.0453\n",
      "Epoch [1/1], Step [3544/8897], Loss: 5.9872\n",
      "Epoch [1/1], Step [3545/8897], Loss: 5.7916\n",
      "Epoch [1/1], Step [3546/8897], Loss: 6.1604\n",
      "Epoch [1/1], Step [3547/8897], Loss: 6.2733\n",
      "Epoch [1/1], Step [3548/8897], Loss: 5.8825\n",
      "Epoch [1/1], Step [3549/8897], Loss: 5.9184\n",
      "Epoch [1/1], Step [3550/8897], Loss: 5.7348\n",
      "Epoch [1/1], Step [3551/8897], Loss: 6.0953\n",
      "Epoch [1/1], Step [3552/8897], Loss: 6.0163\n",
      "Epoch [1/1], Step [3553/8897], Loss: 6.0446\n",
      "Epoch [1/1], Step [3554/8897], Loss: 5.8484\n",
      "Epoch [1/1], Step [3555/8897], Loss: 6.1038\n",
      "Epoch [1/1], Step [3556/8897], Loss: 5.9296\n",
      "Epoch [1/1], Step [3557/8897], Loss: 5.9247\n",
      "Epoch [1/1], Step [3558/8897], Loss: 5.9506\n",
      "Epoch [1/1], Step [3559/8897], Loss: 5.8542\n",
      "Epoch [1/1], Step [3560/8897], Loss: 5.9084\n",
      "Epoch [1/1], Step [3561/8897], Loss: 5.8211\n",
      "Epoch [1/1], Step [3562/8897], Loss: 6.0197\n",
      "Epoch [1/1], Step [3563/8897], Loss: 5.6782\n",
      "Epoch [1/1], Step [3564/8897], Loss: 6.0192\n",
      "Epoch [1/1], Step [3565/8897], Loss: 5.8349\n",
      "Epoch [1/1], Step [3566/8897], Loss: 5.8832\n",
      "Epoch [1/1], Step [3567/8897], Loss: 5.7772\n",
      "Epoch [1/1], Step [3568/8897], Loss: 5.9117\n",
      "Epoch [1/1], Step [3569/8897], Loss: 6.0457\n",
      "Epoch [1/1], Step [3570/8897], Loss: 5.9055\n",
      "Epoch [1/1], Step [3571/8897], Loss: 5.8668\n",
      "Epoch [1/1], Step [3572/8897], Loss: 6.0573\n",
      "Epoch [1/1], Step [3573/8897], Loss: 5.9473\n",
      "Epoch [1/1], Step [3574/8897], Loss: 5.9028\n",
      "Epoch [1/1], Step [3575/8897], Loss: 6.0330\n",
      "Epoch [1/1], Step [3576/8897], Loss: 5.9768\n",
      "Epoch [1/1], Step [3577/8897], Loss: 5.7481\n",
      "Epoch [1/1], Step [3578/8897], Loss: 5.7096\n",
      "Epoch [1/1], Step [3579/8897], Loss: 5.8267\n",
      "Epoch [1/1], Step [3580/8897], Loss: 6.0820\n",
      "Epoch [1/1], Step [3581/8897], Loss: 6.1232\n",
      "Epoch [1/1], Step [3582/8897], Loss: 6.1302\n",
      "Epoch [1/1], Step [3583/8897], Loss: 5.8723\n",
      "Epoch [1/1], Step [3584/8897], Loss: 5.7980\n",
      "Epoch [1/1], Step [3585/8897], Loss: 5.8792\n",
      "Epoch [1/1], Step [3586/8897], Loss: 5.7501\n",
      "Epoch [1/1], Step [3587/8897], Loss: 6.0530\n",
      "Epoch [1/1], Step [3588/8897], Loss: 5.7115\n",
      "Epoch [1/1], Step [3589/8897], Loss: 5.9048\n",
      "Epoch [1/1], Step [3590/8897], Loss: 5.9270\n",
      "Epoch [1/1], Step [3591/8897], Loss: 5.9249\n",
      "Epoch [1/1], Step [3592/8897], Loss: 5.8419\n",
      "Epoch [1/1], Step [3593/8897], Loss: 6.0524\n",
      "Epoch [1/1], Step [3594/8897], Loss: 5.8961\n",
      "Epoch [1/1], Step [3595/8897], Loss: 6.0083\n",
      "Epoch [1/1], Step [3596/8897], Loss: 6.0655\n",
      "Epoch [1/1], Step [3597/8897], Loss: 5.6812\n",
      "Epoch [1/1], Step [3598/8897], Loss: 6.0914\n",
      "Epoch [1/1], Step [3599/8897], Loss: 5.8521\n",
      "Epoch [1/1], Step [3600/8897], Loss: 5.7282\n",
      "Epoch [1/1], Step [3601/8897], Loss: 5.9337\n",
      "Epoch [1/1], Step [3602/8897], Loss: 6.0727\n",
      "Epoch [1/1], Step [3603/8897], Loss: 5.7728\n",
      "Epoch [1/1], Step [3604/8897], Loss: 5.9352\n",
      "Epoch [1/1], Step [3605/8897], Loss: 6.0002\n",
      "Epoch [1/1], Step [3606/8897], Loss: 5.9672\n",
      "Epoch [1/1], Step [3607/8897], Loss: 5.8283\n",
      "Epoch [1/1], Step [3608/8897], Loss: 5.9079\n",
      "Epoch [1/1], Step [3609/8897], Loss: 5.8377\n",
      "Epoch [1/1], Step [3610/8897], Loss: 6.0201\n",
      "Epoch [1/1], Step [3611/8897], Loss: 5.8793\n",
      "Epoch [1/1], Step [3612/8897], Loss: 5.8907\n",
      "Epoch [1/1], Step [3613/8897], Loss: 5.9100\n",
      "Epoch [1/1], Step [3614/8897], Loss: 5.9342\n",
      "Epoch [1/1], Step [3615/8897], Loss: 5.7776\n",
      "Epoch [1/1], Step [3616/8897], Loss: 6.0660\n",
      "Epoch [1/1], Step [3617/8897], Loss: 5.8421\n",
      "Epoch [1/1], Step [3618/8897], Loss: 6.0585\n",
      "Epoch [1/1], Step [3619/8897], Loss: 5.8252\n",
      "Epoch [1/1], Step [3620/8897], Loss: 5.8276\n",
      "Epoch [1/1], Step [3621/8897], Loss: 5.8590\n",
      "Epoch [1/1], Step [3622/8897], Loss: 5.7317\n",
      "Epoch [1/1], Step [3623/8897], Loss: 5.9726\n",
      "Epoch [1/1], Step [3624/8897], Loss: 6.1360\n",
      "Epoch [1/1], Step [3625/8897], Loss: 6.0406\n",
      "Epoch [1/1], Step [3626/8897], Loss: 5.9252\n",
      "Epoch [1/1], Step [3627/8897], Loss: 5.9742\n",
      "Epoch [1/1], Step [3628/8897], Loss: 5.9531\n",
      "Epoch [1/1], Step [3629/8897], Loss: 5.9880\n",
      "Epoch [1/1], Step [3630/8897], Loss: 6.0046\n",
      "Epoch [1/1], Step [3631/8897], Loss: 5.9593\n",
      "Epoch [1/1], Step [3632/8897], Loss: 5.9140\n",
      "Epoch [1/1], Step [3633/8897], Loss: 5.7636\n",
      "Epoch [1/1], Step [3634/8897], Loss: 5.8295\n",
      "Epoch [1/1], Step [3635/8897], Loss: 5.9320\n",
      "Epoch [1/1], Step [3636/8897], Loss: 6.0761\n",
      "Epoch [1/1], Step [3637/8897], Loss: 5.8751\n",
      "Epoch [1/1], Step [3638/8897], Loss: 6.1269\n",
      "Epoch [1/1], Step [3639/8897], Loss: 6.1885\n",
      "Epoch [1/1], Step [3640/8897], Loss: 5.8760\n",
      "Epoch [1/1], Step [3641/8897], Loss: 5.9976\n",
      "Epoch [1/1], Step [3642/8897], Loss: 6.0943\n",
      "Epoch [1/1], Step [3643/8897], Loss: 5.9477\n",
      "Epoch [1/1], Step [3644/8897], Loss: 5.9496\n",
      "Epoch [1/1], Step [3645/8897], Loss: 6.0238\n",
      "Epoch [1/1], Step [3646/8897], Loss: 6.0212\n",
      "Epoch [1/1], Step [3647/8897], Loss: 5.8199\n",
      "Epoch [1/1], Step [3648/8897], Loss: 5.8357\n",
      "Epoch [1/1], Step [3649/8897], Loss: 5.8615\n",
      "Epoch [1/1], Step [3650/8897], Loss: 5.9876\n",
      "Epoch [1/1], Step [3651/8897], Loss: 6.0089\n",
      "Epoch [1/1], Step [3652/8897], Loss: 5.8494\n",
      "Epoch [1/1], Step [3653/8897], Loss: 6.0538\n",
      "Epoch [1/1], Step [3654/8897], Loss: 6.0581\n",
      "Epoch [1/1], Step [3655/8897], Loss: 5.6804\n",
      "Epoch [1/1], Step [3656/8897], Loss: 5.9276\n",
      "Epoch [1/1], Step [3657/8897], Loss: 5.9069\n",
      "Epoch [1/1], Step [3658/8897], Loss: 6.0807\n",
      "Epoch [1/1], Step [3659/8897], Loss: 6.1109\n",
      "Epoch [1/1], Step [3660/8897], Loss: 5.8940\n",
      "Epoch [1/1], Step [3661/8897], Loss: 5.8932\n",
      "Epoch [1/1], Step [3662/8897], Loss: 6.0487\n",
      "Epoch [1/1], Step [3663/8897], Loss: 5.8945\n",
      "Epoch [1/1], Step [3664/8897], Loss: 5.9418\n",
      "Epoch [1/1], Step [3665/8897], Loss: 6.0401\n",
      "Epoch [1/1], Step [3666/8897], Loss: 5.7577\n",
      "Epoch [1/1], Step [3667/8897], Loss: 5.9081\n",
      "Epoch [1/1], Step [3668/8897], Loss: 5.9528\n",
      "Epoch [1/1], Step [3669/8897], Loss: 5.9976\n",
      "Epoch [1/1], Step [3670/8897], Loss: 5.8745\n",
      "Epoch [1/1], Step [3671/8897], Loss: 5.9124\n",
      "Epoch [1/1], Step [3672/8897], Loss: 5.9008\n",
      "Epoch [1/1], Step [3673/8897], Loss: 5.9840\n",
      "Epoch [1/1], Step [3674/8897], Loss: 5.9599\n",
      "Epoch [1/1], Step [3675/8897], Loss: 6.0791\n",
      "Epoch [1/1], Step [3676/8897], Loss: 5.8522\n",
      "Epoch [1/1], Step [3677/8897], Loss: 5.9196\n",
      "Epoch [1/1], Step [3678/8897], Loss: 5.8585\n",
      "Epoch [1/1], Step [3679/8897], Loss: 5.8567\n",
      "Epoch [1/1], Step [3680/8897], Loss: 5.8748\n",
      "Epoch [1/1], Step [3681/8897], Loss: 5.8052\n",
      "Epoch [1/1], Step [3682/8897], Loss: 6.1417\n",
      "Epoch [1/1], Step [3683/8897], Loss: 6.0775\n",
      "Epoch [1/1], Step [3684/8897], Loss: 5.8744\n",
      "Epoch [1/1], Step [3685/8897], Loss: 5.9465\n",
      "Epoch [1/1], Step [3686/8897], Loss: 5.8609\n",
      "Epoch [1/1], Step [3687/8897], Loss: 5.7152\n",
      "Epoch [1/1], Step [3688/8897], Loss: 5.9566\n",
      "Epoch [1/1], Step [3689/8897], Loss: 5.7484\n",
      "Epoch [1/1], Step [3690/8897], Loss: 5.9269\n",
      "Epoch [1/1], Step [3691/8897], Loss: 5.8126\n",
      "Epoch [1/1], Step [3692/8897], Loss: 5.8911\n",
      "Epoch [1/1], Step [3693/8897], Loss: 5.9906\n",
      "Epoch [1/1], Step [3694/8897], Loss: 5.7992\n",
      "Epoch [1/1], Step [3695/8897], Loss: 6.0124\n",
      "Epoch [1/1], Step [3696/8897], Loss: 5.9947\n",
      "Epoch [1/1], Step [3697/8897], Loss: 5.7925\n",
      "Epoch [1/1], Step [3698/8897], Loss: 5.9185\n",
      "Epoch [1/1], Step [3699/8897], Loss: 5.9862\n",
      "Epoch [1/1], Step [3700/8897], Loss: 5.9133\n",
      "Epoch [1/1], Step [3701/8897], Loss: 5.8511\n",
      "Epoch [1/1], Step [3702/8897], Loss: 6.0661\n",
      "Epoch [1/1], Step [3703/8897], Loss: 5.9056\n",
      "Epoch [1/1], Step [3704/8897], Loss: 6.0542\n",
      "Epoch [1/1], Step [3705/8897], Loss: 5.7237\n",
      "Epoch [1/1], Step [3706/8897], Loss: 6.0971\n",
      "Epoch [1/1], Step [3707/8897], Loss: 5.7862\n",
      "Epoch [1/1], Step [3708/8897], Loss: 5.9016\n",
      "Epoch [1/1], Step [3709/8897], Loss: 5.9063\n",
      "Epoch [1/1], Step [3710/8897], Loss: 5.9556\n",
      "Epoch [1/1], Step [3711/8897], Loss: 5.8941\n",
      "Epoch [1/1], Step [3712/8897], Loss: 5.7759\n",
      "Epoch [1/1], Step [3713/8897], Loss: 5.9210\n",
      "Epoch [1/1], Step [3714/8897], Loss: 5.8087\n",
      "Epoch [1/1], Step [3715/8897], Loss: 5.9468\n",
      "Epoch [1/1], Step [3716/8897], Loss: 5.9803\n",
      "Epoch [1/1], Step [3717/8897], Loss: 5.9617\n",
      "Epoch [1/1], Step [3718/8897], Loss: 5.8786\n",
      "Epoch [1/1], Step [3719/8897], Loss: 5.8947\n",
      "Epoch [1/1], Step [3720/8897], Loss: 5.7634\n",
      "Epoch [1/1], Step [3721/8897], Loss: 5.8737\n",
      "Epoch [1/1], Step [3722/8897], Loss: 5.7984\n",
      "Epoch [1/1], Step [3723/8897], Loss: 5.8818\n",
      "Epoch [1/1], Step [3724/8897], Loss: 5.8209\n",
      "Epoch [1/1], Step [3725/8897], Loss: 5.7948\n",
      "Epoch [1/1], Step [3726/8897], Loss: 5.9398\n",
      "Epoch [1/1], Step [3727/8897], Loss: 6.0003\n",
      "Epoch [1/1], Step [3728/8897], Loss: 5.8631\n",
      "Epoch [1/1], Step [3729/8897], Loss: 5.9459\n",
      "Epoch [1/1], Step [3730/8897], Loss: 5.7057\n",
      "Epoch [1/1], Step [3731/8897], Loss: 5.9089\n",
      "Epoch [1/1], Step [3732/8897], Loss: 5.7821\n",
      "Epoch [1/1], Step [3733/8897], Loss: 6.0049\n",
      "Epoch [1/1], Step [3734/8897], Loss: 5.9900\n",
      "Epoch [1/1], Step [3735/8897], Loss: 5.8989\n",
      "Epoch [1/1], Step [3736/8897], Loss: 6.1236\n",
      "Epoch [1/1], Step [3737/8897], Loss: 5.9399\n",
      "Epoch [1/1], Step [3738/8897], Loss: 5.7349\n",
      "Epoch [1/1], Step [3739/8897], Loss: 5.7178\n",
      "Epoch [1/1], Step [3740/8897], Loss: 5.9661\n",
      "Epoch [1/1], Step [3741/8897], Loss: 5.9878\n",
      "Epoch [1/1], Step [3742/8897], Loss: 5.8607\n",
      "Epoch [1/1], Step [3743/8897], Loss: 6.0388\n",
      "Epoch [1/1], Step [3744/8897], Loss: 5.9589\n",
      "Epoch [1/1], Step [3745/8897], Loss: 5.9426\n",
      "Epoch [1/1], Step [3746/8897], Loss: 6.0528\n",
      "Epoch [1/1], Step [3747/8897], Loss: 5.9983\n",
      "Epoch [1/1], Step [3748/8897], Loss: 5.7673\n",
      "Epoch [1/1], Step [3749/8897], Loss: 5.9831\n",
      "Epoch [1/1], Step [3750/8897], Loss: 5.9171\n",
      "Epoch [1/1], Step [3751/8897], Loss: 5.8075\n",
      "Epoch [1/1], Step [3752/8897], Loss: 5.8127\n",
      "Epoch [1/1], Step [3753/8897], Loss: 6.0974\n",
      "Epoch [1/1], Step [3754/8897], Loss: 5.8978\n",
      "Epoch [1/1], Step [3755/8897], Loss: 5.9654\n",
      "Epoch [1/1], Step [3756/8897], Loss: 5.8950\n",
      "Epoch [1/1], Step [3757/8897], Loss: 5.9866\n",
      "Epoch [1/1], Step [3758/8897], Loss: 6.1039\n",
      "Epoch [1/1], Step [3759/8897], Loss: 5.9639\n",
      "Epoch [1/1], Step [3760/8897], Loss: 5.9467\n",
      "Epoch [1/1], Step [3761/8897], Loss: 6.1937\n",
      "Epoch [1/1], Step [3762/8897], Loss: 5.9519\n",
      "Epoch [1/1], Step [3763/8897], Loss: 5.8968\n",
      "Epoch [1/1], Step [3764/8897], Loss: 6.0511\n",
      "Epoch [1/1], Step [3765/8897], Loss: 5.8736\n",
      "Epoch [1/1], Step [3766/8897], Loss: 6.0325\n",
      "Epoch [1/1], Step [3767/8897], Loss: 5.8705\n",
      "Epoch [1/1], Step [3768/8897], Loss: 5.9645\n",
      "Epoch [1/1], Step [3769/8897], Loss: 5.9531\n",
      "Epoch [1/1], Step [3770/8897], Loss: 5.9411\n",
      "Epoch [1/1], Step [3771/8897], Loss: 6.0097\n",
      "Epoch [1/1], Step [3772/8897], Loss: 5.8795\n",
      "Epoch [1/1], Step [3773/8897], Loss: 6.0531\n",
      "Epoch [1/1], Step [3774/8897], Loss: 5.6843\n",
      "Epoch [1/1], Step [3775/8897], Loss: 5.9718\n",
      "Epoch [1/1], Step [3776/8897], Loss: 6.0382\n",
      "Epoch [1/1], Step [3777/8897], Loss: 5.7564\n",
      "Epoch [1/1], Step [3778/8897], Loss: 5.9293\n",
      "Epoch [1/1], Step [3779/8897], Loss: 6.0551\n",
      "Epoch [1/1], Step [3780/8897], Loss: 5.8650\n",
      "Epoch [1/1], Step [3781/8897], Loss: 5.7456\n",
      "Epoch [1/1], Step [3782/8897], Loss: 6.0054\n",
      "Epoch [1/1], Step [3783/8897], Loss: 5.9003\n",
      "Epoch [1/1], Step [3784/8897], Loss: 6.1111\n",
      "Epoch [1/1], Step [3785/8897], Loss: 5.9463\n",
      "Epoch [1/1], Step [3786/8897], Loss: 5.8943\n",
      "Epoch [1/1], Step [3787/8897], Loss: 6.0642\n",
      "Epoch [1/1], Step [3788/8897], Loss: 6.0423\n",
      "Epoch [1/1], Step [3789/8897], Loss: 6.0031\n",
      "Epoch [1/1], Step [3790/8897], Loss: 5.8289\n",
      "Epoch [1/1], Step [3791/8897], Loss: 5.9377\n",
      "Epoch [1/1], Step [3792/8897], Loss: 5.9049\n",
      "Epoch [1/1], Step [3793/8897], Loss: 5.9839\n",
      "Epoch [1/1], Step [3794/8897], Loss: 5.7936\n",
      "Epoch [1/1], Step [3795/8897], Loss: 6.2676\n",
      "Epoch [1/1], Step [3796/8897], Loss: 5.8027\n",
      "Epoch [1/1], Step [3797/8897], Loss: 6.0900\n",
      "Epoch [1/1], Step [3798/8897], Loss: 5.8507\n",
      "Epoch [1/1], Step [3799/8897], Loss: 5.8947\n",
      "Epoch [1/1], Step [3800/8897], Loss: 5.9266\n",
      "Epoch [1/1], Step [3801/8897], Loss: 5.8787\n",
      "Epoch [1/1], Step [3802/8897], Loss: 5.9450\n",
      "Epoch [1/1], Step [3803/8897], Loss: 5.8883\n",
      "Epoch [1/1], Step [3804/8897], Loss: 5.7197\n",
      "Epoch [1/1], Step [3805/8897], Loss: 5.9300\n",
      "Epoch [1/1], Step [3806/8897], Loss: 5.9638\n",
      "Epoch [1/1], Step [3807/8897], Loss: 5.7732\n",
      "Epoch [1/1], Step [3808/8897], Loss: 5.9090\n",
      "Epoch [1/1], Step [3809/8897], Loss: 5.8591\n",
      "Epoch [1/1], Step [3810/8897], Loss: 5.6479\n",
      "Epoch [1/1], Step [3811/8897], Loss: 5.9570\n",
      "Epoch [1/1], Step [3812/8897], Loss: 5.9546\n",
      "Epoch [1/1], Step [3813/8897], Loss: 5.9181\n",
      "Epoch [1/1], Step [3814/8897], Loss: 5.7426\n",
      "Epoch [1/1], Step [3815/8897], Loss: 5.6441\n",
      "Epoch [1/1], Step [3816/8897], Loss: 5.8990\n",
      "Epoch [1/1], Step [3817/8897], Loss: 5.9502\n",
      "Epoch [1/1], Step [3818/8897], Loss: 5.7448\n",
      "Epoch [1/1], Step [3819/8897], Loss: 6.1475\n",
      "Epoch [1/1], Step [3820/8897], Loss: 5.9572\n",
      "Epoch [1/1], Step [3821/8897], Loss: 6.0959\n",
      "Epoch [1/1], Step [3822/8897], Loss: 6.0719\n",
      "Epoch [1/1], Step [3823/8897], Loss: 5.8932\n",
      "Epoch [1/1], Step [3824/8897], Loss: 5.7332\n",
      "Epoch [1/1], Step [3825/8897], Loss: 5.8731\n",
      "Epoch [1/1], Step [3826/8897], Loss: 5.9361\n",
      "Epoch [1/1], Step [3827/8897], Loss: 6.1700\n",
      "Epoch [1/1], Step [3828/8897], Loss: 5.6095\n",
      "Epoch [1/1], Step [3829/8897], Loss: 5.9790\n",
      "Epoch [1/1], Step [3830/8897], Loss: 5.7395\n",
      "Epoch [1/1], Step [3831/8897], Loss: 5.8369\n",
      "Epoch [1/1], Step [3832/8897], Loss: 5.9967\n",
      "Epoch [1/1], Step [3833/8897], Loss: 6.0676\n",
      "Epoch [1/1], Step [3834/8897], Loss: 5.9494\n",
      "Epoch [1/1], Step [3835/8897], Loss: 5.6171\n",
      "Epoch [1/1], Step [3836/8897], Loss: 5.6816\n",
      "Epoch [1/1], Step [3837/8897], Loss: 5.9690\n",
      "Epoch [1/1], Step [3838/8897], Loss: 5.8305\n",
      "Epoch [1/1], Step [3839/8897], Loss: 5.9879\n",
      "Epoch [1/1], Step [3840/8897], Loss: 5.9378\n",
      "Epoch [1/1], Step [3841/8897], Loss: 5.6778\n",
      "Epoch [1/1], Step [3842/8897], Loss: 5.9722\n",
      "Epoch [1/1], Step [3843/8897], Loss: 5.8119\n",
      "Epoch [1/1], Step [3844/8897], Loss: 5.9607\n",
      "Epoch [1/1], Step [3845/8897], Loss: 5.7655\n",
      "Epoch [1/1], Step [3846/8897], Loss: 5.8429\n",
      "Epoch [1/1], Step [3847/8897], Loss: 6.0195\n",
      "Epoch [1/1], Step [3848/8897], Loss: 5.9410\n",
      "Epoch [1/1], Step [3849/8897], Loss: 5.8851\n",
      "Epoch [1/1], Step [3850/8897], Loss: 5.9597\n",
      "Epoch [1/1], Step [3851/8897], Loss: 6.1825\n",
      "Epoch [1/1], Step [3852/8897], Loss: 5.9111\n",
      "Epoch [1/1], Step [3853/8897], Loss: 5.8015\n",
      "Epoch [1/1], Step [3854/8897], Loss: 6.0591\n",
      "Epoch [1/1], Step [3855/8897], Loss: 5.9151\n",
      "Epoch [1/1], Step [3856/8897], Loss: 5.7799\n",
      "Epoch [1/1], Step [3857/8897], Loss: 6.1123\n",
      "Epoch [1/1], Step [3858/8897], Loss: 6.0173\n",
      "Epoch [1/1], Step [3859/8897], Loss: 5.9571\n",
      "Epoch [1/1], Step [3860/8897], Loss: 6.0500\n",
      "Epoch [1/1], Step [3861/8897], Loss: 5.8999\n",
      "Epoch [1/1], Step [3862/8897], Loss: 5.9420\n",
      "Epoch [1/1], Step [3863/8897], Loss: 5.8995\n",
      "Epoch [1/1], Step [3864/8897], Loss: 6.0138\n",
      "Epoch [1/1], Step [3865/8897], Loss: 5.8432\n",
      "Epoch [1/1], Step [3866/8897], Loss: 5.9079\n",
      "Epoch [1/1], Step [3867/8897], Loss: 5.9561\n",
      "Epoch [1/1], Step [3868/8897], Loss: 5.8939\n",
      "Epoch [1/1], Step [3869/8897], Loss: 5.9135\n",
      "Epoch [1/1], Step [3870/8897], Loss: 6.0572\n",
      "Epoch [1/1], Step [3871/8897], Loss: 5.9865\n",
      "Epoch [1/1], Step [3872/8897], Loss: 5.7490\n",
      "Epoch [1/1], Step [3873/8897], Loss: 5.8596\n",
      "Epoch [1/1], Step [3874/8897], Loss: 5.7118\n",
      "Epoch [1/1], Step [3875/8897], Loss: 5.9024\n",
      "Epoch [1/1], Step [3876/8897], Loss: 5.8338\n",
      "Epoch [1/1], Step [3877/8897], Loss: 6.0088\n",
      "Epoch [1/1], Step [3878/8897], Loss: 6.0351\n",
      "Epoch [1/1], Step [3879/8897], Loss: 5.7938\n",
      "Epoch [1/1], Step [3880/8897], Loss: 5.8283\n",
      "Epoch [1/1], Step [3881/8897], Loss: 6.0061\n",
      "Epoch [1/1], Step [3882/8897], Loss: 5.8492\n",
      "Epoch [1/1], Step [3883/8897], Loss: 5.7380\n",
      "Epoch [1/1], Step [3884/8897], Loss: 5.7634\n",
      "Epoch [1/1], Step [3885/8897], Loss: 5.6745\n",
      "Epoch [1/1], Step [3886/8897], Loss: 5.9329\n",
      "Epoch [1/1], Step [3887/8897], Loss: 6.0999\n",
      "Epoch [1/1], Step [3888/8897], Loss: 5.8287\n",
      "Epoch [1/1], Step [3889/8897], Loss: 5.8225\n",
      "Epoch [1/1], Step [3890/8897], Loss: 5.8469\n",
      "Epoch [1/1], Step [3891/8897], Loss: 6.0188\n",
      "Epoch [1/1], Step [3892/8897], Loss: 5.9345\n",
      "Epoch [1/1], Step [3893/8897], Loss: 6.0969\n",
      "Epoch [1/1], Step [3894/8897], Loss: 5.6825\n",
      "Epoch [1/1], Step [3895/8897], Loss: 5.7056\n",
      "Epoch [1/1], Step [3896/8897], Loss: 5.8015\n",
      "Epoch [1/1], Step [3897/8897], Loss: 5.8098\n",
      "Epoch [1/1], Step [3898/8897], Loss: 5.8946\n",
      "Epoch [1/1], Step [3899/8897], Loss: 5.8541\n",
      "Epoch [1/1], Step [3900/8897], Loss: 5.9044\n",
      "Epoch [1/1], Step [3901/8897], Loss: 5.8958\n",
      "Epoch [1/1], Step [3902/8897], Loss: 6.0092\n",
      "Epoch [1/1], Step [3903/8897], Loss: 6.2044\n",
      "Epoch [1/1], Step [3904/8897], Loss: 5.9363\n",
      "Epoch [1/1], Step [3905/8897], Loss: 5.7595\n",
      "Epoch [1/1], Step [3906/8897], Loss: 5.9578\n",
      "Epoch [1/1], Step [3907/8897], Loss: 5.9599\n",
      "Epoch [1/1], Step [3908/8897], Loss: 5.7469\n",
      "Epoch [1/1], Step [3909/8897], Loss: 5.9923\n",
      "Epoch [1/1], Step [3910/8897], Loss: 5.8868\n",
      "Epoch [1/1], Step [3911/8897], Loss: 5.9491\n",
      "Epoch [1/1], Step [3912/8897], Loss: 5.8383\n",
      "Epoch [1/1], Step [3913/8897], Loss: 5.6173\n",
      "Epoch [1/1], Step [3914/8897], Loss: 5.7469\n",
      "Epoch [1/1], Step [3915/8897], Loss: 5.8158\n",
      "Epoch [1/1], Step [3916/8897], Loss: 5.9145\n",
      "Epoch [1/1], Step [3917/8897], Loss: 5.8815\n",
      "Epoch [1/1], Step [3918/8897], Loss: 5.6345\n",
      "Epoch [1/1], Step [3919/8897], Loss: 5.9943\n",
      "Epoch [1/1], Step [3920/8897], Loss: 5.9240\n",
      "Epoch [1/1], Step [3921/8897], Loss: 5.9108\n",
      "Epoch [1/1], Step [3922/8897], Loss: 6.0219\n",
      "Epoch [1/1], Step [3923/8897], Loss: 5.7157\n",
      "Epoch [1/1], Step [3924/8897], Loss: 5.7595\n",
      "Epoch [1/1], Step [3925/8897], Loss: 5.8246\n",
      "Epoch [1/1], Step [3926/8897], Loss: 6.0603\n",
      "Epoch [1/1], Step [3927/8897], Loss: 5.7843\n",
      "Epoch [1/1], Step [3928/8897], Loss: 5.5893\n",
      "Epoch [1/1], Step [3929/8897], Loss: 5.8581\n",
      "Epoch [1/1], Step [3930/8897], Loss: 5.9122\n",
      "Epoch [1/1], Step [3931/8897], Loss: 6.1201\n",
      "Epoch [1/1], Step [3932/8897], Loss: 5.9263\n",
      "Epoch [1/1], Step [3933/8897], Loss: 5.9261\n",
      "Epoch [1/1], Step [3934/8897], Loss: 5.6224\n",
      "Epoch [1/1], Step [3935/8897], Loss: 6.0116\n",
      "Epoch [1/1], Step [3936/8897], Loss: 5.8273\n",
      "Epoch [1/1], Step [3937/8897], Loss: 6.2043\n",
      "Epoch [1/1], Step [3938/8897], Loss: 5.9955\n",
      "Epoch [1/1], Step [3939/8897], Loss: 5.7757\n",
      "Epoch [1/1], Step [3940/8897], Loss: 5.8326\n",
      "Epoch [1/1], Step [3941/8897], Loss: 5.8806\n",
      "Epoch [1/1], Step [3942/8897], Loss: 5.8696\n",
      "Epoch [1/1], Step [3943/8897], Loss: 5.8206\n",
      "Epoch [1/1], Step [3944/8897], Loss: 5.9304\n",
      "Epoch [1/1], Step [3945/8897], Loss: 5.8518\n",
      "Epoch [1/1], Step [3946/8897], Loss: 6.1127\n",
      "Epoch [1/1], Step [3947/8897], Loss: 5.7905\n",
      "Epoch [1/1], Step [3948/8897], Loss: 5.7196\n",
      "Epoch [1/1], Step [3949/8897], Loss: 5.8588\n",
      "Epoch [1/1], Step [3950/8897], Loss: 6.1660\n",
      "Epoch [1/1], Step [3951/8897], Loss: 6.0914\n",
      "Epoch [1/1], Step [3952/8897], Loss: 5.9355\n",
      "Epoch [1/1], Step [3953/8897], Loss: 5.9480\n",
      "Epoch [1/1], Step [3954/8897], Loss: 5.6817\n",
      "Epoch [1/1], Step [3955/8897], Loss: 5.8260\n",
      "Epoch [1/1], Step [3956/8897], Loss: 5.8314\n",
      "Epoch [1/1], Step [3957/8897], Loss: 5.9753\n",
      "Epoch [1/1], Step [3958/8897], Loss: 5.9478\n",
      "Epoch [1/1], Step [3959/8897], Loss: 6.0201\n",
      "Epoch [1/1], Step [3960/8897], Loss: 5.9869\n",
      "Epoch [1/1], Step [3961/8897], Loss: 6.0272\n",
      "Epoch [1/1], Step [3962/8897], Loss: 5.7429\n",
      "Epoch [1/1], Step [3963/8897], Loss: 5.9269\n",
      "Epoch [1/1], Step [3964/8897], Loss: 5.8494\n",
      "Epoch [1/1], Step [3965/8897], Loss: 5.9362\n",
      "Epoch [1/1], Step [3966/8897], Loss: 5.8245\n",
      "Epoch [1/1], Step [3967/8897], Loss: 5.9898\n",
      "Epoch [1/1], Step [3968/8897], Loss: 5.9700\n",
      "Epoch [1/1], Step [3969/8897], Loss: 5.8462\n",
      "Epoch [1/1], Step [3970/8897], Loss: 5.8334\n",
      "Epoch [1/1], Step [3971/8897], Loss: 6.1426\n",
      "Epoch [1/1], Step [3972/8897], Loss: 5.8747\n",
      "Epoch [1/1], Step [3973/8897], Loss: 6.0148\n",
      "Epoch [1/1], Step [3974/8897], Loss: 5.7511\n",
      "Epoch [1/1], Step [3975/8897], Loss: 5.8958\n",
      "Epoch [1/1], Step [3976/8897], Loss: 5.9631\n",
      "Epoch [1/1], Step [3977/8897], Loss: 5.7669\n",
      "Epoch [1/1], Step [3978/8897], Loss: 5.9450\n",
      "Epoch [1/1], Step [3979/8897], Loss: 5.9431\n",
      "Epoch [1/1], Step [3980/8897], Loss: 6.0122\n",
      "Epoch [1/1], Step [3981/8897], Loss: 5.8483\n",
      "Epoch [1/1], Step [3982/8897], Loss: 5.8279\n",
      "Epoch [1/1], Step [3983/8897], Loss: 6.1038\n",
      "Epoch [1/1], Step [3984/8897], Loss: 6.0037\n",
      "Epoch [1/1], Step [3985/8897], Loss: 5.9372\n",
      "Epoch [1/1], Step [3986/8897], Loss: 5.9791\n",
      "Epoch [1/1], Step [3987/8897], Loss: 5.9380\n",
      "Epoch [1/1], Step [3988/8897], Loss: 5.7424\n",
      "Epoch [1/1], Step [3989/8897], Loss: 5.8087\n",
      "Epoch [1/1], Step [3990/8897], Loss: 5.7992\n",
      "Epoch [1/1], Step [3991/8897], Loss: 5.9174\n",
      "Epoch [1/1], Step [3992/8897], Loss: 6.0566\n",
      "Epoch [1/1], Step [3993/8897], Loss: 5.9195\n",
      "Epoch [1/1], Step [3994/8897], Loss: 5.8780\n",
      "Epoch [1/1], Step [3995/8897], Loss: 5.8916\n",
      "Epoch [1/1], Step [3996/8897], Loss: 5.7972\n",
      "Epoch [1/1], Step [3997/8897], Loss: 5.9156\n",
      "Epoch [1/1], Step [3998/8897], Loss: 5.8521\n",
      "Epoch [1/1], Step [3999/8897], Loss: 5.7488\n",
      "Epoch [1/1], Step [4000/8897], Loss: 5.9357\n",
      "Epoch [1/1], Step [4001/8897], Loss: 5.9575\n",
      "Epoch [1/1], Step [4002/8897], Loss: 5.9613\n",
      "Epoch [1/1], Step [4003/8897], Loss: 6.0396\n",
      "Epoch [1/1], Step [4004/8897], Loss: 5.7431\n",
      "Epoch [1/1], Step [4005/8897], Loss: 5.8651\n",
      "Epoch [1/1], Step [4006/8897], Loss: 5.9520\n",
      "Epoch [1/1], Step [4007/8897], Loss: 5.8429\n",
      "Epoch [1/1], Step [4008/8897], Loss: 5.8322\n",
      "Epoch [1/1], Step [4009/8897], Loss: 5.9702\n",
      "Epoch [1/1], Step [4010/8897], Loss: 6.0035\n",
      "Epoch [1/1], Step [4011/8897], Loss: 5.7755\n",
      "Epoch [1/1], Step [4012/8897], Loss: 6.0340\n",
      "Epoch [1/1], Step [4013/8897], Loss: 5.6857\n",
      "Epoch [1/1], Step [4014/8897], Loss: 5.8237\n",
      "Epoch [1/1], Step [4015/8897], Loss: 5.8658\n",
      "Epoch [1/1], Step [4016/8897], Loss: 5.9594\n",
      "Epoch [1/1], Step [4017/8897], Loss: 5.9715\n",
      "Epoch [1/1], Step [4018/8897], Loss: 5.8551\n",
      "Epoch [1/1], Step [4019/8897], Loss: 5.9103\n",
      "Epoch [1/1], Step [4020/8897], Loss: 5.9424\n",
      "Epoch [1/1], Step [4021/8897], Loss: 5.8139\n",
      "Epoch [1/1], Step [4022/8897], Loss: 5.8347\n",
      "Epoch [1/1], Step [4023/8897], Loss: 5.8626\n",
      "Epoch [1/1], Step [4024/8897], Loss: 5.8747\n",
      "Epoch [1/1], Step [4025/8897], Loss: 5.7183\n",
      "Epoch [1/1], Step [4026/8897], Loss: 5.8335\n",
      "Epoch [1/1], Step [4027/8897], Loss: 5.8531\n",
      "Epoch [1/1], Step [4028/8897], Loss: 5.9603\n",
      "Epoch [1/1], Step [4029/8897], Loss: 6.0570\n",
      "Epoch [1/1], Step [4030/8897], Loss: 5.7075\n",
      "Epoch [1/1], Step [4031/8897], Loss: 5.9983\n",
      "Epoch [1/1], Step [4032/8897], Loss: 6.0726\n",
      "Epoch [1/1], Step [4033/8897], Loss: 5.9708\n",
      "Epoch [1/1], Step [4034/8897], Loss: 5.9733\n",
      "Epoch [1/1], Step [4035/8897], Loss: 5.8498\n",
      "Epoch [1/1], Step [4036/8897], Loss: 5.7342\n",
      "Epoch [1/1], Step [4037/8897], Loss: 5.9426\n",
      "Epoch [1/1], Step [4038/8897], Loss: 5.7795\n",
      "Epoch [1/1], Step [4039/8897], Loss: 5.8093\n",
      "Epoch [1/1], Step [4040/8897], Loss: 5.7495\n",
      "Epoch [1/1], Step [4041/8897], Loss: 5.8528\n",
      "Epoch [1/1], Step [4042/8897], Loss: 5.8162\n",
      "Epoch [1/1], Step [4043/8897], Loss: 5.9663\n",
      "Epoch [1/1], Step [4044/8897], Loss: 5.7722\n",
      "Epoch [1/1], Step [4045/8897], Loss: 5.9650\n",
      "Epoch [1/1], Step [4046/8897], Loss: 5.9538\n",
      "Epoch [1/1], Step [4047/8897], Loss: 5.9160\n",
      "Epoch [1/1], Step [4048/8897], Loss: 5.8309\n",
      "Epoch [1/1], Step [4049/8897], Loss: 5.8846\n",
      "Epoch [1/1], Step [4050/8897], Loss: 5.6703\n",
      "Epoch [1/1], Step [4051/8897], Loss: 5.8782\n",
      "Epoch [1/1], Step [4052/8897], Loss: 6.1174\n",
      "Epoch [1/1], Step [4053/8897], Loss: 6.0714\n",
      "Epoch [1/1], Step [4054/8897], Loss: 5.8559\n",
      "Epoch [1/1], Step [4055/8897], Loss: 6.0294\n",
      "Epoch [1/1], Step [4056/8897], Loss: 6.0494\n",
      "Epoch [1/1], Step [4057/8897], Loss: 5.7751\n",
      "Epoch [1/1], Step [4058/8897], Loss: 5.7216\n",
      "Epoch [1/1], Step [4059/8897], Loss: 5.8001\n",
      "Epoch [1/1], Step [4060/8897], Loss: 5.7855\n",
      "Epoch [1/1], Step [4061/8897], Loss: 6.0315\n",
      "Epoch [1/1], Step [4062/8897], Loss: 5.9253\n",
      "Epoch [1/1], Step [4063/8897], Loss: 5.6731\n",
      "Epoch [1/1], Step [4064/8897], Loss: 5.9523\n",
      "Epoch [1/1], Step [4065/8897], Loss: 5.8663\n",
      "Epoch [1/1], Step [4066/8897], Loss: 6.1294\n",
      "Epoch [1/1], Step [4067/8897], Loss: 5.8165\n",
      "Epoch [1/1], Step [4068/8897], Loss: 5.7448\n",
      "Epoch [1/1], Step [4069/8897], Loss: 5.7680\n",
      "Epoch [1/1], Step [4070/8897], Loss: 5.6995\n",
      "Epoch [1/1], Step [4071/8897], Loss: 5.9880\n",
      "Epoch [1/1], Step [4072/8897], Loss: 5.7826\n",
      "Epoch [1/1], Step [4073/8897], Loss: 6.0076\n",
      "Epoch [1/1], Step [4074/8897], Loss: 5.8494\n",
      "Epoch [1/1], Step [4075/8897], Loss: 5.9183\n",
      "Epoch [1/1], Step [4076/8897], Loss: 5.9766\n",
      "Epoch [1/1], Step [4077/8897], Loss: 5.8373\n",
      "Epoch [1/1], Step [4078/8897], Loss: 5.9272\n",
      "Epoch [1/1], Step [4079/8897], Loss: 6.0124\n",
      "Epoch [1/1], Step [4080/8897], Loss: 6.0860\n",
      "Epoch [1/1], Step [4081/8897], Loss: 5.9806\n",
      "Epoch [1/1], Step [4082/8897], Loss: 5.9603\n",
      "Epoch [1/1], Step [4083/8897], Loss: 5.8156\n",
      "Epoch [1/1], Step [4084/8897], Loss: 5.8644\n",
      "Epoch [1/1], Step [4085/8897], Loss: 5.7851\n",
      "Epoch [1/1], Step [4086/8897], Loss: 5.6858\n",
      "Epoch [1/1], Step [4087/8897], Loss: 5.7724\n",
      "Epoch [1/1], Step [4088/8897], Loss: 6.0378\n",
      "Epoch [1/1], Step [4089/8897], Loss: 5.9258\n",
      "Epoch [1/1], Step [4090/8897], Loss: 6.0906\n",
      "Epoch [1/1], Step [4091/8897], Loss: 5.9255\n",
      "Epoch [1/1], Step [4092/8897], Loss: 6.0092\n",
      "Epoch [1/1], Step [4093/8897], Loss: 5.8000\n",
      "Epoch [1/1], Step [4094/8897], Loss: 5.6937\n",
      "Epoch [1/1], Step [4095/8897], Loss: 6.0913\n",
      "Epoch [1/1], Step [4096/8897], Loss: 5.7678\n",
      "Epoch [1/1], Step [4097/8897], Loss: 5.7949\n",
      "Epoch [1/1], Step [4098/8897], Loss: 6.0506\n",
      "Epoch [1/1], Step [4099/8897], Loss: 5.9122\n",
      "Epoch [1/1], Step [4100/8897], Loss: 5.7726\n",
      "Epoch [1/1], Step [4101/8897], Loss: 5.7594\n",
      "Epoch [1/1], Step [4102/8897], Loss: 5.7585\n",
      "Epoch [1/1], Step [4103/8897], Loss: 5.8896\n",
      "Epoch [1/1], Step [4104/8897], Loss: 5.6953\n",
      "Epoch [1/1], Step [4105/8897], Loss: 5.9626\n",
      "Epoch [1/1], Step [4106/8897], Loss: 5.6982\n",
      "Epoch [1/1], Step [4107/8897], Loss: 6.0557\n",
      "Epoch [1/1], Step [4108/8897], Loss: 5.8082\n",
      "Epoch [1/1], Step [4109/8897], Loss: 5.9646\n",
      "Epoch [1/1], Step [4110/8897], Loss: 5.8527\n",
      "Epoch [1/1], Step [4111/8897], Loss: 5.8082\n",
      "Epoch [1/1], Step [4112/8897], Loss: 6.0840\n",
      "Epoch [1/1], Step [4113/8897], Loss: 5.8225\n",
      "Epoch [1/1], Step [4114/8897], Loss: 5.9291\n",
      "Epoch [1/1], Step [4115/8897], Loss: 5.8333\n",
      "Epoch [1/1], Step [4116/8897], Loss: 5.8873\n",
      "Epoch [1/1], Step [4117/8897], Loss: 5.9911\n",
      "Epoch [1/1], Step [4118/8897], Loss: 5.9389\n",
      "Epoch [1/1], Step [4119/8897], Loss: 6.0626\n",
      "Epoch [1/1], Step [4120/8897], Loss: 5.9076\n",
      "Epoch [1/1], Step [4121/8897], Loss: 5.8047\n",
      "Epoch [1/1], Step [4122/8897], Loss: 5.9553\n",
      "Epoch [1/1], Step [4123/8897], Loss: 5.9966\n",
      "Epoch [1/1], Step [4124/8897], Loss: 5.8096\n",
      "Epoch [1/1], Step [4125/8897], Loss: 5.8715\n",
      "Epoch [1/1], Step [4126/8897], Loss: 5.7238\n",
      "Epoch [1/1], Step [4127/8897], Loss: 5.8142\n",
      "Epoch [1/1], Step [4128/8897], Loss: 5.7551\n",
      "Epoch [1/1], Step [4129/8897], Loss: 5.6921\n",
      "Epoch [1/1], Step [4130/8897], Loss: 5.8218\n",
      "Epoch [1/1], Step [4131/8897], Loss: 5.8294\n",
      "Epoch [1/1], Step [4132/8897], Loss: 6.0600\n",
      "Epoch [1/1], Step [4133/8897], Loss: 5.9445\n",
      "Epoch [1/1], Step [4134/8897], Loss: 5.8899\n",
      "Epoch [1/1], Step [4135/8897], Loss: 5.8483\n",
      "Epoch [1/1], Step [4136/8897], Loss: 5.6355\n",
      "Epoch [1/1], Step [4137/8897], Loss: 5.7744\n",
      "Epoch [1/1], Step [4138/8897], Loss: 5.7993\n",
      "Epoch [1/1], Step [4139/8897], Loss: 5.8029\n",
      "Epoch [1/1], Step [4140/8897], Loss: 5.9773\n",
      "Epoch [1/1], Step [4141/8897], Loss: 5.9475\n",
      "Epoch [1/1], Step [4142/8897], Loss: 5.7935\n",
      "Epoch [1/1], Step [4143/8897], Loss: 6.0697\n",
      "Epoch [1/1], Step [4144/8897], Loss: 5.6628\n",
      "Epoch [1/1], Step [4145/8897], Loss: 5.9817\n",
      "Epoch [1/1], Step [4146/8897], Loss: 5.8341\n",
      "Epoch [1/1], Step [4147/8897], Loss: 6.0510\n",
      "Epoch [1/1], Step [4148/8897], Loss: 5.5974\n",
      "Epoch [1/1], Step [4149/8897], Loss: 5.8780\n",
      "Epoch [1/1], Step [4150/8897], Loss: 5.7933\n",
      "Epoch [1/1], Step [4151/8897], Loss: 5.9989\n",
      "Epoch [1/1], Step [4152/8897], Loss: 5.9126\n",
      "Epoch [1/1], Step [4153/8897], Loss: 5.9405\n",
      "Epoch [1/1], Step [4154/8897], Loss: 6.0659\n",
      "Epoch [1/1], Step [4155/8897], Loss: 6.0370\n",
      "Epoch [1/1], Step [4156/8897], Loss: 5.9456\n",
      "Epoch [1/1], Step [4157/8897], Loss: 5.6104\n",
      "Epoch [1/1], Step [4158/8897], Loss: 5.7760\n",
      "Epoch [1/1], Step [4159/8897], Loss: 5.6606\n",
      "Epoch [1/1], Step [4160/8897], Loss: 5.8168\n",
      "Epoch [1/1], Step [4161/8897], Loss: 5.9459\n",
      "Epoch [1/1], Step [4162/8897], Loss: 5.8740\n",
      "Epoch [1/1], Step [4163/8897], Loss: 5.8779\n",
      "Epoch [1/1], Step [4164/8897], Loss: 5.7408\n",
      "Epoch [1/1], Step [4165/8897], Loss: 5.7741\n",
      "Epoch [1/1], Step [4166/8897], Loss: 6.0379\n",
      "Epoch [1/1], Step [4167/8897], Loss: 5.8470\n",
      "Epoch [1/1], Step [4168/8897], Loss: 5.7684\n",
      "Epoch [1/1], Step [4169/8897], Loss: 5.7698\n",
      "Epoch [1/1], Step [4170/8897], Loss: 5.8452\n",
      "Epoch [1/1], Step [4171/8897], Loss: 5.7059\n",
      "Epoch [1/1], Step [4172/8897], Loss: 5.5426\n",
      "Epoch [1/1], Step [4173/8897], Loss: 5.8503\n",
      "Epoch [1/1], Step [4174/8897], Loss: 5.8487\n",
      "Epoch [1/1], Step [4175/8897], Loss: 6.0203\n",
      "Epoch [1/1], Step [4176/8897], Loss: 5.8398\n",
      "Epoch [1/1], Step [4177/8897], Loss: 5.8016\n",
      "Epoch [1/1], Step [4178/8897], Loss: 5.7965\n",
      "Epoch [1/1], Step [4179/8897], Loss: 5.7468\n",
      "Epoch [1/1], Step [4180/8897], Loss: 6.0148\n",
      "Epoch [1/1], Step [4181/8897], Loss: 5.6194\n",
      "Epoch [1/1], Step [4182/8897], Loss: 5.8067\n",
      "Epoch [1/1], Step [4183/8897], Loss: 5.8523\n",
      "Epoch [1/1], Step [4184/8897], Loss: 5.9271\n",
      "Epoch [1/1], Step [4185/8897], Loss: 5.9728\n",
      "Epoch [1/1], Step [4186/8897], Loss: 5.7582\n",
      "Epoch [1/1], Step [4187/8897], Loss: 5.9481\n",
      "Epoch [1/1], Step [4188/8897], Loss: 5.7778\n",
      "Epoch [1/1], Step [4189/8897], Loss: 5.9079\n",
      "Epoch [1/1], Step [4190/8897], Loss: 5.8564\n",
      "Epoch [1/1], Step [4191/8897], Loss: 5.6182\n",
      "Epoch [1/1], Step [4192/8897], Loss: 5.9087\n",
      "Epoch [1/1], Step [4193/8897], Loss: 5.5635\n",
      "Epoch [1/1], Step [4194/8897], Loss: 5.9619\n",
      "Epoch [1/1], Step [4195/8897], Loss: 5.6276\n",
      "Epoch [1/1], Step [4196/8897], Loss: 5.8364\n",
      "Epoch [1/1], Step [4197/8897], Loss: 6.0895\n",
      "Epoch [1/1], Step [4198/8897], Loss: 5.9513\n",
      "Epoch [1/1], Step [4199/8897], Loss: 5.9623\n",
      "Epoch [1/1], Step [4200/8897], Loss: 5.7833\n",
      "Epoch [1/1], Step [4201/8897], Loss: 6.0613\n",
      "Epoch [1/1], Step [4202/8897], Loss: 5.8868\n",
      "Epoch [1/1], Step [4203/8897], Loss: 5.8237\n",
      "Epoch [1/1], Step [4204/8897], Loss: 5.8708\n",
      "Epoch [1/1], Step [4205/8897], Loss: 5.7572\n",
      "Epoch [1/1], Step [4206/8897], Loss: 5.9054\n",
      "Epoch [1/1], Step [4207/8897], Loss: 5.7095\n",
      "Epoch [1/1], Step [4208/8897], Loss: 6.0062\n",
      "Epoch [1/1], Step [4209/8897], Loss: 6.0040\n",
      "Epoch [1/1], Step [4210/8897], Loss: 5.9319\n",
      "Epoch [1/1], Step [4211/8897], Loss: 5.8130\n",
      "Epoch [1/1], Step [4212/8897], Loss: 5.6893\n",
      "Epoch [1/1], Step [4213/8897], Loss: 5.7438\n",
      "Epoch [1/1], Step [4214/8897], Loss: 5.9745\n",
      "Epoch [1/1], Step [4215/8897], Loss: 5.8403\n",
      "Epoch [1/1], Step [4216/8897], Loss: 5.8755\n",
      "Epoch [1/1], Step [4217/8897], Loss: 5.7293\n",
      "Epoch [1/1], Step [4218/8897], Loss: 5.7063\n",
      "Epoch [1/1], Step [4219/8897], Loss: 5.7406\n",
      "Epoch [1/1], Step [4220/8897], Loss: 5.8627\n",
      "Epoch [1/1], Step [4221/8897], Loss: 5.8063\n",
      "Epoch [1/1], Step [4222/8897], Loss: 6.0495\n",
      "Epoch [1/1], Step [4223/8897], Loss: 5.8344\n",
      "Epoch [1/1], Step [4224/8897], Loss: 5.9306\n",
      "Epoch [1/1], Step [4225/8897], Loss: 5.8338\n",
      "Epoch [1/1], Step [4226/8897], Loss: 5.9778\n",
      "Epoch [1/1], Step [4227/8897], Loss: 5.9498\n",
      "Epoch [1/1], Step [4228/8897], Loss: 5.9645\n",
      "Epoch [1/1], Step [4229/8897], Loss: 6.0390\n",
      "Epoch [1/1], Step [4230/8897], Loss: 5.7319\n",
      "Epoch [1/1], Step [4231/8897], Loss: 5.8751\n",
      "Epoch [1/1], Step [4232/8897], Loss: 5.6983\n",
      "Epoch [1/1], Step [4233/8897], Loss: 5.8568\n",
      "Epoch [1/1], Step [4234/8897], Loss: 5.8280\n",
      "Epoch [1/1], Step [4235/8897], Loss: 5.8533\n",
      "Epoch [1/1], Step [4236/8897], Loss: 5.6354\n",
      "Epoch [1/1], Step [4237/8897], Loss: 5.8637\n",
      "Epoch [1/1], Step [4238/8897], Loss: 5.7052\n",
      "Epoch [1/1], Step [4239/8897], Loss: 5.8709\n",
      "Epoch [1/1], Step [4240/8897], Loss: 6.0823\n",
      "Epoch [1/1], Step [4241/8897], Loss: 5.8415\n",
      "Epoch [1/1], Step [4242/8897], Loss: 5.8593\n",
      "Epoch [1/1], Step [4243/8897], Loss: 5.6745\n",
      "Epoch [1/1], Step [4244/8897], Loss: 5.8503\n",
      "Epoch [1/1], Step [4245/8897], Loss: 5.8953\n",
      "Epoch [1/1], Step [4246/8897], Loss: 5.6590\n",
      "Epoch [1/1], Step [4247/8897], Loss: 6.0067\n",
      "Epoch [1/1], Step [4248/8897], Loss: 5.8938\n",
      "Epoch [1/1], Step [4249/8897], Loss: 5.7881\n",
      "Epoch [1/1], Step [4250/8897], Loss: 5.9329\n",
      "Epoch [1/1], Step [4251/8897], Loss: 5.8957\n",
      "Epoch [1/1], Step [4252/8897], Loss: 5.7422\n",
      "Epoch [1/1], Step [4253/8897], Loss: 5.8120\n",
      "Epoch [1/1], Step [4254/8897], Loss: 5.8196\n",
      "Epoch [1/1], Step [4255/8897], Loss: 5.7937\n",
      "Epoch [1/1], Step [4256/8897], Loss: 5.7410\n",
      "Epoch [1/1], Step [4257/8897], Loss: 5.8977\n",
      "Epoch [1/1], Step [4258/8897], Loss: 5.8411\n",
      "Epoch [1/1], Step [4259/8897], Loss: 5.8960\n",
      "Epoch [1/1], Step [4260/8897], Loss: 5.7756\n",
      "Epoch [1/1], Step [4261/8897], Loss: 5.7973\n",
      "Epoch [1/1], Step [4262/8897], Loss: 5.9231\n",
      "Epoch [1/1], Step [4263/8897], Loss: 5.7017\n",
      "Epoch [1/1], Step [4264/8897], Loss: 5.9347\n",
      "Epoch [1/1], Step [4265/8897], Loss: 5.8282\n",
      "Epoch [1/1], Step [4266/8897], Loss: 5.6800\n",
      "Epoch [1/1], Step [4267/8897], Loss: 5.7824\n",
      "Epoch [1/1], Step [4268/8897], Loss: 5.9551\n",
      "Epoch [1/1], Step [4269/8897], Loss: 5.6365\n",
      "Epoch [1/1], Step [4270/8897], Loss: 5.8143\n",
      "Epoch [1/1], Step [4271/8897], Loss: 5.7600\n",
      "Epoch [1/1], Step [4272/8897], Loss: 5.8395\n",
      "Epoch [1/1], Step [4273/8897], Loss: 5.9065\n",
      "Epoch [1/1], Step [4274/8897], Loss: 5.6793\n",
      "Epoch [1/1], Step [4275/8897], Loss: 5.6850\n",
      "Epoch [1/1], Step [4276/8897], Loss: 5.7849\n",
      "Epoch [1/1], Step [4277/8897], Loss: 5.9319\n",
      "Epoch [1/1], Step [4278/8897], Loss: 5.8560\n",
      "Epoch [1/1], Step [4279/8897], Loss: 5.8788\n",
      "Epoch [1/1], Step [4280/8897], Loss: 5.5518\n",
      "Epoch [1/1], Step [4281/8897], Loss: 5.8130\n",
      "Epoch [1/1], Step [4282/8897], Loss: 5.7494\n",
      "Epoch [1/1], Step [4283/8897], Loss: 5.8363\n",
      "Epoch [1/1], Step [4284/8897], Loss: 5.9641\n",
      "Epoch [1/1], Step [4285/8897], Loss: 5.9270\n",
      "Epoch [1/1], Step [4286/8897], Loss: 5.8638\n",
      "Epoch [1/1], Step [4287/8897], Loss: 5.8085\n",
      "Epoch [1/1], Step [4288/8897], Loss: 6.0336\n",
      "Epoch [1/1], Step [4289/8897], Loss: 5.8011\n",
      "Epoch [1/1], Step [4290/8897], Loss: 6.0220\n",
      "Epoch [1/1], Step [4291/8897], Loss: 5.9604\n",
      "Epoch [1/1], Step [4292/8897], Loss: 5.7023\n",
      "Epoch [1/1], Step [4293/8897], Loss: 5.9119\n",
      "Epoch [1/1], Step [4294/8897], Loss: 5.6927\n",
      "Epoch [1/1], Step [4295/8897], Loss: 5.9999\n",
      "Epoch [1/1], Step [4296/8897], Loss: 5.7477\n",
      "Epoch [1/1], Step [4297/8897], Loss: 5.7188\n",
      "Epoch [1/1], Step [4298/8897], Loss: 5.8649\n",
      "Epoch [1/1], Step [4299/8897], Loss: 5.8587\n",
      "Epoch [1/1], Step [4300/8897], Loss: 5.8945\n",
      "Epoch [1/1], Step [4301/8897], Loss: 5.8794\n",
      "Epoch [1/1], Step [4302/8897], Loss: 5.6822\n",
      "Epoch [1/1], Step [4303/8897], Loss: 5.9218\n",
      "Epoch [1/1], Step [4304/8897], Loss: 5.6979\n",
      "Epoch [1/1], Step [4305/8897], Loss: 5.7964\n",
      "Epoch [1/1], Step [4306/8897], Loss: 5.5613\n",
      "Epoch [1/1], Step [4307/8897], Loss: 6.0112\n",
      "Epoch [1/1], Step [4308/8897], Loss: 5.8490\n",
      "Epoch [1/1], Step [4309/8897], Loss: 5.9374\n",
      "Epoch [1/1], Step [4310/8897], Loss: 5.6703\n",
      "Epoch [1/1], Step [4311/8897], Loss: 5.9375\n",
      "Epoch [1/1], Step [4312/8897], Loss: 5.6629\n",
      "Epoch [1/1], Step [4313/8897], Loss: 6.0373\n",
      "Epoch [1/1], Step [4314/8897], Loss: 5.8506\n",
      "Epoch [1/1], Step [4315/8897], Loss: 5.9094\n",
      "Epoch [1/1], Step [4316/8897], Loss: 5.9077\n",
      "Epoch [1/1], Step [4317/8897], Loss: 5.5363\n",
      "Epoch [1/1], Step [4318/8897], Loss: 5.7305\n",
      "Epoch [1/1], Step [4319/8897], Loss: 5.8314\n",
      "Epoch [1/1], Step [4320/8897], Loss: 6.0121\n",
      "Epoch [1/1], Step [4321/8897], Loss: 5.9241\n",
      "Epoch [1/1], Step [4322/8897], Loss: 5.9716\n",
      "Epoch [1/1], Step [4323/8897], Loss: 5.7886\n",
      "Epoch [1/1], Step [4324/8897], Loss: 5.6149\n",
      "Epoch [1/1], Step [4325/8897], Loss: 5.7502\n",
      "Epoch [1/1], Step [4326/8897], Loss: 5.8586\n",
      "Epoch [1/1], Step [4327/8897], Loss: 5.7409\n",
      "Epoch [1/1], Step [4328/8897], Loss: 6.0335\n",
      "Epoch [1/1], Step [4329/8897], Loss: 5.6849\n",
      "Epoch [1/1], Step [4330/8897], Loss: 5.8433\n",
      "Epoch [1/1], Step [4331/8897], Loss: 6.0260\n",
      "Epoch [1/1], Step [4332/8897], Loss: 5.8227\n",
      "Epoch [1/1], Step [4333/8897], Loss: 5.9751\n",
      "Epoch [1/1], Step [4334/8897], Loss: 5.6534\n",
      "Epoch [1/1], Step [4335/8897], Loss: 5.7349\n",
      "Epoch [1/1], Step [4336/8897], Loss: 5.6089\n",
      "Epoch [1/1], Step [4337/8897], Loss: 5.6050\n",
      "Epoch [1/1], Step [4338/8897], Loss: 5.6492\n",
      "Epoch [1/1], Step [4339/8897], Loss: 5.6601\n",
      "Epoch [1/1], Step [4340/8897], Loss: 5.6308\n",
      "Epoch [1/1], Step [4341/8897], Loss: 5.9609\n",
      "Epoch [1/1], Step [4342/8897], Loss: 6.0567\n",
      "Epoch [1/1], Step [4343/8897], Loss: 5.8929\n",
      "Epoch [1/1], Step [4344/8897], Loss: 5.7690\n",
      "Epoch [1/1], Step [4345/8897], Loss: 5.6472\n",
      "Epoch [1/1], Step [4346/8897], Loss: 5.6620\n",
      "Epoch [1/1], Step [4347/8897], Loss: 5.7602\n",
      "Epoch [1/1], Step [4348/8897], Loss: 5.8662\n",
      "Epoch [1/1], Step [4349/8897], Loss: 5.9456\n",
      "Epoch [1/1], Step [4350/8897], Loss: 5.7525\n",
      "Epoch [1/1], Step [4351/8897], Loss: 5.5051\n",
      "Epoch [1/1], Step [4352/8897], Loss: 5.7419\n",
      "Epoch [1/1], Step [4353/8897], Loss: 5.7022\n",
      "Epoch [1/1], Step [4354/8897], Loss: 5.7850\n",
      "Epoch [1/1], Step [4355/8897], Loss: 5.9281\n",
      "Epoch [1/1], Step [4356/8897], Loss: 5.9400\n",
      "Epoch [1/1], Step [4357/8897], Loss: 5.8745\n",
      "Epoch [1/1], Step [4358/8897], Loss: 5.5821\n",
      "Epoch [1/1], Step [4359/8897], Loss: 5.9275\n",
      "Epoch [1/1], Step [4360/8897], Loss: 5.8588\n",
      "Epoch [1/1], Step [4361/8897], Loss: 5.8601\n",
      "Epoch [1/1], Step [4362/8897], Loss: 5.7713\n",
      "Epoch [1/1], Step [4363/8897], Loss: 5.5908\n",
      "Epoch [1/1], Step [4364/8897], Loss: 5.8622\n",
      "Epoch [1/1], Step [4365/8897], Loss: 5.8287\n",
      "Epoch [1/1], Step [4366/8897], Loss: 5.9279\n",
      "Epoch [1/1], Step [4367/8897], Loss: 5.9176\n",
      "Epoch [1/1], Step [4368/8897], Loss: 6.0409\n",
      "Epoch [1/1], Step [4369/8897], Loss: 5.9305\n",
      "Epoch [1/1], Step [4370/8897], Loss: 6.1847\n",
      "Epoch [1/1], Step [4371/8897], Loss: 5.9212\n",
      "Epoch [1/1], Step [4372/8897], Loss: 5.6546\n",
      "Epoch [1/1], Step [4373/8897], Loss: 5.6355\n",
      "Epoch [1/1], Step [4374/8897], Loss: 5.9288\n",
      "Epoch [1/1], Step [4375/8897], Loss: 5.9046\n",
      "Epoch [1/1], Step [4376/8897], Loss: 5.8058\n",
      "Epoch [1/1], Step [4377/8897], Loss: 5.9169\n",
      "Epoch [1/1], Step [4378/8897], Loss: 5.6411\n",
      "Epoch [1/1], Step [4379/8897], Loss: 6.1109\n",
      "Epoch [1/1], Step [4380/8897], Loss: 5.8659\n",
      "Epoch [1/1], Step [4381/8897], Loss: 5.8285\n",
      "Epoch [1/1], Step [4382/8897], Loss: 5.9543\n",
      "Epoch [1/1], Step [4383/8897], Loss: 5.9421\n",
      "Epoch [1/1], Step [4384/8897], Loss: 5.7613\n",
      "Epoch [1/1], Step [4385/8897], Loss: 5.7047\n",
      "Epoch [1/1], Step [4386/8897], Loss: 5.9068\n",
      "Epoch [1/1], Step [4387/8897], Loss: 5.8338\n",
      "Epoch [1/1], Step [4388/8897], Loss: 5.7288\n",
      "Epoch [1/1], Step [4389/8897], Loss: 5.8170\n",
      "Epoch [1/1], Step [4390/8897], Loss: 5.9310\n",
      "Epoch [1/1], Step [4391/8897], Loss: 5.8539\n",
      "Epoch [1/1], Step [4392/8897], Loss: 5.8264\n",
      "Epoch [1/1], Step [4393/8897], Loss: 6.0055\n",
      "Epoch [1/1], Step [4394/8897], Loss: 5.7155\n",
      "Epoch [1/1], Step [4395/8897], Loss: 5.8162\n",
      "Epoch [1/1], Step [4396/8897], Loss: 5.9194\n",
      "Epoch [1/1], Step [4397/8897], Loss: 5.7272\n",
      "Epoch [1/1], Step [4398/8897], Loss: 5.9159\n",
      "Epoch [1/1], Step [4399/8897], Loss: 6.0572\n",
      "Epoch [1/1], Step [4400/8897], Loss: 5.7964\n",
      "Epoch [1/1], Step [4401/8897], Loss: 5.8537\n",
      "Epoch [1/1], Step [4402/8897], Loss: 5.6499\n",
      "Epoch [1/1], Step [4403/8897], Loss: 5.8664\n",
      "Epoch [1/1], Step [4404/8897], Loss: 5.7830\n",
      "Epoch [1/1], Step [4405/8897], Loss: 5.8420\n",
      "Epoch [1/1], Step [4406/8897], Loss: 5.7602\n",
      "Epoch [1/1], Step [4407/8897], Loss: 5.8000\n",
      "Epoch [1/1], Step [4408/8897], Loss: 5.8255\n",
      "Epoch [1/1], Step [4409/8897], Loss: 5.8706\n",
      "Epoch [1/1], Step [4410/8897], Loss: 5.9036\n",
      "Epoch [1/1], Step [4411/8897], Loss: 5.7675\n",
      "Epoch [1/1], Step [4412/8897], Loss: 5.8910\n",
      "Epoch [1/1], Step [4413/8897], Loss: 5.8528\n",
      "Epoch [1/1], Step [4414/8897], Loss: 5.7038\n",
      "Epoch [1/1], Step [4415/8897], Loss: 5.6512\n",
      "Epoch [1/1], Step [4416/8897], Loss: 5.9277\n",
      "Epoch [1/1], Step [4417/8897], Loss: 5.9505\n",
      "Epoch [1/1], Step [4418/8897], Loss: 5.8130\n",
      "Epoch [1/1], Step [4419/8897], Loss: 5.7974\n",
      "Epoch [1/1], Step [4420/8897], Loss: 5.8376\n",
      "Epoch [1/1], Step [4421/8897], Loss: 5.9668\n",
      "Epoch [1/1], Step [4422/8897], Loss: 5.9023\n",
      "Epoch [1/1], Step [4423/8897], Loss: 5.9249\n",
      "Epoch [1/1], Step [4424/8897], Loss: 5.8048\n",
      "Epoch [1/1], Step [4425/8897], Loss: 5.6979\n",
      "Epoch [1/1], Step [4426/8897], Loss: 5.7333\n",
      "Epoch [1/1], Step [4427/8897], Loss: 5.8882\n",
      "Epoch [1/1], Step [4428/8897], Loss: 5.6528\n",
      "Epoch [1/1], Step [4429/8897], Loss: 5.7502\n",
      "Epoch [1/1], Step [4430/8897], Loss: 6.1096\n",
      "Epoch [1/1], Step [4431/8897], Loss: 5.8890\n",
      "Epoch [1/1], Step [4432/8897], Loss: 5.7809\n",
      "Epoch [1/1], Step [4433/8897], Loss: 5.8148\n",
      "Epoch [1/1], Step [4434/8897], Loss: 5.5414\n",
      "Epoch [1/1], Step [4435/8897], Loss: 5.9150\n",
      "Epoch [1/1], Step [4436/8897], Loss: 5.8541\n",
      "Epoch [1/1], Step [4437/8897], Loss: 5.9049\n",
      "Epoch [1/1], Step [4438/8897], Loss: 5.5362\n",
      "Epoch [1/1], Step [4439/8897], Loss: 5.7260\n",
      "Epoch [1/1], Step [4440/8897], Loss: 6.0519\n",
      "Epoch [1/1], Step [4441/8897], Loss: 5.7883\n",
      "Epoch [1/1], Step [4442/8897], Loss: 5.5496\n",
      "Epoch [1/1], Step [4443/8897], Loss: 5.7746\n",
      "Epoch [1/1], Step [4444/8897], Loss: 5.8900\n",
      "Epoch [1/1], Step [4445/8897], Loss: 6.0821\n",
      "Epoch [1/1], Step [4446/8897], Loss: 5.7672\n",
      "Epoch [1/1], Step [4447/8897], Loss: 5.6811\n",
      "Epoch [1/1], Step [4448/8897], Loss: 5.6802\n",
      "Epoch [1/1], Step [4449/8897], Loss: 5.9073\n",
      "Epoch [1/1], Step [4450/8897], Loss: 5.8227\n",
      "Epoch [1/1], Step [4451/8897], Loss: 5.7454\n",
      "Epoch [1/1], Step [4452/8897], Loss: 6.0961\n",
      "Epoch [1/1], Step [4453/8897], Loss: 5.8305\n",
      "Epoch [1/1], Step [4454/8897], Loss: 5.8393\n",
      "Epoch [1/1], Step [4455/8897], Loss: 5.8775\n",
      "Epoch [1/1], Step [4456/8897], Loss: 5.7787\n",
      "Epoch [1/1], Step [4457/8897], Loss: 5.7011\n",
      "Epoch [1/1], Step [4458/8897], Loss: 5.8797\n",
      "Epoch [1/1], Step [4459/8897], Loss: 6.2561\n",
      "Epoch [1/1], Step [4460/8897], Loss: 5.9670\n",
      "Epoch [1/1], Step [4461/8897], Loss: 5.8558\n",
      "Epoch [1/1], Step [4462/8897], Loss: 5.8279\n",
      "Epoch [1/1], Step [4463/8897], Loss: 5.7709\n",
      "Epoch [1/1], Step [4464/8897], Loss: 5.7845\n",
      "Epoch [1/1], Step [4465/8897], Loss: 5.7625\n",
      "Epoch [1/1], Step [4466/8897], Loss: 5.7230\n",
      "Epoch [1/1], Step [4467/8897], Loss: 5.9788\n",
      "Epoch [1/1], Step [4468/8897], Loss: 5.7535\n",
      "Epoch [1/1], Step [4469/8897], Loss: 5.8404\n",
      "Epoch [1/1], Step [4470/8897], Loss: 5.8699\n",
      "Epoch [1/1], Step [4471/8897], Loss: 5.7070\n",
      "Epoch [1/1], Step [4472/8897], Loss: 6.1726\n",
      "Epoch [1/1], Step [4473/8897], Loss: 5.8521\n",
      "Epoch [1/1], Step [4474/8897], Loss: 6.0044\n",
      "Epoch [1/1], Step [4475/8897], Loss: 5.8886\n",
      "Epoch [1/1], Step [4476/8897], Loss: 5.7923\n",
      "Epoch [1/1], Step [4477/8897], Loss: 5.7536\n",
      "Epoch [1/1], Step [4478/8897], Loss: 5.8117\n",
      "Epoch [1/1], Step [4479/8897], Loss: 5.9661\n",
      "Epoch [1/1], Step [4480/8897], Loss: 5.7481\n",
      "Epoch [1/1], Step [4481/8897], Loss: 5.9315\n",
      "Epoch [1/1], Step [4482/8897], Loss: 5.8866\n",
      "Epoch [1/1], Step [4483/8897], Loss: 5.7889\n",
      "Epoch [1/1], Step [4484/8897], Loss: 6.0387\n",
      "Epoch [1/1], Step [4485/8897], Loss: 5.7435\n",
      "Epoch [1/1], Step [4486/8897], Loss: 5.9947\n",
      "Epoch [1/1], Step [4487/8897], Loss: 5.9936\n",
      "Epoch [1/1], Step [4488/8897], Loss: 5.7846\n",
      "Epoch [1/1], Step [4489/8897], Loss: 5.7471\n",
      "Epoch [1/1], Step [4490/8897], Loss: 6.1700\n",
      "Epoch [1/1], Step [4491/8897], Loss: 5.7632\n",
      "Epoch [1/1], Step [4492/8897], Loss: 5.9145\n",
      "Epoch [1/1], Step [4493/8897], Loss: 5.8277\n",
      "Epoch [1/1], Step [4494/8897], Loss: 5.6366\n",
      "Epoch [1/1], Step [4495/8897], Loss: 5.7520\n",
      "Epoch [1/1], Step [4496/8897], Loss: 5.8925\n",
      "Epoch [1/1], Step [4497/8897], Loss: 5.7986\n",
      "Epoch [1/1], Step [4498/8897], Loss: 6.0022\n",
      "Epoch [1/1], Step [4499/8897], Loss: 5.9027\n",
      "Epoch [1/1], Step [4500/8897], Loss: 5.8486\n",
      "Epoch [1/1], Step [4501/8897], Loss: 5.7419\n",
      "Epoch [1/1], Step [4502/8897], Loss: 5.6475\n",
      "Epoch [1/1], Step [4503/8897], Loss: 5.8414\n",
      "Epoch [1/1], Step [4504/8897], Loss: 5.8720\n",
      "Epoch [1/1], Step [4505/8897], Loss: 5.7166\n",
      "Epoch [1/1], Step [4506/8897], Loss: 5.5620\n",
      "Epoch [1/1], Step [4507/8897], Loss: 5.6474\n",
      "Epoch [1/1], Step [4508/8897], Loss: 5.9182\n",
      "Epoch [1/1], Step [4509/8897], Loss: 6.0023\n",
      "Epoch [1/1], Step [4510/8897], Loss: 5.9324\n",
      "Epoch [1/1], Step [4511/8897], Loss: 6.0441\n",
      "Epoch [1/1], Step [4512/8897], Loss: 5.7730\n",
      "Epoch [1/1], Step [4513/8897], Loss: 5.8766\n",
      "Epoch [1/1], Step [4514/8897], Loss: 5.7264\n",
      "Epoch [1/1], Step [4515/8897], Loss: 5.7183\n",
      "Epoch [1/1], Step [4516/8897], Loss: 5.8163\n",
      "Epoch [1/1], Step [4517/8897], Loss: 5.7511\n",
      "Epoch [1/1], Step [4518/8897], Loss: 5.8551\n",
      "Epoch [1/1], Step [4519/8897], Loss: 5.7263\n",
      "Epoch [1/1], Step [4520/8897], Loss: 5.5608\n",
      "Epoch [1/1], Step [4521/8897], Loss: 5.6975\n",
      "Epoch [1/1], Step [4522/8897], Loss: 5.8249\n",
      "Epoch [1/1], Step [4523/8897], Loss: 5.8121\n",
      "Epoch [1/1], Step [4524/8897], Loss: 5.7904\n",
      "Epoch [1/1], Step [4525/8897], Loss: 5.8919\n",
      "Epoch [1/1], Step [4526/8897], Loss: 5.7558\n",
      "Epoch [1/1], Step [4527/8897], Loss: 5.9621\n",
      "Epoch [1/1], Step [4528/8897], Loss: 5.7137\n",
      "Epoch [1/1], Step [4529/8897], Loss: 5.7957\n",
      "Epoch [1/1], Step [4530/8897], Loss: 5.8753\n",
      "Epoch [1/1], Step [4531/8897], Loss: 6.1829\n",
      "Epoch [1/1], Step [4532/8897], Loss: 5.8309\n",
      "Epoch [1/1], Step [4533/8897], Loss: 6.0399\n",
      "Epoch [1/1], Step [4534/8897], Loss: 5.8317\n",
      "Epoch [1/1], Step [4535/8897], Loss: 6.0542\n",
      "Epoch [1/1], Step [4536/8897], Loss: 6.0617\n",
      "Epoch [1/1], Step [4537/8897], Loss: 5.7501\n",
      "Epoch [1/1], Step [4538/8897], Loss: 5.7313\n",
      "Epoch [1/1], Step [4539/8897], Loss: 5.8831\n",
      "Epoch [1/1], Step [4540/8897], Loss: 5.8099\n",
      "Epoch [1/1], Step [4541/8897], Loss: 5.7386\n",
      "Epoch [1/1], Step [4542/8897], Loss: 6.1230\n",
      "Epoch [1/1], Step [4543/8897], Loss: 6.0566\n",
      "Epoch [1/1], Step [4544/8897], Loss: 5.8370\n",
      "Epoch [1/1], Step [4545/8897], Loss: 5.8732\n",
      "Epoch [1/1], Step [4546/8897], Loss: 5.8644\n",
      "Epoch [1/1], Step [4547/8897], Loss: 5.9404\n",
      "Epoch [1/1], Step [4548/8897], Loss: 5.7916\n",
      "Epoch [1/1], Step [4549/8897], Loss: 5.8229\n",
      "Epoch [1/1], Step [4550/8897], Loss: 5.9384\n",
      "Epoch [1/1], Step [4551/8897], Loss: 5.7015\n",
      "Epoch [1/1], Step [4552/8897], Loss: 5.8753\n",
      "Epoch [1/1], Step [4553/8897], Loss: 5.6359\n",
      "Epoch [1/1], Step [4554/8897], Loss: 5.7410\n",
      "Epoch [1/1], Step [4555/8897], Loss: 5.7439\n",
      "Epoch [1/1], Step [4556/8897], Loss: 6.0580\n",
      "Epoch [1/1], Step [4557/8897], Loss: 5.7531\n",
      "Epoch [1/1], Step [4558/8897], Loss: 5.7542\n",
      "Epoch [1/1], Step [4559/8897], Loss: 6.0341\n",
      "Epoch [1/1], Step [4560/8897], Loss: 5.8337\n",
      "Epoch [1/1], Step [4561/8897], Loss: 5.8621\n",
      "Epoch [1/1], Step [4562/8897], Loss: 6.2023\n",
      "Epoch [1/1], Step [4563/8897], Loss: 5.8355\n",
      "Epoch [1/1], Step [4564/8897], Loss: 5.8850\n",
      "Epoch [1/1], Step [4565/8897], Loss: 5.5444\n",
      "Epoch [1/1], Step [4566/8897], Loss: 5.7807\n",
      "Epoch [1/1], Step [4567/8897], Loss: 5.8948\n",
      "Epoch [1/1], Step [4568/8897], Loss: 5.8409\n",
      "Epoch [1/1], Step [4569/8897], Loss: 5.6484\n",
      "Epoch [1/1], Step [4570/8897], Loss: 5.6879\n",
      "Epoch [1/1], Step [4571/8897], Loss: 5.7915\n",
      "Epoch [1/1], Step [4572/8897], Loss: 5.5712\n",
      "Epoch [1/1], Step [4573/8897], Loss: 5.8400\n",
      "Epoch [1/1], Step [4574/8897], Loss: 5.7803\n",
      "Epoch [1/1], Step [4575/8897], Loss: 5.9024\n",
      "Epoch [1/1], Step [4576/8897], Loss: 5.9423\n",
      "Epoch [1/1], Step [4577/8897], Loss: 5.9371\n",
      "Epoch [1/1], Step [4578/8897], Loss: 5.9062\n",
      "Epoch [1/1], Step [4579/8897], Loss: 5.8155\n",
      "Epoch [1/1], Step [4580/8897], Loss: 5.6754\n",
      "Epoch [1/1], Step [4581/8897], Loss: 5.9228\n",
      "Epoch [1/1], Step [4582/8897], Loss: 5.8372\n",
      "Epoch [1/1], Step [4583/8897], Loss: 5.8024\n",
      "Epoch [1/1], Step [4584/8897], Loss: 5.8661\n",
      "Epoch [1/1], Step [4585/8897], Loss: 5.7045\n",
      "Epoch [1/1], Step [4586/8897], Loss: 5.7827\n",
      "Epoch [1/1], Step [4587/8897], Loss: 5.6852\n",
      "Epoch [1/1], Step [4588/8897], Loss: 5.8616\n",
      "Epoch [1/1], Step [4589/8897], Loss: 5.7508\n",
      "Epoch [1/1], Step [4590/8897], Loss: 5.5986\n",
      "Epoch [1/1], Step [4591/8897], Loss: 5.7859\n",
      "Epoch [1/1], Step [4592/8897], Loss: 5.8483\n",
      "Epoch [1/1], Step [4593/8897], Loss: 5.7991\n",
      "Epoch [1/1], Step [4594/8897], Loss: 5.6367\n",
      "Epoch [1/1], Step [4595/8897], Loss: 5.9583\n",
      "Epoch [1/1], Step [4596/8897], Loss: 5.8595\n",
      "Epoch [1/1], Step [4597/8897], Loss: 5.9625\n",
      "Epoch [1/1], Step [4598/8897], Loss: 5.9248\n",
      "Epoch [1/1], Step [4599/8897], Loss: 5.6286\n",
      "Epoch [1/1], Step [4600/8897], Loss: 5.8041\n",
      "Epoch [1/1], Step [4601/8897], Loss: 5.7560\n",
      "Epoch [1/1], Step [4602/8897], Loss: 5.6655\n",
      "Epoch [1/1], Step [4603/8897], Loss: 5.9534\n",
      "Epoch [1/1], Step [4604/8897], Loss: 5.7773\n",
      "Epoch [1/1], Step [4605/8897], Loss: 5.6739\n",
      "Epoch [1/1], Step [4606/8897], Loss: 5.7788\n",
      "Epoch [1/1], Step [4607/8897], Loss: 5.8634\n",
      "Epoch [1/1], Step [4608/8897], Loss: 5.6505\n",
      "Epoch [1/1], Step [4609/8897], Loss: 5.6234\n",
      "Epoch [1/1], Step [4610/8897], Loss: 5.9411\n",
      "Epoch [1/1], Step [4611/8897], Loss: 5.6127\n",
      "Epoch [1/1], Step [4612/8897], Loss: 5.7401\n",
      "Epoch [1/1], Step [4613/8897], Loss: 5.7711\n",
      "Epoch [1/1], Step [4614/8897], Loss: 5.7636\n",
      "Epoch [1/1], Step [4615/8897], Loss: 5.7047\n",
      "Epoch [1/1], Step [4616/8897], Loss: 5.7620\n",
      "Epoch [1/1], Step [4617/8897], Loss: 5.7308\n",
      "Epoch [1/1], Step [4618/8897], Loss: 5.8430\n",
      "Epoch [1/1], Step [4619/8897], Loss: 5.8142\n",
      "Epoch [1/1], Step [4620/8897], Loss: 5.8964\n",
      "Epoch [1/1], Step [4621/8897], Loss: 5.8376\n",
      "Epoch [1/1], Step [4622/8897], Loss: 5.7799\n",
      "Epoch [1/1], Step [4623/8897], Loss: 6.0690\n",
      "Epoch [1/1], Step [4624/8897], Loss: 5.5840\n",
      "Epoch [1/1], Step [4625/8897], Loss: 5.6111\n",
      "Epoch [1/1], Step [4626/8897], Loss: 5.8588\n",
      "Epoch [1/1], Step [4627/8897], Loss: 5.8761\n",
      "Epoch [1/1], Step [4628/8897], Loss: 5.9009\n",
      "Epoch [1/1], Step [4629/8897], Loss: 5.7482\n",
      "Epoch [1/1], Step [4630/8897], Loss: 5.7563\n",
      "Epoch [1/1], Step [4631/8897], Loss: 5.7630\n",
      "Epoch [1/1], Step [4632/8897], Loss: 5.8510\n",
      "Epoch [1/1], Step [4633/8897], Loss: 5.6979\n",
      "Epoch [1/1], Step [4634/8897], Loss: 5.9433\n",
      "Epoch [1/1], Step [4635/8897], Loss: 5.9548\n",
      "Epoch [1/1], Step [4636/8897], Loss: 6.0412\n",
      "Epoch [1/1], Step [4637/8897], Loss: 5.9579\n",
      "Epoch [1/1], Step [4638/8897], Loss: 5.9194\n",
      "Epoch [1/1], Step [4639/8897], Loss: 5.8098\n",
      "Epoch [1/1], Step [4640/8897], Loss: 5.7121\n",
      "Epoch [1/1], Step [4641/8897], Loss: 5.7624\n",
      "Epoch [1/1], Step [4642/8897], Loss: 5.8310\n",
      "Epoch [1/1], Step [4643/8897], Loss: 5.6708\n",
      "Epoch [1/1], Step [4644/8897], Loss: 5.7501\n",
      "Epoch [1/1], Step [4645/8897], Loss: 5.9192\n",
      "Epoch [1/1], Step [4646/8897], Loss: 5.9313\n",
      "Epoch [1/1], Step [4647/8897], Loss: 5.7258\n",
      "Epoch [1/1], Step [4648/8897], Loss: 5.9285\n",
      "Epoch [1/1], Step [4649/8897], Loss: 5.7861\n",
      "Epoch [1/1], Step [4650/8897], Loss: 5.8820\n",
      "Epoch [1/1], Step [4651/8897], Loss: 5.9349\n",
      "Epoch [1/1], Step [4652/8897], Loss: 5.9456\n",
      "Epoch [1/1], Step [4653/8897], Loss: 5.6628\n",
      "Epoch [1/1], Step [4654/8897], Loss: 5.7896\n",
      "Epoch [1/1], Step [4655/8897], Loss: 5.6691\n",
      "Epoch [1/1], Step [4656/8897], Loss: 5.8926\n",
      "Epoch [1/1], Step [4657/8897], Loss: 5.8435\n",
      "Epoch [1/1], Step [4658/8897], Loss: 5.8295\n",
      "Epoch [1/1], Step [4659/8897], Loss: 5.9327\n",
      "Epoch [1/1], Step [4660/8897], Loss: 5.8343\n",
      "Epoch [1/1], Step [4661/8897], Loss: 5.7339\n",
      "Epoch [1/1], Step [4662/8897], Loss: 6.1168\n",
      "Epoch [1/1], Step [4663/8897], Loss: 5.8081\n",
      "Epoch [1/1], Step [4664/8897], Loss: 5.8366\n",
      "Epoch [1/1], Step [4665/8897], Loss: 5.6355\n",
      "Epoch [1/1], Step [4666/8897], Loss: 5.7620\n",
      "Epoch [1/1], Step [4667/8897], Loss: 5.8069\n",
      "Epoch [1/1], Step [4668/8897], Loss: 5.5521\n",
      "Epoch [1/1], Step [4669/8897], Loss: 5.7611\n",
      "Epoch [1/1], Step [4670/8897], Loss: 5.7029\n",
      "Epoch [1/1], Step [4671/8897], Loss: 5.7912\n",
      "Epoch [1/1], Step [4672/8897], Loss: 5.8631\n",
      "Epoch [1/1], Step [4673/8897], Loss: 5.9646\n",
      "Epoch [1/1], Step [4674/8897], Loss: 5.6691\n",
      "Epoch [1/1], Step [4675/8897], Loss: 5.7852\n",
      "Epoch [1/1], Step [4676/8897], Loss: 5.6105\n",
      "Epoch [1/1], Step [4677/8897], Loss: 5.7403\n",
      "Epoch [1/1], Step [4678/8897], Loss: 5.8857\n",
      "Epoch [1/1], Step [4679/8897], Loss: 5.7811\n",
      "Epoch [1/1], Step [4680/8897], Loss: 5.7427\n",
      "Epoch [1/1], Step [4681/8897], Loss: 5.9014\n",
      "Epoch [1/1], Step [4682/8897], Loss: 5.6034\n",
      "Epoch [1/1], Step [4683/8897], Loss: 5.6799\n",
      "Epoch [1/1], Step [4684/8897], Loss: 5.8106\n",
      "Epoch [1/1], Step [4685/8897], Loss: 5.8855\n",
      "Epoch [1/1], Step [4686/8897], Loss: 5.7467\n",
      "Epoch [1/1], Step [4687/8897], Loss: 5.7334\n",
      "Epoch [1/1], Step [4688/8897], Loss: 5.8384\n",
      "Epoch [1/1], Step [4689/8897], Loss: 5.6939\n",
      "Epoch [1/1], Step [4690/8897], Loss: 5.7160\n",
      "Epoch [1/1], Step [4691/8897], Loss: 5.9601\n",
      "Epoch [1/1], Step [4692/8897], Loss: 5.8533\n",
      "Epoch [1/1], Step [4693/8897], Loss: 5.6307\n",
      "Epoch [1/1], Step [4694/8897], Loss: 5.8333\n",
      "Epoch [1/1], Step [4695/8897], Loss: 5.7949\n",
      "Epoch [1/1], Step [4696/8897], Loss: 5.6991\n",
      "Epoch [1/1], Step [4697/8897], Loss: 5.6259\n",
      "Epoch [1/1], Step [4698/8897], Loss: 5.6243\n",
      "Epoch [1/1], Step [4699/8897], Loss: 5.9710\n",
      "Epoch [1/1], Step [4700/8897], Loss: 5.7367\n",
      "Epoch [1/1], Step [4701/8897], Loss: 5.8912\n",
      "Epoch [1/1], Step [4702/8897], Loss: 5.9034\n",
      "Epoch [1/1], Step [4703/8897], Loss: 5.6303\n",
      "Epoch [1/1], Step [4704/8897], Loss: 5.9836\n",
      "Epoch [1/1], Step [4705/8897], Loss: 5.6427\n",
      "Epoch [1/1], Step [4706/8897], Loss: 5.7174\n",
      "Epoch [1/1], Step [4707/8897], Loss: 5.6085\n",
      "Epoch [1/1], Step [4708/8897], Loss: 5.7688\n",
      "Epoch [1/1], Step [4709/8897], Loss: 5.9411\n",
      "Epoch [1/1], Step [4710/8897], Loss: 5.8582\n",
      "Epoch [1/1], Step [4711/8897], Loss: 5.6553\n",
      "Epoch [1/1], Step [4712/8897], Loss: 5.8205\n",
      "Epoch [1/1], Step [4713/8897], Loss: 5.7418\n",
      "Epoch [1/1], Step [4714/8897], Loss: 5.8123\n",
      "Epoch [1/1], Step [4715/8897], Loss: 5.7559\n",
      "Epoch [1/1], Step [4716/8897], Loss: 5.7463\n",
      "Epoch [1/1], Step [4717/8897], Loss: 5.8626\n",
      "Epoch [1/1], Step [4718/8897], Loss: 5.8990\n",
      "Epoch [1/1], Step [4719/8897], Loss: 5.7541\n",
      "Epoch [1/1], Step [4720/8897], Loss: 5.7835\n",
      "Epoch [1/1], Step [4721/8897], Loss: 5.8483\n",
      "Epoch [1/1], Step [4722/8897], Loss: 6.0646\n",
      "Epoch [1/1], Step [4723/8897], Loss: 5.8652\n",
      "Epoch [1/1], Step [4724/8897], Loss: 5.8894\n",
      "Epoch [1/1], Step [4725/8897], Loss: 5.6259\n",
      "Epoch [1/1], Step [4726/8897], Loss: 5.9476\n",
      "Epoch [1/1], Step [4727/8897], Loss: 5.7371\n",
      "Epoch [1/1], Step [4728/8897], Loss: 5.7233\n",
      "Epoch [1/1], Step [4729/8897], Loss: 5.6436\n",
      "Epoch [1/1], Step [4730/8897], Loss: 5.7092\n",
      "Epoch [1/1], Step [4731/8897], Loss: 5.9733\n",
      "Epoch [1/1], Step [4732/8897], Loss: 5.6966\n",
      "Epoch [1/1], Step [4733/8897], Loss: 6.0026\n",
      "Epoch [1/1], Step [4734/8897], Loss: 5.6837\n",
      "Epoch [1/1], Step [4735/8897], Loss: 5.6183\n",
      "Epoch [1/1], Step [4736/8897], Loss: 5.5641\n",
      "Epoch [1/1], Step [4737/8897], Loss: 5.6898\n",
      "Epoch [1/1], Step [4738/8897], Loss: 5.9636\n",
      "Epoch [1/1], Step [4739/8897], Loss: 5.8636\n",
      "Epoch [1/1], Step [4740/8897], Loss: 5.8403\n",
      "Epoch [1/1], Step [4741/8897], Loss: 5.9413\n",
      "Epoch [1/1], Step [4742/8897], Loss: 6.1025\n",
      "Epoch [1/1], Step [4743/8897], Loss: 5.9103\n",
      "Epoch [1/1], Step [4744/8897], Loss: 5.8862\n",
      "Epoch [1/1], Step [4745/8897], Loss: 5.9714\n",
      "Epoch [1/1], Step [4746/8897], Loss: 5.7642\n",
      "Epoch [1/1], Step [4747/8897], Loss: 5.6307\n",
      "Epoch [1/1], Step [4748/8897], Loss: 5.7248\n",
      "Epoch [1/1], Step [4749/8897], Loss: 5.6797\n",
      "Epoch [1/1], Step [4750/8897], Loss: 5.9210\n",
      "Epoch [1/1], Step [4751/8897], Loss: 5.9476\n",
      "Epoch [1/1], Step [4752/8897], Loss: 5.9517\n",
      "Epoch [1/1], Step [4753/8897], Loss: 5.8790\n",
      "Epoch [1/1], Step [4754/8897], Loss: 5.6855\n",
      "Epoch [1/1], Step [4755/8897], Loss: 5.8483\n",
      "Epoch [1/1], Step [4756/8897], Loss: 5.8802\n",
      "Epoch [1/1], Step [4757/8897], Loss: 5.7504\n",
      "Epoch [1/1], Step [4758/8897], Loss: 5.7256\n",
      "Epoch [1/1], Step [4759/8897], Loss: 5.7374\n",
      "Epoch [1/1], Step [4760/8897], Loss: 5.7325\n",
      "Epoch [1/1], Step [4761/8897], Loss: 5.9338\n",
      "Epoch [1/1], Step [4762/8897], Loss: 5.6007\n",
      "Epoch [1/1], Step [4763/8897], Loss: 5.9326\n",
      "Epoch [1/1], Step [4764/8897], Loss: 5.9020\n",
      "Epoch [1/1], Step [4765/8897], Loss: 5.8351\n",
      "Epoch [1/1], Step [4766/8897], Loss: 5.4390\n",
      "Epoch [1/1], Step [4767/8897], Loss: 5.8270\n",
      "Epoch [1/1], Step [4768/8897], Loss: 5.8964\n",
      "Epoch [1/1], Step [4769/8897], Loss: 5.7666\n",
      "Epoch [1/1], Step [4770/8897], Loss: 5.8577\n",
      "Epoch [1/1], Step [4771/8897], Loss: 5.6896\n",
      "Epoch [1/1], Step [4772/8897], Loss: 5.7220\n",
      "Epoch [1/1], Step [4773/8897], Loss: 5.9695\n",
      "Epoch [1/1], Step [4774/8897], Loss: 5.8912\n",
      "Epoch [1/1], Step [4775/8897], Loss: 5.8535\n",
      "Epoch [1/1], Step [4776/8897], Loss: 5.8108\n",
      "Epoch [1/1], Step [4777/8897], Loss: 5.7418\n",
      "Epoch [1/1], Step [4778/8897], Loss: 5.6519\n",
      "Epoch [1/1], Step [4779/8897], Loss: 5.6643\n",
      "Epoch [1/1], Step [4780/8897], Loss: 5.6553\n",
      "Epoch [1/1], Step [4781/8897], Loss: 5.7466\n",
      "Epoch [1/1], Step [4782/8897], Loss: 5.7586\n",
      "Epoch [1/1], Step [4783/8897], Loss: 5.7317\n",
      "Epoch [1/1], Step [4784/8897], Loss: 5.6512\n",
      "Epoch [1/1], Step [4785/8897], Loss: 5.6954\n",
      "Epoch [1/1], Step [4786/8897], Loss: 5.8322\n",
      "Epoch [1/1], Step [4787/8897], Loss: 5.5364\n",
      "Epoch [1/1], Step [4788/8897], Loss: 5.6421\n",
      "Epoch [1/1], Step [4789/8897], Loss: 5.6437\n",
      "Epoch [1/1], Step [4790/8897], Loss: 5.9217\n",
      "Epoch [1/1], Step [4791/8897], Loss: 5.6590\n",
      "Epoch [1/1], Step [4792/8897], Loss: 5.7421\n",
      "Epoch [1/1], Step [4793/8897], Loss: 5.9127\n",
      "Epoch [1/1], Step [4794/8897], Loss: 5.9293\n",
      "Epoch [1/1], Step [4795/8897], Loss: 5.7776\n",
      "Epoch [1/1], Step [4796/8897], Loss: 5.5902\n",
      "Epoch [1/1], Step [4797/8897], Loss: 5.5901\n",
      "Epoch [1/1], Step [4798/8897], Loss: 5.9711\n",
      "Epoch [1/1], Step [4799/8897], Loss: 5.7744\n",
      "Epoch [1/1], Step [4800/8897], Loss: 5.8309\n",
      "Epoch [1/1], Step [4801/8897], Loss: 5.9239\n",
      "Epoch [1/1], Step [4802/8897], Loss: 5.7409\n",
      "Epoch [1/1], Step [4803/8897], Loss: 5.6844\n",
      "Epoch [1/1], Step [4804/8897], Loss: 5.6720\n",
      "Epoch [1/1], Step [4805/8897], Loss: 5.8214\n",
      "Epoch [1/1], Step [4806/8897], Loss: 5.8925\n",
      "Epoch [1/1], Step [4807/8897], Loss: 5.7091\n",
      "Epoch [1/1], Step [4808/8897], Loss: 5.8203\n",
      "Epoch [1/1], Step [4809/8897], Loss: 5.7240\n",
      "Epoch [1/1], Step [4810/8897], Loss: 5.7436\n",
      "Epoch [1/1], Step [4811/8897], Loss: 5.9498\n",
      "Epoch [1/1], Step [4812/8897], Loss: 5.6658\n",
      "Epoch [1/1], Step [4813/8897], Loss: 5.8264\n",
      "Epoch [1/1], Step [4814/8897], Loss: 5.6162\n",
      "Epoch [1/1], Step [4815/8897], Loss: 5.7670\n",
      "Epoch [1/1], Step [4816/8897], Loss: 5.9359\n",
      "Epoch [1/1], Step [4817/8897], Loss: 5.9880\n",
      "Epoch [1/1], Step [4818/8897], Loss: 5.6413\n",
      "Epoch [1/1], Step [4819/8897], Loss: 5.8854\n",
      "Epoch [1/1], Step [4820/8897], Loss: 5.8514\n",
      "Epoch [1/1], Step [4821/8897], Loss: 5.7626\n",
      "Epoch [1/1], Step [4822/8897], Loss: 5.6947\n",
      "Epoch [1/1], Step [4823/8897], Loss: 5.9224\n",
      "Epoch [1/1], Step [4824/8897], Loss: 5.8457\n",
      "Epoch [1/1], Step [4825/8897], Loss: 5.7657\n",
      "Epoch [1/1], Step [4826/8897], Loss: 5.9518\n",
      "Epoch [1/1], Step [4827/8897], Loss: 6.0250\n",
      "Epoch [1/1], Step [4828/8897], Loss: 5.8056\n",
      "Epoch [1/1], Step [4829/8897], Loss: 5.8582\n",
      "Epoch [1/1], Step [4830/8897], Loss: 5.6936\n",
      "Epoch [1/1], Step [4831/8897], Loss: 5.6424\n",
      "Epoch [1/1], Step [4832/8897], Loss: 5.9932\n",
      "Epoch [1/1], Step [4833/8897], Loss: 5.5196\n",
      "Epoch [1/1], Step [4834/8897], Loss: 5.5608\n",
      "Epoch [1/1], Step [4835/8897], Loss: 5.7262\n",
      "Epoch [1/1], Step [4836/8897], Loss: 5.8711\n",
      "Epoch [1/1], Step [4837/8897], Loss: 5.8085\n",
      "Epoch [1/1], Step [4838/8897], Loss: 5.8491\n",
      "Epoch [1/1], Step [4839/8897], Loss: 5.9200\n",
      "Epoch [1/1], Step [4840/8897], Loss: 5.8803\n",
      "Epoch [1/1], Step [4841/8897], Loss: 5.8119\n",
      "Epoch [1/1], Step [4842/8897], Loss: 5.8010\n",
      "Epoch [1/1], Step [4843/8897], Loss: 5.6396\n",
      "Epoch [1/1], Step [4844/8897], Loss: 5.6053\n",
      "Epoch [1/1], Step [4845/8897], Loss: 5.8532\n",
      "Epoch [1/1], Step [4846/8897], Loss: 5.7367\n",
      "Epoch [1/1], Step [4847/8897], Loss: 5.7541\n",
      "Epoch [1/1], Step [4848/8897], Loss: 5.6083\n",
      "Epoch [1/1], Step [4849/8897], Loss: 5.8380\n",
      "Epoch [1/1], Step [4850/8897], Loss: 5.7678\n",
      "Epoch [1/1], Step [4851/8897], Loss: 5.8260\n",
      "Epoch [1/1], Step [4852/8897], Loss: 5.7813\n",
      "Epoch [1/1], Step [4853/8897], Loss: 5.6298\n",
      "Epoch [1/1], Step [4854/8897], Loss: 5.8123\n",
      "Epoch [1/1], Step [4855/8897], Loss: 5.6317\n",
      "Epoch [1/1], Step [4856/8897], Loss: 5.9074\n",
      "Epoch [1/1], Step [4857/8897], Loss: 5.8677\n",
      "Epoch [1/1], Step [4858/8897], Loss: 5.7840\n",
      "Epoch [1/1], Step [4859/8897], Loss: 5.7874\n",
      "Epoch [1/1], Step [4860/8897], Loss: 5.8116\n",
      "Epoch [1/1], Step [4861/8897], Loss: 5.4290\n",
      "Epoch [1/1], Step [4862/8897], Loss: 5.8349\n",
      "Epoch [1/1], Step [4863/8897], Loss: 5.6281\n",
      "Epoch [1/1], Step [4864/8897], Loss: 5.8451\n",
      "Epoch [1/1], Step [4865/8897], Loss: 5.7597\n",
      "Epoch [1/1], Step [4866/8897], Loss: 5.9411\n",
      "Epoch [1/1], Step [4867/8897], Loss: 5.5721\n",
      "Epoch [1/1], Step [4868/8897], Loss: 5.5453\n",
      "Epoch [1/1], Step [4869/8897], Loss: 5.7660\n",
      "Epoch [1/1], Step [4870/8897], Loss: 5.8472\n",
      "Epoch [1/1], Step [4871/8897], Loss: 5.6867\n",
      "Epoch [1/1], Step [4872/8897], Loss: 5.7043\n",
      "Epoch [1/1], Step [4873/8897], Loss: 5.7630\n",
      "Epoch [1/1], Step [4874/8897], Loss: 5.7555\n",
      "Epoch [1/1], Step [4875/8897], Loss: 5.7538\n",
      "Epoch [1/1], Step [4876/8897], Loss: 5.7115\n",
      "Epoch [1/1], Step [4877/8897], Loss: 5.4135\n",
      "Epoch [1/1], Step [4878/8897], Loss: 5.7655\n",
      "Epoch [1/1], Step [4879/8897], Loss: 5.9163\n",
      "Epoch [1/1], Step [4880/8897], Loss: 5.6699\n",
      "Epoch [1/1], Step [4881/8897], Loss: 5.6077\n",
      "Epoch [1/1], Step [4882/8897], Loss: 5.8155\n",
      "Epoch [1/1], Step [4883/8897], Loss: 5.7205\n",
      "Epoch [1/1], Step [4884/8897], Loss: 5.8998\n",
      "Epoch [1/1], Step [4885/8897], Loss: 6.0673\n",
      "Epoch [1/1], Step [4886/8897], Loss: 5.8032\n",
      "Epoch [1/1], Step [4887/8897], Loss: 5.7811\n",
      "Epoch [1/1], Step [4888/8897], Loss: 5.8004\n",
      "Epoch [1/1], Step [4889/8897], Loss: 5.7782\n",
      "Epoch [1/1], Step [4890/8897], Loss: 5.7547\n",
      "Epoch [1/1], Step [4891/8897], Loss: 5.6694\n",
      "Epoch [1/1], Step [4892/8897], Loss: 5.9273\n",
      "Epoch [1/1], Step [4893/8897], Loss: 5.9932\n",
      "Epoch [1/1], Step [4894/8897], Loss: 6.0525\n",
      "Epoch [1/1], Step [4895/8897], Loss: 5.5388\n",
      "Epoch [1/1], Step [4896/8897], Loss: 5.7151\n",
      "Epoch [1/1], Step [4897/8897], Loss: 5.7701\n",
      "Epoch [1/1], Step [4898/8897], Loss: 5.8682\n",
      "Epoch [1/1], Step [4899/8897], Loss: 5.9326\n",
      "Epoch [1/1], Step [4900/8897], Loss: 5.8610\n",
      "Epoch [1/1], Step [4901/8897], Loss: 5.7045\n",
      "Epoch [1/1], Step [4902/8897], Loss: 5.8894\n",
      "Epoch [1/1], Step [4903/8897], Loss: 5.5956\n",
      "Epoch [1/1], Step [4904/8897], Loss: 5.7464\n",
      "Epoch [1/1], Step [4905/8897], Loss: 5.8367\n",
      "Epoch [1/1], Step [4906/8897], Loss: 5.8131\n",
      "Epoch [1/1], Step [4907/8897], Loss: 5.7894\n",
      "Epoch [1/1], Step [4908/8897], Loss: 5.6307\n",
      "Epoch [1/1], Step [4909/8897], Loss: 5.8454\n",
      "Epoch [1/1], Step [4910/8897], Loss: 5.7869\n",
      "Epoch [1/1], Step [4911/8897], Loss: 5.8077\n",
      "Epoch [1/1], Step [4912/8897], Loss: 5.7565\n",
      "Epoch [1/1], Step [4913/8897], Loss: 6.0572\n",
      "Epoch [1/1], Step [4914/8897], Loss: 5.8470\n",
      "Epoch [1/1], Step [4915/8897], Loss: 5.7005\n",
      "Epoch [1/1], Step [4916/8897], Loss: 5.6493\n",
      "Epoch [1/1], Step [4917/8897], Loss: 5.7667\n",
      "Epoch [1/1], Step [4918/8897], Loss: 5.6348\n",
      "Epoch [1/1], Step [4919/8897], Loss: 5.8347\n",
      "Epoch [1/1], Step [4920/8897], Loss: 5.6652\n",
      "Epoch [1/1], Step [4921/8897], Loss: 5.8224\n",
      "Epoch [1/1], Step [4922/8897], Loss: 5.6481\n",
      "Epoch [1/1], Step [4923/8897], Loss: 5.6307\n",
      "Epoch [1/1], Step [4924/8897], Loss: 5.7727\n",
      "Epoch [1/1], Step [4925/8897], Loss: 5.8401\n",
      "Epoch [1/1], Step [4926/8897], Loss: 5.5310\n",
      "Epoch [1/1], Step [4927/8897], Loss: 5.8068\n",
      "Epoch [1/1], Step [4928/8897], Loss: 5.5731\n",
      "Epoch [1/1], Step [4929/8897], Loss: 5.6208\n",
      "Epoch [1/1], Step [4930/8897], Loss: 5.7781\n",
      "Epoch [1/1], Step [4931/8897], Loss: 5.7348\n",
      "Epoch [1/1], Step [4932/8897], Loss: 5.7418\n",
      "Epoch [1/1], Step [4933/8897], Loss: 5.8159\n",
      "Epoch [1/1], Step [4934/8897], Loss: 5.8794\n",
      "Epoch [1/1], Step [4935/8897], Loss: 5.5862\n",
      "Epoch [1/1], Step [4936/8897], Loss: 5.7569\n",
      "Epoch [1/1], Step [4937/8897], Loss: 5.5684\n",
      "Epoch [1/1], Step [4938/8897], Loss: 5.7406\n",
      "Epoch [1/1], Step [4939/8897], Loss: 5.7720\n",
      "Epoch [1/1], Step [4940/8897], Loss: 5.7283\n",
      "Epoch [1/1], Step [4941/8897], Loss: 5.7218\n",
      "Epoch [1/1], Step [4942/8897], Loss: 5.6469\n",
      "Epoch [1/1], Step [4943/8897], Loss: 5.8335\n",
      "Epoch [1/1], Step [4944/8897], Loss: 5.7385\n",
      "Epoch [1/1], Step [4945/8897], Loss: 5.6548\n",
      "Epoch [1/1], Step [4946/8897], Loss: 5.6893\n",
      "Epoch [1/1], Step [4947/8897], Loss: 5.6083\n",
      "Epoch [1/1], Step [4948/8897], Loss: 5.9379\n",
      "Epoch [1/1], Step [4949/8897], Loss: 5.8111\n",
      "Epoch [1/1], Step [4950/8897], Loss: 5.9378\n",
      "Epoch [1/1], Step [4951/8897], Loss: 5.7503\n",
      "Epoch [1/1], Step [4952/8897], Loss: 5.7941\n",
      "Epoch [1/1], Step [4953/8897], Loss: 5.8360\n",
      "Epoch [1/1], Step [4954/8897], Loss: 5.9806\n",
      "Epoch [1/1], Step [4955/8897], Loss: 5.8751\n",
      "Epoch [1/1], Step [4956/8897], Loss: 5.8173\n",
      "Epoch [1/1], Step [4957/8897], Loss: 5.7167\n",
      "Epoch [1/1], Step [4958/8897], Loss: 5.6886\n",
      "Epoch [1/1], Step [4959/8897], Loss: 5.7764\n",
      "Epoch [1/1], Step [4960/8897], Loss: 5.6904\n",
      "Epoch [1/1], Step [4961/8897], Loss: 5.9579\n",
      "Epoch [1/1], Step [4962/8897], Loss: 5.9283\n",
      "Epoch [1/1], Step [4963/8897], Loss: 5.6546\n",
      "Epoch [1/1], Step [4964/8897], Loss: 5.8565\n",
      "Epoch [1/1], Step [4965/8897], Loss: 5.7039\n",
      "Epoch [1/1], Step [4966/8897], Loss: 5.6961\n",
      "Epoch [1/1], Step [4967/8897], Loss: 5.6523\n",
      "Epoch [1/1], Step [4968/8897], Loss: 6.0185\n",
      "Epoch [1/1], Step [4969/8897], Loss: 5.7534\n",
      "Epoch [1/1], Step [4970/8897], Loss: 5.8848\n",
      "Epoch [1/1], Step [4971/8897], Loss: 5.6128\n",
      "Epoch [1/1], Step [4972/8897], Loss: 5.6855\n",
      "Epoch [1/1], Step [4973/8897], Loss: 5.9407\n",
      "Epoch [1/1], Step [4974/8897], Loss: 5.7615\n",
      "Epoch [1/1], Step [4975/8897], Loss: 5.8799\n",
      "Epoch [1/1], Step [4976/8897], Loss: 5.9010\n",
      "Epoch [1/1], Step [4977/8897], Loss: 5.5849\n",
      "Epoch [1/1], Step [4978/8897], Loss: 5.8388\n",
      "Epoch [1/1], Step [4979/8897], Loss: 5.8700\n",
      "Epoch [1/1], Step [4980/8897], Loss: 5.8676\n",
      "Epoch [1/1], Step [4981/8897], Loss: 5.6798\n",
      "Epoch [1/1], Step [4982/8897], Loss: 5.6816\n",
      "Epoch [1/1], Step [4983/8897], Loss: 5.8586\n",
      "Epoch [1/1], Step [4984/8897], Loss: 5.6271\n",
      "Epoch [1/1], Step [4985/8897], Loss: 5.5580\n",
      "Epoch [1/1], Step [4986/8897], Loss: 5.8263\n",
      "Epoch [1/1], Step [4987/8897], Loss: 5.5130\n",
      "Epoch [1/1], Step [4988/8897], Loss: 5.7277\n",
      "Epoch [1/1], Step [4989/8897], Loss: 5.9194\n",
      "Epoch [1/1], Step [4990/8897], Loss: 5.6703\n",
      "Epoch [1/1], Step [4991/8897], Loss: 5.8018\n",
      "Epoch [1/1], Step [4992/8897], Loss: 5.8746\n",
      "Epoch [1/1], Step [4993/8897], Loss: 5.5172\n",
      "Epoch [1/1], Step [4994/8897], Loss: 5.7898\n",
      "Epoch [1/1], Step [4995/8897], Loss: 5.8702\n",
      "Epoch [1/1], Step [4996/8897], Loss: 5.8749\n",
      "Epoch [1/1], Step [4997/8897], Loss: 5.6908\n",
      "Epoch [1/1], Step [4998/8897], Loss: 5.7271\n",
      "Epoch [1/1], Step [4999/8897], Loss: 5.7272\n",
      "Epoch [1/1], Step [5000/8897], Loss: 5.9067\n",
      "Epoch [1/1], Step [5001/8897], Loss: 5.7064\n",
      "Epoch [1/1], Step [5002/8897], Loss: 5.7023\n",
      "Epoch [1/1], Step [5003/8897], Loss: 5.6168\n",
      "Epoch [1/1], Step [5004/8897], Loss: 5.8128\n",
      "Epoch [1/1], Step [5005/8897], Loss: 5.4926\n",
      "Epoch [1/1], Step [5006/8897], Loss: 5.4607\n",
      "Epoch [1/1], Step [5007/8897], Loss: 5.8354\n",
      "Epoch [1/1], Step [5008/8897], Loss: 5.8414\n",
      "Epoch [1/1], Step [5009/8897], Loss: 5.7831\n",
      "Epoch [1/1], Step [5010/8897], Loss: 5.6856\n",
      "Epoch [1/1], Step [5011/8897], Loss: 5.6765\n",
      "Epoch [1/1], Step [5012/8897], Loss: 5.7038\n",
      "Epoch [1/1], Step [5013/8897], Loss: 5.7933\n",
      "Epoch [1/1], Step [5014/8897], Loss: 5.9058\n",
      "Epoch [1/1], Step [5015/8897], Loss: 5.7390\n",
      "Epoch [1/1], Step [5016/8897], Loss: 5.7220\n",
      "Epoch [1/1], Step [5017/8897], Loss: 5.5098\n",
      "Epoch [1/1], Step [5018/8897], Loss: 6.1055\n",
      "Epoch [1/1], Step [5019/8897], Loss: 5.7873\n",
      "Epoch [1/1], Step [5020/8897], Loss: 5.6998\n",
      "Epoch [1/1], Step [5021/8897], Loss: 5.7502\n",
      "Epoch [1/1], Step [5022/8897], Loss: 5.9131\n",
      "Epoch [1/1], Step [5023/8897], Loss: 5.7534\n",
      "Epoch [1/1], Step [5024/8897], Loss: 6.0209\n",
      "Epoch [1/1], Step [5025/8897], Loss: 5.8299\n",
      "Epoch [1/1], Step [5026/8897], Loss: 5.6654\n",
      "Epoch [1/1], Step [5027/8897], Loss: 5.7412\n",
      "Epoch [1/1], Step [5028/8897], Loss: 5.6772\n",
      "Epoch [1/1], Step [5029/8897], Loss: 5.6174\n",
      "Epoch [1/1], Step [5030/8897], Loss: 5.9511\n",
      "Epoch [1/1], Step [5031/8897], Loss: 5.9630\n",
      "Epoch [1/1], Step [5032/8897], Loss: 5.5454\n",
      "Epoch [1/1], Step [5033/8897], Loss: 5.9591\n",
      "Epoch [1/1], Step [5034/8897], Loss: 5.7515\n",
      "Epoch [1/1], Step [5035/8897], Loss: 5.7629\n",
      "Epoch [1/1], Step [5036/8897], Loss: 5.6110\n",
      "Epoch [1/1], Step [5037/8897], Loss: 5.6838\n",
      "Epoch [1/1], Step [5038/8897], Loss: 5.6594\n",
      "Epoch [1/1], Step [5039/8897], Loss: 5.5683\n",
      "Epoch [1/1], Step [5040/8897], Loss: 5.9554\n",
      "Epoch [1/1], Step [5041/8897], Loss: 5.8806\n",
      "Epoch [1/1], Step [5042/8897], Loss: 5.9233\n",
      "Epoch [1/1], Step [5043/8897], Loss: 6.0145\n",
      "Epoch [1/1], Step [5044/8897], Loss: 5.8320\n",
      "Epoch [1/1], Step [5045/8897], Loss: 5.7856\n",
      "Epoch [1/1], Step [5046/8897], Loss: 5.9947\n",
      "Epoch [1/1], Step [5047/8897], Loss: 5.8116\n",
      "Epoch [1/1], Step [5048/8897], Loss: 5.7940\n",
      "Epoch [1/1], Step [5049/8897], Loss: 5.6965\n",
      "Epoch [1/1], Step [5050/8897], Loss: 5.5469\n",
      "Epoch [1/1], Step [5051/8897], Loss: 5.5418\n",
      "Epoch [1/1], Step [5052/8897], Loss: 6.0681\n",
      "Epoch [1/1], Step [5053/8897], Loss: 5.7343\n",
      "Epoch [1/1], Step [5054/8897], Loss: 5.9644\n",
      "Epoch [1/1], Step [5055/8897], Loss: 5.8641\n",
      "Epoch [1/1], Step [5056/8897], Loss: 5.7943\n",
      "Epoch [1/1], Step [5057/8897], Loss: 5.6965\n",
      "Epoch [1/1], Step [5058/8897], Loss: 5.7810\n",
      "Epoch [1/1], Step [5059/8897], Loss: 5.6675\n",
      "Epoch [1/1], Step [5060/8897], Loss: 5.8049\n",
      "Epoch [1/1], Step [5061/8897], Loss: 5.7872\n",
      "Epoch [1/1], Step [5062/8897], Loss: 5.5922\n",
      "Epoch [1/1], Step [5063/8897], Loss: 5.9025\n",
      "Epoch [1/1], Step [5064/8897], Loss: 5.8229\n",
      "Epoch [1/1], Step [5065/8897], Loss: 5.5909\n",
      "Epoch [1/1], Step [5066/8897], Loss: 5.6440\n",
      "Epoch [1/1], Step [5067/8897], Loss: 5.6010\n",
      "Epoch [1/1], Step [5068/8897], Loss: 5.6145\n",
      "Epoch [1/1], Step [5069/8897], Loss: 5.6180\n",
      "Epoch [1/1], Step [5070/8897], Loss: 5.7814\n",
      "Epoch [1/1], Step [5071/8897], Loss: 5.7106\n",
      "Epoch [1/1], Step [5072/8897], Loss: 5.7178\n",
      "Epoch [1/1], Step [5073/8897], Loss: 5.7933\n",
      "Epoch [1/1], Step [5074/8897], Loss: 5.8325\n",
      "Epoch [1/1], Step [5075/8897], Loss: 5.6415\n",
      "Epoch [1/1], Step [5076/8897], Loss: 6.0801\n",
      "Epoch [1/1], Step [5077/8897], Loss: 5.9087\n",
      "Epoch [1/1], Step [5078/8897], Loss: 5.5615\n",
      "Epoch [1/1], Step [5079/8897], Loss: 5.8340\n",
      "Epoch [1/1], Step [5080/8897], Loss: 5.8437\n",
      "Epoch [1/1], Step [5081/8897], Loss: 5.9089\n",
      "Epoch [1/1], Step [5082/8897], Loss: 5.8073\n",
      "Epoch [1/1], Step [5083/8897], Loss: 5.8454\n",
      "Epoch [1/1], Step [5084/8897], Loss: 5.8098\n",
      "Epoch [1/1], Step [5085/8897], Loss: 5.6682\n",
      "Epoch [1/1], Step [5086/8897], Loss: 5.5707\n",
      "Epoch [1/1], Step [5087/8897], Loss: 5.8074\n",
      "Epoch [1/1], Step [5088/8897], Loss: 5.8431\n",
      "Epoch [1/1], Step [5089/8897], Loss: 5.9475\n",
      "Epoch [1/1], Step [5090/8897], Loss: 5.9078\n",
      "Epoch [1/1], Step [5091/8897], Loss: 5.7927\n",
      "Epoch [1/1], Step [5092/8897], Loss: 5.9335\n",
      "Epoch [1/1], Step [5093/8897], Loss: 5.7749\n",
      "Epoch [1/1], Step [5094/8897], Loss: 5.7665\n",
      "Epoch [1/1], Step [5095/8897], Loss: 5.7486\n",
      "Epoch [1/1], Step [5096/8897], Loss: 5.9533\n",
      "Epoch [1/1], Step [5097/8897], Loss: 5.7003\n",
      "Epoch [1/1], Step [5098/8897], Loss: 5.8005\n",
      "Epoch [1/1], Step [5099/8897], Loss: 5.6603\n",
      "Epoch [1/1], Step [5100/8897], Loss: 5.9146\n",
      "Epoch [1/1], Step [5101/8897], Loss: 5.7239\n",
      "Epoch [1/1], Step [5102/8897], Loss: 5.9267\n",
      "Epoch [1/1], Step [5103/8897], Loss: 5.7611\n",
      "Epoch [1/1], Step [5104/8897], Loss: 5.9003\n",
      "Epoch [1/1], Step [5105/8897], Loss: 5.8582\n",
      "Epoch [1/1], Step [5106/8897], Loss: 5.6173\n",
      "Epoch [1/1], Step [5107/8897], Loss: 5.6022\n",
      "Epoch [1/1], Step [5108/8897], Loss: 5.6998\n",
      "Epoch [1/1], Step [5109/8897], Loss: 5.7288\n",
      "Epoch [1/1], Step [5110/8897], Loss: 5.7952\n",
      "Epoch [1/1], Step [5111/8897], Loss: 5.7766\n",
      "Epoch [1/1], Step [5112/8897], Loss: 5.7344\n",
      "Epoch [1/1], Step [5113/8897], Loss: 5.6587\n",
      "Epoch [1/1], Step [5114/8897], Loss: 6.0265\n",
      "Epoch [1/1], Step [5115/8897], Loss: 5.7261\n",
      "Epoch [1/1], Step [5116/8897], Loss: 5.7489\n",
      "Epoch [1/1], Step [5117/8897], Loss: 5.7043\n",
      "Epoch [1/1], Step [5118/8897], Loss: 5.6744\n",
      "Epoch [1/1], Step [5119/8897], Loss: 5.9293\n",
      "Epoch [1/1], Step [5120/8897], Loss: 5.8503\n",
      "Epoch [1/1], Step [5121/8897], Loss: 5.5595\n",
      "Epoch [1/1], Step [5122/8897], Loss: 5.6175\n",
      "Epoch [1/1], Step [5123/8897], Loss: 5.7294\n",
      "Epoch [1/1], Step [5124/8897], Loss: 5.4492\n",
      "Epoch [1/1], Step [5125/8897], Loss: 5.6310\n",
      "Epoch [1/1], Step [5126/8897], Loss: 5.6612\n",
      "Epoch [1/1], Step [5127/8897], Loss: 5.7359\n",
      "Epoch [1/1], Step [5128/8897], Loss: 5.9234\n",
      "Epoch [1/1], Step [5129/8897], Loss: 5.6652\n",
      "Epoch [1/1], Step [5130/8897], Loss: 5.8041\n",
      "Epoch [1/1], Step [5131/8897], Loss: 5.5688\n",
      "Epoch [1/1], Step [5132/8897], Loss: 6.0272\n",
      "Epoch [1/1], Step [5133/8897], Loss: 5.8049\n",
      "Epoch [1/1], Step [5134/8897], Loss: 5.6629\n",
      "Epoch [1/1], Step [5135/8897], Loss: 5.7148\n",
      "Epoch [1/1], Step [5136/8897], Loss: 5.6553\n",
      "Epoch [1/1], Step [5137/8897], Loss: 5.8854\n",
      "Epoch [1/1], Step [5138/8897], Loss: 5.7760\n",
      "Epoch [1/1], Step [5139/8897], Loss: 5.8250\n",
      "Epoch [1/1], Step [5140/8897], Loss: 5.9169\n",
      "Epoch [1/1], Step [5141/8897], Loss: 5.7368\n",
      "Epoch [1/1], Step [5142/8897], Loss: 5.8409\n",
      "Epoch [1/1], Step [5143/8897], Loss: 5.6745\n",
      "Epoch [1/1], Step [5144/8897], Loss: 5.9580\n",
      "Epoch [1/1], Step [5145/8897], Loss: 5.7417\n",
      "Epoch [1/1], Step [5146/8897], Loss: 5.5213\n",
      "Epoch [1/1], Step [5147/8897], Loss: 5.6076\n",
      "Epoch [1/1], Step [5148/8897], Loss: 6.0933\n",
      "Epoch [1/1], Step [5149/8897], Loss: 5.7731\n",
      "Epoch [1/1], Step [5150/8897], Loss: 5.7412\n",
      "Epoch [1/1], Step [5151/8897], Loss: 5.8591\n",
      "Epoch [1/1], Step [5152/8897], Loss: 5.7396\n",
      "Epoch [1/1], Step [5153/8897], Loss: 5.7127\n",
      "Epoch [1/1], Step [5154/8897], Loss: 6.0293\n",
      "Epoch [1/1], Step [5155/8897], Loss: 5.9078\n",
      "Epoch [1/1], Step [5156/8897], Loss: 5.4376\n",
      "Epoch [1/1], Step [5157/8897], Loss: 5.8373\n",
      "Epoch [1/1], Step [5158/8897], Loss: 5.8709\n",
      "Epoch [1/1], Step [5159/8897], Loss: 5.9986\n",
      "Epoch [1/1], Step [5160/8897], Loss: 5.5988\n",
      "Epoch [1/1], Step [5161/8897], Loss: 5.7339\n",
      "Epoch [1/1], Step [5162/8897], Loss: 5.8819\n",
      "Epoch [1/1], Step [5163/8897], Loss: 5.7722\n",
      "Epoch [1/1], Step [5164/8897], Loss: 5.7318\n",
      "Epoch [1/1], Step [5165/8897], Loss: 5.5296\n",
      "Epoch [1/1], Step [5166/8897], Loss: 5.7221\n",
      "Epoch [1/1], Step [5167/8897], Loss: 5.6441\n",
      "Epoch [1/1], Step [5168/8897], Loss: 5.6014\n",
      "Epoch [1/1], Step [5169/8897], Loss: 5.9765\n",
      "Epoch [1/1], Step [5170/8897], Loss: 5.7407\n",
      "Epoch [1/1], Step [5171/8897], Loss: 5.7561\n",
      "Epoch [1/1], Step [5172/8897], Loss: 5.8008\n",
      "Epoch [1/1], Step [5173/8897], Loss: 5.7808\n",
      "Epoch [1/1], Step [5174/8897], Loss: 5.5639\n",
      "Epoch [1/1], Step [5175/8897], Loss: 5.7492\n",
      "Epoch [1/1], Step [5176/8897], Loss: 5.6782\n",
      "Epoch [1/1], Step [5177/8897], Loss: 5.6876\n",
      "Epoch [1/1], Step [5178/8897], Loss: 5.7425\n",
      "Epoch [1/1], Step [5179/8897], Loss: 5.8177\n",
      "Epoch [1/1], Step [5180/8897], Loss: 5.6904\n",
      "Epoch [1/1], Step [5181/8897], Loss: 5.5034\n",
      "Epoch [1/1], Step [5182/8897], Loss: 5.3608\n",
      "Epoch [1/1], Step [5183/8897], Loss: 5.5639\n",
      "Epoch [1/1], Step [5184/8897], Loss: 5.7955\n",
      "Epoch [1/1], Step [5185/8897], Loss: 5.6989\n",
      "Epoch [1/1], Step [5186/8897], Loss: 5.8792\n",
      "Epoch [1/1], Step [5187/8897], Loss: 5.7003\n",
      "Epoch [1/1], Step [5188/8897], Loss: 5.6182\n",
      "Epoch [1/1], Step [5189/8897], Loss: 5.6204\n",
      "Epoch [1/1], Step [5190/8897], Loss: 5.7827\n",
      "Epoch [1/1], Step [5191/8897], Loss: 5.8495\n",
      "Epoch [1/1], Step [5192/8897], Loss: 5.9214\n",
      "Epoch [1/1], Step [5193/8897], Loss: 5.7252\n",
      "Epoch [1/1], Step [5194/8897], Loss: 5.6620\n",
      "Epoch [1/1], Step [5195/8897], Loss: 5.6171\n",
      "Epoch [1/1], Step [5196/8897], Loss: 5.4655\n",
      "Epoch [1/1], Step [5197/8897], Loss: 6.1555\n",
      "Epoch [1/1], Step [5198/8897], Loss: 5.9139\n",
      "Epoch [1/1], Step [5199/8897], Loss: 5.7921\n",
      "Epoch [1/1], Step [5200/8897], Loss: 5.8460\n",
      "Epoch [1/1], Step [5201/8897], Loss: 5.7611\n",
      "Epoch [1/1], Step [5202/8897], Loss: 5.6547\n",
      "Epoch [1/1], Step [5203/8897], Loss: 5.8820\n",
      "Epoch [1/1], Step [5204/8897], Loss: 5.4966\n",
      "Epoch [1/1], Step [5205/8897], Loss: 5.6990\n",
      "Epoch [1/1], Step [5206/8897], Loss: 5.7322\n",
      "Epoch [1/1], Step [5207/8897], Loss: 5.8541\n",
      "Epoch [1/1], Step [5208/8897], Loss: 5.6768\n",
      "Epoch [1/1], Step [5209/8897], Loss: 5.7002\n",
      "Epoch [1/1], Step [5210/8897], Loss: 5.8606\n",
      "Epoch [1/1], Step [5211/8897], Loss: 5.6581\n",
      "Epoch [1/1], Step [5212/8897], Loss: 5.8553\n",
      "Epoch [1/1], Step [5213/8897], Loss: 5.6672\n",
      "Epoch [1/1], Step [5214/8897], Loss: 5.8359\n",
      "Epoch [1/1], Step [5215/8897], Loss: 5.8930\n",
      "Epoch [1/1], Step [5216/8897], Loss: 5.7258\n",
      "Epoch [1/1], Step [5217/8897], Loss: 5.7537\n",
      "Epoch [1/1], Step [5218/8897], Loss: 5.7467\n",
      "Epoch [1/1], Step [5219/8897], Loss: 5.7748\n",
      "Epoch [1/1], Step [5220/8897], Loss: 5.8369\n",
      "Epoch [1/1], Step [5221/8897], Loss: 5.3742\n",
      "Epoch [1/1], Step [5222/8897], Loss: 5.7511\n",
      "Epoch [1/1], Step [5223/8897], Loss: 5.8185\n",
      "Epoch [1/1], Step [5224/8897], Loss: 5.7016\n",
      "Epoch [1/1], Step [5225/8897], Loss: 5.6241\n",
      "Epoch [1/1], Step [5226/8897], Loss: 5.7278\n",
      "Epoch [1/1], Step [5227/8897], Loss: 5.6873\n",
      "Epoch [1/1], Step [5228/8897], Loss: 5.6460\n",
      "Epoch [1/1], Step [5229/8897], Loss: 5.7136\n",
      "Epoch [1/1], Step [5230/8897], Loss: 6.0209\n",
      "Epoch [1/1], Step [5231/8897], Loss: 5.5569\n",
      "Epoch [1/1], Step [5232/8897], Loss: 5.6503\n",
      "Epoch [1/1], Step [5233/8897], Loss: 5.4858\n",
      "Epoch [1/1], Step [5234/8897], Loss: 5.6564\n",
      "Epoch [1/1], Step [5235/8897], Loss: 5.7414\n",
      "Epoch [1/1], Step [5236/8897], Loss: 5.7328\n",
      "Epoch [1/1], Step [5237/8897], Loss: 5.9361\n",
      "Epoch [1/1], Step [5238/8897], Loss: 5.9857\n",
      "Epoch [1/1], Step [5239/8897], Loss: 5.7542\n",
      "Epoch [1/1], Step [5240/8897], Loss: 5.6789\n",
      "Epoch [1/1], Step [5241/8897], Loss: 5.7479\n",
      "Epoch [1/1], Step [5242/8897], Loss: 5.9164\n",
      "Epoch [1/1], Step [5243/8897], Loss: 5.9370\n",
      "Epoch [1/1], Step [5244/8897], Loss: 5.8055\n",
      "Epoch [1/1], Step [5245/8897], Loss: 5.7221\n",
      "Epoch [1/1], Step [5246/8897], Loss: 5.7466\n",
      "Epoch [1/1], Step [5247/8897], Loss: 5.8220\n",
      "Epoch [1/1], Step [5248/8897], Loss: 6.1175\n",
      "Epoch [1/1], Step [5249/8897], Loss: 5.8586\n",
      "Epoch [1/1], Step [5250/8897], Loss: 6.0133\n",
      "Epoch [1/1], Step [5251/8897], Loss: 5.5601\n",
      "Epoch [1/1], Step [5252/8897], Loss: 5.8324\n",
      "Epoch [1/1], Step [5253/8897], Loss: 5.7131\n",
      "Epoch [1/1], Step [5254/8897], Loss: 5.7634\n",
      "Epoch [1/1], Step [5255/8897], Loss: 5.9810\n",
      "Epoch [1/1], Step [5256/8897], Loss: 5.7820\n",
      "Epoch [1/1], Step [5257/8897], Loss: 5.9893\n",
      "Epoch [1/1], Step [5258/8897], Loss: 5.7105\n",
      "Epoch [1/1], Step [5259/8897], Loss: 5.8879\n",
      "Epoch [1/1], Step [5260/8897], Loss: 5.6857\n",
      "Epoch [1/1], Step [5261/8897], Loss: 5.7675\n",
      "Epoch [1/1], Step [5262/8897], Loss: 5.8871\n",
      "Epoch [1/1], Step [5263/8897], Loss: 5.9135\n",
      "Epoch [1/1], Step [5264/8897], Loss: 5.5240\n",
      "Epoch [1/1], Step [5265/8897], Loss: 5.7518\n",
      "Epoch [1/1], Step [5266/8897], Loss: 5.6691\n",
      "Epoch [1/1], Step [5267/8897], Loss: 5.8561\n",
      "Epoch [1/1], Step [5268/8897], Loss: 5.6776\n",
      "Epoch [1/1], Step [5269/8897], Loss: 5.6999\n",
      "Epoch [1/1], Step [5270/8897], Loss: 5.8258\n",
      "Epoch [1/1], Step [5271/8897], Loss: 5.6933\n",
      "Epoch [1/1], Step [5272/8897], Loss: 5.8064\n",
      "Epoch [1/1], Step [5273/8897], Loss: 5.5307\n",
      "Epoch [1/1], Step [5274/8897], Loss: 5.8509\n",
      "Epoch [1/1], Step [5275/8897], Loss: 5.6836\n",
      "Epoch [1/1], Step [5276/8897], Loss: 5.6638\n",
      "Epoch [1/1], Step [5277/8897], Loss: 5.6627\n",
      "Epoch [1/1], Step [5278/8897], Loss: 5.5590\n",
      "Epoch [1/1], Step [5279/8897], Loss: 5.4524\n",
      "Epoch [1/1], Step [5280/8897], Loss: 5.7844\n",
      "Epoch [1/1], Step [5281/8897], Loss: 5.6515\n",
      "Epoch [1/1], Step [5282/8897], Loss: 5.6654\n",
      "Epoch [1/1], Step [5283/8897], Loss: 5.6383\n",
      "Epoch [1/1], Step [5284/8897], Loss: 5.8136\n",
      "Epoch [1/1], Step [5285/8897], Loss: 5.4970\n",
      "Epoch [1/1], Step [5286/8897], Loss: 5.6919\n",
      "Epoch [1/1], Step [5287/8897], Loss: 5.7109\n",
      "Epoch [1/1], Step [5288/8897], Loss: 5.6369\n",
      "Epoch [1/1], Step [5289/8897], Loss: 5.7585\n",
      "Epoch [1/1], Step [5290/8897], Loss: 5.6434\n",
      "Epoch [1/1], Step [5291/8897], Loss: 5.6361\n",
      "Epoch [1/1], Step [5292/8897], Loss: 5.5879\n",
      "Epoch [1/1], Step [5293/8897], Loss: 5.4762\n",
      "Epoch [1/1], Step [5294/8897], Loss: 5.7585\n",
      "Epoch [1/1], Step [5295/8897], Loss: 5.6686\n",
      "Epoch [1/1], Step [5296/8897], Loss: 5.7794\n",
      "Epoch [1/1], Step [5297/8897], Loss: 5.4863\n",
      "Epoch [1/1], Step [5298/8897], Loss: 5.4932\n",
      "Epoch [1/1], Step [5299/8897], Loss: 5.6574\n",
      "Epoch [1/1], Step [5300/8897], Loss: 5.7393\n",
      "Epoch [1/1], Step [5301/8897], Loss: 5.5105\n",
      "Epoch [1/1], Step [5302/8897], Loss: 5.9270\n",
      "Epoch [1/1], Step [5303/8897], Loss: 5.8369\n",
      "Epoch [1/1], Step [5304/8897], Loss: 5.7548\n",
      "Epoch [1/1], Step [5305/8897], Loss: 5.7442\n",
      "Epoch [1/1], Step [5306/8897], Loss: 5.8738\n",
      "Epoch [1/1], Step [5307/8897], Loss: 5.6163\n",
      "Epoch [1/1], Step [5308/8897], Loss: 5.8786\n",
      "Epoch [1/1], Step [5309/8897], Loss: 5.4715\n",
      "Epoch [1/1], Step [5310/8897], Loss: 5.8656\n",
      "Epoch [1/1], Step [5311/8897], Loss: 5.7513\n",
      "Epoch [1/1], Step [5312/8897], Loss: 5.6595\n",
      "Epoch [1/1], Step [5313/8897], Loss: 5.8790\n",
      "Epoch [1/1], Step [5314/8897], Loss: 5.7487\n",
      "Epoch [1/1], Step [5315/8897], Loss: 5.7024\n",
      "Epoch [1/1], Step [5316/8897], Loss: 5.5917\n",
      "Epoch [1/1], Step [5317/8897], Loss: 5.8627\n",
      "Epoch [1/1], Step [5318/8897], Loss: 5.7535\n",
      "Epoch [1/1], Step [5319/8897], Loss: 5.6891\n",
      "Epoch [1/1], Step [5320/8897], Loss: 5.7357\n",
      "Epoch [1/1], Step [5321/8897], Loss: 5.6654\n",
      "Epoch [1/1], Step [5322/8897], Loss: 5.7384\n",
      "Epoch [1/1], Step [5323/8897], Loss: 5.8042\n",
      "Epoch [1/1], Step [5324/8897], Loss: 5.8523\n",
      "Epoch [1/1], Step [5325/8897], Loss: 5.8560\n",
      "Epoch [1/1], Step [5326/8897], Loss: 5.5869\n",
      "Epoch [1/1], Step [5327/8897], Loss: 5.6211\n",
      "Epoch [1/1], Step [5328/8897], Loss: 5.7877\n",
      "Epoch [1/1], Step [5329/8897], Loss: 5.6282\n",
      "Epoch [1/1], Step [5330/8897], Loss: 5.9051\n",
      "Epoch [1/1], Step [5331/8897], Loss: 5.7027\n",
      "Epoch [1/1], Step [5332/8897], Loss: 5.6257\n",
      "Epoch [1/1], Step [5333/8897], Loss: 5.5399\n",
      "Epoch [1/1], Step [5334/8897], Loss: 5.5804\n",
      "Epoch [1/1], Step [5335/8897], Loss: 5.6254\n",
      "Epoch [1/1], Step [5336/8897], Loss: 6.0329\n",
      "Epoch [1/1], Step [5337/8897], Loss: 5.7838\n",
      "Epoch [1/1], Step [5338/8897], Loss: 5.8387\n",
      "Epoch [1/1], Step [5339/8897], Loss: 5.7808\n",
      "Epoch [1/1], Step [5340/8897], Loss: 5.7400\n",
      "Epoch [1/1], Step [5341/8897], Loss: 5.8911\n",
      "Epoch [1/1], Step [5342/8897], Loss: 6.0381\n",
      "Epoch [1/1], Step [5343/8897], Loss: 5.6255\n",
      "Epoch [1/1], Step [5344/8897], Loss: 5.6726\n",
      "Epoch [1/1], Step [5345/8897], Loss: 5.8764\n",
      "Epoch [1/1], Step [5346/8897], Loss: 5.9584\n",
      "Epoch [1/1], Step [5347/8897], Loss: 5.5454\n",
      "Epoch [1/1], Step [5348/8897], Loss: 5.7266\n",
      "Epoch [1/1], Step [5349/8897], Loss: 5.8307\n",
      "Epoch [1/1], Step [5350/8897], Loss: 5.5559\n",
      "Epoch [1/1], Step [5351/8897], Loss: 5.6381\n",
      "Epoch [1/1], Step [5352/8897], Loss: 5.6578\n",
      "Epoch [1/1], Step [5353/8897], Loss: 5.8540\n",
      "Epoch [1/1], Step [5354/8897], Loss: 5.4459\n",
      "Epoch [1/1], Step [5355/8897], Loss: 5.7409\n",
      "Epoch [1/1], Step [5356/8897], Loss: 5.6364\n",
      "Epoch [1/1], Step [5357/8897], Loss: 5.7704\n",
      "Epoch [1/1], Step [5358/8897], Loss: 5.6916\n",
      "Epoch [1/1], Step [5359/8897], Loss: 5.8558\n",
      "Epoch [1/1], Step [5360/8897], Loss: 5.7467\n",
      "Epoch [1/1], Step [5361/8897], Loss: 5.6068\n",
      "Epoch [1/1], Step [5362/8897], Loss: 5.4933\n",
      "Epoch [1/1], Step [5363/8897], Loss: 5.4591\n",
      "Epoch [1/1], Step [5364/8897], Loss: 5.8818\n",
      "Epoch [1/1], Step [5365/8897], Loss: 5.7997\n",
      "Epoch [1/1], Step [5366/8897], Loss: 5.6624\n",
      "Epoch [1/1], Step [5367/8897], Loss: 5.6888\n",
      "Epoch [1/1], Step [5368/8897], Loss: 5.8463\n",
      "Epoch [1/1], Step [5369/8897], Loss: 5.8316\n",
      "Epoch [1/1], Step [5370/8897], Loss: 5.6992\n",
      "Epoch [1/1], Step [5371/8897], Loss: 5.9574\n",
      "Epoch [1/1], Step [5372/8897], Loss: 5.6483\n",
      "Epoch [1/1], Step [5373/8897], Loss: 5.6064\n",
      "Epoch [1/1], Step [5374/8897], Loss: 5.8625\n",
      "Epoch [1/1], Step [5375/8897], Loss: 5.8930\n",
      "Epoch [1/1], Step [5376/8897], Loss: 5.6197\n",
      "Epoch [1/1], Step [5377/8897], Loss: 5.5059\n",
      "Epoch [1/1], Step [5378/8897], Loss: 5.8620\n",
      "Epoch [1/1], Step [5379/8897], Loss: 5.5541\n",
      "Epoch [1/1], Step [5380/8897], Loss: 5.9041\n",
      "Epoch [1/1], Step [5381/8897], Loss: 5.9072\n",
      "Epoch [1/1], Step [5382/8897], Loss: 5.8794\n",
      "Epoch [1/1], Step [5383/8897], Loss: 5.6831\n",
      "Epoch [1/1], Step [5384/8897], Loss: 5.9684\n",
      "Epoch [1/1], Step [5385/8897], Loss: 5.7143\n",
      "Epoch [1/1], Step [5386/8897], Loss: 6.0488\n",
      "Epoch [1/1], Step [5387/8897], Loss: 5.6735\n",
      "Epoch [1/1], Step [5388/8897], Loss: 5.6299\n",
      "Epoch [1/1], Step [5389/8897], Loss: 5.7891\n",
      "Epoch [1/1], Step [5390/8897], Loss: 5.5716\n",
      "Epoch [1/1], Step [5391/8897], Loss: 5.7343\n",
      "Epoch [1/1], Step [5392/8897], Loss: 5.7875\n",
      "Epoch [1/1], Step [5393/8897], Loss: 5.7245\n",
      "Epoch [1/1], Step [5394/8897], Loss: 6.0271\n",
      "Epoch [1/1], Step [5395/8897], Loss: 5.5005\n",
      "Epoch [1/1], Step [5396/8897], Loss: 5.7826\n",
      "Epoch [1/1], Step [5397/8897], Loss: 5.7970\n",
      "Epoch [1/1], Step [5398/8897], Loss: 5.7945\n",
      "Epoch [1/1], Step [5399/8897], Loss: 5.8776\n",
      "Epoch [1/1], Step [5400/8897], Loss: 5.9601\n",
      "Epoch [1/1], Step [5401/8897], Loss: 5.8461\n",
      "Epoch [1/1], Step [5402/8897], Loss: 5.5494\n",
      "Epoch [1/1], Step [5403/8897], Loss: 5.5768\n",
      "Epoch [1/1], Step [5404/8897], Loss: 5.6637\n",
      "Epoch [1/1], Step [5405/8897], Loss: 5.6213\n",
      "Epoch [1/1], Step [5406/8897], Loss: 5.9045\n",
      "Epoch [1/1], Step [5407/8897], Loss: 5.7514\n",
      "Epoch [1/1], Step [5408/8897], Loss: 5.8174\n",
      "Epoch [1/1], Step [5409/8897], Loss: 5.9096\n",
      "Epoch [1/1], Step [5410/8897], Loss: 5.9745\n",
      "Epoch [1/1], Step [5411/8897], Loss: 5.5144\n",
      "Epoch [1/1], Step [5412/8897], Loss: 5.7784\n",
      "Epoch [1/1], Step [5413/8897], Loss: 5.7561\n",
      "Epoch [1/1], Step [5414/8897], Loss: 5.8278\n",
      "Epoch [1/1], Step [5415/8897], Loss: 5.7686\n",
      "Epoch [1/1], Step [5416/8897], Loss: 5.6302\n",
      "Epoch [1/1], Step [5417/8897], Loss: 5.4365\n",
      "Epoch [1/1], Step [5418/8897], Loss: 5.8346\n",
      "Epoch [1/1], Step [5419/8897], Loss: 5.4752\n",
      "Epoch [1/1], Step [5420/8897], Loss: 5.8083\n",
      "Epoch [1/1], Step [5421/8897], Loss: 5.6234\n",
      "Epoch [1/1], Step [5422/8897], Loss: 5.6898\n",
      "Epoch [1/1], Step [5423/8897], Loss: 5.8843\n",
      "Epoch [1/1], Step [5424/8897], Loss: 5.7778\n",
      "Epoch [1/1], Step [5425/8897], Loss: 5.8162\n",
      "Epoch [1/1], Step [5426/8897], Loss: 5.8341\n",
      "Epoch [1/1], Step [5427/8897], Loss: 5.8627\n",
      "Epoch [1/1], Step [5428/8897], Loss: 5.8249\n",
      "Epoch [1/1], Step [5429/8897], Loss: 5.4491\n",
      "Epoch [1/1], Step [5430/8897], Loss: 5.6839\n",
      "Epoch [1/1], Step [5431/8897], Loss: 5.9201\n",
      "Epoch [1/1], Step [5432/8897], Loss: 5.6934\n",
      "Epoch [1/1], Step [5433/8897], Loss: 5.7670\n",
      "Epoch [1/1], Step [5434/8897], Loss: 5.6117\n",
      "Epoch [1/1], Step [5435/8897], Loss: 5.6464\n",
      "Epoch [1/1], Step [5436/8897], Loss: 5.7212\n",
      "Epoch [1/1], Step [5437/8897], Loss: 5.7495\n",
      "Epoch [1/1], Step [5438/8897], Loss: 5.7570\n",
      "Epoch [1/1], Step [5439/8897], Loss: 5.7102\n",
      "Epoch [1/1], Step [5440/8897], Loss: 5.6014\n",
      "Epoch [1/1], Step [5441/8897], Loss: 5.7196\n",
      "Epoch [1/1], Step [5442/8897], Loss: 5.8143\n",
      "Epoch [1/1], Step [5443/8897], Loss: 5.7379\n",
      "Epoch [1/1], Step [5444/8897], Loss: 5.8822\n",
      "Epoch [1/1], Step [5445/8897], Loss: 5.6300\n",
      "Epoch [1/1], Step [5446/8897], Loss: 5.4902\n",
      "Epoch [1/1], Step [5447/8897], Loss: 5.6172\n",
      "Epoch [1/1], Step [5448/8897], Loss: 5.8472\n",
      "Epoch [1/1], Step [5449/8897], Loss: 5.7511\n",
      "Epoch [1/1], Step [5450/8897], Loss: 5.7863\n",
      "Epoch [1/1], Step [5451/8897], Loss: 5.7673\n",
      "Epoch [1/1], Step [5452/8897], Loss: 5.7902\n",
      "Epoch [1/1], Step [5453/8897], Loss: 5.8295\n",
      "Epoch [1/1], Step [5454/8897], Loss: 5.9060\n",
      "Epoch [1/1], Step [5455/8897], Loss: 5.8084\n",
      "Epoch [1/1], Step [5456/8897], Loss: 5.6587\n",
      "Epoch [1/1], Step [5457/8897], Loss: 5.7956\n",
      "Epoch [1/1], Step [5458/8897], Loss: 5.7233\n",
      "Epoch [1/1], Step [5459/8897], Loss: 5.5779\n",
      "Epoch [1/1], Step [5460/8897], Loss: 5.6506\n",
      "Epoch [1/1], Step [5461/8897], Loss: 5.7240\n",
      "Epoch [1/1], Step [5462/8897], Loss: 5.9363\n",
      "Epoch [1/1], Step [5463/8897], Loss: 5.5691\n",
      "Epoch [1/1], Step [5464/8897], Loss: 5.7379\n",
      "Epoch [1/1], Step [5465/8897], Loss: 5.5250\n",
      "Epoch [1/1], Step [5466/8897], Loss: 5.6388\n",
      "Epoch [1/1], Step [5467/8897], Loss: 5.4914\n",
      "Epoch [1/1], Step [5468/8897], Loss: 5.7549\n",
      "Epoch [1/1], Step [5469/8897], Loss: 5.4830\n",
      "Epoch [1/1], Step [5470/8897], Loss: 5.4909\n",
      "Epoch [1/1], Step [5471/8897], Loss: 5.5501\n",
      "Epoch [1/1], Step [5472/8897], Loss: 5.6747\n",
      "Epoch [1/1], Step [5473/8897], Loss: 5.7752\n",
      "Epoch [1/1], Step [5474/8897], Loss: 5.8657\n",
      "Epoch [1/1], Step [5475/8897], Loss: 5.8341\n",
      "Epoch [1/1], Step [5476/8897], Loss: 5.8305\n",
      "Epoch [1/1], Step [5477/8897], Loss: 5.6720\n",
      "Epoch [1/1], Step [5478/8897], Loss: 5.6781\n",
      "Epoch [1/1], Step [5479/8897], Loss: 5.6056\n",
      "Epoch [1/1], Step [5480/8897], Loss: 5.7525\n",
      "Epoch [1/1], Step [5481/8897], Loss: 5.8002\n",
      "Epoch [1/1], Step [5482/8897], Loss: 5.7567\n",
      "Epoch [1/1], Step [5483/8897], Loss: 5.6630\n",
      "Epoch [1/1], Step [5484/8897], Loss: 5.7781\n",
      "Epoch [1/1], Step [5485/8897], Loss: 5.8831\n",
      "Epoch [1/1], Step [5486/8897], Loss: 5.7046\n",
      "Epoch [1/1], Step [5487/8897], Loss: 5.7356\n",
      "Epoch [1/1], Step [5488/8897], Loss: 5.8340\n",
      "Epoch [1/1], Step [5489/8897], Loss: 5.6405\n",
      "Epoch [1/1], Step [5490/8897], Loss: 5.5385\n",
      "Epoch [1/1], Step [5491/8897], Loss: 5.7029\n",
      "Epoch [1/1], Step [5492/8897], Loss: 5.7550\n",
      "Epoch [1/1], Step [5493/8897], Loss: 5.7221\n",
      "Epoch [1/1], Step [5494/8897], Loss: 5.7686\n",
      "Epoch [1/1], Step [5495/8897], Loss: 5.6827\n",
      "Epoch [1/1], Step [5496/8897], Loss: 5.8486\n",
      "Epoch [1/1], Step [5497/8897], Loss: 5.4681\n",
      "Epoch [1/1], Step [5498/8897], Loss: 5.8402\n",
      "Epoch [1/1], Step [5499/8897], Loss: 5.8852\n",
      "Epoch [1/1], Step [5500/8897], Loss: 5.7402\n",
      "Epoch [1/1], Step [5501/8897], Loss: 5.9567\n",
      "Epoch [1/1], Step [5502/8897], Loss: 5.4848\n",
      "Epoch [1/1], Step [5503/8897], Loss: 5.5642\n",
      "Epoch [1/1], Step [5504/8897], Loss: 5.8660\n",
      "Epoch [1/1], Step [5505/8897], Loss: 5.8136\n",
      "Epoch [1/1], Step [5506/8897], Loss: 5.7375\n",
      "Epoch [1/1], Step [5507/8897], Loss: 5.7511\n",
      "Epoch [1/1], Step [5508/8897], Loss: 5.6853\n",
      "Epoch [1/1], Step [5509/8897], Loss: 6.1046\n",
      "Epoch [1/1], Step [5510/8897], Loss: 5.6708\n",
      "Epoch [1/1], Step [5511/8897], Loss: 5.6354\n",
      "Epoch [1/1], Step [5512/8897], Loss: 5.7334\n",
      "Epoch [1/1], Step [5513/8897], Loss: 5.8382\n",
      "Epoch [1/1], Step [5514/8897], Loss: 5.6677\n",
      "Epoch [1/1], Step [5515/8897], Loss: 5.5345\n",
      "Epoch [1/1], Step [5516/8897], Loss: 5.7053\n",
      "Epoch [1/1], Step [5517/8897], Loss: 5.5273\n",
      "Epoch [1/1], Step [5518/8897], Loss: 5.6536\n",
      "Epoch [1/1], Step [5519/8897], Loss: 5.5983\n",
      "Epoch [1/1], Step [5520/8897], Loss: 5.9262\n",
      "Epoch [1/1], Step [5521/8897], Loss: 5.5258\n",
      "Epoch [1/1], Step [5522/8897], Loss: 5.6159\n",
      "Epoch [1/1], Step [5523/8897], Loss: 5.5963\n",
      "Epoch [1/1], Step [5524/8897], Loss: 5.7001\n",
      "Epoch [1/1], Step [5525/8897], Loss: 5.9434\n",
      "Epoch [1/1], Step [5526/8897], Loss: 5.7515\n",
      "Epoch [1/1], Step [5527/8897], Loss: 5.6078\n",
      "Epoch [1/1], Step [5528/8897], Loss: 5.6403\n",
      "Epoch [1/1], Step [5529/8897], Loss: 5.7430\n",
      "Epoch [1/1], Step [5530/8897], Loss: 5.6354\n",
      "Epoch [1/1], Step [5531/8897], Loss: 5.7699\n",
      "Epoch [1/1], Step [5532/8897], Loss: 5.5601\n",
      "Epoch [1/1], Step [5533/8897], Loss: 5.6186\n",
      "Epoch [1/1], Step [5534/8897], Loss: 5.5060\n",
      "Epoch [1/1], Step [5535/8897], Loss: 5.7373\n",
      "Epoch [1/1], Step [5536/8897], Loss: 5.8202\n",
      "Epoch [1/1], Step [5537/8897], Loss: 5.8261\n",
      "Epoch [1/1], Step [5538/8897], Loss: 5.6711\n",
      "Epoch [1/1], Step [5539/8897], Loss: 5.7877\n",
      "Epoch [1/1], Step [5540/8897], Loss: 5.4035\n",
      "Epoch [1/1], Step [5541/8897], Loss: 5.6297\n",
      "Epoch [1/1], Step [5542/8897], Loss: 5.7833\n",
      "Epoch [1/1], Step [5543/8897], Loss: 5.7755\n",
      "Epoch [1/1], Step [5544/8897], Loss: 5.9954\n",
      "Epoch [1/1], Step [5545/8897], Loss: 5.8925\n",
      "Epoch [1/1], Step [5546/8897], Loss: 5.8150\n",
      "Epoch [1/1], Step [5547/8897], Loss: 5.7666\n",
      "Epoch [1/1], Step [5548/8897], Loss: 6.0897\n",
      "Epoch [1/1], Step [5549/8897], Loss: 5.6535\n",
      "Epoch [1/1], Step [5550/8897], Loss: 5.8588\n",
      "Epoch [1/1], Step [5551/8897], Loss: 5.5954\n",
      "Epoch [1/1], Step [5552/8897], Loss: 5.4797\n",
      "Epoch [1/1], Step [5553/8897], Loss: 5.7135\n",
      "Epoch [1/1], Step [5554/8897], Loss: 5.8943\n",
      "Epoch [1/1], Step [5555/8897], Loss: 5.6176\n",
      "Epoch [1/1], Step [5556/8897], Loss: 5.8920\n",
      "Epoch [1/1], Step [5557/8897], Loss: 5.5751\n",
      "Epoch [1/1], Step [5558/8897], Loss: 5.6554\n",
      "Epoch [1/1], Step [5559/8897], Loss: 5.9673\n",
      "Epoch [1/1], Step [5560/8897], Loss: 5.7574\n",
      "Epoch [1/1], Step [5561/8897], Loss: 5.8927\n",
      "Epoch [1/1], Step [5562/8897], Loss: 5.5837\n",
      "Epoch [1/1], Step [5563/8897], Loss: 5.6916\n",
      "Epoch [1/1], Step [5564/8897], Loss: 5.7032\n",
      "Epoch [1/1], Step [5565/8897], Loss: 5.5095\n",
      "Epoch [1/1], Step [5566/8897], Loss: 5.7716\n",
      "Epoch [1/1], Step [5567/8897], Loss: 5.6942\n",
      "Epoch [1/1], Step [5568/8897], Loss: 5.7032\n",
      "Epoch [1/1], Step [5569/8897], Loss: 5.5327\n",
      "Epoch [1/1], Step [5570/8897], Loss: 5.7300\n",
      "Epoch [1/1], Step [5571/8897], Loss: 5.5922\n",
      "Epoch [1/1], Step [5572/8897], Loss: 5.5353\n",
      "Epoch [1/1], Step [5573/8897], Loss: 5.8095\n",
      "Epoch [1/1], Step [5574/8897], Loss: 5.8017\n",
      "Epoch [1/1], Step [5575/8897], Loss: 5.5750\n",
      "Epoch [1/1], Step [5576/8897], Loss: 5.7146\n",
      "Epoch [1/1], Step [5577/8897], Loss: 5.4744\n",
      "Epoch [1/1], Step [5578/8897], Loss: 5.7442\n",
      "Epoch [1/1], Step [5579/8897], Loss: 5.6840\n",
      "Epoch [1/1], Step [5580/8897], Loss: 5.6807\n",
      "Epoch [1/1], Step [5581/8897], Loss: 5.5659\n",
      "Epoch [1/1], Step [5582/8897], Loss: 5.5793\n",
      "Epoch [1/1], Step [5583/8897], Loss: 5.8159\n",
      "Epoch [1/1], Step [5584/8897], Loss: 5.6833\n",
      "Epoch [1/1], Step [5585/8897], Loss: 5.7149\n",
      "Epoch [1/1], Step [5586/8897], Loss: 5.7038\n",
      "Epoch [1/1], Step [5587/8897], Loss: 5.8237\n",
      "Epoch [1/1], Step [5588/8897], Loss: 5.7255\n",
      "Epoch [1/1], Step [5589/8897], Loss: 5.3799\n",
      "Epoch [1/1], Step [5590/8897], Loss: 5.7570\n",
      "Epoch [1/1], Step [5591/8897], Loss: 5.5977\n",
      "Epoch [1/1], Step [5592/8897], Loss: 5.9566\n",
      "Epoch [1/1], Step [5593/8897], Loss: 5.7350\n",
      "Epoch [1/1], Step [5594/8897], Loss: 6.0832\n",
      "Epoch [1/1], Step [5595/8897], Loss: 5.8415\n",
      "Epoch [1/1], Step [5596/8897], Loss: 5.7862\n",
      "Epoch [1/1], Step [5597/8897], Loss: 5.7607\n",
      "Epoch [1/1], Step [5598/8897], Loss: 5.7126\n",
      "Epoch [1/1], Step [5599/8897], Loss: 5.7038\n",
      "Epoch [1/1], Step [5600/8897], Loss: 5.6393\n",
      "Epoch [1/1], Step [5601/8897], Loss: 5.6816\n",
      "Epoch [1/1], Step [5602/8897], Loss: 5.8243\n",
      "Epoch [1/1], Step [5603/8897], Loss: 5.6784\n",
      "Epoch [1/1], Step [5604/8897], Loss: 5.5679\n",
      "Epoch [1/1], Step [5605/8897], Loss: 5.6484\n",
      "Epoch [1/1], Step [5606/8897], Loss: 5.6079\n",
      "Epoch [1/1], Step [5607/8897], Loss: 5.6998\n",
      "Epoch [1/1], Step [5608/8897], Loss: 5.7409\n",
      "Epoch [1/1], Step [5609/8897], Loss: 5.5547\n",
      "Epoch [1/1], Step [5610/8897], Loss: 5.6644\n",
      "Epoch [1/1], Step [5611/8897], Loss: 5.8647\n",
      "Epoch [1/1], Step [5612/8897], Loss: 5.7424\n",
      "Epoch [1/1], Step [5613/8897], Loss: 5.6784\n",
      "Epoch [1/1], Step [5614/8897], Loss: 5.9503\n",
      "Epoch [1/1], Step [5615/8897], Loss: 5.7596\n",
      "Epoch [1/1], Step [5616/8897], Loss: 5.4360\n",
      "Epoch [1/1], Step [5617/8897], Loss: 5.5643\n",
      "Epoch [1/1], Step [5618/8897], Loss: 5.7739\n",
      "Epoch [1/1], Step [5619/8897], Loss: 5.5998\n",
      "Epoch [1/1], Step [5620/8897], Loss: 5.6655\n",
      "Epoch [1/1], Step [5621/8897], Loss: 5.6685\n",
      "Epoch [1/1], Step [5622/8897], Loss: 5.7974\n",
      "Epoch [1/1], Step [5623/8897], Loss: 5.8110\n",
      "Epoch [1/1], Step [5624/8897], Loss: 5.7034\n",
      "Epoch [1/1], Step [5625/8897], Loss: 5.7847\n",
      "Epoch [1/1], Step [5626/8897], Loss: 5.8242\n",
      "Epoch [1/1], Step [5627/8897], Loss: 5.5676\n",
      "Epoch [1/1], Step [5628/8897], Loss: 5.8190\n",
      "Epoch [1/1], Step [5629/8897], Loss: 5.5610\n",
      "Epoch [1/1], Step [5630/8897], Loss: 5.7848\n",
      "Epoch [1/1], Step [5631/8897], Loss: 5.7269\n",
      "Epoch [1/1], Step [5632/8897], Loss: 5.6213\n",
      "Epoch [1/1], Step [5633/8897], Loss: 5.7425\n",
      "Epoch [1/1], Step [5634/8897], Loss: 5.6598\n",
      "Epoch [1/1], Step [5635/8897], Loss: 5.7973\n",
      "Epoch [1/1], Step [5636/8897], Loss: 5.4436\n",
      "Epoch [1/1], Step [5637/8897], Loss: 5.7972\n",
      "Epoch [1/1], Step [5638/8897], Loss: 5.8293\n",
      "Epoch [1/1], Step [5639/8897], Loss: 5.5016\n",
      "Epoch [1/1], Step [5640/8897], Loss: 5.7048\n",
      "Epoch [1/1], Step [5641/8897], Loss: 5.7218\n",
      "Epoch [1/1], Step [5642/8897], Loss: 5.6652\n",
      "Epoch [1/1], Step [5643/8897], Loss: 5.6212\n",
      "Epoch [1/1], Step [5644/8897], Loss: 5.7172\n",
      "Epoch [1/1], Step [5645/8897], Loss: 5.7812\n",
      "Epoch [1/1], Step [5646/8897], Loss: 5.9031\n",
      "Epoch [1/1], Step [5647/8897], Loss: 5.5171\n",
      "Epoch [1/1], Step [5648/8897], Loss: 5.6778\n",
      "Epoch [1/1], Step [5649/8897], Loss: 5.4962\n",
      "Epoch [1/1], Step [5650/8897], Loss: 5.7510\n",
      "Epoch [1/1], Step [5651/8897], Loss: 5.7806\n",
      "Epoch [1/1], Step [5652/8897], Loss: 5.4914\n",
      "Epoch [1/1], Step [5653/8897], Loss: 5.5210\n",
      "Epoch [1/1], Step [5654/8897], Loss: 5.6395\n",
      "Epoch [1/1], Step [5655/8897], Loss: 5.8856\n",
      "Epoch [1/1], Step [5656/8897], Loss: 5.9583\n",
      "Epoch [1/1], Step [5657/8897], Loss: 5.8106\n",
      "Epoch [1/1], Step [5658/8897], Loss: 5.8029\n",
      "Epoch [1/1], Step [5659/8897], Loss: 5.6838\n",
      "Epoch [1/1], Step [5660/8897], Loss: 5.6463\n",
      "Epoch [1/1], Step [5661/8897], Loss: 5.6885\n",
      "Epoch [1/1], Step [5662/8897], Loss: 5.4394\n",
      "Epoch [1/1], Step [5663/8897], Loss: 5.5194\n",
      "Epoch [1/1], Step [5664/8897], Loss: 5.8325\n",
      "Epoch [1/1], Step [5665/8897], Loss: 5.6373\n",
      "Epoch [1/1], Step [5666/8897], Loss: 5.6813\n",
      "Epoch [1/1], Step [5667/8897], Loss: 5.7597\n",
      "Epoch [1/1], Step [5668/8897], Loss: 5.6364\n",
      "Epoch [1/1], Step [5669/8897], Loss: 5.6021\n",
      "Epoch [1/1], Step [5670/8897], Loss: 5.5633\n",
      "Epoch [1/1], Step [5671/8897], Loss: 5.8460\n",
      "Epoch [1/1], Step [5672/8897], Loss: 5.7121\n",
      "Epoch [1/1], Step [5673/8897], Loss: 5.8898\n",
      "Epoch [1/1], Step [5674/8897], Loss: 5.5436\n",
      "Epoch [1/1], Step [5675/8897], Loss: 5.6021\n",
      "Epoch [1/1], Step [5676/8897], Loss: 5.5016\n",
      "Epoch [1/1], Step [5677/8897], Loss: 5.6675\n",
      "Epoch [1/1], Step [5678/8897], Loss: 5.8608\n",
      "Epoch [1/1], Step [5679/8897], Loss: 5.7153\n",
      "Epoch [1/1], Step [5680/8897], Loss: 5.5647\n",
      "Epoch [1/1], Step [5681/8897], Loss: 5.6759\n",
      "Epoch [1/1], Step [5682/8897], Loss: 5.7555\n",
      "Epoch [1/1], Step [5683/8897], Loss: 5.7172\n",
      "Epoch [1/1], Step [5684/8897], Loss: 5.7732\n",
      "Epoch [1/1], Step [5685/8897], Loss: 5.8218\n",
      "Epoch [1/1], Step [5686/8897], Loss: 5.7357\n",
      "Epoch [1/1], Step [5687/8897], Loss: 5.7675\n",
      "Epoch [1/1], Step [5688/8897], Loss: 5.7490\n",
      "Epoch [1/1], Step [5689/8897], Loss: 5.9066\n",
      "Epoch [1/1], Step [5690/8897], Loss: 5.9230\n",
      "Epoch [1/1], Step [5691/8897], Loss: 5.5793\n",
      "Epoch [1/1], Step [5692/8897], Loss: 5.6493\n",
      "Epoch [1/1], Step [5693/8897], Loss: 5.6654\n",
      "Epoch [1/1], Step [5694/8897], Loss: 5.6902\n",
      "Epoch [1/1], Step [5695/8897], Loss: 5.7192\n",
      "Epoch [1/1], Step [5696/8897], Loss: 5.7938\n",
      "Epoch [1/1], Step [5697/8897], Loss: 5.6353\n",
      "Epoch [1/1], Step [5698/8897], Loss: 5.9462\n",
      "Epoch [1/1], Step [5699/8897], Loss: 5.5859\n",
      "Epoch [1/1], Step [5700/8897], Loss: 5.6519\n",
      "Epoch [1/1], Step [5701/8897], Loss: 5.5772\n",
      "Epoch [1/1], Step [5702/8897], Loss: 5.5251\n",
      "Epoch [1/1], Step [5703/8897], Loss: 5.9650\n",
      "Epoch [1/1], Step [5704/8897], Loss: 5.9111\n",
      "Epoch [1/1], Step [5705/8897], Loss: 5.7786\n",
      "Epoch [1/1], Step [5706/8897], Loss: 5.8226\n",
      "Epoch [1/1], Step [5707/8897], Loss: 5.7270\n",
      "Epoch [1/1], Step [5708/8897], Loss: 5.9328\n",
      "Epoch [1/1], Step [5709/8897], Loss: 5.7578\n",
      "Epoch [1/1], Step [5710/8897], Loss: 5.8142\n",
      "Epoch [1/1], Step [5711/8897], Loss: 5.8885\n",
      "Epoch [1/1], Step [5712/8897], Loss: 5.7892\n",
      "Epoch [1/1], Step [5713/8897], Loss: 5.6349\n",
      "Epoch [1/1], Step [5714/8897], Loss: 6.0080\n",
      "Epoch [1/1], Step [5715/8897], Loss: 5.5226\n",
      "Epoch [1/1], Step [5716/8897], Loss: 5.8845\n",
      "Epoch [1/1], Step [5717/8897], Loss: 5.6291\n",
      "Epoch [1/1], Step [5718/8897], Loss: 5.7663\n",
      "Epoch [1/1], Step [5719/8897], Loss: 5.5325\n",
      "Epoch [1/1], Step [5720/8897], Loss: 5.6709\n",
      "Epoch [1/1], Step [5721/8897], Loss: 5.9024\n",
      "Epoch [1/1], Step [5722/8897], Loss: 5.5032\n",
      "Epoch [1/1], Step [5723/8897], Loss: 5.7150\n",
      "Epoch [1/1], Step [5724/8897], Loss: 5.6329\n",
      "Epoch [1/1], Step [5725/8897], Loss: 5.7887\n",
      "Epoch [1/1], Step [5726/8897], Loss: 5.4575\n",
      "Epoch [1/1], Step [5727/8897], Loss: 5.7836\n",
      "Epoch [1/1], Step [5728/8897], Loss: 5.4903\n",
      "Epoch [1/1], Step [5729/8897], Loss: 5.7117\n",
      "Epoch [1/1], Step [5730/8897], Loss: 5.7221\n",
      "Epoch [1/1], Step [5731/8897], Loss: 5.7719\n",
      "Epoch [1/1], Step [5732/8897], Loss: 5.9926\n",
      "Epoch [1/1], Step [5733/8897], Loss: 5.3891\n",
      "Epoch [1/1], Step [5734/8897], Loss: 5.5557\n",
      "Epoch [1/1], Step [5735/8897], Loss: 5.8340\n",
      "Epoch [1/1], Step [5736/8897], Loss: 5.8904\n",
      "Epoch [1/1], Step [5737/8897], Loss: 5.6205\n",
      "Epoch [1/1], Step [5738/8897], Loss: 5.6549\n",
      "Epoch [1/1], Step [5739/8897], Loss: 5.5404\n",
      "Epoch [1/1], Step [5740/8897], Loss: 5.6073\n",
      "Epoch [1/1], Step [5741/8897], Loss: 5.6648\n",
      "Epoch [1/1], Step [5742/8897], Loss: 5.7779\n",
      "Epoch [1/1], Step [5743/8897], Loss: 5.8326\n",
      "Epoch [1/1], Step [5744/8897], Loss: 5.7728\n",
      "Epoch [1/1], Step [5745/8897], Loss: 5.7155\n",
      "Epoch [1/1], Step [5746/8897], Loss: 5.7495\n",
      "Epoch [1/1], Step [5747/8897], Loss: 5.6914\n",
      "Epoch [1/1], Step [5748/8897], Loss: 5.6850\n",
      "Epoch [1/1], Step [5749/8897], Loss: 5.8215\n",
      "Epoch [1/1], Step [5750/8897], Loss: 5.6267\n",
      "Epoch [1/1], Step [5751/8897], Loss: 5.7775\n",
      "Epoch [1/1], Step [5752/8897], Loss: 5.6706\n",
      "Epoch [1/1], Step [5753/8897], Loss: 5.8530\n",
      "Epoch [1/1], Step [5754/8897], Loss: 5.9798\n",
      "Epoch [1/1], Step [5755/8897], Loss: 5.7005\n",
      "Epoch [1/1], Step [5756/8897], Loss: 5.8544\n",
      "Epoch [1/1], Step [5757/8897], Loss: 5.6057\n",
      "Epoch [1/1], Step [5758/8897], Loss: 5.6904\n",
      "Epoch [1/1], Step [5759/8897], Loss: 5.5877\n",
      "Epoch [1/1], Step [5760/8897], Loss: 5.5239\n",
      "Epoch [1/1], Step [5761/8897], Loss: 5.5529\n",
      "Epoch [1/1], Step [5762/8897], Loss: 5.5109\n",
      "Epoch [1/1], Step [5763/8897], Loss: 5.8375\n",
      "Epoch [1/1], Step [5764/8897], Loss: 5.8566\n",
      "Epoch [1/1], Step [5765/8897], Loss: 5.7938\n",
      "Epoch [1/1], Step [5766/8897], Loss: 5.6706\n",
      "Epoch [1/1], Step [5767/8897], Loss: 5.7069\n",
      "Epoch [1/1], Step [5768/8897], Loss: 5.4542\n",
      "Epoch [1/1], Step [5769/8897], Loss: 5.8980\n",
      "Epoch [1/1], Step [5770/8897], Loss: 5.7834\n",
      "Epoch [1/1], Step [5771/8897], Loss: 5.8617\n",
      "Epoch [1/1], Step [5772/8897], Loss: 5.7126\n",
      "Epoch [1/1], Step [5773/8897], Loss: 5.6499\n",
      "Epoch [1/1], Step [5774/8897], Loss: 5.7154\n",
      "Epoch [1/1], Step [5775/8897], Loss: 5.7093\n",
      "Epoch [1/1], Step [5776/8897], Loss: 5.7309\n",
      "Epoch [1/1], Step [5777/8897], Loss: 5.6763\n",
      "Epoch [1/1], Step [5778/8897], Loss: 5.7160\n",
      "Epoch [1/1], Step [5779/8897], Loss: 5.8473\n",
      "Epoch [1/1], Step [5780/8897], Loss: 5.8306\n",
      "Epoch [1/1], Step [5781/8897], Loss: 5.7591\n",
      "Epoch [1/1], Step [5782/8897], Loss: 5.5378\n",
      "Epoch [1/1], Step [5783/8897], Loss: 5.8069\n",
      "Epoch [1/1], Step [5784/8897], Loss: 5.7466\n",
      "Epoch [1/1], Step [5785/8897], Loss: 5.7044\n",
      "Epoch [1/1], Step [5786/8897], Loss: 5.6756\n",
      "Epoch [1/1], Step [5787/8897], Loss: 5.6910\n",
      "Epoch [1/1], Step [5788/8897], Loss: 5.8270\n",
      "Epoch [1/1], Step [5789/8897], Loss: 5.8140\n",
      "Epoch [1/1], Step [5790/8897], Loss: 5.6860\n",
      "Epoch [1/1], Step [5791/8897], Loss: 5.6806\n",
      "Epoch [1/1], Step [5792/8897], Loss: 5.5937\n",
      "Epoch [1/1], Step [5793/8897], Loss: 5.7319\n",
      "Epoch [1/1], Step [5794/8897], Loss: 5.8932\n",
      "Epoch [1/1], Step [5795/8897], Loss: 5.7351\n",
      "Epoch [1/1], Step [5796/8897], Loss: 5.7470\n",
      "Epoch [1/1], Step [5797/8897], Loss: 5.7537\n",
      "Epoch [1/1], Step [5798/8897], Loss: 5.6929\n",
      "Epoch [1/1], Step [5799/8897], Loss: 5.8369\n",
      "Epoch [1/1], Step [5800/8897], Loss: 5.6687\n",
      "Epoch [1/1], Step [5801/8897], Loss: 5.4735\n",
      "Epoch [1/1], Step [5802/8897], Loss: 5.6359\n",
      "Epoch [1/1], Step [5803/8897], Loss: 5.7985\n",
      "Epoch [1/1], Step [5804/8897], Loss: 5.6501\n",
      "Epoch [1/1], Step [5805/8897], Loss: 5.5124\n",
      "Epoch [1/1], Step [5806/8897], Loss: 5.7642\n",
      "Epoch [1/1], Step [5807/8897], Loss: 5.6494\n",
      "Epoch [1/1], Step [5808/8897], Loss: 5.6172\n",
      "Epoch [1/1], Step [5809/8897], Loss: 5.6889\n",
      "Epoch [1/1], Step [5810/8897], Loss: 5.6197\n",
      "Epoch [1/1], Step [5811/8897], Loss: 5.7650\n",
      "Epoch [1/1], Step [5812/8897], Loss: 5.6076\n",
      "Epoch [1/1], Step [5813/8897], Loss: 5.6955\n",
      "Epoch [1/1], Step [5814/8897], Loss: 5.8405\n",
      "Epoch [1/1], Step [5815/8897], Loss: 5.6257\n",
      "Epoch [1/1], Step [5816/8897], Loss: 5.6675\n",
      "Epoch [1/1], Step [5817/8897], Loss: 5.6685\n",
      "Epoch [1/1], Step [5818/8897], Loss: 5.6499\n",
      "Epoch [1/1], Step [5819/8897], Loss: 5.7205\n",
      "Epoch [1/1], Step [5820/8897], Loss: 5.3861\n",
      "Epoch [1/1], Step [5821/8897], Loss: 5.4615\n",
      "Epoch [1/1], Step [5822/8897], Loss: 5.6571\n",
      "Epoch [1/1], Step [5823/8897], Loss: 5.7696\n",
      "Epoch [1/1], Step [5824/8897], Loss: 5.8379\n",
      "Epoch [1/1], Step [5825/8897], Loss: 5.6723\n",
      "Epoch [1/1], Step [5826/8897], Loss: 5.6841\n",
      "Epoch [1/1], Step [5827/8897], Loss: 5.6476\n",
      "Epoch [1/1], Step [5828/8897], Loss: 5.8405\n",
      "Epoch [1/1], Step [5829/8897], Loss: 5.7660\n",
      "Epoch [1/1], Step [5830/8897], Loss: 5.6032\n",
      "Epoch [1/1], Step [5831/8897], Loss: 5.9377\n",
      "Epoch [1/1], Step [5832/8897], Loss: 5.7119\n",
      "Epoch [1/1], Step [5833/8897], Loss: 5.5317\n",
      "Epoch [1/1], Step [5834/8897], Loss: 5.7313\n",
      "Epoch [1/1], Step [5835/8897], Loss: 5.5146\n",
      "Epoch [1/1], Step [5836/8897], Loss: 5.6646\n",
      "Epoch [1/1], Step [5837/8897], Loss: 5.8511\n",
      "Epoch [1/1], Step [5838/8897], Loss: 5.6254\n",
      "Epoch [1/1], Step [5839/8897], Loss: 5.6723\n",
      "Epoch [1/1], Step [5840/8897], Loss: 5.7341\n",
      "Epoch [1/1], Step [5841/8897], Loss: 5.4800\n",
      "Epoch [1/1], Step [5842/8897], Loss: 5.7072\n",
      "Epoch [1/1], Step [5843/8897], Loss: 5.5062\n",
      "Epoch [1/1], Step [5844/8897], Loss: 5.6796\n",
      "Epoch [1/1], Step [5845/8897], Loss: 5.5721\n",
      "Epoch [1/1], Step [5846/8897], Loss: 5.4542\n",
      "Epoch [1/1], Step [5847/8897], Loss: 5.6570\n",
      "Epoch [1/1], Step [5848/8897], Loss: 5.8104\n",
      "Epoch [1/1], Step [5849/8897], Loss: 5.5229\n",
      "Epoch [1/1], Step [5850/8897], Loss: 5.5961\n",
      "Epoch [1/1], Step [5851/8897], Loss: 5.6287\n",
      "Epoch [1/1], Step [5852/8897], Loss: 5.6629\n",
      "Epoch [1/1], Step [5853/8897], Loss: 5.9814\n",
      "Epoch [1/1], Step [5854/8897], Loss: 5.8186\n",
      "Epoch [1/1], Step [5855/8897], Loss: 5.6733\n",
      "Epoch [1/1], Step [5856/8897], Loss: 5.8193\n",
      "Epoch [1/1], Step [5857/8897], Loss: 5.6315\n",
      "Epoch [1/1], Step [5858/8897], Loss: 5.5803\n",
      "Epoch [1/1], Step [5859/8897], Loss: 5.7236\n",
      "Epoch [1/1], Step [5860/8897], Loss: 5.6053\n",
      "Epoch [1/1], Step [5861/8897], Loss: 5.6308\n",
      "Epoch [1/1], Step [5862/8897], Loss: 5.6388\n",
      "Epoch [1/1], Step [5863/8897], Loss: 5.7991\n",
      "Epoch [1/1], Step [5864/8897], Loss: 5.7548\n",
      "Epoch [1/1], Step [5865/8897], Loss: 5.5804\n",
      "Epoch [1/1], Step [5866/8897], Loss: 5.7089\n",
      "Epoch [1/1], Step [5867/8897], Loss: 5.8691\n",
      "Epoch [1/1], Step [5868/8897], Loss: 5.7710\n",
      "Epoch [1/1], Step [5869/8897], Loss: 5.8463\n",
      "Epoch [1/1], Step [5870/8897], Loss: 5.6681\n",
      "Epoch [1/1], Step [5871/8897], Loss: 5.6593\n",
      "Epoch [1/1], Step [5872/8897], Loss: 5.7762\n",
      "Epoch [1/1], Step [5873/8897], Loss: 5.6472\n",
      "Epoch [1/1], Step [5874/8897], Loss: 5.6222\n",
      "Epoch [1/1], Step [5875/8897], Loss: 5.7710\n",
      "Epoch [1/1], Step [5876/8897], Loss: 5.4040\n",
      "Epoch [1/1], Step [5877/8897], Loss: 5.6447\n",
      "Epoch [1/1], Step [5878/8897], Loss: 5.6910\n",
      "Epoch [1/1], Step [5879/8897], Loss: 5.7303\n",
      "Epoch [1/1], Step [5880/8897], Loss: 5.5451\n",
      "Epoch [1/1], Step [5881/8897], Loss: 5.8123\n",
      "Epoch [1/1], Step [5882/8897], Loss: 6.0657\n",
      "Epoch [1/1], Step [5883/8897], Loss: 5.8108\n",
      "Epoch [1/1], Step [5884/8897], Loss: 5.7321\n",
      "Epoch [1/1], Step [5885/8897], Loss: 5.7788\n",
      "Epoch [1/1], Step [5886/8897], Loss: 5.7379\n",
      "Epoch [1/1], Step [5887/8897], Loss: 5.5176\n",
      "Epoch [1/1], Step [5888/8897], Loss: 5.9178\n",
      "Epoch [1/1], Step [5889/8897], Loss: 5.4939\n",
      "Epoch [1/1], Step [5890/8897], Loss: 5.7789\n",
      "Epoch [1/1], Step [5891/8897], Loss: 5.9355\n",
      "Epoch [1/1], Step [5892/8897], Loss: 5.8200\n",
      "Epoch [1/1], Step [5893/8897], Loss: 5.5849\n",
      "Epoch [1/1], Step [5894/8897], Loss: 5.8148\n",
      "Epoch [1/1], Step [5895/8897], Loss: 5.7463\n",
      "Epoch [1/1], Step [5896/8897], Loss: 5.8926\n",
      "Epoch [1/1], Step [5897/8897], Loss: 5.8617\n",
      "Epoch [1/1], Step [5898/8897], Loss: 5.7838\n",
      "Epoch [1/1], Step [5899/8897], Loss: 5.7537\n",
      "Epoch [1/1], Step [5900/8897], Loss: 5.5395\n",
      "Epoch [1/1], Step [5901/8897], Loss: 5.6127\n",
      "Epoch [1/1], Step [5902/8897], Loss: 5.7962\n",
      "Epoch [1/1], Step [5903/8897], Loss: 6.0762\n",
      "Epoch [1/1], Step [5904/8897], Loss: 5.6802\n",
      "Epoch [1/1], Step [5905/8897], Loss: 5.8433\n",
      "Epoch [1/1], Step [5906/8897], Loss: 5.7042\n",
      "Epoch [1/1], Step [5907/8897], Loss: 5.4393\n",
      "Epoch [1/1], Step [5908/8897], Loss: 5.5710\n",
      "Epoch [1/1], Step [5909/8897], Loss: 5.5323\n",
      "Epoch [1/1], Step [5910/8897], Loss: 5.7221\n",
      "Epoch [1/1], Step [5911/8897], Loss: 5.6064\n",
      "Epoch [1/1], Step [5912/8897], Loss: 5.6425\n",
      "Epoch [1/1], Step [5913/8897], Loss: 5.6706\n",
      "Epoch [1/1], Step [5914/8897], Loss: 5.7253\n",
      "Epoch [1/1], Step [5915/8897], Loss: 5.9115\n",
      "Epoch [1/1], Step [5916/8897], Loss: 5.4631\n",
      "Epoch [1/1], Step [5917/8897], Loss: 5.5312\n",
      "Epoch [1/1], Step [5918/8897], Loss: 5.7212\n",
      "Epoch [1/1], Step [5919/8897], Loss: 5.6169\n",
      "Epoch [1/1], Step [5920/8897], Loss: 5.6310\n",
      "Epoch [1/1], Step [5921/8897], Loss: 5.7761\n",
      "Epoch [1/1], Step [5922/8897], Loss: 5.7733\n",
      "Epoch [1/1], Step [5923/8897], Loss: 5.5692\n",
      "Epoch [1/1], Step [5924/8897], Loss: 5.7574\n",
      "Epoch [1/1], Step [5925/8897], Loss: 5.5274\n",
      "Epoch [1/1], Step [5926/8897], Loss: 5.8227\n",
      "Epoch [1/1], Step [5927/8897], Loss: 5.5990\n",
      "Epoch [1/1], Step [5928/8897], Loss: 5.8779\n",
      "Epoch [1/1], Step [5929/8897], Loss: 5.4563\n",
      "Epoch [1/1], Step [5930/8897], Loss: 5.9071\n",
      "Epoch [1/1], Step [5931/8897], Loss: 5.6813\n",
      "Epoch [1/1], Step [5932/8897], Loss: 5.6913\n",
      "Epoch [1/1], Step [5933/8897], Loss: 5.7360\n",
      "Epoch [1/1], Step [5934/8897], Loss: 5.8203\n",
      "Epoch [1/1], Step [5935/8897], Loss: 5.4819\n",
      "Epoch [1/1], Step [5936/8897], Loss: 5.7191\n",
      "Epoch [1/1], Step [5937/8897], Loss: 5.6880\n",
      "Epoch [1/1], Step [5938/8897], Loss: 5.5956\n",
      "Epoch [1/1], Step [5939/8897], Loss: 5.6609\n",
      "Epoch [1/1], Step [5940/8897], Loss: 5.7018\n",
      "Epoch [1/1], Step [5941/8897], Loss: 5.5671\n",
      "Epoch [1/1], Step [5942/8897], Loss: 5.4412\n",
      "Epoch [1/1], Step [5943/8897], Loss: 5.5008\n",
      "Epoch [1/1], Step [5944/8897], Loss: 5.4705\n",
      "Epoch [1/1], Step [5945/8897], Loss: 5.8279\n",
      "Epoch [1/1], Step [5946/8897], Loss: 5.5043\n",
      "Epoch [1/1], Step [5947/8897], Loss: 5.6426\n",
      "Epoch [1/1], Step [5948/8897], Loss: 5.7483\n",
      "Epoch [1/1], Step [5949/8897], Loss: 5.5451\n",
      "Epoch [1/1], Step [5950/8897], Loss: 5.6279\n",
      "Epoch [1/1], Step [5951/8897], Loss: 5.5510\n",
      "Epoch [1/1], Step [5952/8897], Loss: 5.7385\n",
      "Epoch [1/1], Step [5953/8897], Loss: 5.8456\n",
      "Epoch [1/1], Step [5954/8897], Loss: 5.5611\n",
      "Epoch [1/1], Step [5955/8897], Loss: 5.5498\n",
      "Epoch [1/1], Step [5956/8897], Loss: 5.4478\n",
      "Epoch [1/1], Step [5957/8897], Loss: 5.5259\n",
      "Epoch [1/1], Step [5958/8897], Loss: 5.8216\n",
      "Epoch [1/1], Step [5959/8897], Loss: 5.7007\n",
      "Epoch [1/1], Step [5960/8897], Loss: 5.7487\n",
      "Epoch [1/1], Step [5961/8897], Loss: 5.6107\n",
      "Epoch [1/1], Step [5962/8897], Loss: 5.6883\n",
      "Epoch [1/1], Step [5963/8897], Loss: 5.6890\n",
      "Epoch [1/1], Step [5964/8897], Loss: 5.7347\n",
      "Epoch [1/1], Step [5965/8897], Loss: 5.5978\n",
      "Epoch [1/1], Step [5966/8897], Loss: 5.4789\n",
      "Epoch [1/1], Step [5967/8897], Loss: 5.7516\n",
      "Epoch [1/1], Step [5968/8897], Loss: 5.7979\n",
      "Epoch [1/1], Step [5969/8897], Loss: 5.8622\n",
      "Epoch [1/1], Step [5970/8897], Loss: 5.7398\n",
      "Epoch [1/1], Step [5971/8897], Loss: 5.9244\n",
      "Epoch [1/1], Step [5972/8897], Loss: 5.7894\n",
      "Epoch [1/1], Step [5973/8897], Loss: 5.6336\n",
      "Epoch [1/1], Step [5974/8897], Loss: 5.6074\n",
      "Epoch [1/1], Step [5975/8897], Loss: 5.5522\n",
      "Epoch [1/1], Step [5976/8897], Loss: 6.0516\n",
      "Epoch [1/1], Step [5977/8897], Loss: 5.5953\n",
      "Epoch [1/1], Step [5978/8897], Loss: 5.6698\n",
      "Epoch [1/1], Step [5979/8897], Loss: 5.4459\n",
      "Epoch [1/1], Step [5980/8897], Loss: 5.5078\n",
      "Epoch [1/1], Step [5981/8897], Loss: 5.5551\n",
      "Epoch [1/1], Step [5982/8897], Loss: 5.8627\n",
      "Epoch [1/1], Step [5983/8897], Loss: 5.9084\n",
      "Epoch [1/1], Step [5984/8897], Loss: 5.6462\n",
      "Epoch [1/1], Step [5985/8897], Loss: 5.9724\n",
      "Epoch [1/1], Step [5986/8897], Loss: 5.5821\n",
      "Epoch [1/1], Step [5987/8897], Loss: 5.9169\n",
      "Epoch [1/1], Step [5988/8897], Loss: 5.9463\n",
      "Epoch [1/1], Step [5989/8897], Loss: 5.7902\n",
      "Epoch [1/1], Step [5990/8897], Loss: 5.4754\n",
      "Epoch [1/1], Step [5991/8897], Loss: 5.8334\n",
      "Epoch [1/1], Step [5992/8897], Loss: 5.4809\n",
      "Epoch [1/1], Step [5993/8897], Loss: 5.7806\n",
      "Epoch [1/1], Step [5994/8897], Loss: 5.7753\n",
      "Epoch [1/1], Step [5995/8897], Loss: 5.5908\n",
      "Epoch [1/1], Step [5996/8897], Loss: 5.7510\n",
      "Epoch [1/1], Step [5997/8897], Loss: 5.7118\n",
      "Epoch [1/1], Step [5998/8897], Loss: 5.6504\n",
      "Epoch [1/1], Step [5999/8897], Loss: 5.7267\n",
      "Epoch [1/1], Step [6000/8897], Loss: 5.4160\n",
      "Epoch [1/1], Step [6001/8897], Loss: 5.4370\n",
      "Epoch [1/1], Step [6002/8897], Loss: 5.5378\n",
      "Epoch [1/1], Step [6003/8897], Loss: 5.4225\n",
      "Epoch [1/1], Step [6004/8897], Loss: 5.9278\n",
      "Epoch [1/1], Step [6005/8897], Loss: 5.7538\n",
      "Epoch [1/1], Step [6006/8897], Loss: 5.8456\n",
      "Epoch [1/1], Step [6007/8897], Loss: 5.7211\n",
      "Epoch [1/1], Step [6008/8897], Loss: 5.7780\n",
      "Epoch [1/1], Step [6009/8897], Loss: 5.6341\n",
      "Epoch [1/1], Step [6010/8897], Loss: 5.6563\n",
      "Epoch [1/1], Step [6011/8897], Loss: 5.8354\n",
      "Epoch [1/1], Step [6012/8897], Loss: 5.8429\n",
      "Epoch [1/1], Step [6013/8897], Loss: 5.7225\n",
      "Epoch [1/1], Step [6014/8897], Loss: 5.6400\n",
      "Epoch [1/1], Step [6015/8897], Loss: 6.0076\n",
      "Epoch [1/1], Step [6016/8897], Loss: 5.6687\n",
      "Epoch [1/1], Step [6017/8897], Loss: 5.6852\n",
      "Epoch [1/1], Step [6018/8897], Loss: 5.8144\n",
      "Epoch [1/1], Step [6019/8897], Loss: 5.6704\n",
      "Epoch [1/1], Step [6020/8897], Loss: 5.5487\n",
      "Epoch [1/1], Step [6021/8897], Loss: 5.8675\n",
      "Epoch [1/1], Step [6022/8897], Loss: 5.7926\n",
      "Epoch [1/1], Step [6023/8897], Loss: 5.7116\n",
      "Epoch [1/1], Step [6024/8897], Loss: 5.5334\n",
      "Epoch [1/1], Step [6025/8897], Loss: 5.8319\n",
      "Epoch [1/1], Step [6026/8897], Loss: 5.8564\n",
      "Epoch [1/1], Step [6027/8897], Loss: 5.7355\n",
      "Epoch [1/1], Step [6028/8897], Loss: 5.8457\n",
      "Epoch [1/1], Step [6029/8897], Loss: 5.8136\n",
      "Epoch [1/1], Step [6030/8897], Loss: 5.5079\n",
      "Epoch [1/1], Step [6031/8897], Loss: 5.8297\n",
      "Epoch [1/1], Step [6032/8897], Loss: 5.6559\n",
      "Epoch [1/1], Step [6033/8897], Loss: 5.7047\n",
      "Epoch [1/1], Step [6034/8897], Loss: 5.7058\n",
      "Epoch [1/1], Step [6035/8897], Loss: 5.5175\n",
      "Epoch [1/1], Step [6036/8897], Loss: 5.7187\n",
      "Epoch [1/1], Step [6037/8897], Loss: 5.8700\n",
      "Epoch [1/1], Step [6038/8897], Loss: 5.6892\n",
      "Epoch [1/1], Step [6039/8897], Loss: 5.8614\n",
      "Epoch [1/1], Step [6040/8897], Loss: 5.6519\n",
      "Epoch [1/1], Step [6041/8897], Loss: 5.7201\n",
      "Epoch [1/1], Step [6042/8897], Loss: 5.6128\n",
      "Epoch [1/1], Step [6043/8897], Loss: 5.8611\n",
      "Epoch [1/1], Step [6044/8897], Loss: 5.6127\n",
      "Epoch [1/1], Step [6045/8897], Loss: 5.5707\n",
      "Epoch [1/1], Step [6046/8897], Loss: 5.6650\n",
      "Epoch [1/1], Step [6047/8897], Loss: 5.4710\n",
      "Epoch [1/1], Step [6048/8897], Loss: 5.6261\n",
      "Epoch [1/1], Step [6049/8897], Loss: 6.0072\n",
      "Epoch [1/1], Step [6050/8897], Loss: 5.6081\n",
      "Epoch [1/1], Step [6051/8897], Loss: 5.5216\n",
      "Epoch [1/1], Step [6052/8897], Loss: 5.7343\n",
      "Epoch [1/1], Step [6053/8897], Loss: 5.6616\n",
      "Epoch [1/1], Step [6054/8897], Loss: 5.8251\n",
      "Epoch [1/1], Step [6055/8897], Loss: 5.6759\n",
      "Epoch [1/1], Step [6056/8897], Loss: 5.7052\n",
      "Epoch [1/1], Step [6057/8897], Loss: 5.5935\n",
      "Epoch [1/1], Step [6058/8897], Loss: 5.6919\n",
      "Epoch [1/1], Step [6059/8897], Loss: 5.5617\n",
      "Epoch [1/1], Step [6060/8897], Loss: 5.8168\n",
      "Epoch [1/1], Step [6061/8897], Loss: 5.6853\n",
      "Epoch [1/1], Step [6062/8897], Loss: 5.6360\n",
      "Epoch [1/1], Step [6063/8897], Loss: 6.0226\n",
      "Epoch [1/1], Step [6064/8897], Loss: 5.8278\n",
      "Epoch [1/1], Step [6065/8897], Loss: 5.4693\n",
      "Epoch [1/1], Step [6066/8897], Loss: 5.5812\n",
      "Epoch [1/1], Step [6067/8897], Loss: 5.4885\n",
      "Epoch [1/1], Step [6068/8897], Loss: 5.5546\n",
      "Epoch [1/1], Step [6069/8897], Loss: 5.6643\n",
      "Epoch [1/1], Step [6070/8897], Loss: 5.8098\n",
      "Epoch [1/1], Step [6071/8897], Loss: 5.6230\n",
      "Epoch [1/1], Step [6072/8897], Loss: 5.4186\n",
      "Epoch [1/1], Step [6073/8897], Loss: 5.6344\n",
      "Epoch [1/1], Step [6074/8897], Loss: 5.7906\n",
      "Epoch [1/1], Step [6075/8897], Loss: 5.5390\n",
      "Epoch [1/1], Step [6076/8897], Loss: 5.9359\n",
      "Epoch [1/1], Step [6077/8897], Loss: 5.6094\n",
      "Epoch [1/1], Step [6078/8897], Loss: 5.6813\n",
      "Epoch [1/1], Step [6079/8897], Loss: 5.8210\n",
      "Epoch [1/1], Step [6080/8897], Loss: 5.6436\n",
      "Epoch [1/1], Step [6081/8897], Loss: 5.8268\n",
      "Epoch [1/1], Step [6082/8897], Loss: 5.4497\n",
      "Epoch [1/1], Step [6083/8897], Loss: 5.5734\n",
      "Epoch [1/1], Step [6084/8897], Loss: 5.7468\n",
      "Epoch [1/1], Step [6085/8897], Loss: 5.5313\n",
      "Epoch [1/1], Step [6086/8897], Loss: 5.5962\n",
      "Epoch [1/1], Step [6087/8897], Loss: 5.9592\n",
      "Epoch [1/1], Step [6088/8897], Loss: 5.4388\n",
      "Epoch [1/1], Step [6089/8897], Loss: 5.6087\n",
      "Epoch [1/1], Step [6090/8897], Loss: 5.6108\n",
      "Epoch [1/1], Step [6091/8897], Loss: 5.7358\n",
      "Epoch [1/1], Step [6092/8897], Loss: 5.6418\n",
      "Epoch [1/1], Step [6093/8897], Loss: 5.7588\n",
      "Epoch [1/1], Step [6094/8897], Loss: 5.5468\n",
      "Epoch [1/1], Step [6095/8897], Loss: 5.6707\n",
      "Epoch [1/1], Step [6096/8897], Loss: 5.7482\n",
      "Epoch [1/1], Step [6097/8897], Loss: 5.4707\n",
      "Epoch [1/1], Step [6098/8897], Loss: 5.6450\n",
      "Epoch [1/1], Step [6099/8897], Loss: 5.7247\n",
      "Epoch [1/1], Step [6100/8897], Loss: 5.8376\n",
      "Epoch [1/1], Step [6101/8897], Loss: 5.6809\n",
      "Epoch [1/1], Step [6102/8897], Loss: 5.6619\n",
      "Epoch [1/1], Step [6103/8897], Loss: 5.6053\n",
      "Epoch [1/1], Step [6104/8897], Loss: 5.8576\n",
      "Epoch [1/1], Step [6105/8897], Loss: 5.5594\n",
      "Epoch [1/1], Step [6106/8897], Loss: 5.7767\n",
      "Epoch [1/1], Step [6107/8897], Loss: 5.6002\n",
      "Epoch [1/1], Step [6108/8897], Loss: 5.6766\n",
      "Epoch [1/1], Step [6109/8897], Loss: 5.7043\n",
      "Epoch [1/1], Step [6110/8897], Loss: 5.5165\n",
      "Epoch [1/1], Step [6111/8897], Loss: 5.4937\n",
      "Epoch [1/1], Step [6112/8897], Loss: 5.6053\n",
      "Epoch [1/1], Step [6113/8897], Loss: 5.7429\n",
      "Epoch [1/1], Step [6114/8897], Loss: 5.6847\n",
      "Epoch [1/1], Step [6115/8897], Loss: 5.5735\n",
      "Epoch [1/1], Step [6116/8897], Loss: 5.5467\n",
      "Epoch [1/1], Step [6117/8897], Loss: 5.5338\n",
      "Epoch [1/1], Step [6118/8897], Loss: 5.8516\n",
      "Epoch [1/1], Step [6119/8897], Loss: 5.7562\n",
      "Epoch [1/1], Step [6120/8897], Loss: 5.6373\n",
      "Epoch [1/1], Step [6121/8897], Loss: 5.6166\n",
      "Epoch [1/1], Step [6122/8897], Loss: 5.6673\n",
      "Epoch [1/1], Step [6123/8897], Loss: 5.6979\n",
      "Epoch [1/1], Step [6124/8897], Loss: 5.6107\n",
      "Epoch [1/1], Step [6125/8897], Loss: 5.7421\n",
      "Epoch [1/1], Step [6126/8897], Loss: 5.7526\n",
      "Epoch [1/1], Step [6127/8897], Loss: 5.8951\n",
      "Epoch [1/1], Step [6128/8897], Loss: 5.8519\n",
      "Epoch [1/1], Step [6129/8897], Loss: 5.8251\n",
      "Epoch [1/1], Step [6130/8897], Loss: 5.6539\n",
      "Epoch [1/1], Step [6131/8897], Loss: 5.6487\n",
      "Epoch [1/1], Step [6132/8897], Loss: 5.7420\n",
      "Epoch [1/1], Step [6133/8897], Loss: 5.8094\n",
      "Epoch [1/1], Step [6134/8897], Loss: 5.6278\n",
      "Epoch [1/1], Step [6135/8897], Loss: 5.4536\n",
      "Epoch [1/1], Step [6136/8897], Loss: 5.8837\n",
      "Epoch [1/1], Step [6137/8897], Loss: 5.7812\n",
      "Epoch [1/1], Step [6138/8897], Loss: 5.5119\n",
      "Epoch [1/1], Step [6139/8897], Loss: 5.5670\n",
      "Epoch [1/1], Step [6140/8897], Loss: 5.6773\n",
      "Epoch [1/1], Step [6141/8897], Loss: 5.5640\n",
      "Epoch [1/1], Step [6142/8897], Loss: 5.9250\n",
      "Epoch [1/1], Step [6143/8897], Loss: 5.6342\n",
      "Epoch [1/1], Step [6144/8897], Loss: 5.8438\n",
      "Epoch [1/1], Step [6145/8897], Loss: 5.7770\n",
      "Epoch [1/1], Step [6146/8897], Loss: 5.7404\n",
      "Epoch [1/1], Step [6147/8897], Loss: 5.5731\n",
      "Epoch [1/1], Step [6148/8897], Loss: 5.7032\n",
      "Epoch [1/1], Step [6149/8897], Loss: 5.6454\n",
      "Epoch [1/1], Step [6150/8897], Loss: 5.5985\n",
      "Epoch [1/1], Step [6151/8897], Loss: 5.5630\n",
      "Epoch [1/1], Step [6152/8897], Loss: 5.6325\n",
      "Epoch [1/1], Step [6153/8897], Loss: 5.7214\n",
      "Epoch [1/1], Step [6154/8897], Loss: 5.6881\n",
      "Epoch [1/1], Step [6155/8897], Loss: 5.6689\n",
      "Epoch [1/1], Step [6156/8897], Loss: 5.6132\n",
      "Epoch [1/1], Step [6157/8897], Loss: 5.6889\n",
      "Epoch [1/1], Step [6158/8897], Loss: 5.4488\n",
      "Epoch [1/1], Step [6159/8897], Loss: 5.8311\n",
      "Epoch [1/1], Step [6160/8897], Loss: 5.7045\n",
      "Epoch [1/1], Step [6161/8897], Loss: 5.6300\n",
      "Epoch [1/1], Step [6162/8897], Loss: 5.4099\n",
      "Epoch [1/1], Step [6163/8897], Loss: 5.6968\n",
      "Epoch [1/1], Step [6164/8897], Loss: 5.7207\n",
      "Epoch [1/1], Step [6165/8897], Loss: 5.6556\n",
      "Epoch [1/1], Step [6166/8897], Loss: 5.5021\n",
      "Epoch [1/1], Step [6167/8897], Loss: 5.6416\n",
      "Epoch [1/1], Step [6168/8897], Loss: 5.6983\n",
      "Epoch [1/1], Step [6169/8897], Loss: 5.8087\n",
      "Epoch [1/1], Step [6170/8897], Loss: 5.7439\n",
      "Epoch [1/1], Step [6171/8897], Loss: 5.8044\n",
      "Epoch [1/1], Step [6172/8897], Loss: 5.8788\n",
      "Epoch [1/1], Step [6173/8897], Loss: 5.6032\n",
      "Epoch [1/1], Step [6174/8897], Loss: 5.6139\n",
      "Epoch [1/1], Step [6175/8897], Loss: 5.7662\n",
      "Epoch [1/1], Step [6176/8897], Loss: 5.5243\n",
      "Epoch [1/1], Step [6177/8897], Loss: 5.7256\n",
      "Epoch [1/1], Step [6178/8897], Loss: 5.8901\n",
      "Epoch [1/1], Step [6179/8897], Loss: 5.5755\n",
      "Epoch [1/1], Step [6180/8897], Loss: 5.6681\n",
      "Epoch [1/1], Step [6181/8897], Loss: 5.8056\n",
      "Epoch [1/1], Step [6182/8897], Loss: 5.7387\n",
      "Epoch [1/1], Step [6183/8897], Loss: 5.6111\n",
      "Epoch [1/1], Step [6184/8897], Loss: 5.7558\n",
      "Epoch [1/1], Step [6185/8897], Loss: 5.8013\n",
      "Epoch [1/1], Step [6186/8897], Loss: 5.4379\n",
      "Epoch [1/1], Step [6187/8897], Loss: 5.6198\n",
      "Epoch [1/1], Step [6188/8897], Loss: 5.4115\n",
      "Epoch [1/1], Step [6189/8897], Loss: 5.6777\n",
      "Epoch [1/1], Step [6190/8897], Loss: 5.5096\n",
      "Epoch [1/1], Step [6191/8897], Loss: 5.6481\n",
      "Epoch [1/1], Step [6192/8897], Loss: 5.5314\n",
      "Epoch [1/1], Step [6193/8897], Loss: 5.8812\n",
      "Epoch [1/1], Step [6194/8897], Loss: 5.5382\n",
      "Epoch [1/1], Step [6195/8897], Loss: 5.5535\n",
      "Epoch [1/1], Step [6196/8897], Loss: 5.7645\n",
      "Epoch [1/1], Step [6197/8897], Loss: 5.7825\n",
      "Epoch [1/1], Step [6198/8897], Loss: 5.4948\n",
      "Epoch [1/1], Step [6199/8897], Loss: 5.6988\n",
      "Epoch [1/1], Step [6200/8897], Loss: 5.8749\n",
      "Epoch [1/1], Step [6201/8897], Loss: 5.6614\n",
      "Epoch [1/1], Step [6202/8897], Loss: 5.6361\n",
      "Epoch [1/1], Step [6203/8897], Loss: 5.7649\n",
      "Epoch [1/1], Step [6204/8897], Loss: 5.8325\n",
      "Epoch [1/1], Step [6205/8897], Loss: 5.7683\n",
      "Epoch [1/1], Step [6206/8897], Loss: 5.4173\n",
      "Epoch [1/1], Step [6207/8897], Loss: 5.6559\n",
      "Epoch [1/1], Step [6208/8897], Loss: 5.7067\n",
      "Epoch [1/1], Step [6209/8897], Loss: 5.5274\n",
      "Epoch [1/1], Step [6210/8897], Loss: 5.7634\n",
      "Epoch [1/1], Step [6211/8897], Loss: 5.6747\n",
      "Epoch [1/1], Step [6212/8897], Loss: 5.6922\n",
      "Epoch [1/1], Step [6213/8897], Loss: 5.4216\n",
      "Epoch [1/1], Step [6214/8897], Loss: 5.6823\n",
      "Epoch [1/1], Step [6215/8897], Loss: 5.5545\n",
      "Epoch [1/1], Step [6216/8897], Loss: 5.9615\n",
      "Epoch [1/1], Step [6217/8897], Loss: 5.7127\n",
      "Epoch [1/1], Step [6218/8897], Loss: 5.5963\n",
      "Epoch [1/1], Step [6219/8897], Loss: 5.7203\n",
      "Epoch [1/1], Step [6220/8897], Loss: 5.7258\n",
      "Epoch [1/1], Step [6221/8897], Loss: 5.8397\n",
      "Epoch [1/1], Step [6222/8897], Loss: 5.6392\n",
      "Epoch [1/1], Step [6223/8897], Loss: 5.6083\n",
      "Epoch [1/1], Step [6224/8897], Loss: 5.6832\n",
      "Epoch [1/1], Step [6225/8897], Loss: 5.4604\n",
      "Epoch [1/1], Step [6226/8897], Loss: 5.7259\n",
      "Epoch [1/1], Step [6227/8897], Loss: 5.5868\n",
      "Epoch [1/1], Step [6228/8897], Loss: 5.5891\n",
      "Epoch [1/1], Step [6229/8897], Loss: 5.7389\n",
      "Epoch [1/1], Step [6230/8897], Loss: 5.3678\n",
      "Epoch [1/1], Step [6231/8897], Loss: 5.3923\n",
      "Epoch [1/1], Step [6232/8897], Loss: 5.5986\n",
      "Epoch [1/1], Step [6233/8897], Loss: 5.5384\n",
      "Epoch [1/1], Step [6234/8897], Loss: 5.5964\n",
      "Epoch [1/1], Step [6235/8897], Loss: 5.5676\n",
      "Epoch [1/1], Step [6236/8897], Loss: 5.5737\n",
      "Epoch [1/1], Step [6237/8897], Loss: 5.7880\n",
      "Epoch [1/1], Step [6238/8897], Loss: 5.5964\n",
      "Epoch [1/1], Step [6239/8897], Loss: 5.4646\n",
      "Epoch [1/1], Step [6240/8897], Loss: 5.5848\n",
      "Epoch [1/1], Step [6241/8897], Loss: 5.6336\n",
      "Epoch [1/1], Step [6242/8897], Loss: 5.6817\n",
      "Epoch [1/1], Step [6243/8897], Loss: 5.8261\n",
      "Epoch [1/1], Step [6244/8897], Loss: 5.6213\n",
      "Epoch [1/1], Step [6245/8897], Loss: 5.7466\n",
      "Epoch [1/1], Step [6246/8897], Loss: 5.4531\n",
      "Epoch [1/1], Step [6247/8897], Loss: 5.7074\n",
      "Epoch [1/1], Step [6248/8897], Loss: 5.3860\n",
      "Epoch [1/1], Step [6249/8897], Loss: 5.5065\n",
      "Epoch [1/1], Step [6250/8897], Loss: 5.3674\n",
      "Epoch [1/1], Step [6251/8897], Loss: 5.5048\n",
      "Epoch [1/1], Step [6252/8897], Loss: 5.5217\n",
      "Epoch [1/1], Step [6253/8897], Loss: 5.6854\n",
      "Epoch [1/1], Step [6254/8897], Loss: 5.6552\n",
      "Epoch [1/1], Step [6255/8897], Loss: 5.7237\n",
      "Epoch [1/1], Step [6256/8897], Loss: 5.5366\n",
      "Epoch [1/1], Step [6257/8897], Loss: 5.6610\n",
      "Epoch [1/1], Step [6258/8897], Loss: 5.6599\n",
      "Epoch [1/1], Step [6259/8897], Loss: 5.5320\n",
      "Epoch [1/1], Step [6260/8897], Loss: 5.8248\n",
      "Epoch [1/1], Step [6261/8897], Loss: 5.6268\n",
      "Epoch [1/1], Step [6262/8897], Loss: 5.6542\n",
      "Epoch [1/1], Step [6263/8897], Loss: 5.4779\n",
      "Epoch [1/1], Step [6264/8897], Loss: 5.7460\n",
      "Epoch [1/1], Step [6265/8897], Loss: 5.6022\n",
      "Epoch [1/1], Step [6266/8897], Loss: 5.6234\n",
      "Epoch [1/1], Step [6267/8897], Loss: 5.5814\n",
      "Epoch [1/1], Step [6268/8897], Loss: 5.8701\n",
      "Epoch [1/1], Step [6269/8897], Loss: 5.4551\n",
      "Epoch [1/1], Step [6270/8897], Loss: 5.7056\n",
      "Epoch [1/1], Step [6271/8897], Loss: 5.4246\n",
      "Epoch [1/1], Step [6272/8897], Loss: 5.7646\n",
      "Epoch [1/1], Step [6273/8897], Loss: 5.3673\n",
      "Epoch [1/1], Step [6274/8897], Loss: 5.7395\n",
      "Epoch [1/1], Step [6275/8897], Loss: 5.7290\n",
      "Epoch [1/1], Step [6276/8897], Loss: 5.5351\n",
      "Epoch [1/1], Step [6277/8897], Loss: 5.5413\n",
      "Epoch [1/1], Step [6278/8897], Loss: 5.7281\n",
      "Epoch [1/1], Step [6279/8897], Loss: 5.8684\n",
      "Epoch [1/1], Step [6280/8897], Loss: 5.6616\n",
      "Epoch [1/1], Step [6281/8897], Loss: 5.6953\n",
      "Epoch [1/1], Step [6282/8897], Loss: 5.3697\n",
      "Epoch [1/1], Step [6283/8897], Loss: 5.5654\n",
      "Epoch [1/1], Step [6284/8897], Loss: 5.5608\n",
      "Epoch [1/1], Step [6285/8897], Loss: 5.5298\n",
      "Epoch [1/1], Step [6286/8897], Loss: 5.8336\n",
      "Epoch [1/1], Step [6287/8897], Loss: 5.7714\n",
      "Epoch [1/1], Step [6288/8897], Loss: 5.5645\n",
      "Epoch [1/1], Step [6289/8897], Loss: 5.8794\n",
      "Epoch [1/1], Step [6290/8897], Loss: 5.5000\n",
      "Epoch [1/1], Step [6291/8897], Loss: 5.5345\n",
      "Epoch [1/1], Step [6292/8897], Loss: 5.5594\n",
      "Epoch [1/1], Step [6293/8897], Loss: 5.4977\n",
      "Epoch [1/1], Step [6294/8897], Loss: 5.8017\n",
      "Epoch [1/1], Step [6295/8897], Loss: 5.5923\n",
      "Epoch [1/1], Step [6296/8897], Loss: 5.4985\n",
      "Epoch [1/1], Step [6297/8897], Loss: 5.6880\n",
      "Epoch [1/1], Step [6298/8897], Loss: 5.7795\n",
      "Epoch [1/1], Step [6299/8897], Loss: 5.8449\n",
      "Epoch [1/1], Step [6300/8897], Loss: 5.5161\n",
      "Epoch [1/1], Step [6301/8897], Loss: 5.5567\n",
      "Epoch [1/1], Step [6302/8897], Loss: 5.8015\n",
      "Epoch [1/1], Step [6303/8897], Loss: 5.7302\n",
      "Epoch [1/1], Step [6304/8897], Loss: 5.5187\n",
      "Epoch [1/1], Step [6305/8897], Loss: 5.6675\n",
      "Epoch [1/1], Step [6306/8897], Loss: 5.6847\n",
      "Epoch [1/1], Step [6307/8897], Loss: 5.7258\n",
      "Epoch [1/1], Step [6308/8897], Loss: 5.7212\n",
      "Epoch [1/1], Step [6309/8897], Loss: 5.5407\n",
      "Epoch [1/1], Step [6310/8897], Loss: 5.5805\n",
      "Epoch [1/1], Step [6311/8897], Loss: 5.6447\n",
      "Epoch [1/1], Step [6312/8897], Loss: 5.6930\n",
      "Epoch [1/1], Step [6313/8897], Loss: 5.7050\n",
      "Epoch [1/1], Step [6314/8897], Loss: 5.5675\n",
      "Epoch [1/1], Step [6315/8897], Loss: 5.4658\n",
      "Epoch [1/1], Step [6316/8897], Loss: 5.7440\n",
      "Epoch [1/1], Step [6317/8897], Loss: 5.7344\n",
      "Epoch [1/1], Step [6318/8897], Loss: 5.6360\n",
      "Epoch [1/1], Step [6319/8897], Loss: 5.4920\n",
      "Epoch [1/1], Step [6320/8897], Loss: 5.5421\n",
      "Epoch [1/1], Step [6321/8897], Loss: 5.6790\n",
      "Epoch [1/1], Step [6322/8897], Loss: 5.4694\n",
      "Epoch [1/1], Step [6323/8897], Loss: 5.7872\n",
      "Epoch [1/1], Step [6324/8897], Loss: 5.8390\n",
      "Epoch [1/1], Step [6325/8897], Loss: 5.6856\n",
      "Epoch [1/1], Step [6326/8897], Loss: 5.8881\n",
      "Epoch [1/1], Step [6327/8897], Loss: 5.5135\n",
      "Epoch [1/1], Step [6328/8897], Loss: 5.6576\n",
      "Epoch [1/1], Step [6329/8897], Loss: 5.5313\n",
      "Epoch [1/1], Step [6330/8897], Loss: 5.5771\n",
      "Epoch [1/1], Step [6331/8897], Loss: 5.4587\n",
      "Epoch [1/1], Step [6332/8897], Loss: 5.5522\n",
      "Epoch [1/1], Step [6333/8897], Loss: 5.3980\n",
      "Epoch [1/1], Step [6334/8897], Loss: 5.5712\n",
      "Epoch [1/1], Step [6335/8897], Loss: 5.9312\n",
      "Epoch [1/1], Step [6336/8897], Loss: 5.5553\n",
      "Epoch [1/1], Step [6337/8897], Loss: 5.5380\n",
      "Epoch [1/1], Step [6338/8897], Loss: 5.6233\n",
      "Epoch [1/1], Step [6339/8897], Loss: 5.7601\n",
      "Epoch [1/1], Step [6340/8897], Loss: 5.7191\n",
      "Epoch [1/1], Step [6341/8897], Loss: 5.5077\n",
      "Epoch [1/1], Step [6342/8897], Loss: 5.3565\n",
      "Epoch [1/1], Step [6343/8897], Loss: 5.6814\n",
      "Epoch [1/1], Step [6344/8897], Loss: 5.5475\n",
      "Epoch [1/1], Step [6345/8897], Loss: 5.7598\n",
      "Epoch [1/1], Step [6346/8897], Loss: 5.7297\n",
      "Epoch [1/1], Step [6347/8897], Loss: 5.6898\n",
      "Epoch [1/1], Step [6348/8897], Loss: 5.5037\n",
      "Epoch [1/1], Step [6349/8897], Loss: 5.6213\n",
      "Epoch [1/1], Step [6350/8897], Loss: 5.4445\n",
      "Epoch [1/1], Step [6351/8897], Loss: 5.4574\n",
      "Epoch [1/1], Step [6352/8897], Loss: 5.7583\n",
      "Epoch [1/1], Step [6353/8897], Loss: 5.4689\n",
      "Epoch [1/1], Step [6354/8897], Loss: 5.8537\n",
      "Epoch [1/1], Step [6355/8897], Loss: 5.7183\n",
      "Epoch [1/1], Step [6356/8897], Loss: 5.7422\n",
      "Epoch [1/1], Step [6357/8897], Loss: 5.8268\n",
      "Epoch [1/1], Step [6358/8897], Loss: 5.8215\n",
      "Epoch [1/1], Step [6359/8897], Loss: 5.6810\n",
      "Epoch [1/1], Step [6360/8897], Loss: 5.4781\n",
      "Epoch [1/1], Step [6361/8897], Loss: 5.7400\n",
      "Epoch [1/1], Step [6362/8897], Loss: 5.7125\n",
      "Epoch [1/1], Step [6363/8897], Loss: 5.5785\n",
      "Epoch [1/1], Step [6364/8897], Loss: 5.5487\n",
      "Epoch [1/1], Step [6365/8897], Loss: 5.6656\n",
      "Epoch [1/1], Step [6366/8897], Loss: 5.6029\n",
      "Epoch [1/1], Step [6367/8897], Loss: 5.9689\n",
      "Epoch [1/1], Step [6368/8897], Loss: 5.6734\n",
      "Epoch [1/1], Step [6369/8897], Loss: 5.8661\n",
      "Epoch [1/1], Step [6370/8897], Loss: 5.3642\n",
      "Epoch [1/1], Step [6371/8897], Loss: 6.0683\n",
      "Epoch [1/1], Step [6372/8897], Loss: 5.6099\n",
      "Epoch [1/1], Step [6373/8897], Loss: 5.5745\n",
      "Epoch [1/1], Step [6374/8897], Loss: 5.5846\n",
      "Epoch [1/1], Step [6375/8897], Loss: 5.5529\n",
      "Epoch [1/1], Step [6376/8897], Loss: 5.8225\n",
      "Epoch [1/1], Step [6377/8897], Loss: 5.9203\n",
      "Epoch [1/1], Step [6378/8897], Loss: 5.6051\n",
      "Epoch [1/1], Step [6379/8897], Loss: 5.7404\n",
      "Epoch [1/1], Step [6380/8897], Loss: 5.7696\n",
      "Epoch [1/1], Step [6381/8897], Loss: 5.5288\n",
      "Epoch [1/1], Step [6382/8897], Loss: 5.7799\n",
      "Epoch [1/1], Step [6383/8897], Loss: 5.7164\n",
      "Epoch [1/1], Step [6384/8897], Loss: 5.7925\n",
      "Epoch [1/1], Step [6385/8897], Loss: 5.5528\n",
      "Epoch [1/1], Step [6386/8897], Loss: 5.5133\n",
      "Epoch [1/1], Step [6387/8897], Loss: 5.7727\n",
      "Epoch [1/1], Step [6388/8897], Loss: 5.7726\n",
      "Epoch [1/1], Step [6389/8897], Loss: 5.7256\n",
      "Epoch [1/1], Step [6390/8897], Loss: 5.6996\n",
      "Epoch [1/1], Step [6391/8897], Loss: 5.6246\n",
      "Epoch [1/1], Step [6392/8897], Loss: 5.6248\n",
      "Epoch [1/1], Step [6393/8897], Loss: 5.6599\n",
      "Epoch [1/1], Step [6394/8897], Loss: 5.7260\n",
      "Epoch [1/1], Step [6395/8897], Loss: 5.6636\n",
      "Epoch [1/1], Step [6396/8897], Loss: 5.5398\n",
      "Epoch [1/1], Step [6397/8897], Loss: 5.6882\n",
      "Epoch [1/1], Step [6398/8897], Loss: 5.9030\n",
      "Epoch [1/1], Step [6399/8897], Loss: 5.7006\n",
      "Epoch [1/1], Step [6400/8897], Loss: 5.7396\n",
      "Epoch [1/1], Step [6401/8897], Loss: 5.6605\n",
      "Epoch [1/1], Step [6402/8897], Loss: 5.4867\n",
      "Epoch [1/1], Step [6403/8897], Loss: 5.6625\n",
      "Epoch [1/1], Step [6404/8897], Loss: 5.6980\n",
      "Epoch [1/1], Step [6405/8897], Loss: 5.8673\n",
      "Epoch [1/1], Step [6406/8897], Loss: 5.6829\n",
      "Epoch [1/1], Step [6407/8897], Loss: 5.7378\n",
      "Epoch [1/1], Step [6408/8897], Loss: 5.6715\n",
      "Epoch [1/1], Step [6409/8897], Loss: 5.7932\n",
      "Epoch [1/1], Step [6410/8897], Loss: 5.6405\n",
      "Epoch [1/1], Step [6411/8897], Loss: 5.6180\n",
      "Epoch [1/1], Step [6412/8897], Loss: 5.6353\n",
      "Epoch [1/1], Step [6413/8897], Loss: 5.3821\n",
      "Epoch [1/1], Step [6414/8897], Loss: 5.3421\n",
      "Epoch [1/1], Step [6415/8897], Loss: 5.5650\n",
      "Epoch [1/1], Step [6416/8897], Loss: 5.6754\n",
      "Epoch [1/1], Step [6417/8897], Loss: 5.5685\n",
      "Epoch [1/1], Step [6418/8897], Loss: 5.5646\n",
      "Epoch [1/1], Step [6419/8897], Loss: 5.6047\n",
      "Epoch [1/1], Step [6420/8897], Loss: 5.6502\n",
      "Epoch [1/1], Step [6421/8897], Loss: 5.5373\n",
      "Epoch [1/1], Step [6422/8897], Loss: 5.7674\n",
      "Epoch [1/1], Step [6423/8897], Loss: 5.7979\n",
      "Epoch [1/1], Step [6424/8897], Loss: 5.4603\n",
      "Epoch [1/1], Step [6425/8897], Loss: 5.6091\n",
      "Epoch [1/1], Step [6426/8897], Loss: 5.7939\n",
      "Epoch [1/1], Step [6427/8897], Loss: 5.6065\n",
      "Epoch [1/1], Step [6428/8897], Loss: 5.3476\n",
      "Epoch [1/1], Step [6429/8897], Loss: 5.7200\n",
      "Epoch [1/1], Step [6430/8897], Loss: 5.6052\n",
      "Epoch [1/1], Step [6431/8897], Loss: 5.6843\n",
      "Epoch [1/1], Step [6432/8897], Loss: 5.8193\n",
      "Epoch [1/1], Step [6433/8897], Loss: 5.4891\n",
      "Epoch [1/1], Step [6434/8897], Loss: 5.8379\n",
      "Epoch [1/1], Step [6435/8897], Loss: 5.7824\n",
      "Epoch [1/1], Step [6436/8897], Loss: 5.5942\n",
      "Epoch [1/1], Step [6437/8897], Loss: 5.3540\n",
      "Epoch [1/1], Step [6438/8897], Loss: 5.8383\n",
      "Epoch [1/1], Step [6439/8897], Loss: 5.2469\n",
      "Epoch [1/1], Step [6440/8897], Loss: 5.8483\n",
      "Epoch [1/1], Step [6441/8897], Loss: 5.7037\n",
      "Epoch [1/1], Step [6442/8897], Loss: 5.6233\n",
      "Epoch [1/1], Step [6443/8897], Loss: 5.4948\n",
      "Epoch [1/1], Step [6444/8897], Loss: 5.7219\n",
      "Epoch [1/1], Step [6445/8897], Loss: 5.4387\n",
      "Epoch [1/1], Step [6446/8897], Loss: 5.6219\n",
      "Epoch [1/1], Step [6447/8897], Loss: 5.6099\n",
      "Epoch [1/1], Step [6448/8897], Loss: 5.5140\n",
      "Epoch [1/1], Step [6449/8897], Loss: 5.7644\n",
      "Epoch [1/1], Step [6450/8897], Loss: 5.7295\n",
      "Epoch [1/1], Step [6451/8897], Loss: 5.6707\n",
      "Epoch [1/1], Step [6452/8897], Loss: 5.6959\n",
      "Epoch [1/1], Step [6453/8897], Loss: 5.5286\n",
      "Epoch [1/1], Step [6454/8897], Loss: 5.5542\n",
      "Epoch [1/1], Step [6455/8897], Loss: 5.9159\n",
      "Epoch [1/1], Step [6456/8897], Loss: 5.6565\n",
      "Epoch [1/1], Step [6457/8897], Loss: 5.5563\n",
      "Epoch [1/1], Step [6458/8897], Loss: 5.5288\n",
      "Epoch [1/1], Step [6459/8897], Loss: 5.6721\n",
      "Epoch [1/1], Step [6460/8897], Loss: 5.5610\n",
      "Epoch [1/1], Step [6461/8897], Loss: 5.5408\n",
      "Epoch [1/1], Step [6462/8897], Loss: 5.7905\n",
      "Epoch [1/1], Step [6463/8897], Loss: 5.9168\n",
      "Epoch [1/1], Step [6464/8897], Loss: 5.7443\n",
      "Epoch [1/1], Step [6465/8897], Loss: 5.4373\n",
      "Epoch [1/1], Step [6466/8897], Loss: 5.8284\n",
      "Epoch [1/1], Step [6467/8897], Loss: 5.7071\n",
      "Epoch [1/1], Step [6468/8897], Loss: 5.5665\n",
      "Epoch [1/1], Step [6469/8897], Loss: 5.6518\n",
      "Epoch [1/1], Step [6470/8897], Loss: 5.4369\n",
      "Epoch [1/1], Step [6471/8897], Loss: 5.3769\n",
      "Epoch [1/1], Step [6472/8897], Loss: 5.7115\n",
      "Epoch [1/1], Step [6473/8897], Loss: 5.5526\n",
      "Epoch [1/1], Step [6474/8897], Loss: 5.5498\n",
      "Epoch [1/1], Step [6475/8897], Loss: 5.5719\n",
      "Epoch [1/1], Step [6476/8897], Loss: 5.7859\n",
      "Epoch [1/1], Step [6477/8897], Loss: 5.8020\n",
      "Epoch [1/1], Step [6478/8897], Loss: 5.4979\n",
      "Epoch [1/1], Step [6479/8897], Loss: 6.0188\n",
      "Epoch [1/1], Step [6480/8897], Loss: 5.5453\n",
      "Epoch [1/1], Step [6481/8897], Loss: 5.8301\n",
      "Epoch [1/1], Step [6482/8897], Loss: 5.6438\n",
      "Epoch [1/1], Step [6483/8897], Loss: 5.5776\n",
      "Epoch [1/1], Step [6484/8897], Loss: 5.5087\n",
      "Epoch [1/1], Step [6485/8897], Loss: 5.5545\n",
      "Epoch [1/1], Step [6486/8897], Loss: 5.5490\n",
      "Epoch [1/1], Step [6487/8897], Loss: 5.5578\n",
      "Epoch [1/1], Step [6488/8897], Loss: 5.2741\n",
      "Epoch [1/1], Step [6489/8897], Loss: 5.6910\n",
      "Epoch [1/1], Step [6490/8897], Loss: 5.6667\n",
      "Epoch [1/1], Step [6491/8897], Loss: 6.0754\n",
      "Epoch [1/1], Step [6492/8897], Loss: 5.7098\n",
      "Epoch [1/1], Step [6493/8897], Loss: 5.6310\n",
      "Epoch [1/1], Step [6494/8897], Loss: 5.5704\n",
      "Epoch [1/1], Step [6495/8897], Loss: 5.7969\n",
      "Epoch [1/1], Step [6496/8897], Loss: 5.7394\n",
      "Epoch [1/1], Step [6497/8897], Loss: 5.7114\n",
      "Epoch [1/1], Step [6498/8897], Loss: 5.5423\n",
      "Epoch [1/1], Step [6499/8897], Loss: 5.7727\n",
      "Epoch [1/1], Step [6500/8897], Loss: 5.6419\n",
      "Epoch [1/1], Step [6501/8897], Loss: 5.6076\n",
      "Epoch [1/1], Step [6502/8897], Loss: 5.5434\n",
      "Epoch [1/1], Step [6503/8897], Loss: 5.5224\n",
      "Epoch [1/1], Step [6504/8897], Loss: 5.8370\n",
      "Epoch [1/1], Step [6505/8897], Loss: 5.4975\n",
      "Epoch [1/1], Step [6506/8897], Loss: 5.7136\n",
      "Epoch [1/1], Step [6507/8897], Loss: 5.4512\n",
      "Epoch [1/1], Step [6508/8897], Loss: 5.6241\n",
      "Epoch [1/1], Step [6509/8897], Loss: 5.5434\n",
      "Epoch [1/1], Step [6510/8897], Loss: 5.7277\n",
      "Epoch [1/1], Step [6511/8897], Loss: 5.6669\n",
      "Epoch [1/1], Step [6512/8897], Loss: 5.8846\n",
      "Epoch [1/1], Step [6513/8897], Loss: 5.5750\n",
      "Epoch [1/1], Step [6514/8897], Loss: 5.3961\n",
      "Epoch [1/1], Step [6515/8897], Loss: 5.8649\n",
      "Epoch [1/1], Step [6516/8897], Loss: 5.6883\n",
      "Epoch [1/1], Step [6517/8897], Loss: 5.5368\n",
      "Epoch [1/1], Step [6518/8897], Loss: 5.8895\n",
      "Epoch [1/1], Step [6519/8897], Loss: 5.4037\n",
      "Epoch [1/1], Step [6520/8897], Loss: 5.5477\n",
      "Epoch [1/1], Step [6521/8897], Loss: 5.6046\n",
      "Epoch [1/1], Step [6522/8897], Loss: 5.3673\n",
      "Epoch [1/1], Step [6523/8897], Loss: 5.6809\n",
      "Epoch [1/1], Step [6524/8897], Loss: 5.6007\n",
      "Epoch [1/1], Step [6525/8897], Loss: 5.5770\n",
      "Epoch [1/1], Step [6526/8897], Loss: 5.5872\n",
      "Epoch [1/1], Step [6527/8897], Loss: 5.6890\n",
      "Epoch [1/1], Step [6528/8897], Loss: 5.9576\n",
      "Epoch [1/1], Step [6529/8897], Loss: 5.4515\n",
      "Epoch [1/1], Step [6530/8897], Loss: 5.6590\n",
      "Epoch [1/1], Step [6531/8897], Loss: 5.6491\n",
      "Epoch [1/1], Step [6532/8897], Loss: 5.2970\n",
      "Epoch [1/1], Step [6533/8897], Loss: 5.7495\n",
      "Epoch [1/1], Step [6534/8897], Loss: 5.4436\n",
      "Epoch [1/1], Step [6535/8897], Loss: 5.4656\n",
      "Epoch [1/1], Step [6536/8897], Loss: 5.8932\n",
      "Epoch [1/1], Step [6537/8897], Loss: 5.5233\n",
      "Epoch [1/1], Step [6538/8897], Loss: 5.7888\n",
      "Epoch [1/1], Step [6539/8897], Loss: 5.5860\n",
      "Epoch [1/1], Step [6540/8897], Loss: 5.8871\n",
      "Epoch [1/1], Step [6541/8897], Loss: 5.7337\n",
      "Epoch [1/1], Step [6542/8897], Loss: 5.9035\n",
      "Epoch [1/1], Step [6543/8897], Loss: 5.7008\n",
      "Epoch [1/1], Step [6544/8897], Loss: 5.3306\n",
      "Epoch [1/1], Step [6545/8897], Loss: 5.9161\n",
      "Epoch [1/1], Step [6546/8897], Loss: 5.8228\n",
      "Epoch [1/1], Step [6547/8897], Loss: 5.8624\n",
      "Epoch [1/1], Step [6548/8897], Loss: 5.5943\n",
      "Epoch [1/1], Step [6549/8897], Loss: 5.5928\n",
      "Epoch [1/1], Step [6550/8897], Loss: 5.8029\n",
      "Epoch [1/1], Step [6551/8897], Loss: 5.8750\n",
      "Epoch [1/1], Step [6552/8897], Loss: 5.5585\n",
      "Epoch [1/1], Step [6553/8897], Loss: 5.7952\n",
      "Epoch [1/1], Step [6554/8897], Loss: 5.4917\n",
      "Epoch [1/1], Step [6555/8897], Loss: 5.7638\n",
      "Epoch [1/1], Step [6556/8897], Loss: 5.6917\n",
      "Epoch [1/1], Step [6557/8897], Loss: 5.5013\n",
      "Epoch [1/1], Step [6558/8897], Loss: 5.6909\n",
      "Epoch [1/1], Step [6559/8897], Loss: 5.6663\n",
      "Epoch [1/1], Step [6560/8897], Loss: 5.5108\n",
      "Epoch [1/1], Step [6561/8897], Loss: 5.7366\n",
      "Epoch [1/1], Step [6562/8897], Loss: 5.4555\n",
      "Epoch [1/1], Step [6563/8897], Loss: 5.3918\n",
      "Epoch [1/1], Step [6564/8897], Loss: 5.6458\n",
      "Epoch [1/1], Step [6565/8897], Loss: 5.5001\n",
      "Epoch [1/1], Step [6566/8897], Loss: 5.6863\n",
      "Epoch [1/1], Step [6567/8897], Loss: 5.4942\n",
      "Epoch [1/1], Step [6568/8897], Loss: 5.5640\n",
      "Epoch [1/1], Step [6569/8897], Loss: 5.5899\n",
      "Epoch [1/1], Step [6570/8897], Loss: 5.6102\n",
      "Epoch [1/1], Step [6571/8897], Loss: 5.8750\n",
      "Epoch [1/1], Step [6572/8897], Loss: 5.4405\n",
      "Epoch [1/1], Step [6573/8897], Loss: 5.7280\n",
      "Epoch [1/1], Step [6574/8897], Loss: 5.6928\n",
      "Epoch [1/1], Step [6575/8897], Loss: 5.5720\n",
      "Epoch [1/1], Step [6576/8897], Loss: 5.9210\n",
      "Epoch [1/1], Step [6577/8897], Loss: 5.6597\n",
      "Epoch [1/1], Step [6578/8897], Loss: 5.6806\n",
      "Epoch [1/1], Step [6579/8897], Loss: 5.5565\n",
      "Epoch [1/1], Step [6580/8897], Loss: 5.6275\n",
      "Epoch [1/1], Step [6581/8897], Loss: 5.5227\n",
      "Epoch [1/1], Step [6582/8897], Loss: 5.4654\n",
      "Epoch [1/1], Step [6583/8897], Loss: 5.6388\n",
      "Epoch [1/1], Step [6584/8897], Loss: 5.5598\n",
      "Epoch [1/1], Step [6585/8897], Loss: 5.4417\n",
      "Epoch [1/1], Step [6586/8897], Loss: 5.5448\n",
      "Epoch [1/1], Step [6587/8897], Loss: 5.7085\n",
      "Epoch [1/1], Step [6588/8897], Loss: 5.8315\n",
      "Epoch [1/1], Step [6589/8897], Loss: 5.5535\n",
      "Epoch [1/1], Step [6590/8897], Loss: 5.5093\n",
      "Epoch [1/1], Step [6591/8897], Loss: 5.7626\n",
      "Epoch [1/1], Step [6592/8897], Loss: 5.5147\n",
      "Epoch [1/1], Step [6593/8897], Loss: 5.6148\n",
      "Epoch [1/1], Step [6594/8897], Loss: 5.7093\n",
      "Epoch [1/1], Step [6595/8897], Loss: 5.6560\n",
      "Epoch [1/1], Step [6596/8897], Loss: 5.6264\n",
      "Epoch [1/1], Step [6597/8897], Loss: 5.6733\n",
      "Epoch [1/1], Step [6598/8897], Loss: 5.7284\n",
      "Epoch [1/1], Step [6599/8897], Loss: 5.3832\n",
      "Epoch [1/1], Step [6600/8897], Loss: 6.1092\n",
      "Epoch [1/1], Step [6601/8897], Loss: 5.7540\n",
      "Epoch [1/1], Step [6602/8897], Loss: 5.5771\n",
      "Epoch [1/1], Step [6603/8897], Loss: 5.9973\n",
      "Epoch [1/1], Step [6604/8897], Loss: 5.5667\n",
      "Epoch [1/1], Step [6605/8897], Loss: 5.8186\n",
      "Epoch [1/1], Step [6606/8897], Loss: 5.7183\n",
      "Epoch [1/1], Step [6607/8897], Loss: 5.5062\n",
      "Epoch [1/1], Step [6608/8897], Loss: 5.5103\n",
      "Epoch [1/1], Step [6609/8897], Loss: 5.7409\n",
      "Epoch [1/1], Step [6610/8897], Loss: 5.5072\n",
      "Epoch [1/1], Step [6611/8897], Loss: 5.4609\n",
      "Epoch [1/1], Step [6612/8897], Loss: 5.5108\n",
      "Epoch [1/1], Step [6613/8897], Loss: 5.6715\n",
      "Epoch [1/1], Step [6614/8897], Loss: 5.4092\n",
      "Epoch [1/1], Step [6615/8897], Loss: 5.7577\n",
      "Epoch [1/1], Step [6616/8897], Loss: 5.5281\n",
      "Epoch [1/1], Step [6617/8897], Loss: 5.6276\n",
      "Epoch [1/1], Step [6618/8897], Loss: 5.4494\n",
      "Epoch [1/1], Step [6619/8897], Loss: 5.5481\n",
      "Epoch [1/1], Step [6620/8897], Loss: 5.9906\n",
      "Epoch [1/1], Step [6621/8897], Loss: 5.6425\n",
      "Epoch [1/1], Step [6622/8897], Loss: 5.7354\n",
      "Epoch [1/1], Step [6623/8897], Loss: 5.3883\n",
      "Epoch [1/1], Step [6624/8897], Loss: 5.6709\n",
      "Epoch [1/1], Step [6625/8897], Loss: 5.5845\n",
      "Epoch [1/1], Step [6626/8897], Loss: 5.3497\n",
      "Epoch [1/1], Step [6627/8897], Loss: 5.7339\n",
      "Epoch [1/1], Step [6628/8897], Loss: 5.7693\n",
      "Epoch [1/1], Step [6629/8897], Loss: 5.6333\n",
      "Epoch [1/1], Step [6630/8897], Loss: 5.4731\n",
      "Epoch [1/1], Step [6631/8897], Loss: 5.4234\n",
      "Epoch [1/1], Step [6632/8897], Loss: 5.7128\n",
      "Epoch [1/1], Step [6633/8897], Loss: 5.6369\n",
      "Epoch [1/1], Step [6634/8897], Loss: 5.5706\n",
      "Epoch [1/1], Step [6635/8897], Loss: 5.8542\n",
      "Epoch [1/1], Step [6636/8897], Loss: 5.8625\n",
      "Epoch [1/1], Step [6637/8897], Loss: 5.6032\n",
      "Epoch [1/1], Step [6638/8897], Loss: 5.8066\n",
      "Epoch [1/1], Step [6639/8897], Loss: 5.4565\n",
      "Epoch [1/1], Step [6640/8897], Loss: 5.6410\n",
      "Epoch [1/1], Step [6641/8897], Loss: 5.5174\n",
      "Epoch [1/1], Step [6642/8897], Loss: 5.7160\n",
      "Epoch [1/1], Step [6643/8897], Loss: 5.8608\n",
      "Epoch [1/1], Step [6644/8897], Loss: 5.4845\n",
      "Epoch [1/1], Step [6645/8897], Loss: 5.7542\n",
      "Epoch [1/1], Step [6646/8897], Loss: 5.6292\n",
      "Epoch [1/1], Step [6647/8897], Loss: 5.4935\n",
      "Epoch [1/1], Step [6648/8897], Loss: 5.8585\n",
      "Epoch [1/1], Step [6649/8897], Loss: 5.6405\n",
      "Epoch [1/1], Step [6650/8897], Loss: 5.6433\n",
      "Epoch [1/1], Step [6651/8897], Loss: 5.5049\n",
      "Epoch [1/1], Step [6652/8897], Loss: 5.7008\n",
      "Epoch [1/1], Step [6653/8897], Loss: 5.4735\n",
      "Epoch [1/1], Step [6654/8897], Loss: 5.4621\n",
      "Epoch [1/1], Step [6655/8897], Loss: 5.5381\n",
      "Epoch [1/1], Step [6656/8897], Loss: 5.6091\n",
      "Epoch [1/1], Step [6657/8897], Loss: 5.5243\n",
      "Epoch [1/1], Step [6658/8897], Loss: 5.4900\n",
      "Epoch [1/1], Step [6659/8897], Loss: 5.6536\n",
      "Epoch [1/1], Step [6660/8897], Loss: 5.6957\n",
      "Epoch [1/1], Step [6661/8897], Loss: 5.7339\n",
      "Epoch [1/1], Step [6662/8897], Loss: 5.6165\n",
      "Epoch [1/1], Step [6663/8897], Loss: 5.5265\n",
      "Epoch [1/1], Step [6664/8897], Loss: 5.5103\n",
      "Epoch [1/1], Step [6665/8897], Loss: 5.6793\n",
      "Epoch [1/1], Step [6666/8897], Loss: 5.6767\n",
      "Epoch [1/1], Step [6667/8897], Loss: 5.7057\n",
      "Epoch [1/1], Step [6668/8897], Loss: 5.6457\n",
      "Epoch [1/1], Step [6669/8897], Loss: 5.8439\n",
      "Epoch [1/1], Step [6670/8897], Loss: 5.5270\n",
      "Epoch [1/1], Step [6671/8897], Loss: 5.4966\n",
      "Epoch [1/1], Step [6672/8897], Loss: 5.7879\n",
      "Epoch [1/1], Step [6673/8897], Loss: 5.9534\n",
      "Epoch [1/1], Step [6674/8897], Loss: 5.7881\n",
      "Epoch [1/1], Step [6675/8897], Loss: 5.7201\n",
      "Epoch [1/1], Step [6676/8897], Loss: 5.5197\n",
      "Epoch [1/1], Step [6677/8897], Loss: 5.8176\n",
      "Epoch [1/1], Step [6678/8897], Loss: 5.5797\n",
      "Epoch [1/1], Step [6679/8897], Loss: 5.6138\n",
      "Epoch [1/1], Step [6680/8897], Loss: 5.3474\n",
      "Epoch [1/1], Step [6681/8897], Loss: 5.3948\n",
      "Epoch [1/1], Step [6682/8897], Loss: 5.5521\n",
      "Epoch [1/1], Step [6683/8897], Loss: 5.4488\n",
      "Epoch [1/1], Step [6684/8897], Loss: 5.5748\n",
      "Epoch [1/1], Step [6685/8897], Loss: 5.6633\n",
      "Epoch [1/1], Step [6686/8897], Loss: 5.3806\n",
      "Epoch [1/1], Step [6687/8897], Loss: 5.3204\n",
      "Epoch [1/1], Step [6688/8897], Loss: 5.5415\n",
      "Epoch [1/1], Step [6689/8897], Loss: 5.6322\n",
      "Epoch [1/1], Step [6690/8897], Loss: 5.5384\n",
      "Epoch [1/1], Step [6691/8897], Loss: 5.6739\n",
      "Epoch [1/1], Step [6692/8897], Loss: 5.6701\n",
      "Epoch [1/1], Step [6693/8897], Loss: 5.7915\n",
      "Epoch [1/1], Step [6694/8897], Loss: 5.4432\n",
      "Epoch [1/1], Step [6695/8897], Loss: 5.7293\n",
      "Epoch [1/1], Step [6696/8897], Loss: 5.7996\n",
      "Epoch [1/1], Step [6697/8897], Loss: 5.5834\n",
      "Epoch [1/1], Step [6698/8897], Loss: 5.7935\n",
      "Epoch [1/1], Step [6699/8897], Loss: 5.7093\n",
      "Epoch [1/1], Step [6700/8897], Loss: 5.7601\n",
      "Epoch [1/1], Step [6701/8897], Loss: 5.6490\n",
      "Epoch [1/1], Step [6702/8897], Loss: 5.5269\n",
      "Epoch [1/1], Step [6703/8897], Loss: 5.7871\n",
      "Epoch [1/1], Step [6704/8897], Loss: 5.6305\n",
      "Epoch [1/1], Step [6705/8897], Loss: 5.7108\n",
      "Epoch [1/1], Step [6706/8897], Loss: 5.7110\n",
      "Epoch [1/1], Step [6707/8897], Loss: 5.8283\n",
      "Epoch [1/1], Step [6708/8897], Loss: 5.6734\n",
      "Epoch [1/1], Step [6709/8897], Loss: 5.5481\n",
      "Epoch [1/1], Step [6710/8897], Loss: 5.6917\n",
      "Epoch [1/1], Step [6711/8897], Loss: 5.6877\n",
      "Epoch [1/1], Step [6712/8897], Loss: 5.4456\n",
      "Epoch [1/1], Step [6713/8897], Loss: 5.6441\n",
      "Epoch [1/1], Step [6714/8897], Loss: 5.3914\n",
      "Epoch [1/1], Step [6715/8897], Loss: 5.7469\n",
      "Epoch [1/1], Step [6716/8897], Loss: 5.5459\n",
      "Epoch [1/1], Step [6717/8897], Loss: 5.4415\n",
      "Epoch [1/1], Step [6718/8897], Loss: 5.6213\n",
      "Epoch [1/1], Step [6719/8897], Loss: 5.6084\n",
      "Epoch [1/1], Step [6720/8897], Loss: 5.7063\n",
      "Epoch [1/1], Step [6721/8897], Loss: 5.5052\n",
      "Epoch [1/1], Step [6722/8897], Loss: 5.7595\n",
      "Epoch [1/1], Step [6723/8897], Loss: 5.5609\n",
      "Epoch [1/1], Step [6724/8897], Loss: 5.9580\n",
      "Epoch [1/1], Step [6725/8897], Loss: 5.5825\n",
      "Epoch [1/1], Step [6726/8897], Loss: 5.7342\n",
      "Epoch [1/1], Step [6727/8897], Loss: 5.5290\n",
      "Epoch [1/1], Step [6728/8897], Loss: 5.4083\n",
      "Epoch [1/1], Step [6729/8897], Loss: 5.7588\n",
      "Epoch [1/1], Step [6730/8897], Loss: 5.4057\n",
      "Epoch [1/1], Step [6731/8897], Loss: 5.5689\n",
      "Epoch [1/1], Step [6732/8897], Loss: 5.6405\n",
      "Epoch [1/1], Step [6733/8897], Loss: 5.6484\n",
      "Epoch [1/1], Step [6734/8897], Loss: 5.3887\n",
      "Epoch [1/1], Step [6735/8897], Loss: 5.5980\n",
      "Epoch [1/1], Step [6736/8897], Loss: 5.7470\n",
      "Epoch [1/1], Step [6737/8897], Loss: 5.7657\n",
      "Epoch [1/1], Step [6738/8897], Loss: 5.7908\n",
      "Epoch [1/1], Step [6739/8897], Loss: 5.6594\n",
      "Epoch [1/1], Step [6740/8897], Loss: 5.5531\n",
      "Epoch [1/1], Step [6741/8897], Loss: 5.5934\n",
      "Epoch [1/1], Step [6742/8897], Loss: 5.4834\n",
      "Epoch [1/1], Step [6743/8897], Loss: 5.6495\n",
      "Epoch [1/1], Step [6744/8897], Loss: 5.6234\n",
      "Epoch [1/1], Step [6745/8897], Loss: 5.6070\n",
      "Epoch [1/1], Step [6746/8897], Loss: 5.7808\n",
      "Epoch [1/1], Step [6747/8897], Loss: 5.9051\n",
      "Epoch [1/1], Step [6748/8897], Loss: 5.6062\n",
      "Epoch [1/1], Step [6749/8897], Loss: 5.3851\n",
      "Epoch [1/1], Step [6750/8897], Loss: 5.5534\n",
      "Epoch [1/1], Step [6751/8897], Loss: 5.5824\n",
      "Epoch [1/1], Step [6752/8897], Loss: 5.6387\n",
      "Epoch [1/1], Step [6753/8897], Loss: 5.7296\n",
      "Epoch [1/1], Step [6754/8897], Loss: 5.7768\n",
      "Epoch [1/1], Step [6755/8897], Loss: 5.6437\n",
      "Epoch [1/1], Step [6756/8897], Loss: 5.5798\n",
      "Epoch [1/1], Step [6757/8897], Loss: 5.7210\n",
      "Epoch [1/1], Step [6758/8897], Loss: 5.6694\n",
      "Epoch [1/1], Step [6759/8897], Loss: 5.6022\n",
      "Epoch [1/1], Step [6760/8897], Loss: 5.6998\n",
      "Epoch [1/1], Step [6761/8897], Loss: 5.6938\n",
      "Epoch [1/1], Step [6762/8897], Loss: 5.7702\n",
      "Epoch [1/1], Step [6763/8897], Loss: 5.5369\n",
      "Epoch [1/1], Step [6764/8897], Loss: 5.8056\n",
      "Epoch [1/1], Step [6765/8897], Loss: 5.8197\n",
      "Epoch [1/1], Step [6766/8897], Loss: 5.5595\n",
      "Epoch [1/1], Step [6767/8897], Loss: 5.6373\n",
      "Epoch [1/1], Step [6768/8897], Loss: 5.7480\n",
      "Epoch [1/1], Step [6769/8897], Loss: 5.8645\n",
      "Epoch [1/1], Step [6770/8897], Loss: 5.5781\n",
      "Epoch [1/1], Step [6771/8897], Loss: 5.3597\n",
      "Epoch [1/1], Step [6772/8897], Loss: 5.6569\n",
      "Epoch [1/1], Step [6773/8897], Loss: 5.7386\n",
      "Epoch [1/1], Step [6774/8897], Loss: 5.7079\n",
      "Epoch [1/1], Step [6775/8897], Loss: 5.5998\n",
      "Epoch [1/1], Step [6776/8897], Loss: 5.6007\n",
      "Epoch [1/1], Step [6777/8897], Loss: 5.6222\n",
      "Epoch [1/1], Step [6778/8897], Loss: 5.6965\n",
      "Epoch [1/1], Step [6779/8897], Loss: 5.6164\n",
      "Epoch [1/1], Step [6780/8897], Loss: 5.8808\n",
      "Epoch [1/1], Step [6781/8897], Loss: 5.6427\n",
      "Epoch [1/1], Step [6782/8897], Loss: 5.5000\n",
      "Epoch [1/1], Step [6783/8897], Loss: 5.4439\n",
      "Epoch [1/1], Step [6784/8897], Loss: 5.4872\n",
      "Epoch [1/1], Step [6785/8897], Loss: 5.6096\n",
      "Epoch [1/1], Step [6786/8897], Loss: 5.4166\n",
      "Epoch [1/1], Step [6787/8897], Loss: 5.4741\n",
      "Epoch [1/1], Step [6788/8897], Loss: 5.6634\n",
      "Epoch [1/1], Step [6789/8897], Loss: 5.6330\n",
      "Epoch [1/1], Step [6790/8897], Loss: 5.6083\n",
      "Epoch [1/1], Step [6791/8897], Loss: 5.7326\n",
      "Epoch [1/1], Step [6792/8897], Loss: 5.5241\n",
      "Epoch [1/1], Step [6793/8897], Loss: 5.2670\n",
      "Epoch [1/1], Step [6794/8897], Loss: 5.6946\n",
      "Epoch [1/1], Step [6795/8897], Loss: 5.5402\n",
      "Epoch [1/1], Step [6796/8897], Loss: 5.6707\n",
      "Epoch [1/1], Step [6797/8897], Loss: 5.6103\n",
      "Epoch [1/1], Step [6798/8897], Loss: 5.5060\n",
      "Epoch [1/1], Step [6799/8897], Loss: 5.6916\n",
      "Epoch [1/1], Step [6800/8897], Loss: 5.6993\n",
      "Epoch [1/1], Step [6801/8897], Loss: 5.6530\n",
      "Epoch [1/1], Step [6802/8897], Loss: 5.5023\n",
      "Epoch [1/1], Step [6803/8897], Loss: 5.6026\n",
      "Epoch [1/1], Step [6804/8897], Loss: 5.9817\n",
      "Epoch [1/1], Step [6805/8897], Loss: 5.6670\n",
      "Epoch [1/1], Step [6806/8897], Loss: 5.9033\n",
      "Epoch [1/1], Step [6807/8897], Loss: 5.5749\n",
      "Epoch [1/1], Step [6808/8897], Loss: 5.8639\n",
      "Epoch [1/1], Step [6809/8897], Loss: 5.5625\n",
      "Epoch [1/1], Step [6810/8897], Loss: 5.6769\n",
      "Epoch [1/1], Step [6811/8897], Loss: 5.4307\n",
      "Epoch [1/1], Step [6812/8897], Loss: 5.6361\n",
      "Epoch [1/1], Step [6813/8897], Loss: 5.6251\n",
      "Epoch [1/1], Step [6814/8897], Loss: 5.8773\n",
      "Epoch [1/1], Step [6815/8897], Loss: 5.5738\n",
      "Epoch [1/1], Step [6816/8897], Loss: 5.5362\n",
      "Epoch [1/1], Step [6817/8897], Loss: 5.6188\n",
      "Epoch [1/1], Step [6818/8897], Loss: 5.5108\n",
      "Epoch [1/1], Step [6819/8897], Loss: 5.6649\n",
      "Epoch [1/1], Step [6820/8897], Loss: 5.5627\n",
      "Epoch [1/1], Step [6821/8897], Loss: 5.5700\n",
      "Epoch [1/1], Step [6822/8897], Loss: 5.4120\n",
      "Epoch [1/1], Step [6823/8897], Loss: 5.6502\n",
      "Epoch [1/1], Step [6824/8897], Loss: 5.6246\n",
      "Epoch [1/1], Step [6825/8897], Loss: 5.7804\n",
      "Epoch [1/1], Step [6826/8897], Loss: 5.5080\n",
      "Epoch [1/1], Step [6827/8897], Loss: 5.5945\n",
      "Epoch [1/1], Step [6828/8897], Loss: 5.6998\n",
      "Epoch [1/1], Step [6829/8897], Loss: 5.6889\n",
      "Epoch [1/1], Step [6830/8897], Loss: 5.5683\n",
      "Epoch [1/1], Step [6831/8897], Loss: 5.6900\n",
      "Epoch [1/1], Step [6832/8897], Loss: 5.7281\n",
      "Epoch [1/1], Step [6833/8897], Loss: 5.8231\n",
      "Epoch [1/1], Step [6834/8897], Loss: 5.5588\n",
      "Epoch [1/1], Step [6835/8897], Loss: 5.5563\n",
      "Epoch [1/1], Step [6836/8897], Loss: 5.7070\n",
      "Epoch [1/1], Step [6837/8897], Loss: 5.2874\n",
      "Epoch [1/1], Step [6838/8897], Loss: 5.7179\n",
      "Epoch [1/1], Step [6839/8897], Loss: 5.6282\n",
      "Epoch [1/1], Step [6840/8897], Loss: 5.6801\n",
      "Epoch [1/1], Step [6841/8897], Loss: 5.5694\n",
      "Epoch [1/1], Step [6842/8897], Loss: 5.8037\n",
      "Epoch [1/1], Step [6843/8897], Loss: 5.6414\n",
      "Epoch [1/1], Step [6844/8897], Loss: 5.5561\n",
      "Epoch [1/1], Step [6845/8897], Loss: 5.6357\n",
      "Epoch [1/1], Step [6846/8897], Loss: 5.6628\n",
      "Epoch [1/1], Step [6847/8897], Loss: 5.5715\n",
      "Epoch [1/1], Step [6848/8897], Loss: 5.3232\n",
      "Epoch [1/1], Step [6849/8897], Loss: 5.7205\n",
      "Epoch [1/1], Step [6850/8897], Loss: 5.7367\n",
      "Epoch [1/1], Step [6851/8897], Loss: 5.4915\n",
      "Epoch [1/1], Step [6852/8897], Loss: 5.5862\n",
      "Epoch [1/1], Step [6853/8897], Loss: 5.5588\n",
      "Epoch [1/1], Step [6854/8897], Loss: 5.5298\n",
      "Epoch [1/1], Step [6855/8897], Loss: 5.6419\n",
      "Epoch [1/1], Step [6856/8897], Loss: 5.5579\n",
      "Epoch [1/1], Step [6857/8897], Loss: 5.7991\n",
      "Epoch [1/1], Step [6858/8897], Loss: 5.6093\n",
      "Epoch [1/1], Step [6859/8897], Loss: 5.6310\n",
      "Epoch [1/1], Step [6860/8897], Loss: 5.3950\n",
      "Epoch [1/1], Step [6861/8897], Loss: 5.3787\n",
      "Epoch [1/1], Step [6862/8897], Loss: 5.6462\n",
      "Epoch [1/1], Step [6863/8897], Loss: 5.8661\n",
      "Epoch [1/1], Step [6864/8897], Loss: 5.6235\n",
      "Epoch [1/1], Step [6865/8897], Loss: 5.6000\n",
      "Epoch [1/1], Step [6866/8897], Loss: 5.4881\n",
      "Epoch [1/1], Step [6867/8897], Loss: 5.2975\n",
      "Epoch [1/1], Step [6868/8897], Loss: 5.5395\n",
      "Epoch [1/1], Step [6869/8897], Loss: 5.3757\n",
      "Epoch [1/1], Step [6870/8897], Loss: 5.8350\n",
      "Epoch [1/1], Step [6871/8897], Loss: 5.7862\n",
      "Epoch [1/1], Step [6872/8897], Loss: 5.6235\n",
      "Epoch [1/1], Step [6873/8897], Loss: 5.4792\n",
      "Epoch [1/1], Step [6874/8897], Loss: 5.6491\n",
      "Epoch [1/1], Step [6875/8897], Loss: 5.4907\n",
      "Epoch [1/1], Step [6876/8897], Loss: 5.3598\n",
      "Epoch [1/1], Step [6877/8897], Loss: 5.5088\n",
      "Epoch [1/1], Step [6878/8897], Loss: 5.7930\n",
      "Epoch [1/1], Step [6879/8897], Loss: 5.7623\n",
      "Epoch [1/1], Step [6880/8897], Loss: 5.5409\n",
      "Epoch [1/1], Step [6881/8897], Loss: 5.5779\n",
      "Epoch [1/1], Step [6882/8897], Loss: 5.3777\n",
      "Epoch [1/1], Step [6883/8897], Loss: 5.6656\n",
      "Epoch [1/1], Step [6884/8897], Loss: 5.7631\n",
      "Epoch [1/1], Step [6885/8897], Loss: 5.5372\n",
      "Epoch [1/1], Step [6886/8897], Loss: 5.8268\n",
      "Epoch [1/1], Step [6887/8897], Loss: 5.5182\n",
      "Epoch [1/1], Step [6888/8897], Loss: 5.6581\n",
      "Epoch [1/1], Step [6889/8897], Loss: 5.6807\n",
      "Epoch [1/1], Step [6890/8897], Loss: 5.7278\n",
      "Epoch [1/1], Step [6891/8897], Loss: 5.7002\n",
      "Epoch [1/1], Step [6892/8897], Loss: 5.5128\n",
      "Epoch [1/1], Step [6893/8897], Loss: 5.6875\n",
      "Epoch [1/1], Step [6894/8897], Loss: 5.7595\n",
      "Epoch [1/1], Step [6895/8897], Loss: 5.8164\n",
      "Epoch [1/1], Step [6896/8897], Loss: 5.5408\n",
      "Epoch [1/1], Step [6897/8897], Loss: 5.6600\n",
      "Epoch [1/1], Step [6898/8897], Loss: 5.7799\n",
      "Epoch [1/1], Step [6899/8897], Loss: 5.4935\n",
      "Epoch [1/1], Step [6900/8897], Loss: 5.6349\n",
      "Epoch [1/1], Step [6901/8897], Loss: 5.5311\n",
      "Epoch [1/1], Step [6902/8897], Loss: 5.8014\n",
      "Epoch [1/1], Step [6903/8897], Loss: 5.5951\n",
      "Epoch [1/1], Step [6904/8897], Loss: 5.5261\n",
      "Epoch [1/1], Step [6905/8897], Loss: 5.5500\n",
      "Epoch [1/1], Step [6906/8897], Loss: 5.5573\n",
      "Epoch [1/1], Step [6907/8897], Loss: 5.8315\n",
      "Epoch [1/1], Step [6908/8897], Loss: 5.6585\n",
      "Epoch [1/1], Step [6909/8897], Loss: 5.6699\n",
      "Epoch [1/1], Step [6910/8897], Loss: 5.6027\n",
      "Epoch [1/1], Step [6911/8897], Loss: 5.7825\n",
      "Epoch [1/1], Step [6912/8897], Loss: 5.7076\n",
      "Epoch [1/1], Step [6913/8897], Loss: 5.5807\n",
      "Epoch [1/1], Step [6914/8897], Loss: 5.6029\n",
      "Epoch [1/1], Step [6915/8897], Loss: 5.5006\n",
      "Epoch [1/1], Step [6916/8897], Loss: 5.7919\n",
      "Epoch [1/1], Step [6917/8897], Loss: 5.5669\n",
      "Epoch [1/1], Step [6918/8897], Loss: 5.7285\n",
      "Epoch [1/1], Step [6919/8897], Loss: 5.5929\n",
      "Epoch [1/1], Step [6920/8897], Loss: 5.5518\n",
      "Epoch [1/1], Step [6921/8897], Loss: 5.6543\n",
      "Epoch [1/1], Step [6922/8897], Loss: 5.8209\n",
      "Epoch [1/1], Step [6923/8897], Loss: 5.6023\n",
      "Epoch [1/1], Step [6924/8897], Loss: 5.6585\n",
      "Epoch [1/1], Step [6925/8897], Loss: 5.6987\n",
      "Epoch [1/1], Step [6926/8897], Loss: 5.9060\n",
      "Epoch [1/1], Step [6927/8897], Loss: 5.7204\n",
      "Epoch [1/1], Step [6928/8897], Loss: 5.6894\n",
      "Epoch [1/1], Step [6929/8897], Loss: 5.4716\n",
      "Epoch [1/1], Step [6930/8897], Loss: 5.5846\n",
      "Epoch [1/1], Step [6931/8897], Loss: 5.6526\n",
      "Epoch [1/1], Step [6932/8897], Loss: 5.5880\n",
      "Epoch [1/1], Step [6933/8897], Loss: 5.4899\n",
      "Epoch [1/1], Step [6934/8897], Loss: 5.5338\n",
      "Epoch [1/1], Step [6935/8897], Loss: 5.3013\n",
      "Epoch [1/1], Step [6936/8897], Loss: 5.4639\n",
      "Epoch [1/1], Step [6937/8897], Loss: 5.3010\n",
      "Epoch [1/1], Step [6938/8897], Loss: 5.8385\n",
      "Epoch [1/1], Step [6939/8897], Loss: 5.5705\n",
      "Epoch [1/1], Step [6940/8897], Loss: 5.6127\n",
      "Epoch [1/1], Step [6941/8897], Loss: 5.8408\n",
      "Epoch [1/1], Step [6942/8897], Loss: 5.5060\n",
      "Epoch [1/1], Step [6943/8897], Loss: 5.4097\n",
      "Epoch [1/1], Step [6944/8897], Loss: 5.6700\n",
      "Epoch [1/1], Step [6945/8897], Loss: 5.4771\n",
      "Epoch [1/1], Step [6946/8897], Loss: 5.7548\n",
      "Epoch [1/1], Step [6947/8897], Loss: 5.7780\n",
      "Epoch [1/1], Step [6948/8897], Loss: 5.7270\n",
      "Epoch [1/1], Step [6949/8897], Loss: 5.7708\n",
      "Epoch [1/1], Step [6950/8897], Loss: 5.6153\n",
      "Epoch [1/1], Step [6951/8897], Loss: 5.4233\n",
      "Epoch [1/1], Step [6952/8897], Loss: 5.6543\n",
      "Epoch [1/1], Step [6953/8897], Loss: 5.7484\n",
      "Epoch [1/1], Step [6954/8897], Loss: 5.5125\n",
      "Epoch [1/1], Step [6955/8897], Loss: 5.6700\n",
      "Epoch [1/1], Step [6956/8897], Loss: 5.6140\n",
      "Epoch [1/1], Step [6957/8897], Loss: 5.4930\n",
      "Epoch [1/1], Step [6958/8897], Loss: 5.5359\n",
      "Epoch [1/1], Step [6959/8897], Loss: 5.5385\n",
      "Epoch [1/1], Step [6960/8897], Loss: 5.7505\n",
      "Epoch [1/1], Step [6961/8897], Loss: 5.5777\n",
      "Epoch [1/1], Step [6962/8897], Loss: 5.7990\n",
      "Epoch [1/1], Step [6963/8897], Loss: 5.7500\n",
      "Epoch [1/1], Step [6964/8897], Loss: 5.6498\n",
      "Epoch [1/1], Step [6965/8897], Loss: 5.3907\n",
      "Epoch [1/1], Step [6966/8897], Loss: 5.5667\n",
      "Epoch [1/1], Step [6967/8897], Loss: 5.6080\n",
      "Epoch [1/1], Step [6968/8897], Loss: 5.7574\n",
      "Epoch [1/1], Step [6969/8897], Loss: 5.7038\n",
      "Epoch [1/1], Step [6970/8897], Loss: 5.6666\n",
      "Epoch [1/1], Step [6971/8897], Loss: 5.5476\n",
      "Epoch [1/1], Step [6972/8897], Loss: 5.6214\n",
      "Epoch [1/1], Step [6973/8897], Loss: 5.5359\n",
      "Epoch [1/1], Step [6974/8897], Loss: 5.7171\n",
      "Epoch [1/1], Step [6975/8897], Loss: 5.8069\n",
      "Epoch [1/1], Step [6976/8897], Loss: 5.4074\n",
      "Epoch [1/1], Step [6977/8897], Loss: 5.5416\n",
      "Epoch [1/1], Step [6978/8897], Loss: 5.6205\n",
      "Epoch [1/1], Step [6979/8897], Loss: 5.4815\n",
      "Epoch [1/1], Step [6980/8897], Loss: 5.3421\n",
      "Epoch [1/1], Step [6981/8897], Loss: 5.9019\n",
      "Epoch [1/1], Step [6982/8897], Loss: 5.6019\n",
      "Epoch [1/1], Step [6983/8897], Loss: 5.6901\n",
      "Epoch [1/1], Step [6984/8897], Loss: 5.8199\n",
      "Epoch [1/1], Step [6985/8897], Loss: 5.9348\n",
      "Epoch [1/1], Step [6986/8897], Loss: 5.5090\n",
      "Epoch [1/1], Step [6987/8897], Loss: 5.3829\n",
      "Epoch [1/1], Step [6988/8897], Loss: 5.2935\n",
      "Epoch [1/1], Step [6989/8897], Loss: 5.7887\n",
      "Epoch [1/1], Step [6990/8897], Loss: 5.6433\n",
      "Epoch [1/1], Step [6991/8897], Loss: 5.5631\n",
      "Epoch [1/1], Step [6992/8897], Loss: 5.5500\n",
      "Epoch [1/1], Step [6993/8897], Loss: 5.3731\n",
      "Epoch [1/1], Step [6994/8897], Loss: 5.5299\n",
      "Epoch [1/1], Step [6995/8897], Loss: 5.4912\n",
      "Epoch [1/1], Step [6996/8897], Loss: 5.7206\n",
      "Epoch [1/1], Step [6997/8897], Loss: 5.5389\n",
      "Epoch [1/1], Step [6998/8897], Loss: 5.6389\n",
      "Epoch [1/1], Step [6999/8897], Loss: 5.6600\n",
      "Epoch [1/1], Step [7000/8897], Loss: 5.7409\n",
      "Epoch [1/1], Step [7001/8897], Loss: 5.7144\n",
      "Epoch [1/1], Step [7002/8897], Loss: 5.8288\n",
      "Epoch [1/1], Step [7003/8897], Loss: 5.8042\n",
      "Epoch [1/1], Step [7004/8897], Loss: 5.6444\n",
      "Epoch [1/1], Step [7005/8897], Loss: 5.5347\n",
      "Epoch [1/1], Step [7006/8897], Loss: 5.4924\n",
      "Epoch [1/1], Step [7007/8897], Loss: 5.5805\n",
      "Epoch [1/1], Step [7008/8897], Loss: 5.5841\n",
      "Epoch [1/1], Step [7009/8897], Loss: 5.5008\n",
      "Epoch [1/1], Step [7010/8897], Loss: 5.6132\n",
      "Epoch [1/1], Step [7011/8897], Loss: 5.5018\n",
      "Epoch [1/1], Step [7012/8897], Loss: 5.4462\n",
      "Epoch [1/1], Step [7013/8897], Loss: 5.7867\n",
      "Epoch [1/1], Step [7014/8897], Loss: 5.3580\n",
      "Epoch [1/1], Step [7015/8897], Loss: 5.5056\n",
      "Epoch [1/1], Step [7016/8897], Loss: 5.8092\n",
      "Epoch [1/1], Step [7017/8897], Loss: 5.7554\n",
      "Epoch [1/1], Step [7018/8897], Loss: 5.4839\n",
      "Epoch [1/1], Step [7019/8897], Loss: 5.5173\n",
      "Epoch [1/1], Step [7020/8897], Loss: 5.5884\n",
      "Epoch [1/1], Step [7021/8897], Loss: 5.5879\n",
      "Epoch [1/1], Step [7022/8897], Loss: 5.5690\n",
      "Epoch [1/1], Step [7023/8897], Loss: 5.7308\n",
      "Epoch [1/1], Step [7024/8897], Loss: 5.7442\n",
      "Epoch [1/1], Step [7025/8897], Loss: 5.6859\n",
      "Epoch [1/1], Step [7026/8897], Loss: 5.7328\n",
      "Epoch [1/1], Step [7027/8897], Loss: 5.5879\n",
      "Epoch [1/1], Step [7028/8897], Loss: 5.7648\n",
      "Epoch [1/1], Step [7029/8897], Loss: 5.4827\n",
      "Epoch [1/1], Step [7030/8897], Loss: 5.4613\n",
      "Epoch [1/1], Step [7031/8897], Loss: 5.8100\n",
      "Epoch [1/1], Step [7032/8897], Loss: 5.7644\n",
      "Epoch [1/1], Step [7033/8897], Loss: 5.6646\n",
      "Epoch [1/1], Step [7034/8897], Loss: 5.5669\n",
      "Epoch [1/1], Step [7035/8897], Loss: 5.7649\n",
      "Epoch [1/1], Step [7036/8897], Loss: 5.8597\n",
      "Epoch [1/1], Step [7037/8897], Loss: 5.4406\n",
      "Epoch [1/1], Step [7038/8897], Loss: 5.5236\n",
      "Epoch [1/1], Step [7039/8897], Loss: 5.7378\n",
      "Epoch [1/1], Step [7040/8897], Loss: 5.5336\n",
      "Epoch [1/1], Step [7041/8897], Loss: 5.7000\n",
      "Epoch [1/1], Step [7042/8897], Loss: 5.6018\n",
      "Epoch [1/1], Step [7043/8897], Loss: 5.6761\n",
      "Epoch [1/1], Step [7044/8897], Loss: 5.6820\n",
      "Epoch [1/1], Step [7045/8897], Loss: 5.7353\n",
      "Epoch [1/1], Step [7046/8897], Loss: 5.6885\n",
      "Epoch [1/1], Step [7047/8897], Loss: 5.4818\n",
      "Epoch [1/1], Step [7048/8897], Loss: 5.4253\n",
      "Epoch [1/1], Step [7049/8897], Loss: 5.8455\n",
      "Epoch [1/1], Step [7050/8897], Loss: 5.5320\n",
      "Epoch [1/1], Step [7051/8897], Loss: 5.7972\n",
      "Epoch [1/1], Step [7052/8897], Loss: 5.7514\n",
      "Epoch [1/1], Step [7053/8897], Loss: 5.5862\n",
      "Epoch [1/1], Step [7054/8897], Loss: 5.4399\n",
      "Epoch [1/1], Step [7055/8897], Loss: 5.7497\n",
      "Epoch [1/1], Step [7056/8897], Loss: 5.6659\n",
      "Epoch [1/1], Step [7057/8897], Loss: 5.8968\n",
      "Epoch [1/1], Step [7058/8897], Loss: 5.5945\n",
      "Epoch [1/1], Step [7059/8897], Loss: 5.7093\n",
      "Epoch [1/1], Step [7060/8897], Loss: 5.5659\n",
      "Epoch [1/1], Step [7061/8897], Loss: 5.4372\n",
      "Epoch [1/1], Step [7062/8897], Loss: 5.4381\n",
      "Epoch [1/1], Step [7063/8897], Loss: 5.7737\n",
      "Epoch [1/1], Step [7064/8897], Loss: 5.6227\n",
      "Epoch [1/1], Step [7065/8897], Loss: 5.6426\n",
      "Epoch [1/1], Step [7066/8897], Loss: 5.4394\n",
      "Epoch [1/1], Step [7067/8897], Loss: 5.7454\n",
      "Epoch [1/1], Step [7068/8897], Loss: 5.4974\n",
      "Epoch [1/1], Step [7069/8897], Loss: 5.9236\n",
      "Epoch [1/1], Step [7070/8897], Loss: 5.5882\n",
      "Epoch [1/1], Step [7071/8897], Loss: 5.5285\n",
      "Epoch [1/1], Step [7072/8897], Loss: 5.6582\n",
      "Epoch [1/1], Step [7073/8897], Loss: 5.6394\n",
      "Epoch [1/1], Step [7074/8897], Loss: 5.8526\n",
      "Epoch [1/1], Step [7075/8897], Loss: 5.6929\n",
      "Epoch [1/1], Step [7076/8897], Loss: 5.8588\n",
      "Epoch [1/1], Step [7077/8897], Loss: 5.6596\n",
      "Epoch [1/1], Step [7078/8897], Loss: 5.7070\n",
      "Epoch [1/1], Step [7079/8897], Loss: 5.5247\n",
      "Epoch [1/1], Step [7080/8897], Loss: 5.4891\n",
      "Epoch [1/1], Step [7081/8897], Loss: 5.3494\n",
      "Epoch [1/1], Step [7082/8897], Loss: 5.4868\n",
      "Epoch [1/1], Step [7083/8897], Loss: 5.5869\n",
      "Epoch [1/1], Step [7084/8897], Loss: 5.6307\n",
      "Epoch [1/1], Step [7085/8897], Loss: 5.3448\n",
      "Epoch [1/1], Step [7086/8897], Loss: 5.5138\n",
      "Epoch [1/1], Step [7087/8897], Loss: 5.7403\n",
      "Epoch [1/1], Step [7088/8897], Loss: 5.5869\n",
      "Epoch [1/1], Step [7089/8897], Loss: 5.4736\n",
      "Epoch [1/1], Step [7090/8897], Loss: 5.7166\n",
      "Epoch [1/1], Step [7091/8897], Loss: 5.5782\n",
      "Epoch [1/1], Step [7092/8897], Loss: 5.5742\n",
      "Epoch [1/1], Step [7093/8897], Loss: 5.5410\n",
      "Epoch [1/1], Step [7094/8897], Loss: 5.4996\n",
      "Epoch [1/1], Step [7095/8897], Loss: 5.4505\n",
      "Epoch [1/1], Step [7096/8897], Loss: 5.7723\n",
      "Epoch [1/1], Step [7097/8897], Loss: 5.5862\n",
      "Epoch [1/1], Step [7098/8897], Loss: 5.6776\n",
      "Epoch [1/1], Step [7099/8897], Loss: 5.5911\n",
      "Epoch [1/1], Step [7100/8897], Loss: 5.5690\n",
      "Epoch [1/1], Step [7101/8897], Loss: 5.7064\n",
      "Epoch [1/1], Step [7102/8897], Loss: 5.6386\n",
      "Epoch [1/1], Step [7103/8897], Loss: 5.6291\n",
      "Epoch [1/1], Step [7104/8897], Loss: 5.5063\n",
      "Epoch [1/1], Step [7105/8897], Loss: 5.7242\n",
      "Epoch [1/1], Step [7106/8897], Loss: 5.5104\n",
      "Epoch [1/1], Step [7107/8897], Loss: 5.5295\n",
      "Epoch [1/1], Step [7108/8897], Loss: 5.5745\n",
      "Epoch [1/1], Step [7109/8897], Loss: 5.6475\n",
      "Epoch [1/1], Step [7110/8897], Loss: 5.6951\n",
      "Epoch [1/1], Step [7111/8897], Loss: 5.5282\n",
      "Epoch [1/1], Step [7112/8897], Loss: 5.3533\n",
      "Epoch [1/1], Step [7113/8897], Loss: 5.6722\n",
      "Epoch [1/1], Step [7114/8897], Loss: 5.6413\n",
      "Epoch [1/1], Step [7115/8897], Loss: 5.5404\n",
      "Epoch [1/1], Step [7116/8897], Loss: 5.5218\n",
      "Epoch [1/1], Step [7117/8897], Loss: 5.5399\n",
      "Epoch [1/1], Step [7118/8897], Loss: 5.4635\n",
      "Epoch [1/1], Step [7119/8897], Loss: 5.5071\n",
      "Epoch [1/1], Step [7120/8897], Loss: 5.7019\n",
      "Epoch [1/1], Step [7121/8897], Loss: 5.6532\n",
      "Epoch [1/1], Step [7122/8897], Loss: 5.4710\n",
      "Epoch [1/1], Step [7123/8897], Loss: 5.4004\n",
      "Epoch [1/1], Step [7124/8897], Loss: 5.4499\n",
      "Epoch [1/1], Step [7125/8897], Loss: 5.8303\n",
      "Epoch [1/1], Step [7126/8897], Loss: 5.6882\n",
      "Epoch [1/1], Step [7127/8897], Loss: 5.7113\n",
      "Epoch [1/1], Step [7128/8897], Loss: 5.6735\n",
      "Epoch [1/1], Step [7129/8897], Loss: 5.8116\n",
      "Epoch [1/1], Step [7130/8897], Loss: 5.8478\n",
      "Epoch [1/1], Step [7131/8897], Loss: 5.7573\n",
      "Epoch [1/1], Step [7132/8897], Loss: 5.3371\n",
      "Epoch [1/1], Step [7133/8897], Loss: 5.7601\n",
      "Epoch [1/1], Step [7134/8897], Loss: 5.4965\n",
      "Epoch [1/1], Step [7135/8897], Loss: 5.6226\n",
      "Epoch [1/1], Step [7136/8897], Loss: 5.4884\n",
      "Epoch [1/1], Step [7137/8897], Loss: 5.6305\n",
      "Epoch [1/1], Step [7138/8897], Loss: 5.5347\n",
      "Epoch [1/1], Step [7139/8897], Loss: 5.7232\n",
      "Epoch [1/1], Step [7140/8897], Loss: 5.6305\n",
      "Epoch [1/1], Step [7141/8897], Loss: 5.6631\n",
      "Epoch [1/1], Step [7142/8897], Loss: 5.6235\n",
      "Epoch [1/1], Step [7143/8897], Loss: 5.4754\n",
      "Epoch [1/1], Step [7144/8897], Loss: 5.6504\n",
      "Epoch [1/1], Step [7145/8897], Loss: 5.8002\n",
      "Epoch [1/1], Step [7146/8897], Loss: 5.4716\n",
      "Epoch [1/1], Step [7147/8897], Loss: 5.3778\n",
      "Epoch [1/1], Step [7148/8897], Loss: 5.7222\n",
      "Epoch [1/1], Step [7149/8897], Loss: 5.5142\n",
      "Epoch [1/1], Step [7150/8897], Loss: 5.4135\n",
      "Epoch [1/1], Step [7151/8897], Loss: 5.5949\n",
      "Epoch [1/1], Step [7152/8897], Loss: 5.6959\n",
      "Epoch [1/1], Step [7153/8897], Loss: 5.2793\n",
      "Epoch [1/1], Step [7154/8897], Loss: 5.6683\n",
      "Epoch [1/1], Step [7155/8897], Loss: 5.7003\n",
      "Epoch [1/1], Step [7156/8897], Loss: 5.6851\n",
      "Epoch [1/1], Step [7157/8897], Loss: 5.6759\n",
      "Epoch [1/1], Step [7158/8897], Loss: 5.5445\n",
      "Epoch [1/1], Step [7159/8897], Loss: 5.6760\n",
      "Epoch [1/1], Step [7160/8897], Loss: 5.7889\n",
      "Epoch [1/1], Step [7161/8897], Loss: 5.8944\n",
      "Epoch [1/1], Step [7162/8897], Loss: 5.7516\n",
      "Epoch [1/1], Step [7163/8897], Loss: 5.6014\n",
      "Epoch [1/1], Step [7164/8897], Loss: 5.4511\n",
      "Epoch [1/1], Step [7165/8897], Loss: 5.2804\n",
      "Epoch [1/1], Step [7166/8897], Loss: 5.4044\n",
      "Epoch [1/1], Step [7167/8897], Loss: 5.4953\n",
      "Epoch [1/1], Step [7168/8897], Loss: 5.8558\n",
      "Epoch [1/1], Step [7169/8897], Loss: 5.6127\n",
      "Epoch [1/1], Step [7170/8897], Loss: 5.7847\n",
      "Epoch [1/1], Step [7171/8897], Loss: 5.5935\n",
      "Epoch [1/1], Step [7172/8897], Loss: 5.5178\n",
      "Epoch [1/1], Step [7173/8897], Loss: 5.4858\n",
      "Epoch [1/1], Step [7174/8897], Loss: 5.6367\n",
      "Epoch [1/1], Step [7175/8897], Loss: 5.7873\n",
      "Epoch [1/1], Step [7176/8897], Loss: 5.4479\n",
      "Epoch [1/1], Step [7177/8897], Loss: 5.4823\n",
      "Epoch [1/1], Step [7178/8897], Loss: 5.5135\n",
      "Epoch [1/1], Step [7179/8897], Loss: 5.3984\n",
      "Epoch [1/1], Step [7180/8897], Loss: 5.7241\n",
      "Epoch [1/1], Step [7181/8897], Loss: 5.7567\n",
      "Epoch [1/1], Step [7182/8897], Loss: 5.6896\n",
      "Epoch [1/1], Step [7183/8897], Loss: 5.5545\n",
      "Epoch [1/1], Step [7184/8897], Loss: 5.5340\n",
      "Epoch [1/1], Step [7185/8897], Loss: 5.8704\n",
      "Epoch [1/1], Step [7186/8897], Loss: 5.5565\n",
      "Epoch [1/1], Step [7187/8897], Loss: 5.4328\n",
      "Epoch [1/1], Step [7188/8897], Loss: 5.5796\n",
      "Epoch [1/1], Step [7189/8897], Loss: 5.5177\n",
      "Epoch [1/1], Step [7190/8897], Loss: 5.6914\n",
      "Epoch [1/1], Step [7191/8897], Loss: 5.4516\n",
      "Epoch [1/1], Step [7192/8897], Loss: 5.6758\n",
      "Epoch [1/1], Step [7193/8897], Loss: 5.6961\n",
      "Epoch [1/1], Step [7194/8897], Loss: 5.5764\n",
      "Epoch [1/1], Step [7195/8897], Loss: 5.3612\n",
      "Epoch [1/1], Step [7196/8897], Loss: 5.8302\n",
      "Epoch [1/1], Step [7197/8897], Loss: 5.4837\n",
      "Epoch [1/1], Step [7198/8897], Loss: 5.6474\n",
      "Epoch [1/1], Step [7199/8897], Loss: 5.7860\n",
      "Epoch [1/1], Step [7200/8897], Loss: 5.7300\n",
      "Epoch [1/1], Step [7201/8897], Loss: 5.7536\n",
      "Epoch [1/1], Step [7202/8897], Loss: 5.5116\n",
      "Epoch [1/1], Step [7203/8897], Loss: 5.6575\n",
      "Epoch [1/1], Step [7204/8897], Loss: 5.6301\n",
      "Epoch [1/1], Step [7205/8897], Loss: 5.6170\n",
      "Epoch [1/1], Step [7206/8897], Loss: 5.8124\n",
      "Epoch [1/1], Step [7207/8897], Loss: 5.4533\n",
      "Epoch [1/1], Step [7208/8897], Loss: 5.5447\n",
      "Epoch [1/1], Step [7209/8897], Loss: 5.4538\n",
      "Epoch [1/1], Step [7210/8897], Loss: 5.4865\n",
      "Epoch [1/1], Step [7211/8897], Loss: 5.6148\n",
      "Epoch [1/1], Step [7212/8897], Loss: 5.7389\n",
      "Epoch [1/1], Step [7213/8897], Loss: 5.5636\n",
      "Epoch [1/1], Step [7214/8897], Loss: 5.3971\n",
      "Epoch [1/1], Step [7215/8897], Loss: 5.5149\n",
      "Epoch [1/1], Step [7216/8897], Loss: 5.6136\n",
      "Epoch [1/1], Step [7217/8897], Loss: 5.5931\n",
      "Epoch [1/1], Step [7218/8897], Loss: 5.7900\n",
      "Epoch [1/1], Step [7219/8897], Loss: 5.7400\n",
      "Epoch [1/1], Step [7220/8897], Loss: 5.7408\n",
      "Epoch [1/1], Step [7221/8897], Loss: 5.6775\n",
      "Epoch [1/1], Step [7222/8897], Loss: 5.6078\n",
      "Epoch [1/1], Step [7223/8897], Loss: 5.3936\n",
      "Epoch [1/1], Step [7224/8897], Loss: 5.7132\n",
      "Epoch [1/1], Step [7225/8897], Loss: 5.7347\n",
      "Epoch [1/1], Step [7226/8897], Loss: 5.5929\n",
      "Epoch [1/1], Step [7227/8897], Loss: 5.5920\n",
      "Epoch [1/1], Step [7228/8897], Loss: 5.6557\n",
      "Epoch [1/1], Step [7229/8897], Loss: 5.6296\n",
      "Epoch [1/1], Step [7230/8897], Loss: 5.7640\n",
      "Epoch [1/1], Step [7231/8897], Loss: 5.7420\n",
      "Epoch [1/1], Step [7232/8897], Loss: 5.5626\n",
      "Epoch [1/1], Step [7233/8897], Loss: 5.7876\n",
      "Epoch [1/1], Step [7234/8897], Loss: 5.5829\n",
      "Epoch [1/1], Step [7235/8897], Loss: 5.6458\n",
      "Epoch [1/1], Step [7236/8897], Loss: 5.5968\n",
      "Epoch [1/1], Step [7237/8897], Loss: 5.5860\n",
      "Epoch [1/1], Step [7238/8897], Loss: 5.6790\n",
      "Epoch [1/1], Step [7239/8897], Loss: 5.6516\n",
      "Epoch [1/1], Step [7240/8897], Loss: 5.5491\n",
      "Epoch [1/1], Step [7241/8897], Loss: 5.5744\n",
      "Epoch [1/1], Step [7242/8897], Loss: 5.6131\n",
      "Epoch [1/1], Step [7243/8897], Loss: 6.0211\n",
      "Epoch [1/1], Step [7244/8897], Loss: 5.4731\n",
      "Epoch [1/1], Step [7245/8897], Loss: 5.7262\n",
      "Epoch [1/1], Step [7246/8897], Loss: 5.4115\n",
      "Epoch [1/1], Step [7247/8897], Loss: 5.5258\n",
      "Epoch [1/1], Step [7248/8897], Loss: 5.6550\n",
      "Epoch [1/1], Step [7249/8897], Loss: 5.4459\n",
      "Epoch [1/1], Step [7250/8897], Loss: 5.6754\n",
      "Epoch [1/1], Step [7251/8897], Loss: 5.5636\n",
      "Epoch [1/1], Step [7252/8897], Loss: 5.5754\n",
      "Epoch [1/1], Step [7253/8897], Loss: 5.7838\n",
      "Epoch [1/1], Step [7254/8897], Loss: 5.7295\n",
      "Epoch [1/1], Step [7255/8897], Loss: 5.4761\n",
      "Epoch [1/1], Step [7256/8897], Loss: 5.5501\n",
      "Epoch [1/1], Step [7257/8897], Loss: 5.7064\n",
      "Epoch [1/1], Step [7258/8897], Loss: 5.6480\n",
      "Epoch [1/1], Step [7259/8897], Loss: 5.7158\n",
      "Epoch [1/1], Step [7260/8897], Loss: 5.5604\n",
      "Epoch [1/1], Step [7261/8897], Loss: 5.6098\n",
      "Epoch [1/1], Step [7262/8897], Loss: 5.7070\n",
      "Epoch [1/1], Step [7263/8897], Loss: 5.5097\n",
      "Epoch [1/1], Step [7264/8897], Loss: 5.7044\n",
      "Epoch [1/1], Step [7265/8897], Loss: 5.4931\n",
      "Epoch [1/1], Step [7266/8897], Loss: 5.7265\n",
      "Epoch [1/1], Step [7267/8897], Loss: 5.6448\n",
      "Epoch [1/1], Step [7268/8897], Loss: 5.5513\n",
      "Epoch [1/1], Step [7269/8897], Loss: 5.5588\n",
      "Epoch [1/1], Step [7270/8897], Loss: 5.5380\n",
      "Epoch [1/1], Step [7271/8897], Loss: 5.6949\n",
      "Epoch [1/1], Step [7272/8897], Loss: 5.5208\n",
      "Epoch [1/1], Step [7273/8897], Loss: 5.6648\n",
      "Epoch [1/1], Step [7274/8897], Loss: 5.3621\n",
      "Epoch [1/1], Step [7275/8897], Loss: 5.3674\n",
      "Epoch [1/1], Step [7276/8897], Loss: 5.6316\n",
      "Epoch [1/1], Step [7277/8897], Loss: 5.5537\n",
      "Epoch [1/1], Step [7278/8897], Loss: 5.7087\n",
      "Epoch [1/1], Step [7279/8897], Loss: 5.7591\n",
      "Epoch [1/1], Step [7280/8897], Loss: 5.7703\n",
      "Epoch [1/1], Step [7281/8897], Loss: 5.6257\n",
      "Epoch [1/1], Step [7282/8897], Loss: 5.5303\n",
      "Epoch [1/1], Step [7283/8897], Loss: 5.6767\n",
      "Epoch [1/1], Step [7284/8897], Loss: 5.4639\n",
      "Epoch [1/1], Step [7285/8897], Loss: 5.6639\n",
      "Epoch [1/1], Step [7286/8897], Loss: 5.6722\n",
      "Epoch [1/1], Step [7287/8897], Loss: 5.5187\n",
      "Epoch [1/1], Step [7288/8897], Loss: 5.5878\n",
      "Epoch [1/1], Step [7289/8897], Loss: 5.8362\n",
      "Epoch [1/1], Step [7290/8897], Loss: 5.8133\n",
      "Epoch [1/1], Step [7291/8897], Loss: 5.4573\n",
      "Epoch [1/1], Step [7292/8897], Loss: 5.4825\n",
      "Epoch [1/1], Step [7293/8897], Loss: 5.6454\n",
      "Epoch [1/1], Step [7294/8897], Loss: 5.7549\n",
      "Epoch [1/1], Step [7295/8897], Loss: 5.6284\n",
      "Epoch [1/1], Step [7296/8897], Loss: 5.3718\n",
      "Epoch [1/1], Step [7297/8897], Loss: 5.8116\n",
      "Epoch [1/1], Step [7298/8897], Loss: 5.5981\n",
      "Epoch [1/1], Step [7299/8897], Loss: 5.5527\n",
      "Epoch [1/1], Step [7300/8897], Loss: 5.6835\n",
      "Epoch [1/1], Step [7301/8897], Loss: 5.5728\n",
      "Epoch [1/1], Step [7302/8897], Loss: 5.5685\n",
      "Epoch [1/1], Step [7303/8897], Loss: 5.6644\n",
      "Epoch [1/1], Step [7304/8897], Loss: 5.7252\n",
      "Epoch [1/1], Step [7305/8897], Loss: 5.6190\n",
      "Epoch [1/1], Step [7306/8897], Loss: 5.6154\n",
      "Epoch [1/1], Step [7307/8897], Loss: 5.7637\n",
      "Epoch [1/1], Step [7308/8897], Loss: 5.7096\n",
      "Epoch [1/1], Step [7309/8897], Loss: 5.8216\n",
      "Epoch [1/1], Step [7310/8897], Loss: 5.6600\n",
      "Epoch [1/1], Step [7311/8897], Loss: 5.6680\n",
      "Epoch [1/1], Step [7312/8897], Loss: 5.9621\n",
      "Epoch [1/1], Step [7313/8897], Loss: 5.5574\n",
      "Epoch [1/1], Step [7314/8897], Loss: 5.7432\n",
      "Epoch [1/1], Step [7315/8897], Loss: 5.6950\n",
      "Epoch [1/1], Step [7316/8897], Loss: 5.7427\n",
      "Epoch [1/1], Step [7317/8897], Loss: 5.4907\n",
      "Epoch [1/1], Step [7318/8897], Loss: 5.7417\n",
      "Epoch [1/1], Step [7319/8897], Loss: 5.6504\n",
      "Epoch [1/1], Step [7320/8897], Loss: 5.5681\n",
      "Epoch [1/1], Step [7321/8897], Loss: 5.7022\n",
      "Epoch [1/1], Step [7322/8897], Loss: 5.7095\n",
      "Epoch [1/1], Step [7323/8897], Loss: 5.6726\n",
      "Epoch [1/1], Step [7324/8897], Loss: 5.5273\n",
      "Epoch [1/1], Step [7325/8897], Loss: 5.3515\n",
      "Epoch [1/1], Step [7326/8897], Loss: 5.3695\n",
      "Epoch [1/1], Step [7327/8897], Loss: 5.5656\n",
      "Epoch [1/1], Step [7328/8897], Loss: 5.4657\n",
      "Epoch [1/1], Step [7329/8897], Loss: 5.4937\n",
      "Epoch [1/1], Step [7330/8897], Loss: 5.7526\n",
      "Epoch [1/1], Step [7331/8897], Loss: 5.9149\n",
      "Epoch [1/1], Step [7332/8897], Loss: 5.7042\n",
      "Epoch [1/1], Step [7333/8897], Loss: 5.3270\n",
      "Epoch [1/1], Step [7334/8897], Loss: 5.5309\n",
      "Epoch [1/1], Step [7335/8897], Loss: 5.5112\n",
      "Epoch [1/1], Step [7336/8897], Loss: 5.7528\n",
      "Epoch [1/1], Step [7337/8897], Loss: 5.4777\n",
      "Epoch [1/1], Step [7338/8897], Loss: 5.6053\n",
      "Epoch [1/1], Step [7339/8897], Loss: 5.5800\n",
      "Epoch [1/1], Step [7340/8897], Loss: 5.7478\n",
      "Epoch [1/1], Step [7341/8897], Loss: 5.8642\n",
      "Epoch [1/1], Step [7342/8897], Loss: 5.6841\n",
      "Epoch [1/1], Step [7343/8897], Loss: 5.3807\n",
      "Epoch [1/1], Step [7344/8897], Loss: 5.5974\n",
      "Epoch [1/1], Step [7345/8897], Loss: 5.8411\n",
      "Epoch [1/1], Step [7346/8897], Loss: 5.7317\n",
      "Epoch [1/1], Step [7347/8897], Loss: 5.3654\n",
      "Epoch [1/1], Step [7348/8897], Loss: 5.7609\n",
      "Epoch [1/1], Step [7349/8897], Loss: 5.5943\n",
      "Epoch [1/1], Step [7350/8897], Loss: 5.6575\n",
      "Epoch [1/1], Step [7351/8897], Loss: 5.4766\n",
      "Epoch [1/1], Step [7352/8897], Loss: 5.5658\n",
      "Epoch [1/1], Step [7353/8897], Loss: 5.3828\n",
      "Epoch [1/1], Step [7354/8897], Loss: 5.7495\n",
      "Epoch [1/1], Step [7355/8897], Loss: 5.3853\n",
      "Epoch [1/1], Step [7356/8897], Loss: 5.3546\n",
      "Epoch [1/1], Step [7357/8897], Loss: 5.4789\n",
      "Epoch [1/1], Step [7358/8897], Loss: 5.6383\n",
      "Epoch [1/1], Step [7359/8897], Loss: 5.7379\n",
      "Epoch [1/1], Step [7360/8897], Loss: 5.6419\n",
      "Epoch [1/1], Step [7361/8897], Loss: 5.7330\n",
      "Epoch [1/1], Step [7362/8897], Loss: 5.4546\n",
      "Epoch [1/1], Step [7363/8897], Loss: 5.4892\n",
      "Epoch [1/1], Step [7364/8897], Loss: 5.6102\n",
      "Epoch [1/1], Step [7365/8897], Loss: 5.5886\n",
      "Epoch [1/1], Step [7366/8897], Loss: 5.4125\n",
      "Epoch [1/1], Step [7367/8897], Loss: 5.6350\n",
      "Epoch [1/1], Step [7368/8897], Loss: 5.5198\n",
      "Epoch [1/1], Step [7369/8897], Loss: 5.6661\n",
      "Epoch [1/1], Step [7370/8897], Loss: 5.8753\n",
      "Epoch [1/1], Step [7371/8897], Loss: 5.4326\n",
      "Epoch [1/1], Step [7372/8897], Loss: 5.5458\n",
      "Epoch [1/1], Step [7373/8897], Loss: 5.5904\n",
      "Epoch [1/1], Step [7374/8897], Loss: 5.7211\n",
      "Epoch [1/1], Step [7375/8897], Loss: 5.5054\n",
      "Epoch [1/1], Step [7376/8897], Loss: 5.6184\n",
      "Epoch [1/1], Step [7377/8897], Loss: 5.4885\n",
      "Epoch [1/1], Step [7378/8897], Loss: 5.5870\n",
      "Epoch [1/1], Step [7379/8897], Loss: 5.5624\n",
      "Epoch [1/1], Step [7380/8897], Loss: 5.5397\n",
      "Epoch [1/1], Step [7381/8897], Loss: 5.5087\n",
      "Epoch [1/1], Step [7382/8897], Loss: 5.4266\n",
      "Epoch [1/1], Step [7383/8897], Loss: 5.8525\n",
      "Epoch [1/1], Step [7384/8897], Loss: 5.5302\n",
      "Epoch [1/1], Step [7385/8897], Loss: 5.6755\n",
      "Epoch [1/1], Step [7386/8897], Loss: 5.7374\n",
      "Epoch [1/1], Step [7387/8897], Loss: 5.8002\n",
      "Epoch [1/1], Step [7388/8897], Loss: 5.3700\n",
      "Epoch [1/1], Step [7389/8897], Loss: 5.8338\n",
      "Epoch [1/1], Step [7390/8897], Loss: 5.5702\n",
      "Epoch [1/1], Step [7391/8897], Loss: 5.6472\n",
      "Epoch [1/1], Step [7392/8897], Loss: 5.4593\n",
      "Epoch [1/1], Step [7393/8897], Loss: 5.6106\n",
      "Epoch [1/1], Step [7394/8897], Loss: 5.6131\n",
      "Epoch [1/1], Step [7395/8897], Loss: 5.5926\n",
      "Epoch [1/1], Step [7396/8897], Loss: 5.6957\n",
      "Epoch [1/1], Step [7397/8897], Loss: 5.4473\n",
      "Epoch [1/1], Step [7398/8897], Loss: 5.5484\n",
      "Epoch [1/1], Step [7399/8897], Loss: 5.5479\n",
      "Epoch [1/1], Step [7400/8897], Loss: 5.4102\n",
      "Epoch [1/1], Step [7401/8897], Loss: 5.7501\n",
      "Epoch [1/1], Step [7402/8897], Loss: 5.6466\n",
      "Epoch [1/1], Step [7403/8897], Loss: 5.7426\n",
      "Epoch [1/1], Step [7404/8897], Loss: 5.4032\n",
      "Epoch [1/1], Step [7405/8897], Loss: 5.6055\n",
      "Epoch [1/1], Step [7406/8897], Loss: 5.7266\n",
      "Epoch [1/1], Step [7407/8897], Loss: 5.5662\n",
      "Epoch [1/1], Step [7408/8897], Loss: 5.9118\n",
      "Epoch [1/1], Step [7409/8897], Loss: 5.6166\n",
      "Epoch [1/1], Step [7410/8897], Loss: 5.4483\n",
      "Epoch [1/1], Step [7411/8897], Loss: 5.4890\n",
      "Epoch [1/1], Step [7412/8897], Loss: 5.6811\n",
      "Epoch [1/1], Step [7413/8897], Loss: 5.4923\n",
      "Epoch [1/1], Step [7414/8897], Loss: 5.3961\n",
      "Epoch [1/1], Step [7415/8897], Loss: 5.6336\n",
      "Epoch [1/1], Step [7416/8897], Loss: 5.6685\n",
      "Epoch [1/1], Step [7417/8897], Loss: 5.3176\n",
      "Epoch [1/1], Step [7418/8897], Loss: 5.5730\n",
      "Epoch [1/1], Step [7419/8897], Loss: 5.2417\n",
      "Epoch [1/1], Step [7420/8897], Loss: 5.5764\n",
      "Epoch [1/1], Step [7421/8897], Loss: 5.6306\n",
      "Epoch [1/1], Step [7422/8897], Loss: 5.7780\n",
      "Epoch [1/1], Step [7423/8897], Loss: 5.5401\n",
      "Epoch [1/1], Step [7424/8897], Loss: 5.6784\n",
      "Epoch [1/1], Step [7425/8897], Loss: 5.5352\n",
      "Epoch [1/1], Step [7426/8897], Loss: 5.5021\n",
      "Epoch [1/1], Step [7427/8897], Loss: 5.6247\n",
      "Epoch [1/1], Step [7428/8897], Loss: 5.4409\n",
      "Epoch [1/1], Step [7429/8897], Loss: 5.7319\n",
      "Epoch [1/1], Step [7430/8897], Loss: 5.3931\n",
      "Epoch [1/1], Step [7431/8897], Loss: 5.7646\n",
      "Epoch [1/1], Step [7432/8897], Loss: 5.3464\n",
      "Epoch [1/1], Step [7433/8897], Loss: 5.4557\n",
      "Epoch [1/1], Step [7434/8897], Loss: 5.7052\n",
      "Epoch [1/1], Step [7435/8897], Loss: 5.5873\n",
      "Epoch [1/1], Step [7436/8897], Loss: 5.6049\n",
      "Epoch [1/1], Step [7437/8897], Loss: 5.4962\n",
      "Epoch [1/1], Step [7438/8897], Loss: 5.6508\n",
      "Epoch [1/1], Step [7439/8897], Loss: 5.6796\n",
      "Epoch [1/1], Step [7440/8897], Loss: 5.7563\n",
      "Epoch [1/1], Step [7441/8897], Loss: 5.6868\n",
      "Epoch [1/1], Step [7442/8897], Loss: 5.7100\n",
      "Epoch [1/1], Step [7443/8897], Loss: 5.6233\n",
      "Epoch [1/1], Step [7444/8897], Loss: 5.8018\n",
      "Epoch [1/1], Step [7445/8897], Loss: 5.5372\n",
      "Epoch [1/1], Step [7446/8897], Loss: 5.4625\n",
      "Epoch [1/1], Step [7447/8897], Loss: 5.6332\n",
      "Epoch [1/1], Step [7448/8897], Loss: 5.5359\n",
      "Epoch [1/1], Step [7449/8897], Loss: 5.5651\n",
      "Epoch [1/1], Step [7450/8897], Loss: 5.5060\n",
      "Epoch [1/1], Step [7451/8897], Loss: 5.4854\n",
      "Epoch [1/1], Step [7452/8897], Loss: 5.6821\n",
      "Epoch [1/1], Step [7453/8897], Loss: 5.4238\n",
      "Epoch [1/1], Step [7454/8897], Loss: 5.6298\n",
      "Epoch [1/1], Step [7455/8897], Loss: 5.3387\n",
      "Epoch [1/1], Step [7456/8897], Loss: 5.5729\n",
      "Epoch [1/1], Step [7457/8897], Loss: 5.8556\n",
      "Epoch [1/1], Step [7458/8897], Loss: 5.4942\n",
      "Epoch [1/1], Step [7459/8897], Loss: 5.2592\n",
      "Epoch [1/1], Step [7460/8897], Loss: 5.4832\n",
      "Epoch [1/1], Step [7461/8897], Loss: 5.3973\n",
      "Epoch [1/1], Step [7462/8897], Loss: 5.4291\n",
      "Epoch [1/1], Step [7463/8897], Loss: 5.6235\n",
      "Epoch [1/1], Step [7464/8897], Loss: 5.8272\n",
      "Epoch [1/1], Step [7465/8897], Loss: 5.6582\n",
      "Epoch [1/1], Step [7466/8897], Loss: 5.5934\n",
      "Epoch [1/1], Step [7467/8897], Loss: 5.6700\n",
      "Epoch [1/1], Step [7468/8897], Loss: 5.3044\n",
      "Epoch [1/1], Step [7469/8897], Loss: 5.5480\n",
      "Epoch [1/1], Step [7470/8897], Loss: 5.5415\n",
      "Epoch [1/1], Step [7471/8897], Loss: 5.7260\n",
      "Epoch [1/1], Step [7472/8897], Loss: 5.7462\n",
      "Epoch [1/1], Step [7473/8897], Loss: 5.3917\n",
      "Epoch [1/1], Step [7474/8897], Loss: 5.6570\n",
      "Epoch [1/1], Step [7475/8897], Loss: 5.6128\n",
      "Epoch [1/1], Step [7476/8897], Loss: 5.4179\n",
      "Epoch [1/1], Step [7477/8897], Loss: 5.5063\n",
      "Epoch [1/1], Step [7478/8897], Loss: 5.7060\n",
      "Epoch [1/1], Step [7479/8897], Loss: 5.5043\n",
      "Epoch [1/1], Step [7480/8897], Loss: 5.6236\n",
      "Epoch [1/1], Step [7481/8897], Loss: 5.3953\n",
      "Epoch [1/1], Step [7482/8897], Loss: 5.7989\n",
      "Epoch [1/1], Step [7483/8897], Loss: 5.7485\n",
      "Epoch [1/1], Step [7484/8897], Loss: 5.6723\n",
      "Epoch [1/1], Step [7485/8897], Loss: 5.5216\n",
      "Epoch [1/1], Step [7486/8897], Loss: 5.4282\n",
      "Epoch [1/1], Step [7487/8897], Loss: 5.7606\n",
      "Epoch [1/1], Step [7488/8897], Loss: 5.4057\n",
      "Epoch [1/1], Step [7489/8897], Loss: 5.7748\n",
      "Epoch [1/1], Step [7490/8897], Loss: 5.7448\n",
      "Epoch [1/1], Step [7491/8897], Loss: 5.5757\n",
      "Epoch [1/1], Step [7492/8897], Loss: 5.5736\n",
      "Epoch [1/1], Step [7493/8897], Loss: 5.7211\n",
      "Epoch [1/1], Step [7494/8897], Loss: 5.6149\n",
      "Epoch [1/1], Step [7495/8897], Loss: 5.4393\n",
      "Epoch [1/1], Step [7496/8897], Loss: 5.7920\n",
      "Epoch [1/1], Step [7497/8897], Loss: 5.7373\n",
      "Epoch [1/1], Step [7498/8897], Loss: 5.5326\n",
      "Epoch [1/1], Step [7499/8897], Loss: 5.7053\n",
      "Epoch [1/1], Step [7500/8897], Loss: 5.6410\n",
      "Epoch [1/1], Step [7501/8897], Loss: 5.6397\n",
      "Epoch [1/1], Step [7502/8897], Loss: 5.6054\n",
      "Epoch [1/1], Step [7503/8897], Loss: 5.5652\n",
      "Epoch [1/1], Step [7504/8897], Loss: 5.4349\n",
      "Epoch [1/1], Step [7505/8897], Loss: 5.3372\n",
      "Epoch [1/1], Step [7506/8897], Loss: 5.6601\n",
      "Epoch [1/1], Step [7507/8897], Loss: 5.7709\n",
      "Epoch [1/1], Step [7508/8897], Loss: 5.3132\n",
      "Epoch [1/1], Step [7509/8897], Loss: 5.3728\n",
      "Epoch [1/1], Step [7510/8897], Loss: 5.5914\n",
      "Epoch [1/1], Step [7511/8897], Loss: 5.5576\n",
      "Epoch [1/1], Step [7512/8897], Loss: 5.5206\n",
      "Epoch [1/1], Step [7513/8897], Loss: 5.7246\n",
      "Epoch [1/1], Step [7514/8897], Loss: 5.6566\n",
      "Epoch [1/1], Step [7515/8897], Loss: 5.6486\n",
      "Epoch [1/1], Step [7516/8897], Loss: 5.8683\n",
      "Epoch [1/1], Step [7517/8897], Loss: 5.5535\n",
      "Epoch [1/1], Step [7518/8897], Loss: 5.6790\n",
      "Epoch [1/1], Step [7519/8897], Loss: 5.5575\n",
      "Epoch [1/1], Step [7520/8897], Loss: 5.5172\n",
      "Epoch [1/1], Step [7521/8897], Loss: 5.4458\n",
      "Epoch [1/1], Step [7522/8897], Loss: 5.5421\n",
      "Epoch [1/1], Step [7523/8897], Loss: 5.4100\n",
      "Epoch [1/1], Step [7524/8897], Loss: 5.5516\n",
      "Epoch [1/1], Step [7525/8897], Loss: 5.5356\n",
      "Epoch [1/1], Step [7526/8897], Loss: 5.3949\n",
      "Epoch [1/1], Step [7527/8897], Loss: 5.4191\n",
      "Epoch [1/1], Step [7528/8897], Loss: 5.3968\n",
      "Epoch [1/1], Step [7529/8897], Loss: 5.3379\n",
      "Epoch [1/1], Step [7530/8897], Loss: 5.5667\n",
      "Epoch [1/1], Step [7531/8897], Loss: 5.7355\n",
      "Epoch [1/1], Step [7532/8897], Loss: 5.4655\n",
      "Epoch [1/1], Step [7533/8897], Loss: 5.4642\n",
      "Epoch [1/1], Step [7534/8897], Loss: 5.4213\n",
      "Epoch [1/1], Step [7535/8897], Loss: 5.5239\n",
      "Epoch [1/1], Step [7536/8897], Loss: 5.6145\n",
      "Epoch [1/1], Step [7537/8897], Loss: 5.6591\n",
      "Epoch [1/1], Step [7538/8897], Loss: 5.7748\n",
      "Epoch [1/1], Step [7539/8897], Loss: 5.7351\n",
      "Epoch [1/1], Step [7540/8897], Loss: 5.6945\n",
      "Epoch [1/1], Step [7541/8897], Loss: 5.3000\n",
      "Epoch [1/1], Step [7542/8897], Loss: 5.6506\n",
      "Epoch [1/1], Step [7543/8897], Loss: 5.6614\n",
      "Epoch [1/1], Step [7544/8897], Loss: 5.4762\n",
      "Epoch [1/1], Step [7545/8897], Loss: 5.4449\n",
      "Epoch [1/1], Step [7546/8897], Loss: 5.6521\n",
      "Epoch [1/1], Step [7547/8897], Loss: 5.5549\n",
      "Epoch [1/1], Step [7548/8897], Loss: 5.4749\n",
      "Epoch [1/1], Step [7549/8897], Loss: 5.6486\n",
      "Epoch [1/1], Step [7550/8897], Loss: 5.7122\n",
      "Epoch [1/1], Step [7551/8897], Loss: 5.4081\n",
      "Epoch [1/1], Step [7552/8897], Loss: 5.8287\n",
      "Epoch [1/1], Step [7553/8897], Loss: 5.4588\n",
      "Epoch [1/1], Step [7554/8897], Loss: 5.8699\n",
      "Epoch [1/1], Step [7555/8897], Loss: 5.4851\n",
      "Epoch [1/1], Step [7556/8897], Loss: 5.5548\n",
      "Epoch [1/1], Step [7557/8897], Loss: 5.7001\n",
      "Epoch [1/1], Step [7558/8897], Loss: 5.5559\n",
      "Epoch [1/1], Step [7559/8897], Loss: 5.6960\n",
      "Epoch [1/1], Step [7560/8897], Loss: 5.4075\n",
      "Epoch [1/1], Step [7561/8897], Loss: 5.5314\n",
      "Epoch [1/1], Step [7562/8897], Loss: 5.5971\n",
      "Epoch [1/1], Step [7563/8897], Loss: 5.4274\n",
      "Epoch [1/1], Step [7564/8897], Loss: 5.4852\n",
      "Epoch [1/1], Step [7565/8897], Loss: 5.7257\n",
      "Epoch [1/1], Step [7566/8897], Loss: 5.6152\n",
      "Epoch [1/1], Step [7567/8897], Loss: 5.8055\n",
      "Epoch [1/1], Step [7568/8897], Loss: 5.5929\n",
      "Epoch [1/1], Step [7569/8897], Loss: 5.6441\n",
      "Epoch [1/1], Step [7570/8897], Loss: 5.6016\n",
      "Epoch [1/1], Step [7571/8897], Loss: 5.5105\n",
      "Epoch [1/1], Step [7572/8897], Loss: 5.5452\n",
      "Epoch [1/1], Step [7573/8897], Loss: 5.6716\n",
      "Epoch [1/1], Step [7574/8897], Loss: 5.4610\n",
      "Epoch [1/1], Step [7575/8897], Loss: 5.2971\n",
      "Epoch [1/1], Step [7576/8897], Loss: 5.3422\n",
      "Epoch [1/1], Step [7577/8897], Loss: 5.5100\n",
      "Epoch [1/1], Step [7578/8897], Loss: 5.5629\n",
      "Epoch [1/1], Step [7579/8897], Loss: 5.4039\n",
      "Epoch [1/1], Step [7580/8897], Loss: 5.3796\n",
      "Epoch [1/1], Step [7581/8897], Loss: 5.4123\n",
      "Epoch [1/1], Step [7582/8897], Loss: 5.5182\n",
      "Epoch [1/1], Step [7583/8897], Loss: 5.4655\n",
      "Epoch [1/1], Step [7584/8897], Loss: 5.7581\n",
      "Epoch [1/1], Step [7585/8897], Loss: 5.5850\n",
      "Epoch [1/1], Step [7586/8897], Loss: 5.7941\n",
      "Epoch [1/1], Step [7587/8897], Loss: 5.5035\n",
      "Epoch [1/1], Step [7588/8897], Loss: 5.5268\n",
      "Epoch [1/1], Step [7589/8897], Loss: 5.5352\n",
      "Epoch [1/1], Step [7590/8897], Loss: 5.5566\n",
      "Epoch [1/1], Step [7591/8897], Loss: 5.8663\n",
      "Epoch [1/1], Step [7592/8897], Loss: 5.8070\n",
      "Epoch [1/1], Step [7593/8897], Loss: 5.5787\n",
      "Epoch [1/1], Step [7594/8897], Loss: 5.7895\n",
      "Epoch [1/1], Step [7595/8897], Loss: 5.4402\n",
      "Epoch [1/1], Step [7596/8897], Loss: 5.4411\n",
      "Epoch [1/1], Step [7597/8897], Loss: 5.5815\n",
      "Epoch [1/1], Step [7598/8897], Loss: 5.6161\n",
      "Epoch [1/1], Step [7599/8897], Loss: 5.4849\n",
      "Epoch [1/1], Step [7600/8897], Loss: 5.3626\n",
      "Epoch [1/1], Step [7601/8897], Loss: 5.5170\n",
      "Epoch [1/1], Step [7602/8897], Loss: 5.4665\n",
      "Epoch [1/1], Step [7603/8897], Loss: 5.5437\n",
      "Epoch [1/1], Step [7604/8897], Loss: 5.6929\n",
      "Epoch [1/1], Step [7605/8897], Loss: 5.7515\n",
      "Epoch [1/1], Step [7606/8897], Loss: 5.4659\n",
      "Epoch [1/1], Step [7607/8897], Loss: 5.4810\n",
      "Epoch [1/1], Step [7608/8897], Loss: 5.4658\n",
      "Epoch [1/1], Step [7609/8897], Loss: 5.6153\n",
      "Epoch [1/1], Step [7610/8897], Loss: 5.3962\n",
      "Epoch [1/1], Step [7611/8897], Loss: 5.7304\n",
      "Epoch [1/1], Step [7612/8897], Loss: 5.7350\n",
      "Epoch [1/1], Step [7613/8897], Loss: 5.5281\n",
      "Epoch [1/1], Step [7614/8897], Loss: 5.6197\n",
      "Epoch [1/1], Step [7615/8897], Loss: 5.6560\n",
      "Epoch [1/1], Step [7616/8897], Loss: 5.3855\n",
      "Epoch [1/1], Step [7617/8897], Loss: 5.6798\n",
      "Epoch [1/1], Step [7618/8897], Loss: 5.6326\n",
      "Epoch [1/1], Step [7619/8897], Loss: 5.8045\n",
      "Epoch [1/1], Step [7620/8897], Loss: 5.5708\n",
      "Epoch [1/1], Step [7621/8897], Loss: 5.6743\n",
      "Epoch [1/1], Step [7622/8897], Loss: 5.7677\n",
      "Epoch [1/1], Step [7623/8897], Loss: 5.6307\n",
      "Epoch [1/1], Step [7624/8897], Loss: 5.5385\n",
      "Epoch [1/1], Step [7625/8897], Loss: 5.4393\n",
      "Epoch [1/1], Step [7626/8897], Loss: 5.5089\n",
      "Epoch [1/1], Step [7627/8897], Loss: 5.3453\n",
      "Epoch [1/1], Step [7628/8897], Loss: 5.5538\n",
      "Epoch [1/1], Step [7629/8897], Loss: 5.6177\n",
      "Epoch [1/1], Step [7630/8897], Loss: 5.5181\n",
      "Epoch [1/1], Step [7631/8897], Loss: 5.5381\n",
      "Epoch [1/1], Step [7632/8897], Loss: 5.6226\n",
      "Epoch [1/1], Step [7633/8897], Loss: 5.5627\n",
      "Epoch [1/1], Step [7634/8897], Loss: 5.5528\n",
      "Epoch [1/1], Step [7635/8897], Loss: 5.7928\n",
      "Epoch [1/1], Step [7636/8897], Loss: 5.6215\n",
      "Epoch [1/1], Step [7637/8897], Loss: 5.5374\n",
      "Epoch [1/1], Step [7638/8897], Loss: 5.3903\n",
      "Epoch [1/1], Step [7639/8897], Loss: 5.4529\n",
      "Epoch [1/1], Step [7640/8897], Loss: 5.6466\n",
      "Epoch [1/1], Step [7641/8897], Loss: 5.3554\n",
      "Epoch [1/1], Step [7642/8897], Loss: 5.8474\n",
      "Epoch [1/1], Step [7643/8897], Loss: 5.5718\n",
      "Epoch [1/1], Step [7644/8897], Loss: 5.4651\n",
      "Epoch [1/1], Step [7645/8897], Loss: 5.8100\n",
      "Epoch [1/1], Step [7646/8897], Loss: 5.4197\n",
      "Epoch [1/1], Step [7647/8897], Loss: 5.6034\n",
      "Epoch [1/1], Step [7648/8897], Loss: 5.2417\n",
      "Epoch [1/1], Step [7649/8897], Loss: 5.5248\n",
      "Epoch [1/1], Step [7650/8897], Loss: 5.3992\n",
      "Epoch [1/1], Step [7651/8897], Loss: 5.6599\n",
      "Epoch [1/1], Step [7652/8897], Loss: 5.7342\n",
      "Epoch [1/1], Step [7653/8897], Loss: 5.6342\n",
      "Epoch [1/1], Step [7654/8897], Loss: 5.5455\n",
      "Epoch [1/1], Step [7655/8897], Loss: 5.5295\n",
      "Epoch [1/1], Step [7656/8897], Loss: 5.1723\n",
      "Epoch [1/1], Step [7657/8897], Loss: 5.4166\n",
      "Epoch [1/1], Step [7658/8897], Loss: 5.4537\n",
      "Epoch [1/1], Step [7659/8897], Loss: 5.5202\n",
      "Epoch [1/1], Step [7660/8897], Loss: 5.7607\n",
      "Epoch [1/1], Step [7661/8897], Loss: 5.7114\n",
      "Epoch [1/1], Step [7662/8897], Loss: 5.3716\n",
      "Epoch [1/1], Step [7663/8897], Loss: 5.3870\n",
      "Epoch [1/1], Step [7664/8897], Loss: 5.7345\n",
      "Epoch [1/1], Step [7665/8897], Loss: 5.4470\n",
      "Epoch [1/1], Step [7666/8897], Loss: 5.6490\n",
      "Epoch [1/1], Step [7667/8897], Loss: 5.6104\n",
      "Epoch [1/1], Step [7668/8897], Loss: 5.6586\n",
      "Epoch [1/1], Step [7669/8897], Loss: 5.6871\n",
      "Epoch [1/1], Step [7670/8897], Loss: 5.9019\n",
      "Epoch [1/1], Step [7671/8897], Loss: 5.4196\n",
      "Epoch [1/1], Step [7672/8897], Loss: 5.5466\n",
      "Epoch [1/1], Step [7673/8897], Loss: 5.7054\n",
      "Epoch [1/1], Step [7674/8897], Loss: 5.5854\n",
      "Epoch [1/1], Step [7675/8897], Loss: 5.5987\n",
      "Epoch [1/1], Step [7676/8897], Loss: 5.7187\n",
      "Epoch [1/1], Step [7677/8897], Loss: 5.4847\n",
      "Epoch [1/1], Step [7678/8897], Loss: 5.5190\n",
      "Epoch [1/1], Step [7679/8897], Loss: 5.4240\n",
      "Epoch [1/1], Step [7680/8897], Loss: 5.5311\n",
      "Epoch [1/1], Step [7681/8897], Loss: 5.9071\n",
      "Epoch [1/1], Step [7682/8897], Loss: 5.4684\n",
      "Epoch [1/1], Step [7683/8897], Loss: 5.4343\n",
      "Epoch [1/1], Step [7684/8897], Loss: 5.8583\n",
      "Epoch [1/1], Step [7685/8897], Loss: 5.5835\n",
      "Epoch [1/1], Step [7686/8897], Loss: 5.3873\n",
      "Epoch [1/1], Step [7687/8897], Loss: 5.4465\n",
      "Epoch [1/1], Step [7688/8897], Loss: 5.3843\n",
      "Epoch [1/1], Step [7689/8897], Loss: 5.6774\n",
      "Epoch [1/1], Step [7690/8897], Loss: 5.7068\n",
      "Epoch [1/1], Step [7691/8897], Loss: 5.5344\n",
      "Epoch [1/1], Step [7692/8897], Loss: 5.7698\n",
      "Epoch [1/1], Step [7693/8897], Loss: 5.4193\n",
      "Epoch [1/1], Step [7694/8897], Loss: 5.5575\n",
      "Epoch [1/1], Step [7695/8897], Loss: 5.5998\n",
      "Epoch [1/1], Step [7696/8897], Loss: 5.3394\n",
      "Epoch [1/1], Step [7697/8897], Loss: 5.5950\n",
      "Epoch [1/1], Step [7698/8897], Loss: 5.3853\n",
      "Epoch [1/1], Step [7699/8897], Loss: 5.6834\n",
      "Epoch [1/1], Step [7700/8897], Loss: 5.7486\n",
      "Epoch [1/1], Step [7701/8897], Loss: 5.8348\n",
      "Epoch [1/1], Step [7702/8897], Loss: 5.3386\n",
      "Epoch [1/1], Step [7703/8897], Loss: 5.4432\n",
      "Epoch [1/1], Step [7704/8897], Loss: 5.5723\n",
      "Epoch [1/1], Step [7705/8897], Loss: 5.8389\n",
      "Epoch [1/1], Step [7706/8897], Loss: 5.4520\n",
      "Epoch [1/1], Step [7707/8897], Loss: 5.5475\n",
      "Epoch [1/1], Step [7708/8897], Loss: 5.6603\n",
      "Epoch [1/1], Step [7709/8897], Loss: 5.5160\n",
      "Epoch [1/1], Step [7710/8897], Loss: 5.6498\n",
      "Epoch [1/1], Step [7711/8897], Loss: 5.4937\n",
      "Epoch [1/1], Step [7712/8897], Loss: 5.6440\n",
      "Epoch [1/1], Step [7713/8897], Loss: 5.3279\n",
      "Epoch [1/1], Step [7714/8897], Loss: 5.5509\n",
      "Epoch [1/1], Step [7715/8897], Loss: 5.1222\n",
      "Epoch [1/1], Step [7716/8897], Loss: 5.5824\n",
      "Epoch [1/1], Step [7717/8897], Loss: 5.4528\n",
      "Epoch [1/1], Step [7718/8897], Loss: 5.4796\n",
      "Epoch [1/1], Step [7719/8897], Loss: 5.6773\n",
      "Epoch [1/1], Step [7720/8897], Loss: 5.5068\n",
      "Epoch [1/1], Step [7721/8897], Loss: 5.5042\n",
      "Epoch [1/1], Step [7722/8897], Loss: 5.6222\n",
      "Epoch [1/1], Step [7723/8897], Loss: 5.7530\n",
      "Epoch [1/1], Step [7724/8897], Loss: 5.6265\n",
      "Epoch [1/1], Step [7725/8897], Loss: 5.6560\n",
      "Epoch [1/1], Step [7726/8897], Loss: 5.4706\n",
      "Epoch [1/1], Step [7727/8897], Loss: 5.5029\n",
      "Epoch [1/1], Step [7728/8897], Loss: 5.6285\n",
      "Epoch [1/1], Step [7729/8897], Loss: 5.3724\n",
      "Epoch [1/1], Step [7730/8897], Loss: 5.5788\n",
      "Epoch [1/1], Step [7731/8897], Loss: 5.5710\n",
      "Epoch [1/1], Step [7732/8897], Loss: 5.6329\n",
      "Epoch [1/1], Step [7733/8897], Loss: 5.6273\n",
      "Epoch [1/1], Step [7734/8897], Loss: 5.3795\n",
      "Epoch [1/1], Step [7735/8897], Loss: 5.7087\n",
      "Epoch [1/1], Step [7736/8897], Loss: 5.5847\n",
      "Epoch [1/1], Step [7737/8897], Loss: 5.6000\n",
      "Epoch [1/1], Step [7738/8897], Loss: 5.6644\n",
      "Epoch [1/1], Step [7739/8897], Loss: 5.4501\n",
      "Epoch [1/1], Step [7740/8897], Loss: 5.3796\n",
      "Epoch [1/1], Step [7741/8897], Loss: 5.5869\n",
      "Epoch [1/1], Step [7742/8897], Loss: 5.3458\n",
      "Epoch [1/1], Step [7743/8897], Loss: 5.6255\n",
      "Epoch [1/1], Step [7744/8897], Loss: 5.6762\n",
      "Epoch [1/1], Step [7745/8897], Loss: 5.6165\n",
      "Epoch [1/1], Step [7746/8897], Loss: 5.5301\n",
      "Epoch [1/1], Step [7747/8897], Loss: 5.6023\n",
      "Epoch [1/1], Step [7748/8897], Loss: 5.8205\n",
      "Epoch [1/1], Step [7749/8897], Loss: 5.6538\n",
      "Epoch [1/1], Step [7750/8897], Loss: 5.3763\n",
      "Epoch [1/1], Step [7751/8897], Loss: 5.3361\n",
      "Epoch [1/1], Step [7752/8897], Loss: 5.6105\n",
      "Epoch [1/1], Step [7753/8897], Loss: 5.6266\n",
      "Epoch [1/1], Step [7754/8897], Loss: 5.5721\n",
      "Epoch [1/1], Step [7755/8897], Loss: 5.4612\n",
      "Epoch [1/1], Step [7756/8897], Loss: 5.7491\n",
      "Epoch [1/1], Step [7757/8897], Loss: 5.3474\n",
      "Epoch [1/1], Step [7758/8897], Loss: 5.8401\n",
      "Epoch [1/1], Step [7759/8897], Loss: 5.6166\n",
      "Epoch [1/1], Step [7760/8897], Loss: 5.7702\n",
      "Epoch [1/1], Step [7761/8897], Loss: 5.3819\n",
      "Epoch [1/1], Step [7762/8897], Loss: 5.5451\n",
      "Epoch [1/1], Step [7763/8897], Loss: 5.4004\n",
      "Epoch [1/1], Step [7764/8897], Loss: 5.5838\n",
      "Epoch [1/1], Step [7765/8897], Loss: 5.6238\n",
      "Epoch [1/1], Step [7766/8897], Loss: 5.6285\n",
      "Epoch [1/1], Step [7767/8897], Loss: 5.3680\n",
      "Epoch [1/1], Step [7768/8897], Loss: 5.3712\n",
      "Epoch [1/1], Step [7769/8897], Loss: 5.5276\n",
      "Epoch [1/1], Step [7770/8897], Loss: 5.6345\n",
      "Epoch [1/1], Step [7771/8897], Loss: 5.6842\n",
      "Epoch [1/1], Step [7772/8897], Loss: 5.5640\n",
      "Epoch [1/1], Step [7773/8897], Loss: 5.5981\n",
      "Epoch [1/1], Step [7774/8897], Loss: 5.3664\n",
      "Epoch [1/1], Step [7775/8897], Loss: 5.5769\n",
      "Epoch [1/1], Step [7776/8897], Loss: 5.7909\n",
      "Epoch [1/1], Step [7777/8897], Loss: 5.5129\n",
      "Epoch [1/1], Step [7778/8897], Loss: 5.7579\n",
      "Epoch [1/1], Step [7779/8897], Loss: 5.6115\n",
      "Epoch [1/1], Step [7780/8897], Loss: 5.4826\n",
      "Epoch [1/1], Step [7781/8897], Loss: 5.5664\n",
      "Epoch [1/1], Step [7782/8897], Loss: 5.5498\n",
      "Epoch [1/1], Step [7783/8897], Loss: 5.3622\n",
      "Epoch [1/1], Step [7784/8897], Loss: 5.5392\n",
      "Epoch [1/1], Step [7785/8897], Loss: 5.6916\n",
      "Epoch [1/1], Step [7786/8897], Loss: 5.7370\n",
      "Epoch [1/1], Step [7787/8897], Loss: 5.4718\n",
      "Epoch [1/1], Step [7788/8897], Loss: 5.5470\n",
      "Epoch [1/1], Step [7789/8897], Loss: 5.3051\n",
      "Epoch [1/1], Step [7790/8897], Loss: 5.4531\n",
      "Epoch [1/1], Step [7791/8897], Loss: 5.5656\n",
      "Epoch [1/1], Step [7792/8897], Loss: 5.4043\n",
      "Epoch [1/1], Step [7793/8897], Loss: 5.5963\n",
      "Epoch [1/1], Step [7794/8897], Loss: 5.5380\n",
      "Epoch [1/1], Step [7795/8897], Loss: 5.6719\n",
      "Epoch [1/1], Step [7796/8897], Loss: 5.6520\n",
      "Epoch [1/1], Step [7797/8897], Loss: 5.7579\n",
      "Epoch [1/1], Step [7798/8897], Loss: 5.0639\n",
      "Epoch [1/1], Step [7799/8897], Loss: 5.4349\n",
      "Epoch [1/1], Step [7800/8897], Loss: 5.6133\n",
      "Epoch [1/1], Step [7801/8897], Loss: 5.5637\n",
      "Epoch [1/1], Step [7802/8897], Loss: 5.5007\n",
      "Epoch [1/1], Step [7803/8897], Loss: 5.4624\n",
      "Epoch [1/1], Step [7804/8897], Loss: 5.5885\n",
      "Epoch [1/1], Step [7805/8897], Loss: 5.5143\n",
      "Epoch [1/1], Step [7806/8897], Loss: 5.5033\n",
      "Epoch [1/1], Step [7807/8897], Loss: 5.6938\n",
      "Epoch [1/1], Step [7808/8897], Loss: 5.4706\n",
      "Epoch [1/1], Step [7809/8897], Loss: 5.7274\n",
      "Epoch [1/1], Step [7810/8897], Loss: 5.3837\n",
      "Epoch [1/1], Step [7811/8897], Loss: 5.4787\n",
      "Epoch [1/1], Step [7812/8897], Loss: 5.5529\n",
      "Epoch [1/1], Step [7813/8897], Loss: 5.3734\n",
      "Epoch [1/1], Step [7814/8897], Loss: 5.3362\n",
      "Epoch [1/1], Step [7815/8897], Loss: 5.4976\n",
      "Epoch [1/1], Step [7816/8897], Loss: 5.4305\n",
      "Epoch [1/1], Step [7817/8897], Loss: 5.6669\n",
      "Epoch [1/1], Step [7818/8897], Loss: 5.7449\n",
      "Epoch [1/1], Step [7819/8897], Loss: 5.5784\n",
      "Epoch [1/1], Step [7820/8897], Loss: 5.7369\n",
      "Epoch [1/1], Step [7821/8897], Loss: 5.4909\n",
      "Epoch [1/1], Step [7822/8897], Loss: 5.5078\n",
      "Epoch [1/1], Step [7823/8897], Loss: 5.5298\n",
      "Epoch [1/1], Step [7824/8897], Loss: 5.4585\n",
      "Epoch [1/1], Step [7825/8897], Loss: 5.4946\n",
      "Epoch [1/1], Step [7826/8897], Loss: 5.5834\n",
      "Epoch [1/1], Step [7827/8897], Loss: 5.5656\n",
      "Epoch [1/1], Step [7828/8897], Loss: 5.5024\n",
      "Epoch [1/1], Step [7829/8897], Loss: 5.5281\n",
      "Epoch [1/1], Step [7830/8897], Loss: 5.7160\n",
      "Epoch [1/1], Step [7831/8897], Loss: 5.6891\n",
      "Epoch [1/1], Step [7832/8897], Loss: 5.8030\n",
      "Epoch [1/1], Step [7833/8897], Loss: 5.8913\n",
      "Epoch [1/1], Step [7834/8897], Loss: 5.4351\n",
      "Epoch [1/1], Step [7835/8897], Loss: 5.4937\n",
      "Epoch [1/1], Step [7836/8897], Loss: 5.4117\n",
      "Epoch [1/1], Step [7837/8897], Loss: 5.5438\n",
      "Epoch [1/1], Step [7838/8897], Loss: 5.5250\n",
      "Epoch [1/1], Step [7839/8897], Loss: 5.5486\n",
      "Epoch [1/1], Step [7840/8897], Loss: 5.3787\n",
      "Epoch [1/1], Step [7841/8897], Loss: 5.5372\n",
      "Epoch [1/1], Step [7842/8897], Loss: 5.4745\n",
      "Epoch [1/1], Step [7843/8897], Loss: 5.3551\n",
      "Epoch [1/1], Step [7844/8897], Loss: 5.5265\n",
      "Epoch [1/1], Step [7845/8897], Loss: 5.3762\n",
      "Epoch [1/1], Step [7846/8897], Loss: 5.3256\n",
      "Epoch [1/1], Step [7847/8897], Loss: 5.5491\n",
      "Epoch [1/1], Step [7848/8897], Loss: 5.6268\n",
      "Epoch [1/1], Step [7849/8897], Loss: 5.5263\n",
      "Epoch [1/1], Step [7850/8897], Loss: 5.6341\n",
      "Epoch [1/1], Step [7851/8897], Loss: 5.7031\n",
      "Epoch [1/1], Step [7852/8897], Loss: 5.6639\n",
      "Epoch [1/1], Step [7853/8897], Loss: 5.5553\n",
      "Epoch [1/1], Step [7854/8897], Loss: 5.4163\n",
      "Epoch [1/1], Step [7855/8897], Loss: 5.5615\n",
      "Epoch [1/1], Step [7856/8897], Loss: 5.6893\n",
      "Epoch [1/1], Step [7857/8897], Loss: 5.7492\n",
      "Epoch [1/1], Step [7858/8897], Loss: 5.4987\n",
      "Epoch [1/1], Step [7859/8897], Loss: 5.5194\n",
      "Epoch [1/1], Step [7860/8897], Loss: 5.7601\n",
      "Epoch [1/1], Step [7861/8897], Loss: 5.4334\n",
      "Epoch [1/1], Step [7862/8897], Loss: 5.6170\n",
      "Epoch [1/1], Step [7863/8897], Loss: 5.6814\n",
      "Epoch [1/1], Step [7864/8897], Loss: 5.2672\n",
      "Epoch [1/1], Step [7865/8897], Loss: 5.6448\n",
      "Epoch [1/1], Step [7866/8897], Loss: 5.6506\n",
      "Epoch [1/1], Step [7867/8897], Loss: 5.7672\n",
      "Epoch [1/1], Step [7868/8897], Loss: 5.4689\n",
      "Epoch [1/1], Step [7869/8897], Loss: 5.3930\n",
      "Epoch [1/1], Step [7870/8897], Loss: 5.6654\n",
      "Epoch [1/1], Step [7871/8897], Loss: 5.5156\n",
      "Epoch [1/1], Step [7872/8897], Loss: 5.5180\n",
      "Epoch [1/1], Step [7873/8897], Loss: 5.3665\n",
      "Epoch [1/1], Step [7874/8897], Loss: 5.6183\n",
      "Epoch [1/1], Step [7875/8897], Loss: 5.4978\n",
      "Epoch [1/1], Step [7876/8897], Loss: 5.5494\n",
      "Epoch [1/1], Step [7877/8897], Loss: 5.6113\n",
      "Epoch [1/1], Step [7878/8897], Loss: 5.4947\n",
      "Epoch [1/1], Step [7879/8897], Loss: 5.2261\n",
      "Epoch [1/1], Step [7880/8897], Loss: 5.5394\n",
      "Epoch [1/1], Step [7881/8897], Loss: 5.2963\n",
      "Epoch [1/1], Step [7882/8897], Loss: 5.7120\n",
      "Epoch [1/1], Step [7883/8897], Loss: 5.4249\n",
      "Epoch [1/1], Step [7884/8897], Loss: 5.3218\n",
      "Epoch [1/1], Step [7885/8897], Loss: 5.5216\n",
      "Epoch [1/1], Step [7886/8897], Loss: 5.7062\n",
      "Epoch [1/1], Step [7887/8897], Loss: 5.5702\n",
      "Epoch [1/1], Step [7888/8897], Loss: 5.6667\n",
      "Epoch [1/1], Step [7889/8897], Loss: 5.4370\n",
      "Epoch [1/1], Step [7890/8897], Loss: 5.7177\n",
      "Epoch [1/1], Step [7891/8897], Loss: 5.4266\n",
      "Epoch [1/1], Step [7892/8897], Loss: 5.1205\n",
      "Epoch [1/1], Step [7893/8897], Loss: 5.6847\n",
      "Epoch [1/1], Step [7894/8897], Loss: 5.6464\n",
      "Epoch [1/1], Step [7895/8897], Loss: 5.3525\n",
      "Epoch [1/1], Step [7896/8897], Loss: 5.6657\n",
      "Epoch [1/1], Step [7897/8897], Loss: 5.8290\n",
      "Epoch [1/1], Step [7898/8897], Loss: 5.6785\n",
      "Epoch [1/1], Step [7899/8897], Loss: 5.5436\n",
      "Epoch [1/1], Step [7900/8897], Loss: 5.5384\n",
      "Epoch [1/1], Step [7901/8897], Loss: 5.5423\n",
      "Epoch [1/1], Step [7902/8897], Loss: 5.3545\n",
      "Epoch [1/1], Step [7903/8897], Loss: 5.5320\n",
      "Epoch [1/1], Step [7904/8897], Loss: 5.4649\n",
      "Epoch [1/1], Step [7905/8897], Loss: 5.8692\n",
      "Epoch [1/1], Step [7906/8897], Loss: 5.3303\n",
      "Epoch [1/1], Step [7907/8897], Loss: 5.7498\n",
      "Epoch [1/1], Step [7908/8897], Loss: 5.5142\n",
      "Epoch [1/1], Step [7909/8897], Loss: 5.4336\n",
      "Epoch [1/1], Step [7910/8897], Loss: 5.4891\n",
      "Epoch [1/1], Step [7911/8897], Loss: 5.5051\n",
      "Epoch [1/1], Step [7912/8897], Loss: 5.2288\n",
      "Epoch [1/1], Step [7913/8897], Loss: 5.3938\n",
      "Epoch [1/1], Step [7914/8897], Loss: 5.7441\n",
      "Epoch [1/1], Step [7915/8897], Loss: 5.7756\n",
      "Epoch [1/1], Step [7916/8897], Loss: 5.5337\n",
      "Epoch [1/1], Step [7917/8897], Loss: 5.5861\n",
      "Epoch [1/1], Step [7918/8897], Loss: 5.4843\n",
      "Epoch [1/1], Step [7919/8897], Loss: 5.4547\n",
      "Epoch [1/1], Step [7920/8897], Loss: 5.3533\n",
      "Epoch [1/1], Step [7921/8897], Loss: 5.5354\n",
      "Epoch [1/1], Step [7922/8897], Loss: 5.3911\n",
      "Epoch [1/1], Step [7923/8897], Loss: 5.6244\n",
      "Epoch [1/1], Step [7924/8897], Loss: 5.6727\n",
      "Epoch [1/1], Step [7925/8897], Loss: 5.6420\n",
      "Epoch [1/1], Step [7926/8897], Loss: 5.5098\n",
      "Epoch [1/1], Step [7927/8897], Loss: 5.6608\n",
      "Epoch [1/1], Step [7928/8897], Loss: 5.5429\n",
      "Epoch [1/1], Step [7929/8897], Loss: 5.7662\n",
      "Epoch [1/1], Step [7930/8897], Loss: 5.6720\n",
      "Epoch [1/1], Step [7931/8897], Loss: 5.6220\n",
      "Epoch [1/1], Step [7932/8897], Loss: 5.5494\n",
      "Epoch [1/1], Step [7933/8897], Loss: 5.4285\n",
      "Epoch [1/1], Step [7934/8897], Loss: 5.5199\n",
      "Epoch [1/1], Step [7935/8897], Loss: 5.5103\n",
      "Epoch [1/1], Step [7936/8897], Loss: 5.5058\n",
      "Epoch [1/1], Step [7937/8897], Loss: 5.4822\n",
      "Epoch [1/1], Step [7938/8897], Loss: 5.6623\n",
      "Epoch [1/1], Step [7939/8897], Loss: 5.8781\n",
      "Epoch [1/1], Step [7940/8897], Loss: 5.5884\n",
      "Epoch [1/1], Step [7941/8897], Loss: 5.1589\n",
      "Epoch [1/1], Step [7942/8897], Loss: 5.6279\n",
      "Epoch [1/1], Step [7943/8897], Loss: 5.4498\n",
      "Epoch [1/1], Step [7944/8897], Loss: 5.5605\n",
      "Epoch [1/1], Step [7945/8897], Loss: 5.4297\n",
      "Epoch [1/1], Step [7946/8897], Loss: 5.5809\n",
      "Epoch [1/1], Step [7947/8897], Loss: 5.5759\n",
      "Epoch [1/1], Step [7948/8897], Loss: 5.4998\n",
      "Epoch [1/1], Step [7949/8897], Loss: 5.6413\n",
      "Epoch [1/1], Step [7950/8897], Loss: 5.5534\n",
      "Epoch [1/1], Step [7951/8897], Loss: 5.4190\n",
      "Epoch [1/1], Step [7952/8897], Loss: 5.6968\n",
      "Epoch [1/1], Step [7953/8897], Loss: 5.5741\n",
      "Epoch [1/1], Step [7954/8897], Loss: 5.6212\n",
      "Epoch [1/1], Step [7955/8897], Loss: 5.5592\n",
      "Epoch [1/1], Step [7956/8897], Loss: 5.6813\n",
      "Epoch [1/1], Step [7957/8897], Loss: 5.5655\n",
      "Epoch [1/1], Step [7958/8897], Loss: 5.5864\n",
      "Epoch [1/1], Step [7959/8897], Loss: 5.5273\n",
      "Epoch [1/1], Step [7960/8897], Loss: 5.4128\n",
      "Epoch [1/1], Step [7961/8897], Loss: 5.7735\n",
      "Epoch [1/1], Step [7962/8897], Loss: 5.3967\n",
      "Epoch [1/1], Step [7963/8897], Loss: 5.3802\n",
      "Epoch [1/1], Step [7964/8897], Loss: 5.4484\n",
      "Epoch [1/1], Step [7965/8897], Loss: 5.5108\n",
      "Epoch [1/1], Step [7966/8897], Loss: 5.3544\n",
      "Epoch [1/1], Step [7967/8897], Loss: 5.1465\n",
      "Epoch [1/1], Step [7968/8897], Loss: 5.4837\n",
      "Epoch [1/1], Step [7969/8897], Loss: 5.6795\n",
      "Epoch [1/1], Step [7970/8897], Loss: 5.7737\n",
      "Epoch [1/1], Step [7971/8897], Loss: 5.7945\n",
      "Epoch [1/1], Step [7972/8897], Loss: 5.4597\n",
      "Epoch [1/1], Step [7973/8897], Loss: 5.5956\n",
      "Epoch [1/1], Step [7974/8897], Loss: 5.4074\n",
      "Epoch [1/1], Step [7975/8897], Loss: 5.5376\n",
      "Epoch [1/1], Step [7976/8897], Loss: 5.6200\n",
      "Epoch [1/1], Step [7977/8897], Loss: 5.5025\n",
      "Epoch [1/1], Step [7978/8897], Loss: 5.4885\n",
      "Epoch [1/1], Step [7979/8897], Loss: 5.3379\n",
      "Epoch [1/1], Step [7980/8897], Loss: 5.5762\n",
      "Epoch [1/1], Step [7981/8897], Loss: 5.5351\n",
      "Epoch [1/1], Step [7982/8897], Loss: 5.6316\n",
      "Epoch [1/1], Step [7983/8897], Loss: 5.3371\n",
      "Epoch [1/1], Step [7984/8897], Loss: 5.6451\n",
      "Epoch [1/1], Step [7985/8897], Loss: 5.4673\n",
      "Epoch [1/1], Step [7986/8897], Loss: 5.5937\n",
      "Epoch [1/1], Step [7987/8897], Loss: 5.8878\n",
      "Epoch [1/1], Step [7988/8897], Loss: 5.6313\n",
      "Epoch [1/1], Step [7989/8897], Loss: 5.4573\n",
      "Epoch [1/1], Step [7990/8897], Loss: 5.4997\n",
      "Epoch [1/1], Step [7991/8897], Loss: 5.5966\n",
      "Epoch [1/1], Step [7992/8897], Loss: 5.3446\n",
      "Epoch [1/1], Step [7993/8897], Loss: 5.4107\n",
      "Epoch [1/1], Step [7994/8897], Loss: 5.6939\n",
      "Epoch [1/1], Step [7995/8897], Loss: 5.6102\n",
      "Epoch [1/1], Step [7996/8897], Loss: 5.4510\n",
      "Epoch [1/1], Step [7997/8897], Loss: 5.6386\n",
      "Epoch [1/1], Step [7998/8897], Loss: 5.7606\n",
      "Epoch [1/1], Step [7999/8897], Loss: 5.7435\n",
      "Epoch [1/1], Step [8000/8897], Loss: 5.2234\n",
      "Epoch [1/1], Step [8001/8897], Loss: 5.5915\n",
      "Epoch [1/1], Step [8002/8897], Loss: 5.6351\n",
      "Epoch [1/1], Step [8003/8897], Loss: 5.5132\n",
      "Epoch [1/1], Step [8004/8897], Loss: 5.7738\n",
      "Epoch [1/1], Step [8005/8897], Loss: 5.6482\n",
      "Epoch [1/1], Step [8006/8897], Loss: 5.4356\n",
      "Epoch [1/1], Step [8007/8897], Loss: 5.4180\n",
      "Epoch [1/1], Step [8008/8897], Loss: 5.6355\n",
      "Epoch [1/1], Step [8009/8897], Loss: 5.3013\n",
      "Epoch [1/1], Step [8010/8897], Loss: 5.4225\n",
      "Epoch [1/1], Step [8011/8897], Loss: 5.6759\n",
      "Epoch [1/1], Step [8012/8897], Loss: 5.5752\n",
      "Epoch [1/1], Step [8013/8897], Loss: 5.7260\n",
      "Epoch [1/1], Step [8014/8897], Loss: 5.6618\n",
      "Epoch [1/1], Step [8015/8897], Loss: 5.5411\n",
      "Epoch [1/1], Step [8016/8897], Loss: 5.3969\n",
      "Epoch [1/1], Step [8017/8897], Loss: 5.3911\n",
      "Epoch [1/1], Step [8018/8897], Loss: 5.4356\n",
      "Epoch [1/1], Step [8019/8897], Loss: 5.4873\n",
      "Epoch [1/1], Step [8020/8897], Loss: 5.6401\n",
      "Epoch [1/1], Step [8021/8897], Loss: 5.4757\n",
      "Epoch [1/1], Step [8022/8897], Loss: 5.6735\n",
      "Epoch [1/1], Step [8023/8897], Loss: 5.6343\n",
      "Epoch [1/1], Step [8024/8897], Loss: 5.7053\n",
      "Epoch [1/1], Step [8025/8897], Loss: 5.6369\n",
      "Epoch [1/1], Step [8026/8897], Loss: 5.5993\n",
      "Epoch [1/1], Step [8027/8897], Loss: 5.2438\n",
      "Epoch [1/1], Step [8028/8897], Loss: 5.6871\n",
      "Epoch [1/1], Step [8029/8897], Loss: 5.4546\n",
      "Epoch [1/1], Step [8030/8897], Loss: 5.5000\n",
      "Epoch [1/1], Step [8031/8897], Loss: 5.6589\n",
      "Epoch [1/1], Step [8032/8897], Loss: 5.4526\n",
      "Epoch [1/1], Step [8033/8897], Loss: 5.6768\n",
      "Epoch [1/1], Step [8034/8897], Loss: 5.5005\n",
      "Epoch [1/1], Step [8035/8897], Loss: 5.6005\n",
      "Epoch [1/1], Step [8036/8897], Loss: 5.5295\n",
      "Epoch [1/1], Step [8037/8897], Loss: 5.4126\n",
      "Epoch [1/1], Step [8038/8897], Loss: 5.6424\n",
      "Epoch [1/1], Step [8039/8897], Loss: 5.4108\n",
      "Epoch [1/1], Step [8040/8897], Loss: 5.6502\n",
      "Epoch [1/1], Step [8041/8897], Loss: 5.4740\n",
      "Epoch [1/1], Step [8042/8897], Loss: 5.5788\n",
      "Epoch [1/1], Step [8043/8897], Loss: 5.6048\n",
      "Epoch [1/1], Step [8044/8897], Loss: 5.8132\n",
      "Epoch [1/1], Step [8045/8897], Loss: 5.7580\n",
      "Epoch [1/1], Step [8046/8897], Loss: 5.6995\n",
      "Epoch [1/1], Step [8047/8897], Loss: 5.3302\n",
      "Epoch [1/1], Step [8048/8897], Loss: 5.4418\n",
      "Epoch [1/1], Step [8049/8897], Loss: 5.4828\n",
      "Epoch [1/1], Step [8050/8897], Loss: 5.6775\n",
      "Epoch [1/1], Step [8051/8897], Loss: 5.6955\n",
      "Epoch [1/1], Step [8052/8897], Loss: 5.3629\n",
      "Epoch [1/1], Step [8053/8897], Loss: 5.6035\n",
      "Epoch [1/1], Step [8054/8897], Loss: 5.4684\n",
      "Epoch [1/1], Step [8055/8897], Loss: 5.6882\n",
      "Epoch [1/1], Step [8056/8897], Loss: 5.5976\n",
      "Epoch [1/1], Step [8057/8897], Loss: 5.9041\n",
      "Epoch [1/1], Step [8058/8897], Loss: 5.5859\n",
      "Epoch [1/1], Step [8059/8897], Loss: 5.4736\n",
      "Epoch [1/1], Step [8060/8897], Loss: 5.7006\n",
      "Epoch [1/1], Step [8061/8897], Loss: 5.6353\n",
      "Epoch [1/1], Step [8062/8897], Loss: 5.5744\n",
      "Epoch [1/1], Step [8063/8897], Loss: 5.7589\n",
      "Epoch [1/1], Step [8064/8897], Loss: 5.2490\n",
      "Epoch [1/1], Step [8065/8897], Loss: 5.5294\n",
      "Epoch [1/1], Step [8066/8897], Loss: 5.7124\n",
      "Epoch [1/1], Step [8067/8897], Loss: 5.9182\n",
      "Epoch [1/1], Step [8068/8897], Loss: 5.3883\n",
      "Epoch [1/1], Step [8069/8897], Loss: 5.3767\n",
      "Epoch [1/1], Step [8070/8897], Loss: 5.4443\n",
      "Epoch [1/1], Step [8071/8897], Loss: 5.3364\n",
      "Epoch [1/1], Step [8072/8897], Loss: 5.6695\n",
      "Epoch [1/1], Step [8073/8897], Loss: 5.7304\n",
      "Epoch [1/1], Step [8074/8897], Loss: 5.4398\n",
      "Epoch [1/1], Step [8075/8897], Loss: 5.5139\n",
      "Epoch [1/1], Step [8076/8897], Loss: 5.6268\n",
      "Epoch [1/1], Step [8077/8897], Loss: 5.6477\n",
      "Epoch [1/1], Step [8078/8897], Loss: 5.7116\n",
      "Epoch [1/1], Step [8079/8897], Loss: 5.3685\n",
      "Epoch [1/1], Step [8080/8897], Loss: 5.7899\n",
      "Epoch [1/1], Step [8081/8897], Loss: 5.4647\n",
      "Epoch [1/1], Step [8082/8897], Loss: 5.7437\n",
      "Epoch [1/1], Step [8083/8897], Loss: 5.4536\n",
      "Epoch [1/1], Step [8084/8897], Loss: 5.4992\n",
      "Epoch [1/1], Step [8085/8897], Loss: 5.5049\n",
      "Epoch [1/1], Step [8086/8897], Loss: 5.3015\n",
      "Epoch [1/1], Step [8087/8897], Loss: 5.5681\n",
      "Epoch [1/1], Step [8088/8897], Loss: 5.5760\n",
      "Epoch [1/1], Step [8089/8897], Loss: 5.5845\n",
      "Epoch [1/1], Step [8090/8897], Loss: 5.3677\n",
      "Epoch [1/1], Step [8091/8897], Loss: 5.4752\n",
      "Epoch [1/1], Step [8092/8897], Loss: 5.4718\n",
      "Epoch [1/1], Step [8093/8897], Loss: 5.5281\n",
      "Epoch [1/1], Step [8094/8897], Loss: 5.6564\n",
      "Epoch [1/1], Step [8095/8897], Loss: 5.4627\n",
      "Epoch [1/1], Step [8096/8897], Loss: 5.7989\n",
      "Epoch [1/1], Step [8097/8897], Loss: 5.5675\n",
      "Epoch [1/1], Step [8098/8897], Loss: 5.6765\n",
      "Epoch [1/1], Step [8099/8897], Loss: 5.4081\n",
      "Epoch [1/1], Step [8100/8897], Loss: 5.5833\n",
      "Epoch [1/1], Step [8101/8897], Loss: 5.6170\n",
      "Epoch [1/1], Step [8102/8897], Loss: 5.3004\n",
      "Epoch [1/1], Step [8103/8897], Loss: 5.6347\n",
      "Epoch [1/1], Step [8104/8897], Loss: 5.6784\n",
      "Epoch [1/1], Step [8105/8897], Loss: 5.7090\n",
      "Epoch [1/1], Step [8106/8897], Loss: 5.3281\n",
      "Epoch [1/1], Step [8107/8897], Loss: 5.5217\n",
      "Epoch [1/1], Step [8108/8897], Loss: 5.6311\n",
      "Epoch [1/1], Step [8109/8897], Loss: 5.7899\n",
      "Epoch [1/1], Step [8110/8897], Loss: 5.3358\n",
      "Epoch [1/1], Step [8111/8897], Loss: 5.4423\n",
      "Epoch [1/1], Step [8112/8897], Loss: 5.4680\n",
      "Epoch [1/1], Step [8113/8897], Loss: 5.3198\n",
      "Epoch [1/1], Step [8114/8897], Loss: 5.6041\n",
      "Epoch [1/1], Step [8115/8897], Loss: 5.3849\n",
      "Epoch [1/1], Step [8116/8897], Loss: 5.5151\n",
      "Epoch [1/1], Step [8117/8897], Loss: 5.5718\n",
      "Epoch [1/1], Step [8118/8897], Loss: 5.3034\n",
      "Epoch [1/1], Step [8119/8897], Loss: 5.6303\n",
      "Epoch [1/1], Step [8120/8897], Loss: 5.6977\n",
      "Epoch [1/1], Step [8121/8897], Loss: 5.5238\n",
      "Epoch [1/1], Step [8122/8897], Loss: 5.5844\n",
      "Epoch [1/1], Step [8123/8897], Loss: 5.4798\n",
      "Epoch [1/1], Step [8124/8897], Loss: 5.5792\n",
      "Epoch [1/1], Step [8125/8897], Loss: 5.4181\n",
      "Epoch [1/1], Step [8126/8897], Loss: 5.3663\n",
      "Epoch [1/1], Step [8127/8897], Loss: 5.3138\n",
      "Epoch [1/1], Step [8128/8897], Loss: 5.6206\n",
      "Epoch [1/1], Step [8129/8897], Loss: 5.4832\n",
      "Epoch [1/1], Step [8130/8897], Loss: 5.4505\n",
      "Epoch [1/1], Step [8131/8897], Loss: 5.5560\n",
      "Epoch [1/1], Step [8132/8897], Loss: 5.6411\n",
      "Epoch [1/1], Step [8133/8897], Loss: 5.4362\n",
      "Epoch [1/1], Step [8134/8897], Loss: 5.6935\n",
      "Epoch [1/1], Step [8135/8897], Loss: 5.5559\n",
      "Epoch [1/1], Step [8136/8897], Loss: 5.4620\n",
      "Epoch [1/1], Step [8137/8897], Loss: 5.3866\n",
      "Epoch [1/1], Step [8138/8897], Loss: 5.1680\n",
      "Epoch [1/1], Step [8139/8897], Loss: 5.3032\n",
      "Epoch [1/1], Step [8140/8897], Loss: 5.5669\n",
      "Epoch [1/1], Step [8141/8897], Loss: 5.5319\n",
      "Epoch [1/1], Step [8142/8897], Loss: 5.6156\n",
      "Epoch [1/1], Step [8143/8897], Loss: 5.5561\n",
      "Epoch [1/1], Step [8144/8897], Loss: 5.6927\n",
      "Epoch [1/1], Step [8145/8897], Loss: 5.3824\n",
      "Epoch [1/1], Step [8146/8897], Loss: 5.3533\n",
      "Epoch [1/1], Step [8147/8897], Loss: 5.1071\n",
      "Epoch [1/1], Step [8148/8897], Loss: 5.7102\n",
      "Epoch [1/1], Step [8149/8897], Loss: 5.5204\n",
      "Epoch [1/1], Step [8150/8897], Loss: 5.4323\n",
      "Epoch [1/1], Step [8151/8897], Loss: 5.6324\n",
      "Epoch [1/1], Step [8152/8897], Loss: 5.4445\n",
      "Epoch [1/1], Step [8153/8897], Loss: 5.3544\n",
      "Epoch [1/1], Step [8154/8897], Loss: 5.4343\n",
      "Epoch [1/1], Step [8155/8897], Loss: 5.7374\n",
      "Epoch [1/1], Step [8156/8897], Loss: 5.5782\n",
      "Epoch [1/1], Step [8157/8897], Loss: 5.5303\n",
      "Epoch [1/1], Step [8158/8897], Loss: 5.5993\n",
      "Epoch [1/1], Step [8159/8897], Loss: 5.2405\n",
      "Epoch [1/1], Step [8160/8897], Loss: 5.6817\n",
      "Epoch [1/1], Step [8161/8897], Loss: 5.5342\n",
      "Epoch [1/1], Step [8162/8897], Loss: 5.4936\n",
      "Epoch [1/1], Step [8163/8897], Loss: 5.4710\n",
      "Epoch [1/1], Step [8164/8897], Loss: 5.6132\n",
      "Epoch [1/1], Step [8165/8897], Loss: 5.4683\n",
      "Epoch [1/1], Step [8166/8897], Loss: 5.3703\n",
      "Epoch [1/1], Step [8167/8897], Loss: 5.3649\n",
      "Epoch [1/1], Step [8168/8897], Loss: 5.4463\n",
      "Epoch [1/1], Step [8169/8897], Loss: 5.4508\n",
      "Epoch [1/1], Step [8170/8897], Loss: 5.5484\n",
      "Epoch [1/1], Step [8171/8897], Loss: 5.6023\n",
      "Epoch [1/1], Step [8172/8897], Loss: 5.4682\n",
      "Epoch [1/1], Step [8173/8897], Loss: 5.3656\n",
      "Epoch [1/1], Step [8174/8897], Loss: 5.5992\n",
      "Epoch [1/1], Step [8175/8897], Loss: 5.5219\n",
      "Epoch [1/1], Step [8176/8897], Loss: 5.5145\n",
      "Epoch [1/1], Step [8177/8897], Loss: 5.5713\n",
      "Epoch [1/1], Step [8178/8897], Loss: 5.6505\n",
      "Epoch [1/1], Step [8179/8897], Loss: 5.3861\n",
      "Epoch [1/1], Step [8180/8897], Loss: 5.4932\n",
      "Epoch [1/1], Step [8181/8897], Loss: 5.5261\n",
      "Epoch [1/1], Step [8182/8897], Loss: 5.6956\n",
      "Epoch [1/1], Step [8183/8897], Loss: 5.6677\n",
      "Epoch [1/1], Step [8184/8897], Loss: 5.4310\n",
      "Epoch [1/1], Step [8185/8897], Loss: 5.5502\n",
      "Epoch [1/1], Step [8186/8897], Loss: 5.6706\n",
      "Epoch [1/1], Step [8187/8897], Loss: 5.7126\n",
      "Epoch [1/1], Step [8188/8897], Loss: 5.5882\n",
      "Epoch [1/1], Step [8189/8897], Loss: 5.4889\n",
      "Epoch [1/1], Step [8190/8897], Loss: 5.6229\n",
      "Epoch [1/1], Step [8191/8897], Loss: 5.5683\n",
      "Epoch [1/1], Step [8192/8897], Loss: 5.6434\n",
      "Epoch [1/1], Step [8193/8897], Loss: 5.6405\n",
      "Epoch [1/1], Step [8194/8897], Loss: 5.4258\n",
      "Epoch [1/1], Step [8195/8897], Loss: 5.7229\n",
      "Epoch [1/1], Step [8196/8897], Loss: 5.3162\n",
      "Epoch [1/1], Step [8197/8897], Loss: 5.5523\n",
      "Epoch [1/1], Step [8198/8897], Loss: 5.5455\n",
      "Epoch [1/1], Step [8199/8897], Loss: 5.5211\n",
      "Epoch [1/1], Step [8200/8897], Loss: 5.7026\n",
      "Epoch [1/1], Step [8201/8897], Loss: 5.5513\n",
      "Epoch [1/1], Step [8202/8897], Loss: 5.5053\n",
      "Epoch [1/1], Step [8203/8897], Loss: 5.3908\n",
      "Epoch [1/1], Step [8204/8897], Loss: 5.6082\n",
      "Epoch [1/1], Step [8205/8897], Loss: 5.5916\n",
      "Epoch [1/1], Step [8206/8897], Loss: 5.6917\n",
      "Epoch [1/1], Step [8207/8897], Loss: 5.9038\n",
      "Epoch [1/1], Step [8208/8897], Loss: 5.6392\n",
      "Epoch [1/1], Step [8209/8897], Loss: 5.5244\n",
      "Epoch [1/1], Step [8210/8897], Loss: 5.5640\n",
      "Epoch [1/1], Step [8211/8897], Loss: 5.3675\n",
      "Epoch [1/1], Step [8212/8897], Loss: 5.6709\n",
      "Epoch [1/1], Step [8213/8897], Loss: 5.7595\n",
      "Epoch [1/1], Step [8214/8897], Loss: 5.4281\n",
      "Epoch [1/1], Step [8215/8897], Loss: 5.5545\n",
      "Epoch [1/1], Step [8216/8897], Loss: 5.5775\n",
      "Epoch [1/1], Step [8217/8897], Loss: 5.4359\n",
      "Epoch [1/1], Step [8218/8897], Loss: 5.5128\n",
      "Epoch [1/1], Step [8219/8897], Loss: 5.6448\n",
      "Epoch [1/1], Step [8220/8897], Loss: 5.3935\n",
      "Epoch [1/1], Step [8221/8897], Loss: 5.8429\n",
      "Epoch [1/1], Step [8222/8897], Loss: 5.2163\n",
      "Epoch [1/1], Step [8223/8897], Loss: 5.6006\n",
      "Epoch [1/1], Step [8224/8897], Loss: 5.5033\n",
      "Epoch [1/1], Step [8225/8897], Loss: 5.6345\n",
      "Epoch [1/1], Step [8226/8897], Loss: 5.2979\n",
      "Epoch [1/1], Step [8227/8897], Loss: 5.6806\n",
      "Epoch [1/1], Step [8228/8897], Loss: 5.4824\n",
      "Epoch [1/1], Step [8229/8897], Loss: 5.5809\n",
      "Epoch [1/1], Step [8230/8897], Loss: 5.7624\n",
      "Epoch [1/1], Step [8231/8897], Loss: 5.3720\n",
      "Epoch [1/1], Step [8232/8897], Loss: 5.6754\n",
      "Epoch [1/1], Step [8233/8897], Loss: 5.4326\n",
      "Epoch [1/1], Step [8234/8897], Loss: 5.7366\n",
      "Epoch [1/1], Step [8235/8897], Loss: 5.4337\n",
      "Epoch [1/1], Step [8236/8897], Loss: 5.5064\n",
      "Epoch [1/1], Step [8237/8897], Loss: 5.4034\n",
      "Epoch [1/1], Step [8238/8897], Loss: 5.4141\n",
      "Epoch [1/1], Step [8239/8897], Loss: 5.3557\n",
      "Epoch [1/1], Step [8240/8897], Loss: 5.6544\n",
      "Epoch [1/1], Step [8241/8897], Loss: 5.6508\n",
      "Epoch [1/1], Step [8242/8897], Loss: 5.5420\n",
      "Epoch [1/1], Step [8243/8897], Loss: 5.5049\n",
      "Epoch [1/1], Step [8244/8897], Loss: 5.4840\n",
      "Epoch [1/1], Step [8245/8897], Loss: 5.5963\n",
      "Epoch [1/1], Step [8246/8897], Loss: 5.4880\n",
      "Epoch [1/1], Step [8247/8897], Loss: 5.4227\n",
      "Epoch [1/1], Step [8248/8897], Loss: 5.4740\n",
      "Epoch [1/1], Step [8249/8897], Loss: 5.7160\n",
      "Epoch [1/1], Step [8250/8897], Loss: 5.4164\n",
      "Epoch [1/1], Step [8251/8897], Loss: 5.4337\n",
      "Epoch [1/1], Step [8252/8897], Loss: 5.4041\n",
      "Epoch [1/1], Step [8253/8897], Loss: 5.6942\n",
      "Epoch [1/1], Step [8254/8897], Loss: 5.6205\n",
      "Epoch [1/1], Step [8255/8897], Loss: 5.7646\n",
      "Epoch [1/1], Step [8256/8897], Loss: 5.6182\n",
      "Epoch [1/1], Step [8257/8897], Loss: 5.4319\n",
      "Epoch [1/1], Step [8258/8897], Loss: 5.3744\n",
      "Epoch [1/1], Step [8259/8897], Loss: 5.4676\n",
      "Epoch [1/1], Step [8260/8897], Loss: 5.6885\n",
      "Epoch [1/1], Step [8261/8897], Loss: 5.3346\n",
      "Epoch [1/1], Step [8262/8897], Loss: 5.5879\n",
      "Epoch [1/1], Step [8263/8897], Loss: 5.5941\n",
      "Epoch [1/1], Step [8264/8897], Loss: 5.4228\n",
      "Epoch [1/1], Step [8265/8897], Loss: 5.5085\n",
      "Epoch [1/1], Step [8266/8897], Loss: 5.5753\n",
      "Epoch [1/1], Step [8267/8897], Loss: 5.6283\n",
      "Epoch [1/1], Step [8268/8897], Loss: 5.5383\n",
      "Epoch [1/1], Step [8269/8897], Loss: 5.6585\n",
      "Epoch [1/1], Step [8270/8897], Loss: 5.5795\n",
      "Epoch [1/1], Step [8271/8897], Loss: 5.2690\n",
      "Epoch [1/1], Step [8272/8897], Loss: 5.3172\n",
      "Epoch [1/1], Step [8273/8897], Loss: 5.7847\n",
      "Epoch [1/1], Step [8274/8897], Loss: 5.6610\n",
      "Epoch [1/1], Step [8275/8897], Loss: 5.3797\n",
      "Epoch [1/1], Step [8276/8897], Loss: 5.5582\n",
      "Epoch [1/1], Step [8277/8897], Loss: 5.6510\n",
      "Epoch [1/1], Step [8278/8897], Loss: 5.7232\n",
      "Epoch [1/1], Step [8279/8897], Loss: 5.6518\n",
      "Epoch [1/1], Step [8280/8897], Loss: 5.4704\n",
      "Epoch [1/1], Step [8281/8897], Loss: 5.3947\n",
      "Epoch [1/1], Step [8282/8897], Loss: 5.3464\n",
      "Epoch [1/1], Step [8283/8897], Loss: 5.7931\n",
      "Epoch [1/1], Step [8284/8897], Loss: 5.7174\n",
      "Epoch [1/1], Step [8285/8897], Loss: 5.3357\n",
      "Epoch [1/1], Step [8286/8897], Loss: 5.7638\n",
      "Epoch [1/1], Step [8287/8897], Loss: 5.5747\n",
      "Epoch [1/1], Step [8288/8897], Loss: 5.7186\n",
      "Epoch [1/1], Step [8289/8897], Loss: 5.5445\n",
      "Epoch [1/1], Step [8290/8897], Loss: 5.7688\n",
      "Epoch [1/1], Step [8291/8897], Loss: 5.6109\n",
      "Epoch [1/1], Step [8292/8897], Loss: 5.6943\n",
      "Epoch [1/1], Step [8293/8897], Loss: 5.5457\n",
      "Epoch [1/1], Step [8294/8897], Loss: 5.4685\n",
      "Epoch [1/1], Step [8295/8897], Loss: 5.5482\n",
      "Epoch [1/1], Step [8296/8897], Loss: 5.3387\n",
      "Epoch [1/1], Step [8297/8897], Loss: 5.5125\n",
      "Epoch [1/1], Step [8298/8897], Loss: 5.4264\n",
      "Epoch [1/1], Step [8299/8897], Loss: 5.7352\n",
      "Epoch [1/1], Step [8300/8897], Loss: 5.3714\n",
      "Epoch [1/1], Step [8301/8897], Loss: 5.2996\n",
      "Epoch [1/1], Step [8302/8897], Loss: 5.5276\n",
      "Epoch [1/1], Step [8303/8897], Loss: 5.5669\n",
      "Epoch [1/1], Step [8304/8897], Loss: 5.5353\n",
      "Epoch [1/1], Step [8305/8897], Loss: 5.4893\n",
      "Epoch [1/1], Step [8306/8897], Loss: 5.9310\n",
      "Epoch [1/1], Step [8307/8897], Loss: 5.2850\n",
      "Epoch [1/1], Step [8308/8897], Loss: 5.3046\n",
      "Epoch [1/1], Step [8309/8897], Loss: 5.6562\n",
      "Epoch [1/1], Step [8310/8897], Loss: 5.8950\n",
      "Epoch [1/1], Step [8311/8897], Loss: 5.6108\n",
      "Epoch [1/1], Step [8312/8897], Loss: 5.6657\n",
      "Epoch [1/1], Step [8313/8897], Loss: 5.5741\n",
      "Epoch [1/1], Step [8314/8897], Loss: 5.4561\n",
      "Epoch [1/1], Step [8315/8897], Loss: 5.5510\n",
      "Epoch [1/1], Step [8316/8897], Loss: 5.3646\n",
      "Epoch [1/1], Step [8317/8897], Loss: 5.6204\n",
      "Epoch [1/1], Step [8318/8897], Loss: 5.5919\n",
      "Epoch [1/1], Step [8319/8897], Loss: 5.4878\n",
      "Epoch [1/1], Step [8320/8897], Loss: 5.4508\n",
      "Epoch [1/1], Step [8321/8897], Loss: 5.7308\n",
      "Epoch [1/1], Step [8322/8897], Loss: 5.7595\n",
      "Epoch [1/1], Step [8323/8897], Loss: 5.5557\n",
      "Epoch [1/1], Step [8324/8897], Loss: 5.6511\n",
      "Epoch [1/1], Step [8325/8897], Loss: 5.5974\n",
      "Epoch [1/1], Step [8326/8897], Loss: 5.5888\n",
      "Epoch [1/1], Step [8327/8897], Loss: 5.2916\n",
      "Epoch [1/1], Step [8328/8897], Loss: 5.4641\n",
      "Epoch [1/1], Step [8329/8897], Loss: 5.5828\n",
      "Epoch [1/1], Step [8330/8897], Loss: 5.9437\n",
      "Epoch [1/1], Step [8331/8897], Loss: 5.4065\n",
      "Epoch [1/1], Step [8332/8897], Loss: 5.4670\n",
      "Epoch [1/1], Step [8333/8897], Loss: 5.4383\n",
      "Epoch [1/1], Step [8334/8897], Loss: 5.6522\n",
      "Epoch [1/1], Step [8335/8897], Loss: 5.5554\n",
      "Epoch [1/1], Step [8336/8897], Loss: 5.5996\n",
      "Epoch [1/1], Step [8337/8897], Loss: 5.6993\n",
      "Epoch [1/1], Step [8338/8897], Loss: 5.7619\n",
      "Epoch [1/1], Step [8339/8897], Loss: 5.5065\n",
      "Epoch [1/1], Step [8340/8897], Loss: 5.3605\n",
      "Epoch [1/1], Step [8341/8897], Loss: 5.7652\n",
      "Epoch [1/1], Step [8342/8897], Loss: 5.6632\n",
      "Epoch [1/1], Step [8343/8897], Loss: 5.5995\n",
      "Epoch [1/1], Step [8344/8897], Loss: 5.4894\n",
      "Epoch [1/1], Step [8345/8897], Loss: 5.5997\n",
      "Epoch [1/1], Step [8346/8897], Loss: 5.5710\n",
      "Epoch [1/1], Step [8347/8897], Loss: 5.7396\n",
      "Epoch [1/1], Step [8348/8897], Loss: 5.5014\n",
      "Epoch [1/1], Step [8349/8897], Loss: 5.5437\n",
      "Epoch [1/1], Step [8350/8897], Loss: 5.6559\n",
      "Epoch [1/1], Step [8351/8897], Loss: 5.3478\n",
      "Epoch [1/1], Step [8352/8897], Loss: 5.4368\n",
      "Epoch [1/1], Step [8353/8897], Loss: 5.6045\n",
      "Epoch [1/1], Step [8354/8897], Loss: 5.5162\n",
      "Epoch [1/1], Step [8355/8897], Loss: 5.6882\n",
      "Epoch [1/1], Step [8356/8897], Loss: 5.5783\n",
      "Epoch [1/1], Step [8357/8897], Loss: 5.5384\n",
      "Epoch [1/1], Step [8358/8897], Loss: 5.3933\n",
      "Epoch [1/1], Step [8359/8897], Loss: 5.4099\n",
      "Epoch [1/1], Step [8360/8897], Loss: 5.6187\n",
      "Epoch [1/1], Step [8361/8897], Loss: 5.8786\n",
      "Epoch [1/1], Step [8362/8897], Loss: 5.4521\n",
      "Epoch [1/1], Step [8363/8897], Loss: 5.6167\n",
      "Epoch [1/1], Step [8364/8897], Loss: 5.6234\n",
      "Epoch [1/1], Step [8365/8897], Loss: 5.6524\n",
      "Epoch [1/1], Step [8366/8897], Loss: 5.5213\n",
      "Epoch [1/1], Step [8367/8897], Loss: 5.1959\n",
      "Epoch [1/1], Step [8368/8897], Loss: 5.5052\n",
      "Epoch [1/1], Step [8369/8897], Loss: 5.5227\n",
      "Epoch [1/1], Step [8370/8897], Loss: 5.7544\n",
      "Epoch [1/1], Step [8371/8897], Loss: 5.5274\n",
      "Epoch [1/1], Step [8372/8897], Loss: 5.5466\n",
      "Epoch [1/1], Step [8373/8897], Loss: 5.7602\n",
      "Epoch [1/1], Step [8374/8897], Loss: 5.2878\n",
      "Epoch [1/1], Step [8375/8897], Loss: 5.4046\n",
      "Epoch [1/1], Step [8376/8897], Loss: 5.4995\n",
      "Epoch [1/1], Step [8377/8897], Loss: 5.2958\n",
      "Epoch [1/1], Step [8378/8897], Loss: 5.6016\n",
      "Epoch [1/1], Step [8379/8897], Loss: 5.3914\n",
      "Epoch [1/1], Step [8380/8897], Loss: 5.6925\n",
      "Epoch [1/1], Step [8381/8897], Loss: 5.3208\n",
      "Epoch [1/1], Step [8382/8897], Loss: 5.6259\n",
      "Epoch [1/1], Step [8383/8897], Loss: 5.5555\n",
      "Epoch [1/1], Step [8384/8897], Loss: 5.6193\n",
      "Epoch [1/1], Step [8385/8897], Loss: 5.5375\n",
      "Epoch [1/1], Step [8386/8897], Loss: 5.4840\n",
      "Epoch [1/1], Step [8387/8897], Loss: 5.6463\n",
      "Epoch [1/1], Step [8388/8897], Loss: 5.5704\n",
      "Epoch [1/1], Step [8389/8897], Loss: 5.4953\n",
      "Epoch [1/1], Step [8390/8897], Loss: 5.3697\n",
      "Epoch [1/1], Step [8391/8897], Loss: 5.6293\n",
      "Epoch [1/1], Step [8392/8897], Loss: 5.4841\n",
      "Epoch [1/1], Step [8393/8897], Loss: 5.5137\n",
      "Epoch [1/1], Step [8394/8897], Loss: 5.1205\n",
      "Epoch [1/1], Step [8395/8897], Loss: 5.5597\n",
      "Epoch [1/1], Step [8396/8897], Loss: 5.5727\n",
      "Epoch [1/1], Step [8397/8897], Loss: 5.7066\n",
      "Epoch [1/1], Step [8398/8897], Loss: 5.5694\n",
      "Epoch [1/1], Step [8399/8897], Loss: 5.4104\n",
      "Epoch [1/1], Step [8400/8897], Loss: 5.4451\n",
      "Epoch [1/1], Step [8401/8897], Loss: 5.4999\n",
      "Epoch [1/1], Step [8402/8897], Loss: 5.4564\n",
      "Epoch [1/1], Step [8403/8897], Loss: 5.4845\n",
      "Epoch [1/1], Step [8404/8897], Loss: 5.5535\n",
      "Epoch [1/1], Step [8405/8897], Loss: 5.3802\n",
      "Epoch [1/1], Step [8406/8897], Loss: 5.6227\n",
      "Epoch [1/1], Step [8407/8897], Loss: 5.5199\n",
      "Epoch [1/1], Step [8408/8897], Loss: 5.5826\n",
      "Epoch [1/1], Step [8409/8897], Loss: 5.5101\n",
      "Epoch [1/1], Step [8410/8897], Loss: 5.5831\n",
      "Epoch [1/1], Step [8411/8897], Loss: 5.8666\n",
      "Epoch [1/1], Step [8412/8897], Loss: 5.2756\n",
      "Epoch [1/1], Step [8413/8897], Loss: 5.6177\n",
      "Epoch [1/1], Step [8414/8897], Loss: 5.3259\n",
      "Epoch [1/1], Step [8415/8897], Loss: 5.4575\n",
      "Epoch [1/1], Step [8416/8897], Loss: 5.5678\n",
      "Epoch [1/1], Step [8417/8897], Loss: 5.4990\n",
      "Epoch [1/1], Step [8418/8897], Loss: 5.3873\n",
      "Epoch [1/1], Step [8419/8897], Loss: 5.4801\n",
      "Epoch [1/1], Step [8420/8897], Loss: 5.6464\n",
      "Epoch [1/1], Step [8421/8897], Loss: 5.3081\n",
      "Epoch [1/1], Step [8422/8897], Loss: 5.3917\n",
      "Epoch [1/1], Step [8423/8897], Loss: 5.6522\n",
      "Epoch [1/1], Step [8424/8897], Loss: 5.2077\n",
      "Epoch [1/1], Step [8425/8897], Loss: 5.6253\n",
      "Epoch [1/1], Step [8426/8897], Loss: 5.5268\n",
      "Epoch [1/1], Step [8427/8897], Loss: 5.5149\n",
      "Epoch [1/1], Step [8428/8897], Loss: 5.6371\n",
      "Epoch [1/1], Step [8429/8897], Loss: 5.2363\n",
      "Epoch [1/1], Step [8430/8897], Loss: 5.6409\n",
      "Epoch [1/1], Step [8431/8897], Loss: 5.6980\n",
      "Epoch [1/1], Step [8432/8897], Loss: 5.4037\n",
      "Epoch [1/1], Step [8433/8897], Loss: 5.3955\n",
      "Epoch [1/1], Step [8434/8897], Loss: 5.6396\n",
      "Epoch [1/1], Step [8435/8897], Loss: 5.5801\n",
      "Epoch [1/1], Step [8436/8897], Loss: 5.5665\n",
      "Epoch [1/1], Step [8437/8897], Loss: 5.5173\n",
      "Epoch [1/1], Step [8438/8897], Loss: 5.3154\n",
      "Epoch [1/1], Step [8439/8897], Loss: 5.5974\n",
      "Epoch [1/1], Step [8440/8897], Loss: 5.3940\n",
      "Epoch [1/1], Step [8441/8897], Loss: 5.5661\n",
      "Epoch [1/1], Step [8442/8897], Loss: 5.6535\n",
      "Epoch [1/1], Step [8443/8897], Loss: 5.6347\n",
      "Epoch [1/1], Step [8444/8897], Loss: 5.3599\n",
      "Epoch [1/1], Step [8445/8897], Loss: 5.4054\n",
      "Epoch [1/1], Step [8446/8897], Loss: 5.4308\n",
      "Epoch [1/1], Step [8447/8897], Loss: 5.3484\n",
      "Epoch [1/1], Step [8448/8897], Loss: 5.4710\n",
      "Epoch [1/1], Step [8449/8897], Loss: 5.4671\n",
      "Epoch [1/1], Step [8450/8897], Loss: 5.4902\n",
      "Epoch [1/1], Step [8451/8897], Loss: 5.7149\n",
      "Epoch [1/1], Step [8452/8897], Loss: 5.3853\n",
      "Epoch [1/1], Step [8453/8897], Loss: 5.7052\n",
      "Epoch [1/1], Step [8454/8897], Loss: 5.5609\n",
      "Epoch [1/1], Step [8455/8897], Loss: 5.4894\n",
      "Epoch [1/1], Step [8456/8897], Loss: 5.6013\n",
      "Epoch [1/1], Step [8457/8897], Loss: 5.5930\n",
      "Epoch [1/1], Step [8458/8897], Loss: 5.5277\n",
      "Epoch [1/1], Step [8459/8897], Loss: 5.5050\n",
      "Epoch [1/1], Step [8460/8897], Loss: 5.5557\n",
      "Epoch [1/1], Step [8461/8897], Loss: 5.4252\n",
      "Epoch [1/1], Step [8462/8897], Loss: 5.5484\n",
      "Epoch [1/1], Step [8463/8897], Loss: 5.7367\n",
      "Epoch [1/1], Step [8464/8897], Loss: 5.6135\n",
      "Epoch [1/1], Step [8465/8897], Loss: 5.3788\n",
      "Epoch [1/1], Step [8466/8897], Loss: 5.4355\n",
      "Epoch [1/1], Step [8467/8897], Loss: 5.3091\n",
      "Epoch [1/1], Step [8468/8897], Loss: 5.4555\n",
      "Epoch [1/1], Step [8469/8897], Loss: 5.5948\n",
      "Epoch [1/1], Step [8470/8897], Loss: 5.6214\n",
      "Epoch [1/1], Step [8471/8897], Loss: 5.5741\n",
      "Epoch [1/1], Step [8472/8897], Loss: 5.5797\n",
      "Epoch [1/1], Step [8473/8897], Loss: 5.6069\n",
      "Epoch [1/1], Step [8474/8897], Loss: 5.5413\n",
      "Epoch [1/1], Step [8475/8897], Loss: 5.4016\n",
      "Epoch [1/1], Step [8476/8897], Loss: 5.8200\n",
      "Epoch [1/1], Step [8477/8897], Loss: 5.3495\n",
      "Epoch [1/1], Step [8478/8897], Loss: 5.5751\n",
      "Epoch [1/1], Step [8479/8897], Loss: 5.5187\n",
      "Epoch [1/1], Step [8480/8897], Loss: 5.5405\n",
      "Epoch [1/1], Step [8481/8897], Loss: 5.6060\n",
      "Epoch [1/1], Step [8482/8897], Loss: 5.4874\n",
      "Epoch [1/1], Step [8483/8897], Loss: 5.8331\n",
      "Epoch [1/1], Step [8484/8897], Loss: 5.3262\n",
      "Epoch [1/1], Step [8485/8897], Loss: 5.5388\n",
      "Epoch [1/1], Step [8486/8897], Loss: 5.3693\n",
      "Epoch [1/1], Step [8487/8897], Loss: 5.4663\n",
      "Epoch [1/1], Step [8488/8897], Loss: 5.4181\n",
      "Epoch [1/1], Step [8489/8897], Loss: 5.5858\n",
      "Epoch [1/1], Step [8490/8897], Loss: 5.5927\n",
      "Epoch [1/1], Step [8491/8897], Loss: 5.6355\n",
      "Epoch [1/1], Step [8492/8897], Loss: 5.2669\n",
      "Epoch [1/1], Step [8493/8897], Loss: 5.5830\n",
      "Epoch [1/1], Step [8494/8897], Loss: 5.3895\n",
      "Epoch [1/1], Step [8495/8897], Loss: 5.4488\n",
      "Epoch [1/1], Step [8496/8897], Loss: 5.7323\n",
      "Epoch [1/1], Step [8497/8897], Loss: 5.7235\n",
      "Epoch [1/1], Step [8498/8897], Loss: 5.6402\n",
      "Epoch [1/1], Step [8499/8897], Loss: 5.6686\n",
      "Epoch [1/1], Step [8500/8897], Loss: 5.5537\n",
      "Epoch [1/1], Step [8501/8897], Loss: 5.4332\n",
      "Epoch [1/1], Step [8502/8897], Loss: 5.4063\n",
      "Epoch [1/1], Step [8503/8897], Loss: 5.5250\n",
      "Epoch [1/1], Step [8504/8897], Loss: 5.4290\n",
      "Epoch [1/1], Step [8505/8897], Loss: 5.6800\n",
      "Epoch [1/1], Step [8506/8897], Loss: 5.7026\n",
      "Epoch [1/1], Step [8507/8897], Loss: 5.3778\n",
      "Epoch [1/1], Step [8508/8897], Loss: 5.5648\n",
      "Epoch [1/1], Step [8509/8897], Loss: 5.3738\n",
      "Epoch [1/1], Step [8510/8897], Loss: 5.3407\n",
      "Epoch [1/1], Step [8511/8897], Loss: 5.4947\n",
      "Epoch [1/1], Step [8512/8897], Loss: 5.4540\n",
      "Epoch [1/1], Step [8513/8897], Loss: 5.4063\n",
      "Epoch [1/1], Step [8514/8897], Loss: 5.4315\n",
      "Epoch [1/1], Step [8515/8897], Loss: 5.5549\n",
      "Epoch [1/1], Step [8516/8897], Loss: 5.4602\n",
      "Epoch [1/1], Step [8517/8897], Loss: 5.3860\n",
      "Epoch [1/1], Step [8518/8897], Loss: 5.5348\n",
      "Epoch [1/1], Step [8519/8897], Loss: 5.4719\n",
      "Epoch [1/1], Step [8520/8897], Loss: 5.4783\n",
      "Epoch [1/1], Step [8521/8897], Loss: 5.5146\n",
      "Epoch [1/1], Step [8522/8897], Loss: 5.5882\n",
      "Epoch [1/1], Step [8523/8897], Loss: 5.2914\n",
      "Epoch [1/1], Step [8524/8897], Loss: 5.6686\n",
      "Epoch [1/1], Step [8525/8897], Loss: 5.4621\n",
      "Epoch [1/1], Step [8526/8897], Loss: 5.4381\n",
      "Epoch [1/1], Step [8527/8897], Loss: 5.5648\n",
      "Epoch [1/1], Step [8528/8897], Loss: 5.5949\n",
      "Epoch [1/1], Step [8529/8897], Loss: 5.6066\n",
      "Epoch [1/1], Step [8530/8897], Loss: 5.2892\n",
      "Epoch [1/1], Step [8531/8897], Loss: 5.3905\n",
      "Epoch [1/1], Step [8532/8897], Loss: 5.4436\n",
      "Epoch [1/1], Step [8533/8897], Loss: 5.5557\n",
      "Epoch [1/1], Step [8534/8897], Loss: 5.3550\n",
      "Epoch [1/1], Step [8535/8897], Loss: 5.2350\n",
      "Epoch [1/1], Step [8536/8897], Loss: 5.5968\n",
      "Epoch [1/1], Step [8537/8897], Loss: 5.5765\n",
      "Epoch [1/1], Step [8538/8897], Loss: 5.6983\n",
      "Epoch [1/1], Step [8539/8897], Loss: 5.5273\n",
      "Epoch [1/1], Step [8540/8897], Loss: 5.6246\n",
      "Epoch [1/1], Step [8541/8897], Loss: 5.4561\n",
      "Epoch [1/1], Step [8542/8897], Loss: 5.3644\n",
      "Epoch [1/1], Step [8543/8897], Loss: 5.6015\n",
      "Epoch [1/1], Step [8544/8897], Loss: 5.7316\n",
      "Epoch [1/1], Step [8545/8897], Loss: 5.4808\n",
      "Epoch [1/1], Step [8546/8897], Loss: 5.4932\n",
      "Epoch [1/1], Step [8547/8897], Loss: 5.3274\n",
      "Epoch [1/1], Step [8548/8897], Loss: 5.6039\n",
      "Epoch [1/1], Step [8549/8897], Loss: 5.5217\n",
      "Epoch [1/1], Step [8550/8897], Loss: 5.6703\n",
      "Epoch [1/1], Step [8551/8897], Loss: 5.2298\n",
      "Epoch [1/1], Step [8552/8897], Loss: 5.3974\n",
      "Epoch [1/1], Step [8553/8897], Loss: 5.6425\n",
      "Epoch [1/1], Step [8554/8897], Loss: 5.2751\n",
      "Epoch [1/1], Step [8555/8897], Loss: 5.5259\n",
      "Epoch [1/1], Step [8556/8897], Loss: 5.5994\n",
      "Epoch [1/1], Step [8557/8897], Loss: 5.4593\n",
      "Epoch [1/1], Step [8558/8897], Loss: 5.4589\n",
      "Epoch [1/1], Step [8559/8897], Loss: 5.6159\n",
      "Epoch [1/1], Step [8560/8897], Loss: 5.2874\n",
      "Epoch [1/1], Step [8561/8897], Loss: 5.7433\n",
      "Epoch [1/1], Step [8562/8897], Loss: 5.4330\n",
      "Epoch [1/1], Step [8563/8897], Loss: 5.4580\n",
      "Epoch [1/1], Step [8564/8897], Loss: 5.2795\n",
      "Epoch [1/1], Step [8565/8897], Loss: 5.4860\n",
      "Epoch [1/1], Step [8566/8897], Loss: 5.5110\n",
      "Epoch [1/1], Step [8567/8897], Loss: 5.6655\n",
      "Epoch [1/1], Step [8568/8897], Loss: 5.3812\n",
      "Epoch [1/1], Step [8569/8897], Loss: 5.2970\n",
      "Epoch [1/1], Step [8570/8897], Loss: 5.4294\n",
      "Epoch [1/1], Step [8571/8897], Loss: 5.6029\n",
      "Epoch [1/1], Step [8572/8897], Loss: 5.5604\n",
      "Epoch [1/1], Step [8573/8897], Loss: 5.4238\n",
      "Epoch [1/1], Step [8574/8897], Loss: 5.4995\n",
      "Epoch [1/1], Step [8575/8897], Loss: 5.7641\n",
      "Epoch [1/1], Step [8576/8897], Loss: 5.6525\n",
      "Epoch [1/1], Step [8577/8897], Loss: 5.3756\n",
      "Epoch [1/1], Step [8578/8897], Loss: 5.3403\n",
      "Epoch [1/1], Step [8579/8897], Loss: 5.7021\n",
      "Epoch [1/1], Step [8580/8897], Loss: 5.4961\n",
      "Epoch [1/1], Step [8581/8897], Loss: 5.5573\n",
      "Epoch [1/1], Step [8582/8897], Loss: 5.5099\n",
      "Epoch [1/1], Step [8583/8897], Loss: 5.5640\n",
      "Epoch [1/1], Step [8584/8897], Loss: 5.5988\n",
      "Epoch [1/1], Step [8585/8897], Loss: 5.3810\n",
      "Epoch [1/1], Step [8586/8897], Loss: 5.2899\n",
      "Epoch [1/1], Step [8587/8897], Loss: 5.4357\n",
      "Epoch [1/1], Step [8588/8897], Loss: 5.6969\n",
      "Epoch [1/1], Step [8589/8897], Loss: 5.4710\n",
      "Epoch [1/1], Step [8590/8897], Loss: 5.3940\n",
      "Epoch [1/1], Step [8591/8897], Loss: 5.5530\n",
      "Epoch [1/1], Step [8592/8897], Loss: 5.5492\n",
      "Epoch [1/1], Step [8593/8897], Loss: 5.7948\n",
      "Epoch [1/1], Step [8594/8897], Loss: 5.5873\n",
      "Epoch [1/1], Step [8595/8897], Loss: 5.3658\n",
      "Epoch [1/1], Step [8596/8897], Loss: 5.6312\n",
      "Epoch [1/1], Step [8597/8897], Loss: 5.4923\n",
      "Epoch [1/1], Step [8598/8897], Loss: 5.6250\n",
      "Epoch [1/1], Step [8599/8897], Loss: 5.6320\n",
      "Epoch [1/1], Step [8600/8897], Loss: 5.7736\n",
      "Epoch [1/1], Step [8601/8897], Loss: 5.5932\n",
      "Epoch [1/1], Step [8602/8897], Loss: 5.4571\n",
      "Epoch [1/1], Step [8603/8897], Loss: 5.2399\n",
      "Epoch [1/1], Step [8604/8897], Loss: 5.6165\n",
      "Epoch [1/1], Step [8605/8897], Loss: 5.3153\n",
      "Epoch [1/1], Step [8606/8897], Loss: 5.4399\n",
      "Epoch [1/1], Step [8607/8897], Loss: 5.3461\n",
      "Epoch [1/1], Step [8608/8897], Loss: 5.3329\n",
      "Epoch [1/1], Step [8609/8897], Loss: 5.6043\n",
      "Epoch [1/1], Step [8610/8897], Loss: 5.3237\n",
      "Epoch [1/1], Step [8611/8897], Loss: 5.4554\n",
      "Epoch [1/1], Step [8612/8897], Loss: 5.4360\n",
      "Epoch [1/1], Step [8613/8897], Loss: 5.7671\n",
      "Epoch [1/1], Step [8614/8897], Loss: 5.6798\n",
      "Epoch [1/1], Step [8615/8897], Loss: 5.4522\n",
      "Epoch [1/1], Step [8616/8897], Loss: 5.4802\n",
      "Epoch [1/1], Step [8617/8897], Loss: 5.5694\n",
      "Epoch [1/1], Step [8618/8897], Loss: 5.3530\n",
      "Epoch [1/1], Step [8619/8897], Loss: 5.5347\n",
      "Epoch [1/1], Step [8620/8897], Loss: 5.6592\n",
      "Epoch [1/1], Step [8621/8897], Loss: 5.5082\n",
      "Epoch [1/1], Step [8622/8897], Loss: 5.5918\n",
      "Epoch [1/1], Step [8623/8897], Loss: 5.5825\n",
      "Epoch [1/1], Step [8624/8897], Loss: 5.4102\n",
      "Epoch [1/1], Step [8625/8897], Loss: 5.4889\n",
      "Epoch [1/1], Step [8626/8897], Loss: 5.6222\n",
      "Epoch [1/1], Step [8627/8897], Loss: 5.2840\n",
      "Epoch [1/1], Step [8628/8897], Loss: 5.3014\n",
      "Epoch [1/1], Step [8629/8897], Loss: 5.4588\n",
      "Epoch [1/1], Step [8630/8897], Loss: 5.4793\n",
      "Epoch [1/1], Step [8631/8897], Loss: 5.1219\n",
      "Epoch [1/1], Step [8632/8897], Loss: 5.4927\n",
      "Epoch [1/1], Step [8633/8897], Loss: 5.7401\n",
      "Epoch [1/1], Step [8634/8897], Loss: 5.4506\n",
      "Epoch [1/1], Step [8635/8897], Loss: 5.7992\n",
      "Epoch [1/1], Step [8636/8897], Loss: 5.5098\n",
      "Epoch [1/1], Step [8637/8897], Loss: 5.2499\n",
      "Epoch [1/1], Step [8638/8897], Loss: 5.3996\n",
      "Epoch [1/1], Step [8639/8897], Loss: 5.5112\n",
      "Epoch [1/1], Step [8640/8897], Loss: 5.6052\n",
      "Epoch [1/1], Step [8641/8897], Loss: 5.5839\n",
      "Epoch [1/1], Step [8642/8897], Loss: 5.2633\n",
      "Epoch [1/1], Step [8643/8897], Loss: 5.4407\n",
      "Epoch [1/1], Step [8644/8897], Loss: 5.5083\n",
      "Epoch [1/1], Step [8645/8897], Loss: 5.7500\n",
      "Epoch [1/1], Step [8646/8897], Loss: 5.4704\n",
      "Epoch [1/1], Step [8647/8897], Loss: 5.7611\n",
      "Epoch [1/1], Step [8648/8897], Loss: 5.6230\n",
      "Epoch [1/1], Step [8649/8897], Loss: 5.5758\n",
      "Epoch [1/1], Step [8650/8897], Loss: 5.6236\n",
      "Epoch [1/1], Step [8651/8897], Loss: 5.7138\n",
      "Epoch [1/1], Step [8652/8897], Loss: 5.3747\n",
      "Epoch [1/1], Step [8653/8897], Loss: 5.5641\n",
      "Epoch [1/1], Step [8654/8897], Loss: 5.7686\n",
      "Epoch [1/1], Step [8655/8897], Loss: 5.4848\n",
      "Epoch [1/1], Step [8656/8897], Loss: 5.5030\n",
      "Epoch [1/1], Step [8657/8897], Loss: 5.7062\n",
      "Epoch [1/1], Step [8658/8897], Loss: 5.7927\n",
      "Epoch [1/1], Step [8659/8897], Loss: 5.4451\n",
      "Epoch [1/1], Step [8660/8897], Loss: 5.3660\n",
      "Epoch [1/1], Step [8661/8897], Loss: 5.5597\n",
      "Epoch [1/1], Step [8662/8897], Loss: 5.4684\n",
      "Epoch [1/1], Step [8663/8897], Loss: 5.6119\n",
      "Epoch [1/1], Step [8664/8897], Loss: 5.4014\n",
      "Epoch [1/1], Step [8665/8897], Loss: 5.5471\n",
      "Epoch [1/1], Step [8666/8897], Loss: 5.4120\n",
      "Epoch [1/1], Step [8667/8897], Loss: 5.4604\n",
      "Epoch [1/1], Step [8668/8897], Loss: 5.9491\n",
      "Epoch [1/1], Step [8669/8897], Loss: 5.4794\n",
      "Epoch [1/1], Step [8670/8897], Loss: 5.4108\n",
      "Epoch [1/1], Step [8671/8897], Loss: 5.2535\n",
      "Epoch [1/1], Step [8672/8897], Loss: 5.8878\n",
      "Epoch [1/1], Step [8673/8897], Loss: 5.4756\n",
      "Epoch [1/1], Step [8674/8897], Loss: 5.5623\n",
      "Epoch [1/1], Step [8675/8897], Loss: 5.5818\n",
      "Epoch [1/1], Step [8676/8897], Loss: 5.4192\n",
      "Epoch [1/1], Step [8677/8897], Loss: 5.8344\n",
      "Epoch [1/1], Step [8678/8897], Loss: 5.4228\n",
      "Epoch [1/1], Step [8679/8897], Loss: 5.3331\n",
      "Epoch [1/1], Step [8680/8897], Loss: 5.2800\n",
      "Epoch [1/1], Step [8681/8897], Loss: 5.5605\n",
      "Epoch [1/1], Step [8682/8897], Loss: 5.4803\n",
      "Epoch [1/1], Step [8683/8897], Loss: 5.6069\n",
      "Epoch [1/1], Step [8684/8897], Loss: 5.3734\n",
      "Epoch [1/1], Step [8685/8897], Loss: 5.5731\n",
      "Epoch [1/1], Step [8686/8897], Loss: 5.7241\n",
      "Epoch [1/1], Step [8687/8897], Loss: 5.6756\n",
      "Epoch [1/1], Step [8688/8897], Loss: 5.4086\n",
      "Epoch [1/1], Step [8689/8897], Loss: 5.4347\n",
      "Epoch [1/1], Step [8690/8897], Loss: 5.5559\n",
      "Epoch [1/1], Step [8691/8897], Loss: 5.5682\n",
      "Epoch [1/1], Step [8692/8897], Loss: 5.3439\n",
      "Epoch [1/1], Step [8693/8897], Loss: 5.5526\n",
      "Epoch [1/1], Step [8694/8897], Loss: 5.3904\n",
      "Epoch [1/1], Step [8695/8897], Loss: 5.4909\n",
      "Epoch [1/1], Step [8696/8897], Loss: 5.5956\n",
      "Epoch [1/1], Step [8697/8897], Loss: 5.3130\n",
      "Epoch [1/1], Step [8698/8897], Loss: 5.3125\n",
      "Epoch [1/1], Step [8699/8897], Loss: 5.5025\n",
      "Epoch [1/1], Step [8700/8897], Loss: 5.3599\n",
      "Epoch [1/1], Step [8701/8897], Loss: 5.4646\n",
      "Epoch [1/1], Step [8702/8897], Loss: 5.5979\n",
      "Epoch [1/1], Step [8703/8897], Loss: 5.6185\n",
      "Epoch [1/1], Step [8704/8897], Loss: 5.4794\n",
      "Epoch [1/1], Step [8705/8897], Loss: 5.5321\n",
      "Epoch [1/1], Step [8706/8897], Loss: 5.5244\n",
      "Epoch [1/1], Step [8707/8897], Loss: 5.4255\n",
      "Epoch [1/1], Step [8708/8897], Loss: 5.5503\n",
      "Epoch [1/1], Step [8709/8897], Loss: 5.5281\n",
      "Epoch [1/1], Step [8710/8897], Loss: 5.5418\n",
      "Epoch [1/1], Step [8711/8897], Loss: 5.4399\n",
      "Epoch [1/1], Step [8712/8897], Loss: 5.3324\n",
      "Epoch [1/1], Step [8713/8897], Loss: 5.3921\n",
      "Epoch [1/1], Step [8714/8897], Loss: 5.6290\n",
      "Epoch [1/1], Step [8715/8897], Loss: 5.5953\n",
      "Epoch [1/1], Step [8716/8897], Loss: 5.6200\n",
      "Epoch [1/1], Step [8717/8897], Loss: 5.9600\n",
      "Epoch [1/1], Step [8718/8897], Loss: 5.3909\n",
      "Epoch [1/1], Step [8719/8897], Loss: 5.6667\n",
      "Epoch [1/1], Step [8720/8897], Loss: 5.5208\n",
      "Epoch [1/1], Step [8721/8897], Loss: 5.6017\n",
      "Epoch [1/1], Step [8722/8897], Loss: 5.7461\n",
      "Epoch [1/1], Step [8723/8897], Loss: 5.4360\n",
      "Epoch [1/1], Step [8724/8897], Loss: 5.6117\n",
      "Epoch [1/1], Step [8725/8897], Loss: 5.8173\n",
      "Epoch [1/1], Step [8726/8897], Loss: 5.8688\n",
      "Epoch [1/1], Step [8727/8897], Loss: 5.6520\n",
      "Epoch [1/1], Step [8728/8897], Loss: 5.4202\n",
      "Epoch [1/1], Step [8729/8897], Loss: 5.5565\n",
      "Epoch [1/1], Step [8730/8897], Loss: 5.6083\n",
      "Epoch [1/1], Step [8731/8897], Loss: 5.7702\n",
      "Epoch [1/1], Step [8732/8897], Loss: 5.4052\n",
      "Epoch [1/1], Step [8733/8897], Loss: 5.4415\n",
      "Epoch [1/1], Step [8734/8897], Loss: 5.6387\n",
      "Epoch [1/1], Step [8735/8897], Loss: 5.5493\n",
      "Epoch [1/1], Step [8736/8897], Loss: 5.4750\n",
      "Epoch [1/1], Step [8737/8897], Loss: 5.6715\n",
      "Epoch [1/1], Step [8738/8897], Loss: 5.4809\n",
      "Epoch [1/1], Step [8739/8897], Loss: 5.5344\n",
      "Epoch [1/1], Step [8740/8897], Loss: 5.7283\n",
      "Epoch [1/1], Step [8741/8897], Loss: 5.2966\n",
      "Epoch [1/1], Step [8742/8897], Loss: 5.2604\n",
      "Epoch [1/1], Step [8743/8897], Loss: 5.5289\n",
      "Epoch [1/1], Step [8744/8897], Loss: 5.5581\n",
      "Epoch [1/1], Step [8745/8897], Loss: 5.5046\n",
      "Epoch [1/1], Step [8746/8897], Loss: 5.5640\n",
      "Epoch [1/1], Step [8747/8897], Loss: 5.6212\n",
      "Epoch [1/1], Step [8748/8897], Loss: 5.4957\n",
      "Epoch [1/1], Step [8749/8897], Loss: 5.4189\n",
      "Epoch [1/1], Step [8750/8897], Loss: 5.5236\n",
      "Epoch [1/1], Step [8751/8897], Loss: 5.5496\n",
      "Epoch [1/1], Step [8752/8897], Loss: 5.4687\n",
      "Epoch [1/1], Step [8753/8897], Loss: 5.5699\n",
      "Epoch [1/1], Step [8754/8897], Loss: 5.3616\n",
      "Epoch [1/1], Step [8755/8897], Loss: 5.2757\n",
      "Epoch [1/1], Step [8756/8897], Loss: 5.6915\n",
      "Epoch [1/1], Step [8757/8897], Loss: 5.6161\n",
      "Epoch [1/1], Step [8758/8897], Loss: 5.5107\n",
      "Epoch [1/1], Step [8759/8897], Loss: 5.6748\n",
      "Epoch [1/1], Step [8760/8897], Loss: 5.5593\n",
      "Epoch [1/1], Step [8761/8897], Loss: 5.6440\n",
      "Epoch [1/1], Step [8762/8897], Loss: 5.6019\n",
      "Epoch [1/1], Step [8763/8897], Loss: 5.6551\n",
      "Epoch [1/1], Step [8764/8897], Loss: 5.4235\n",
      "Epoch [1/1], Step [8765/8897], Loss: 5.2936\n",
      "Epoch [1/1], Step [8766/8897], Loss: 5.4253\n",
      "Epoch [1/1], Step [8767/8897], Loss: 5.5932\n",
      "Epoch [1/1], Step [8768/8897], Loss: 5.4101\n",
      "Epoch [1/1], Step [8769/8897], Loss: 5.5169\n",
      "Epoch [1/1], Step [8770/8897], Loss: 5.4470\n",
      "Epoch [1/1], Step [8771/8897], Loss: 5.6552\n",
      "Epoch [1/1], Step [8772/8897], Loss: 5.3373\n",
      "Epoch [1/1], Step [8773/8897], Loss: 5.6422\n",
      "Epoch [1/1], Step [8774/8897], Loss: 5.7774\n",
      "Epoch [1/1], Step [8775/8897], Loss: 5.4343\n",
      "Epoch [1/1], Step [8776/8897], Loss: 5.4902\n",
      "Epoch [1/1], Step [8777/8897], Loss: 5.3516\n",
      "Epoch [1/1], Step [8778/8897], Loss: 5.4041\n",
      "Epoch [1/1], Step [8779/8897], Loss: 5.4357\n",
      "Epoch [1/1], Step [8780/8897], Loss: 5.2846\n",
      "Epoch [1/1], Step [8781/8897], Loss: 5.7687\n",
      "Epoch [1/1], Step [8782/8897], Loss: 5.5402\n",
      "Epoch [1/1], Step [8783/8897], Loss: 5.3806\n",
      "Epoch [1/1], Step [8784/8897], Loss: 5.5878\n",
      "Epoch [1/1], Step [8785/8897], Loss: 5.4548\n",
      "Epoch [1/1], Step [8786/8897], Loss: 5.7386\n",
      "Epoch [1/1], Step [8787/8897], Loss: 5.6567\n",
      "Epoch [1/1], Step [8788/8897], Loss: 5.3935\n",
      "Epoch [1/1], Step [8789/8897], Loss: 5.5900\n",
      "Epoch [1/1], Step [8790/8897], Loss: 5.6534\n",
      "Epoch [1/1], Step [8791/8897], Loss: 5.5440\n",
      "Epoch [1/1], Step [8792/8897], Loss: 5.5920\n",
      "Epoch [1/1], Step [8793/8897], Loss: 5.5229\n",
      "Epoch [1/1], Step [8794/8897], Loss: 5.6725\n",
      "Epoch [1/1], Step [8795/8897], Loss: 5.5715\n",
      "Epoch [1/1], Step [8796/8897], Loss: 5.5314\n",
      "Epoch [1/1], Step [8797/8897], Loss: 5.5898\n",
      "Epoch [1/1], Step [8798/8897], Loss: 5.5445\n",
      "Epoch [1/1], Step [8799/8897], Loss: 5.4300\n",
      "Epoch [1/1], Step [8800/8897], Loss: 5.7874\n",
      "Epoch [1/1], Step [8801/8897], Loss: 5.4601\n",
      "Epoch [1/1], Step [8802/8897], Loss: 5.5949\n",
      "Epoch [1/1], Step [8803/8897], Loss: 5.5027\n",
      "Epoch [1/1], Step [8804/8897], Loss: 5.6677\n",
      "Epoch [1/1], Step [8805/8897], Loss: 5.4670\n",
      "Epoch [1/1], Step [8806/8897], Loss: 5.2639\n",
      "Epoch [1/1], Step [8807/8897], Loss: 5.5537\n",
      "Epoch [1/1], Step [8808/8897], Loss: 5.4866\n",
      "Epoch [1/1], Step [8809/8897], Loss: 5.6639\n",
      "Epoch [1/1], Step [8810/8897], Loss: 5.3748\n",
      "Epoch [1/1], Step [8811/8897], Loss: 5.4314\n",
      "Epoch [1/1], Step [8812/8897], Loss: 5.3958\n",
      "Epoch [1/1], Step [8813/8897], Loss: 5.5288\n",
      "Epoch [1/1], Step [8814/8897], Loss: 5.7133\n",
      "Epoch [1/1], Step [8815/8897], Loss: 5.4988\n",
      "Epoch [1/1], Step [8816/8897], Loss: 5.3773\n",
      "Epoch [1/1], Step [8817/8897], Loss: 5.6978\n",
      "Epoch [1/1], Step [8818/8897], Loss: 5.4995\n",
      "Epoch [1/1], Step [8819/8897], Loss: 5.3933\n",
      "Epoch [1/1], Step [8820/8897], Loss: 5.3944\n",
      "Epoch [1/1], Step [8821/8897], Loss: 5.4834\n",
      "Epoch [1/1], Step [8822/8897], Loss: 5.3218\n",
      "Epoch [1/1], Step [8823/8897], Loss: 5.4213\n",
      "Epoch [1/1], Step [8824/8897], Loss: 5.5742\n",
      "Epoch [1/1], Step [8825/8897], Loss: 5.7791\n",
      "Epoch [1/1], Step [8826/8897], Loss: 5.5372\n",
      "Epoch [1/1], Step [8827/8897], Loss: 5.4075\n",
      "Epoch [1/1], Step [8828/8897], Loss: 5.4838\n",
      "Epoch [1/1], Step [8829/8897], Loss: 5.6570\n",
      "Epoch [1/1], Step [8830/8897], Loss: 5.7689\n",
      "Epoch [1/1], Step [8831/8897], Loss: 5.4996\n",
      "Epoch [1/1], Step [8832/8897], Loss: 5.5606\n",
      "Epoch [1/1], Step [8833/8897], Loss: 5.5152\n",
      "Epoch [1/1], Step [8834/8897], Loss: 5.3059\n",
      "Epoch [1/1], Step [8835/8897], Loss: 5.4260\n",
      "Epoch [1/1], Step [8836/8897], Loss: 5.2280\n",
      "Epoch [1/1], Step [8837/8897], Loss: 5.5121\n",
      "Epoch [1/1], Step [8838/8897], Loss: 5.3602\n",
      "Epoch [1/1], Step [8839/8897], Loss: 5.4846\n",
      "Epoch [1/1], Step [8840/8897], Loss: 5.2901\n",
      "Epoch [1/1], Step [8841/8897], Loss: 5.5871\n",
      "Epoch [1/1], Step [8842/8897], Loss: 5.5686\n",
      "Epoch [1/1], Step [8843/8897], Loss: 5.3823\n",
      "Epoch [1/1], Step [8844/8897], Loss: 5.5109\n",
      "Epoch [1/1], Step [8845/8897], Loss: 5.6042\n",
      "Epoch [1/1], Step [8846/8897], Loss: 5.5063\n",
      "Epoch [1/1], Step [8847/8897], Loss: 5.4998\n",
      "Epoch [1/1], Step [8848/8897], Loss: 5.4800\n",
      "Epoch [1/1], Step [8849/8897], Loss: 5.6901\n",
      "Epoch [1/1], Step [8850/8897], Loss: 5.6412\n",
      "Epoch [1/1], Step [8851/8897], Loss: 5.6781\n",
      "Epoch [1/1], Step [8852/8897], Loss: 5.3971\n",
      "Epoch [1/1], Step [8853/8897], Loss: 5.5106\n",
      "Epoch [1/1], Step [8854/8897], Loss: 5.4429\n",
      "Epoch [1/1], Step [8855/8897], Loss: 5.6301\n",
      "Epoch [1/1], Step [8856/8897], Loss: 5.4781\n",
      "Epoch [1/1], Step [8857/8897], Loss: 5.2812\n",
      "Epoch [1/1], Step [8858/8897], Loss: 5.5553\n",
      "Epoch [1/1], Step [8859/8897], Loss: 5.5130\n",
      "Epoch [1/1], Step [8860/8897], Loss: 5.4631\n",
      "Epoch [1/1], Step [8861/8897], Loss: 5.5820\n",
      "Epoch [1/1], Step [8862/8897], Loss: 5.6680\n",
      "Epoch [1/1], Step [8863/8897], Loss: 5.6179\n",
      "Epoch [1/1], Step [8864/8897], Loss: 5.2242\n",
      "Epoch [1/1], Step [8865/8897], Loss: 5.3214\n",
      "Epoch [1/1], Step [8866/8897], Loss: 5.4052\n",
      "Epoch [1/1], Step [8867/8897], Loss: 5.0914\n",
      "Epoch [1/1], Step [8868/8897], Loss: 5.4357\n",
      "Epoch [1/1], Step [8869/8897], Loss: 5.7404\n",
      "Epoch [1/1], Step [8870/8897], Loss: 5.2426\n",
      "Epoch [1/1], Step [8871/8897], Loss: 5.5357\n",
      "Epoch [1/1], Step [8872/8897], Loss: 5.4697\n",
      "Epoch [1/1], Step [8873/8897], Loss: 5.5380\n",
      "Epoch [1/1], Step [8874/8897], Loss: 5.4699\n",
      "Epoch [1/1], Step [8875/8897], Loss: 5.5878\n",
      "Epoch [1/1], Step [8876/8897], Loss: 5.5695\n",
      "Epoch [1/1], Step [8877/8897], Loss: 5.7359\n",
      "Epoch [1/1], Step [8878/8897], Loss: 5.6795\n",
      "Epoch [1/1], Step [8879/8897], Loss: 5.8426\n",
      "Epoch [1/1], Step [8880/8897], Loss: 5.6388\n",
      "Epoch [1/1], Step [8881/8897], Loss: 5.6201\n",
      "Epoch [1/1], Step [8882/8897], Loss: 5.4004\n",
      "Epoch [1/1], Step [8883/8897], Loss: 5.5421\n",
      "Epoch [1/1], Step [8884/8897], Loss: 5.6439\n",
      "Epoch [1/1], Step [8885/8897], Loss: 5.3544\n",
      "Epoch [1/1], Step [8886/8897], Loss: 5.4537\n",
      "Epoch [1/1], Step [8887/8897], Loss: 5.4135\n",
      "Epoch [1/1], Step [8888/8897], Loss: 5.6170\n",
      "Epoch [1/1], Step [8889/8897], Loss: 5.5690\n",
      "Epoch [1/1], Step [8890/8897], Loss: 5.6114\n",
      "Epoch [1/1], Step [8891/8897], Loss: 5.4373\n",
      "Epoch [1/1], Step [8892/8897], Loss: 5.5328\n",
      "Epoch [1/1], Step [8893/8897], Loss: 5.7761\n",
      "Epoch [1/1], Step [8894/8897], Loss: 5.5204\n",
      "Epoch [1/1], Step [8895/8897], Loss: 5.4194\n",
      "Epoch [1/1], Step [8896/8897], Loss: 5.3628\n",
      "Epoch [1/1], Step [8897/8897], Loss: 5.5296\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "learning_rates = []\n",
    "\n",
    "# Training Loop\n",
    "for epoch_idx in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (train_images, train_labels) in enumerate(train_loader1k):\n",
    "        train_images, train_labels = train_images.to(device), train_labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        train_outputs = model(train_images)\n",
    "\n",
    "        # Compute the loss\n",
    "        train_loss = criterion(train_outputs, train_labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Gradient Clip\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Step optimizer and the scheduler\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss.item())\n",
    "        learning_rates.append(scheduler.get_last_lr()[0])  # Assumes optimizer has a single param group\n",
    "\n",
    "        print(f\"Epoch [{epoch_idx+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader1k)}], Loss: {train_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the validation images: 6.456%\n"
     ]
    }
   ],
   "source": [
    "# Validation Loop\n",
    "# NOTE: LOGITS TO MAX LOGIT FUNCTION MIGHT CHANGE DUE TO SPECIFIC NATURE OF VISION TRANSFORMER ALGORITHM\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for val_images, val_labels in val_loader1k:\n",
    "        val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "\n",
    "        # Logits\n",
    "        val_outputs = model(val_images)\n",
    "\n",
    "        # Let the index of the highest logit be the predicted class \n",
    "        _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "\n",
    "        # Update counts from this batch's values\n",
    "        total_count += val_labels.size(0)\n",
    "        correct_count += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "    # Print accuracy score\n",
    "    print(f'Accuracy of the model on the validation images: {100 * correct_count / total_count}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save(model, './models/vit_16p_1280d_12l.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit21k_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
