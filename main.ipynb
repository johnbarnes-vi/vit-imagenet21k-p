{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure you follow the preprocessing instructions in the README.md file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Lets see the directory structure of imagenet1k\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        jpeg_files = [f for f in files if f.endswith('.JPEG')]\n",
    "        if jpeg_files:  # if the list is not empty\n",
    "            print('{}Number of JPEG files: {}'.format(subindent, len(jpeg_files)))\n",
    "        for f in files:\n",
    "            if f.endswith('.txt'):\n",
    "                print('{}{}'.format(subindent, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet1k_resized/ILSVRC2012_img_train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet1k_resized/ILSVRC2012_img_val/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet21k_resized_new/imagenet21k_train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet21k_resized_new/imagenet21k_val/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is clear from the output of the above cells that preprocessing worked!\n",
    "\n",
    "We are looking to see if the validation and training sets are organized in the same manner and that they are ordered the same.\n",
    "\n",
    "This makes input into the `torchvision.datasets.ImageFolder` class work without a hitch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries to unzip `tiny-imagenet-200.zip`\n",
    "import zipfile\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "# Importing pytorch libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing custom VisionTransformer Model\n",
    "\n",
    "from models.vit import VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cuda')\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "patch_size_ = 16     # to be changed\n",
    "D_ = 1280            # to be changed\n",
    "num_layers_ = 12     # to be changed\n",
    "num_classes_ = 1000\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "model = VisionTransformer(patch_size=patch_size_, D=D_, num_layers=num_layers_, num_classes=num_classes_)\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Define a transform for training data\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Pad(4),  # Pad the image by 4 pixels\n",
    "    transforms.RandomCrop(224),  # Randomly crop a 224x224 region from the padded image\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Define a transform for validation data\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to range [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CPU cores: 24\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of available CPU cores:\", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageNet-1k has 1,281,200 training images and 50,000 validation images!\n"
     ]
    }
   ],
   "source": [
    "# Load ImageNet1k dataset and make DataLoaders\n",
    "train_dataset1k = datasets.ImageFolder(root='/mnt/imagenet1k_resized/ILSVRC2012_img_train', transform=train_transform)\n",
    "val_dataset1k = datasets.ImageFolder(root='/mnt/imagenet1k_resized/ILSVRC2012_img_val', transform=val_transform)\n",
    "\n",
    "train_loader1k = DataLoader(dataset=train_dataset1k, batch_size=batch_size, shuffle=True, num_workers=20, pin_memory=True)\n",
    "val_loader1k = DataLoader(dataset=val_dataset1k, batch_size=batch_size, shuffle=False, num_workers=20, pin_memory=True)\n",
    "\n",
    "print(f\"ImageNet-1k has {len(train_loader1k)*batch_size:,} training images and {len(val_loader1k)*batch_size:,} validation images!\")\n",
    "\n",
    "# Load ImageNet21k dataset and make DataLoaders\n",
    "#train_dataset21k = datasets.ImageFolder(root='/mnt/imagenet21k_resized_new/imagenet21k_train', transform=train_transform)\n",
    "#val_dataset21k = datasets.ImageFolder(root='/mnt/imagenet21k_resized_new/imagenet21k_val', transform=val_transform)\n",
    "\n",
    "#train_loader21k = DataLoader(dataset=train_dataset21k, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "#val_loader21k = DataLoader(dataset=val_dataset21k, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "#print(f\"ImageNet-21k has {len(train_loader21k)*batch_size:,} training images and {len(val_loader21k)*batch_size:,} validation images!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images batch shape: torch.Size([100, 3, 224, 224])\n",
      "Train labels batch shape: torch.Size([100])\n",
      "Train images data type: torch.float32\n",
      "Train labels data type: torch.int64\n",
      "Validation images batch shape: torch.Size([100, 3, 224, 224])\n",
      "Validation labels batch shape: torch.Size([100])\n",
      "Validation images data type: torch.float32\n",
      "Validation labels data type: torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Inspect a batch from train_loader1k\n",
    "train_images, train_labels = next(iter(train_loader1k))\n",
    "train_images = train_images.to(device)\n",
    "\n",
    "print(\"Train images batch shape:\", train_images.shape)\n",
    "print(\"Train labels batch shape:\", train_labels.shape)\n",
    "print(\"Train images data type:\", train_images.dtype)\n",
    "print(\"Train labels data type:\", train_labels.dtype)\n",
    "\n",
    "# Inspect a batch from val_loader1k\n",
    "val_images, val_labels = next(iter(val_loader1k))\n",
    "\n",
    "print(\"Validation images batch shape:\", val_images.shape)\n",
    "print(\"Validation labels batch shape:\", val_labels.shape)\n",
    "print(\"Validation images data type:\", val_images.dtype)\n",
    "print(\"Validation labels data type:\", val_labels.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING COMPONENTS OF vit.py IN IPYNB BEFORE MOVING TO .PY FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Class for Image Preprocessing\n",
    "class ImagePreprocessor(nn.Module):\n",
    "    def __init__(self, patch_size):\n",
    "        super(ImagePreprocessor, self).__init__()\n",
    "        self.patch_size = patch_size  # Size of each patch\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Dynamically get the batch size and channel dimensions\n",
    "        batch_size, channel, _, _ = x.size()\n",
    "\n",
    "        # Using unfold to create patches\n",
    "        x_p = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "\n",
    "        # Reshape into the desired shape\n",
    "        x_p = x_p.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "        x_p = x_p.view(batch_size, -1, self.patch_size * self.patch_size * channel)\n",
    "\n",
    "        # Now x_p should have shape [batch_size, (Height * Width) / (patch_size * patch_size), (patch_size * patch_size * channel)]\n",
    "        \n",
    "        return x_p\n",
    "\n",
    "# Class for Patch Embedding\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_dim, D):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.D = D  # Dimension to project to\n",
    "        self.linear = nn.Linear(patch_dim, D)  # Linear projection layer\n",
    "\n",
    "    def forward(self, x_p):\n",
    "        # Project patches to D dimensions\n",
    "        x_emb = self.linear(x_p)\n",
    "        return x_emb\n",
    "\n",
    "# Class for adding a Class Token\n",
    "class ClassToken(nn.Module):\n",
    "    def __init__(self, D):\n",
    "        super(ClassToken, self).__init__()\n",
    "        self.class_token_embedding = nn.Parameter(torch.randn(1, 1, D))  # Learnable class token\n",
    "\n",
    "    def forward(self, x_emb):\n",
    "        # Prepend class token to patch embeddings\n",
    "        batch_size = x_emb.size(0)\n",
    "        class_token = self.class_token_embedding.repeat(batch_size, 1, 1)\n",
    "        x_class = torch.cat([class_token, x_emb], dim=1)\n",
    "        return x_class\n",
    "\n",
    "# Class for Position Embeddings\n",
    "class PositionEmbedding(nn.Module):\n",
    "    def __init__(self, seq_len, D):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(1, seq_len, D))  # Learnable position embeddings\n",
    "\n",
    "    def forward(self, x_class):\n",
    "        # Add position embeddings\n",
    "        x_pos = x_class + self.position_embeddings\n",
    "        return x_pos\n",
    "\n",
    "# Class for Transformer Encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, D, num_layers):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_norm = nn.LayerNorm(D)\n",
    "        self.multihead_attention = nn.MultiheadAttention(D, num_heads=4)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(D, D),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(D, D)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_pos):\n",
    "        # Transformer Encoder Logic\n",
    "        for _ in range(self.num_layers):\n",
    "            x_norm = self.layer_norm(x_pos)\n",
    "            x_att, _ = self.multihead_attention(x_norm, x_norm, x_norm)\n",
    "            x_pos = x_pos + x_att\n",
    "            x_pos = x_pos + self.mlp(self.layer_norm(x_pos))\n",
    "        return x_pos\n",
    "\n",
    "# Class for Classification Head\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, D, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.linear = nn.Linear(D, num_classes)  # Linear layer for classification\n",
    "\n",
    "    def forward(self, x_transformed):\n",
    "        # Take the class token and perform classification\n",
    "        x_class_token = x_transformed[:, 0, :]\n",
    "        output = self.linear(x_class_token)\n",
    "        return output\n",
    "\n",
    "# Main Vision Transformer Class\n",
    "class VisionTransformerTest(nn.Module):\n",
    "    def __init__(self, patch_size, D, num_layers, num_classes):\n",
    "        super(VisionTransformerTest, self).__init__()\n",
    "        self.image_preprocessor = ImagePreprocessor(patch_size)\n",
    "        self.patch_embedding = PatchEmbedding(patch_size * patch_size * 3, D)  # 3 channels, patch_size x patch_size patches\n",
    "        self.class_token = ClassToken(D)\n",
    "        self.position_embedding = PositionEmbedding(197, D)  # 196 patches + 1 class token\n",
    "        self.transformer_encoder = TransformerEncoder(D, num_layers)\n",
    "        self.classification_head = ClassificationHead(D, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"x.shape\",x.shape)\n",
    "        \n",
    "        # Preprocess the image into patches\n",
    "        x_p = self.image_preprocessor(x)\n",
    "        print(\"x_p.shape: \", x_p.shape)\n",
    "\n",
    "        # Generate patch embeddings\n",
    "        x_emb = self.patch_embedding(x_p)\n",
    "        print(\"x_emb.shape: \", x_emb.shape)\n",
    "\n",
    "        # Prepend the class token\n",
    "        x_class = self.class_token(x_emb)\n",
    "        print(\"x_class.shape: \", x_class.shape)\n",
    "\n",
    "        # Add position embeddings\n",
    "        x_pos = self.position_embedding(x_class)\n",
    "        print(\"x_pos.shape: \", x_pos.shape)\n",
    "\n",
    "        # Pass through the Transformer Encoder\n",
    "        x_transformed = self.transformer_encoder(x_pos)\n",
    "        print(\"x_transformed.shape: \", x_transformed.shape)\n",
    "    \n",
    "        # Perform classification\n",
    "        output = self.classification_head(x_transformed)\n",
    "        print(\"output.shape: \", output.shape)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape torch.Size([100, 3, 224, 224])\n",
      "x_p.shape:  torch.Size([100, 196, 768])\n",
      "x_emb.shape:  torch.Size([100, 196, 1280])\n",
      "x_class.shape:  torch.Size([100, 197, 1280])\n",
      "x_pos.shape:  torch.Size([100, 197, 1280])\n",
      "x_transformed.shape:  torch.Size([100, 197, 1280])\n",
      "output.shape:  torch.Size([100, 1000])\n",
      "Above was the transformation path of the data\n"
     ]
    }
   ],
   "source": [
    "model_test = VisionTransformerTest(patch_size=patch_size_, D=D_, num_layers=num_layers_, num_classes=num_classes_)\n",
    "model_test.to(device)\n",
    "output = model_test(train_images)\n",
    "print(\"Above was the transformation path of the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 196, 1280])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_preprocessor = ImagePreprocessor(patch_size=16)\n",
    "patch_embedder = PatchEmbedding(patch_dim=16*16*3, D=1280)\n",
    "x_p = image_preprocessor(train_images)\n",
    "x_emb = patch_embedder(x_p)\n",
    "x_emb.shape\n",
    "\n",
    "# I dont think this is the right shape the color dim needs to collapse into the last one also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING COMPONENTS OF vit.py IN IPYNB BEFORE MOVING TO .PY FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/12812], Loss: 11.0264\n",
      "Epoch [1/10], Step [2/12812], Loss: 16.6684\n",
      "Epoch [1/10], Step [3/12812], Loss: 37.1431\n",
      "Epoch [1/10], Step [4/12812], Loss: 148.2206\n",
      "Epoch [1/10], Step [5/12812], Loss: 185.9615\n",
      "Epoch [1/10], Step [6/12812], Loss: 68458.3359\n",
      "Epoch [1/10], Step [7/12812], Loss: 104053136.0000\n",
      "Epoch [1/10], Step [8/12812], Loss: 4156940928440860672.0000\n",
      "Epoch [1/10], Step [9/12812], Loss: nan\n",
      "Epoch [1/10], Step [10/12812], Loss: nan\n",
      "Epoch [1/10], Step [11/12812], Loss: nan\n",
      "Epoch [1/10], Step [12/12812], Loss: nan\n",
      "Epoch [1/10], Step [13/12812], Loss: nan\n",
      "Epoch [1/10], Step [14/12812], Loss: nan\n",
      "Epoch [1/10], Step [15/12812], Loss: nan\n",
      "Epoch [1/10], Step [16/12812], Loss: nan\n",
      "Epoch [1/10], Step [17/12812], Loss: nan\n",
      "Epoch [1/10], Step [18/12812], Loss: nan\n",
      "Epoch [1/10], Step [19/12812], Loss: nan\n",
      "Epoch [1/10], Step [20/12812], Loss: nan\n",
      "Epoch [1/10], Step [21/12812], Loss: nan\n",
      "Epoch [1/10], Step [22/12812], Loss: nan\n",
      "Epoch [1/10], Step [23/12812], Loss: nan\n",
      "Epoch [1/10], Step [24/12812], Loss: nan\n",
      "Epoch [1/10], Step [25/12812], Loss: nan\n",
      "Epoch [1/10], Step [26/12812], Loss: nan\n",
      "Epoch [1/10], Step [27/12812], Loss: nan\n",
      "Epoch [1/10], Step [28/12812], Loss: nan\n",
      "Epoch [1/10], Step [29/12812], Loss: nan\n",
      "Epoch [1/10], Step [30/12812], Loss: nan\n",
      "Epoch [1/10], Step [31/12812], Loss: nan\n",
      "Epoch [1/10], Step [32/12812], Loss: nan\n",
      "Epoch [1/10], Step [33/12812], Loss: nan\n",
      "Epoch [1/10], Step [34/12812], Loss: nan\n",
      "Epoch [1/10], Step [35/12812], Loss: nan\n",
      "Epoch [1/10], Step [36/12812], Loss: nan\n",
      "Epoch [1/10], Step [37/12812], Loss: nan\n",
      "Epoch [1/10], Step [38/12812], Loss: nan\n",
      "Epoch [1/10], Step [39/12812], Loss: nan\n",
      "Epoch [1/10], Step [40/12812], Loss: nan\n",
      "Epoch [1/10], Step [41/12812], Loss: nan\n",
      "Epoch [1/10], Step [42/12812], Loss: nan\n",
      "Epoch [1/10], Step [43/12812], Loss: nan\n",
      "Epoch [1/10], Step [44/12812], Loss: nan\n",
      "Epoch [1/10], Step [45/12812], Loss: nan\n",
      "Epoch [1/10], Step [46/12812], Loss: nan\n",
      "Epoch [1/10], Step [47/12812], Loss: nan\n",
      "Epoch [1/10], Step [48/12812], Loss: nan\n",
      "Epoch [1/10], Step [49/12812], Loss: nan\n",
      "Epoch [1/10], Step [50/12812], Loss: nan\n",
      "Epoch [1/10], Step [51/12812], Loss: nan\n",
      "Epoch [1/10], Step [52/12812], Loss: nan\n",
      "Epoch [1/10], Step [53/12812], Loss: nan\n",
      "Epoch [1/10], Step [54/12812], Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb#X23sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m train_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb#X23sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb#X23sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch_idx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m], Step [\u001b[39m\u001b[39m{\u001b[39;00mbatch_idx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(train_loader1k)\u001b[39m}\u001b[39;00m\u001b[39m], Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m.\u001b[39;49mitem()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (train_images, train_labels) in enumerate(train_loader1k):\n",
    "        train_images, train_labels = train_images.to(device), train_labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        train_outputs = model(train_images)\n",
    "\n",
    "        # Compute the loss\n",
    "        train_loss = criterion(train_outputs, train_labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch_idx+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader1k)}], Loss: {train_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Module [VisionTransformer] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m val_images, val_labels \u001b[39m=\u001b[39m val_images\u001b[39m.\u001b[39mto(device), val_labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Logits\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb#X25sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m val_outputs \u001b[39m=\u001b[39m model(val_images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Let the index of the highest logit be the predicted class \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb#X25sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m _, val_predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(val_outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/repos/vit-imagenet21k-p/vit21k_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/vit-imagenet21k-p/vit21k_env/lib/python3.10/site-packages/torch/nn/modules/module.py:363\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_unimplemented\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39minput\u001b[39m: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Defines the computation performed at every call.\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \n\u001b[1;32m    355\u001b[0m \u001b[39m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[39m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModule [\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m] is missing the required \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mforward\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m function\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Module [VisionTransformer] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "# Validation Loop\n",
    "# NOTE: LOGITS TO MAX LOGIT FUNCTION MIGHT CHANGE DUE TO SPECIFIC NATURE OF VISION TRANSFORMER ALGORITHM\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for val_images, val_labels in val_loader1k:\n",
    "        val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "\n",
    "        # Logits\n",
    "        val_outputs = model(val_images)\n",
    "\n",
    "        # Let the index of the highest logit be the predicted class \n",
    "        _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "\n",
    "        # Update counts from this batch's values\n",
    "        total_count += val_labels.size(0)\n",
    "        correct_count += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "    # Print accuracy score\n",
    "    print(f'Accuracy of the model on the validation images: {100 * correct_count / total_count}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit21k_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
