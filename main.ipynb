{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure you follow the preprocessing instructions in the README.md file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Lets see the directory structure of imagenet1k\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        jpeg_files = [f for f in files if f.endswith('.JPEG')]\n",
    "        if jpeg_files:  # if the list is not empty\n",
    "            print('{}Number of JPEG files: {}'.format(subindent, len(jpeg_files)))\n",
    "        for f in files:\n",
    "            if f.endswith('.txt'):\n",
    "                print('{}{}'.format(subindent, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet1k_resized/ILSVRC2012_img_train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet1k_resized/ILSVRC2012_img_val/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet21k_resized_new/imagenet21k_train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet21k_resized_new/imagenet21k_val/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is clear from the output of the above cells that preprocessing worked!\n",
    "\n",
    "We are looking to see if the validation and training sets are organized in the same manner and that they are ordered the same.\n",
    "\n",
    "This makes input into the `torchvision.datasets.ImageFolder` class work without a hitch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries to unzip `tiny-imagenet-200.zip`\n",
    "import zipfile\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "# Importing pytorch libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing custom VisionTransformer Model\n",
    "\n",
    "from models.vit import VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cuda')\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 512 # should be 4096 for ViT paper\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "patch_size_ = 32\n",
    "\n",
    "num_layers_ = 12\n",
    "D_ = 768\n",
    "mlp_size_ = 3072\n",
    "num_heads_ = 12\n",
    "\n",
    "num_classes_ = 1000\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "model = VisionTransformer(patch_size=patch_size_, D=D_, num_layers=num_layers_, num_classes=num_classes_, num_heads=num_heads_, mlp_size=mlp_size_)\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "\n",
    "# Define a transform for training data\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Pad(4),  # Pad the image by 4 pixels\n",
    "    transforms.RandomCrop(224),  # Randomly crop a 224x224 region from the padded image\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Define a transform for validation data\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to range [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CPU cores: 24\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of available CPU cores:\", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageNet-1k has 1,281,168 training images and 50,096 validation images!\n"
     ]
    }
   ],
   "source": [
    "# Load ImageNet1k dataset and make DataLoaders\n",
    "train_dataset1k = datasets.ImageFolder(root='/mnt/imagenet1k_resized/ILSVRC2012_img_train', transform=train_transform)\n",
    "val_dataset1k = datasets.ImageFolder(root='/mnt/imagenet1k_resized/ILSVRC2012_img_val', transform=val_transform)\n",
    "\n",
    "train_loader1k = DataLoader(dataset=train_dataset1k, batch_size=batch_size, shuffle=True, num_workers=20, pin_memory=True)\n",
    "val_loader1k = DataLoader(dataset=val_dataset1k, batch_size=batch_size, shuffle=False, num_workers=20, pin_memory=True)\n",
    "\n",
    "#Calculate total steps\n",
    "total_steps = len(train_loader1k) * num_epochs\n",
    "\n",
    "# StepLR that decays the learning rate every 30 epochs\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.9)\n",
    "\n",
    "print(f\"ImageNet-1k has {len(train_loader1k)*batch_size:,} training images and {len(val_loader1k)*batch_size:,} validation images!\")\n",
    "\n",
    "# Load ImageNet21k dataset and make DataLoaders\n",
    "#train_dataset21k = datasets.ImageFolder(root='/mnt/imagenet21k_resized_new/imagenet21k_train', transform=train_transform)\n",
    "#val_dataset21k = datasets.ImageFolder(root='/mnt/imagenet21k_resized_new/imagenet21k_val', transform=val_transform)\n",
    "\n",
    "#train_loader21k = DataLoader(dataset=train_dataset21k, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "#val_loader21k = DataLoader(dataset=val_dataset21k, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "#print(f\"ImageNet-21k has {len(train_loader21k)*batch_size:,} training images and {len(val_loader21k)*batch_size:,} validation images!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2583"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a batch from train_loader1k\n",
    "train_images, train_labels = next(iter(train_loader1k))\n",
    "train_images, train_labels = train_images.to(device), train_labels.to(device)\n",
    "\n",
    "print(\"Train images batch shape:\", train_images.shape)\n",
    "print(\"Train labels batch shape:\", train_labels.shape)\n",
    "print(\"Train images data type:\", train_images.dtype)\n",
    "print(\"Train labels data type:\", train_labels.dtype)\n",
    "\n",
    "# Inspect a batch from val_loader1k\n",
    "val_images, val_labels = next(iter(val_loader1k))\n",
    "\n",
    "print(\"Validation images batch shape:\", val_images.shape)\n",
    "print(\"Validation labels batch shape:\", val_labels.shape)\n",
    "print(\"Validation images data type:\", val_images.dtype)\n",
    "print(\"Validation labels data type:\", val_labels.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING COMPONENTS OF vit.py IN IPYNB BEFORE MOVING TO .PY FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Class for Image Preprocessing\n",
    "class ImagePreprocessor(nn.Module):\n",
    "    def __init__(self, patch_size):\n",
    "        super(ImagePreprocessor, self).__init__()\n",
    "        self.patch_size = patch_size  # Size of each patch\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Dynamically get the batch size and channel dimensions\n",
    "        batch_size, channel, _, _ = x.size()\n",
    "\n",
    "        # Using unfold to create patches\n",
    "        x_p = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "\n",
    "        # Reshape into the desired shape\n",
    "        x_p = x_p.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "        x_p = x_p.view(batch_size, -1, self.patch_size * self.patch_size * channel)\n",
    "\n",
    "        # Now x_p should have shape [batch_size, (Height * Width) / (patch_size * patch_size), (patch_size * patch_size * channel)]\n",
    "        \n",
    "        return x_p\n",
    "\n",
    "# Class for Patch Embedding\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_dim, D):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.D = D  # Dimension to project to\n",
    "        self.linear = nn.Linear(patch_dim, D)  # Linear projection layer\n",
    "\n",
    "    def forward(self, x_p):\n",
    "        # Project patches to D dimensions\n",
    "        x_emb = self.linear(x_p)\n",
    "        return x_emb\n",
    "\n",
    "# Class for adding a Class Token\n",
    "class ClassToken(nn.Module):\n",
    "    def __init__(self, D):\n",
    "        super(ClassToken, self).__init__()\n",
    "        self.class_token_embedding = nn.Parameter(torch.randn(1, 1, D))  # Learnable class token\n",
    "\n",
    "    def forward(self, x_emb):\n",
    "        # Prepend class token to patch embeddings\n",
    "        batch_size = x_emb.size(0)\n",
    "        class_token = self.class_token_embedding.repeat(batch_size, 1, 1)\n",
    "        x_class = torch.cat([class_token, x_emb], dim=1)\n",
    "        return x_class\n",
    "\n",
    "# Class for Position Embeddings\n",
    "class PositionEmbedding(nn.Module):\n",
    "    def __init__(self, seq_len, D):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(1, seq_len, D))  # Learnable position embeddings\n",
    "\n",
    "    def forward(self, x_class):\n",
    "        # Add position embeddings\n",
    "        x_pos = x_class + self.position_embeddings\n",
    "        return x_pos\n",
    "\n",
    "# Class for Transformer Encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, D, num_layers):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_norm = nn.LayerNorm(D)\n",
    "        self.multihead_attention = nn.MultiheadAttention(D, num_heads=4, batch_first=True)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(D, D),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(D, D)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_pos):\n",
    "        # Transformer Encoder Logic\n",
    "        for _ in range(self.num_layers):\n",
    "            x_norm = self.layer_norm(x_pos)\n",
    "            x_att, _ = self.multihead_attention(x_norm, x_norm, x_norm)\n",
    "            x_pos = x_pos + x_att\n",
    "            x_pos = x_pos + self.mlp(self.layer_norm(x_pos))\n",
    "        return x_pos\n",
    "\n",
    "# Class for Classification Head\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, D, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.linear = nn.Linear(D, num_classes)  # Linear layer for classification\n",
    "\n",
    "    def forward(self, x_transformed):\n",
    "        # Take the class token and perform classification\n",
    "        x_class_token = x_transformed[:, 0, :]\n",
    "        print(\"x_class_token.shape: \", x_class_token.shape)\n",
    "        output = self.linear(x_class_token)\n",
    "        return output\n",
    "\n",
    "# Main Vision Transformer Class\n",
    "class VisionTransformerTest(nn.Module):\n",
    "    def __init__(self, patch_size, D, num_layers, num_classes):\n",
    "        super(VisionTransformerTest, self).__init__()\n",
    "        self.image_preprocessor = ImagePreprocessor(patch_size)\n",
    "        self.patch_embedding = PatchEmbedding(patch_size * patch_size * 3, D)  # 3 channels, patch_size x patch_size patches\n",
    "        self.class_token = ClassToken(D)\n",
    "        self.position_embedding = PositionEmbedding(197, D)  # 196 patches + 1 class token\n",
    "        self.transformer_encoder = TransformerEncoder(D, num_layers)\n",
    "        self.classification_head = ClassificationHead(D, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"x.shape\",x.shape)\n",
    "        \n",
    "        # Preprocess the image into patches\n",
    "        x_p = self.image_preprocessor(x)\n",
    "        print(\"x_p.shape: \", x_p.shape)\n",
    "\n",
    "        # Generate patch embeddings\n",
    "        x_emb = self.patch_embedding(x_p)\n",
    "        print(\"x_emb.shape: \", x_emb.shape)\n",
    "\n",
    "        # Prepend the class token\n",
    "        x_class = self.class_token(x_emb)\n",
    "        print(\"x_class.shape: \", x_class.shape)\n",
    "\n",
    "        # Add position embeddings\n",
    "        x_pos = self.position_embedding(x_class)\n",
    "        print(\"x_pos.shape: \", x_pos.shape)\n",
    "\n",
    "        # Pass through the Transformer Encoder\n",
    "        x_transformed = self.transformer_encoder(x_pos)\n",
    "        print(\"x_transformed.shape: \", x_transformed.shape)\n",
    "    \n",
    "        # Perform classification\n",
    "        output = self.classification_head(x_transformed)\n",
    "        print(\"output.shape: \", output.shape)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = VisionTransformerTest(patch_size=patch_size_, D=D_, num_layers=num_layers_, num_classes=num_classes_)\n",
    "model_test.to(device)\n",
    "output = model_test(train_images)\n",
    "print(\"Above was the transformation path of the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING COMPONENTS OF vit.py IN IPYNB BEFORE MOVING TO .PY FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "learning_rates = []\n",
    "\n",
    "# Training Loop\n",
    "for epoch_idx in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (train_images, train_labels) in enumerate(train_loader1k):\n",
    "        train_images, train_labels = train_images.to(device), train_labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        train_outputs = model(train_images)\n",
    "\n",
    "        # Compute the loss\n",
    "        train_loss = criterion(train_outputs, train_labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Gradient Clip\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Step optimizer and the scheduler\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss.item())\n",
    "        learning_rates.append(scheduler.get_last_lr()[0])  # Assumes optimizer has a single param group\n",
    "\n",
    "        print(f\"Epoch [{epoch_idx+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader1k)}], Loss: {train_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Loop\n",
    "# NOTE: LOGITS TO MAX LOGIT FUNCTION MIGHT CHANGE DUE TO SPECIFIC NATURE OF VISION TRANSFORMER ALGORITHM\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for val_images, val_labels in val_loader1k:\n",
    "        val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "\n",
    "        # Logits\n",
    "        val_outputs = model(val_images)\n",
    "\n",
    "        # Let the index of the highest logit be the predicted class \n",
    "        _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "\n",
    "        # Update counts from this batch's values\n",
    "        total_count += val_labels.size(0)\n",
    "        correct_count += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "    # Print accuracy score\n",
    "    print(f'Accuracy of the model on the validation images: {100 * correct_count / total_count}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save(model, './models/vit-base-32p.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit21k_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
