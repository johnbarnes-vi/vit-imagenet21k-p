{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure you follow the preprocessing instructions in the README.md file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Lets see the directory structure of imagenet1k\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        jpeg_files = [f for f in files if f.endswith('.JPEG')]\n",
    "        if jpeg_files:  # if the list is not empty\n",
    "            print('{}Number of JPEG files: {}'.format(subindent, len(jpeg_files)))\n",
    "        for f in files:\n",
    "            if f.endswith('.txt'):\n",
    "                print('{}{}'.format(subindent, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet1k_resized/ILSVRC2012_img_train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet1k_resized/ILSVRC2012_img_val/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet21k_resized_new/imagenet21k_train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet21k_resized_new/imagenet21k_val/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is clear from the output of the above cells that preprocessing worked!\n",
    "\n",
    "We are looking to see if the validation and training sets are organized in the same manner and that they are ordered the same.\n",
    "\n",
    "This makes input into the `torchvision.datasets.ImageFolder` class work without a hitch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries to unzip `tiny-imagenet-200.zip`\n",
    "import zipfile\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "# Importing pytorch libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing custom VisionTransformer Model\n",
    "\n",
    "from models.vit import VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cuda')\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 144\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "patch_size_ = 16     # to be changed\n",
    "D_ = 1280            # to be changed\n",
    "num_layers_ = 12     # to be changed\n",
    "num_classes_ = 1000\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "model = VisionTransformer(patch_size=patch_size_, D=D_, num_layers=num_layers_, num_classes=num_classes_)\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "\n",
    "# Define a transform for training data\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Pad(4),  # Pad the image by 4 pixels\n",
    "    transforms.RandomCrop(224),  # Randomly crop a 224x224 region from the padded image\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Define a transform for validation data\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to range [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CPU cores: 24\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of available CPU cores:\", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageNet-1k has 1,281,168 training images and 50,112 validation images!\n"
     ]
    }
   ],
   "source": [
    "# Load ImageNet1k dataset and make DataLoaders\n",
    "train_dataset1k = datasets.ImageFolder(root='/mnt/imagenet1k_resized/ILSVRC2012_img_train', transform=train_transform)\n",
    "val_dataset1k = datasets.ImageFolder(root='/mnt/imagenet1k_resized/ILSVRC2012_img_val', transform=val_transform)\n",
    "\n",
    "train_loader1k = DataLoader(dataset=train_dataset1k, batch_size=batch_size, shuffle=True, num_workers=20, pin_memory=True)\n",
    "val_loader1k = DataLoader(dataset=val_dataset1k, batch_size=batch_size, shuffle=False, num_workers=20, pin_memory=True)\n",
    "\n",
    "#Calculate total steps\n",
    "total_steps = len(train_loader1k) * num_epochs\n",
    "\n",
    "# StepLR that decays the learning rate every 30 epochs\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.9)\n",
    "\n",
    "print(f\"ImageNet-1k has {len(train_loader1k)*batch_size:,} training images and {len(val_loader1k)*batch_size:,} validation images!\")\n",
    "\n",
    "# Load ImageNet21k dataset and make DataLoaders\n",
    "#train_dataset21k = datasets.ImageFolder(root='/mnt/imagenet21k_resized_new/imagenet21k_train', transform=train_transform)\n",
    "#val_dataset21k = datasets.ImageFolder(root='/mnt/imagenet21k_resized_new/imagenet21k_val', transform=val_transform)\n",
    "\n",
    "#train_loader21k = DataLoader(dataset=train_dataset21k, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "#val_loader21k = DataLoader(dataset=val_dataset21k, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "#print(f\"ImageNet-21k has {len(train_loader21k)*batch_size:,} training images and {len(val_loader21k)*batch_size:,} validation images!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8897"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images batch shape: torch.Size([144, 3, 224, 224])\n",
      "Train labels batch shape: torch.Size([144])\n",
      "Train images data type: torch.float32\n",
      "Train labels data type: torch.int64\n",
      "Validation images batch shape: torch.Size([144, 3, 224, 224])\n",
      "Validation labels batch shape: torch.Size([144])\n",
      "Validation images data type: torch.float32\n",
      "Validation labels data type: torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Inspect a batch from train_loader1k\n",
    "train_images, train_labels = next(iter(train_loader1k))\n",
    "train_images, train_labels = train_images.to(device), train_labels.to(device)\n",
    "\n",
    "print(\"Train images batch shape:\", train_images.shape)\n",
    "print(\"Train labels batch shape:\", train_labels.shape)\n",
    "print(\"Train images data type:\", train_images.dtype)\n",
    "print(\"Train labels data type:\", train_labels.dtype)\n",
    "\n",
    "# Inspect a batch from val_loader1k\n",
    "val_images, val_labels = next(iter(val_loader1k))\n",
    "\n",
    "print(\"Validation images batch shape:\", val_images.shape)\n",
    "print(\"Validation labels batch shape:\", val_labels.shape)\n",
    "print(\"Validation images data type:\", val_images.dtype)\n",
    "print(\"Validation labels data type:\", val_labels.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING COMPONENTS OF vit.py IN IPYNB BEFORE MOVING TO .PY FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Class for Image Preprocessing\n",
    "class ImagePreprocessor(nn.Module):\n",
    "    def __init__(self, patch_size):\n",
    "        super(ImagePreprocessor, self).__init__()\n",
    "        self.patch_size = patch_size  # Size of each patch\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Dynamically get the batch size and channel dimensions\n",
    "        batch_size, channel, _, _ = x.size()\n",
    "\n",
    "        # Using unfold to create patches\n",
    "        x_p = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "\n",
    "        # Reshape into the desired shape\n",
    "        x_p = x_p.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "        x_p = x_p.view(batch_size, -1, self.patch_size * self.patch_size * channel)\n",
    "\n",
    "        # Now x_p should have shape [batch_size, (Height * Width) / (patch_size * patch_size), (patch_size * patch_size * channel)]\n",
    "        \n",
    "        return x_p\n",
    "\n",
    "# Class for Patch Embedding\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_dim, D):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.D = D  # Dimension to project to\n",
    "        self.linear = nn.Linear(patch_dim, D)  # Linear projection layer\n",
    "\n",
    "    def forward(self, x_p):\n",
    "        # Project patches to D dimensions\n",
    "        x_emb = self.linear(x_p)\n",
    "        return x_emb\n",
    "\n",
    "# Class for adding a Class Token\n",
    "class ClassToken(nn.Module):\n",
    "    def __init__(self, D):\n",
    "        super(ClassToken, self).__init__()\n",
    "        self.class_token_embedding = nn.Parameter(torch.randn(1, 1, D))  # Learnable class token\n",
    "\n",
    "    def forward(self, x_emb):\n",
    "        # Prepend class token to patch embeddings\n",
    "        batch_size = x_emb.size(0)\n",
    "        class_token = self.class_token_embedding.repeat(batch_size, 1, 1)\n",
    "        x_class = torch.cat([class_token, x_emb], dim=1)\n",
    "        return x_class\n",
    "\n",
    "# Class for Position Embeddings\n",
    "class PositionEmbedding(nn.Module):\n",
    "    def __init__(self, seq_len, D):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(1, seq_len, D))  # Learnable position embeddings\n",
    "\n",
    "    def forward(self, x_class):\n",
    "        # Add position embeddings\n",
    "        x_pos = x_class + self.position_embeddings\n",
    "        return x_pos\n",
    "\n",
    "# Class for Transformer Encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, D, num_layers):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_norm = nn.LayerNorm(D)\n",
    "        self.multihead_attention = nn.MultiheadAttention(D, num_heads=4, batch_first=True)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(D, D),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(D, D)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_pos):\n",
    "        # Transformer Encoder Logic\n",
    "        for _ in range(self.num_layers):\n",
    "            x_norm = self.layer_norm(x_pos)\n",
    "            x_att, _ = self.multihead_attention(x_norm, x_norm, x_norm)\n",
    "            x_pos = x_pos + x_att\n",
    "            x_pos = x_pos + self.mlp(self.layer_norm(x_pos))\n",
    "        return x_pos\n",
    "\n",
    "# Class for Classification Head\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, D, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.linear = nn.Linear(D, num_classes)  # Linear layer for classification\n",
    "\n",
    "    def forward(self, x_transformed):\n",
    "        # Take the class token and perform classification\n",
    "        x_class_token = x_transformed[:, 0, :]\n",
    "        print(\"x_class_token.shape: \", x_class_token.shape)\n",
    "        output = self.linear(x_class_token)\n",
    "        return output\n",
    "\n",
    "# Main Vision Transformer Class\n",
    "class VisionTransformerTest(nn.Module):\n",
    "    def __init__(self, patch_size, D, num_layers, num_classes):\n",
    "        super(VisionTransformerTest, self).__init__()\n",
    "        self.image_preprocessor = ImagePreprocessor(patch_size)\n",
    "        self.patch_embedding = PatchEmbedding(patch_size * patch_size * 3, D)  # 3 channels, patch_size x patch_size patches\n",
    "        self.class_token = ClassToken(D)\n",
    "        self.position_embedding = PositionEmbedding(197, D)  # 196 patches + 1 class token\n",
    "        self.transformer_encoder = TransformerEncoder(D, num_layers)\n",
    "        self.classification_head = ClassificationHead(D, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"x.shape\",x.shape)\n",
    "        \n",
    "        # Preprocess the image into patches\n",
    "        x_p = self.image_preprocessor(x)\n",
    "        print(\"x_p.shape: \", x_p.shape)\n",
    "\n",
    "        # Generate patch embeddings\n",
    "        x_emb = self.patch_embedding(x_p)\n",
    "        print(\"x_emb.shape: \", x_emb.shape)\n",
    "\n",
    "        # Prepend the class token\n",
    "        x_class = self.class_token(x_emb)\n",
    "        print(\"x_class.shape: \", x_class.shape)\n",
    "\n",
    "        # Add position embeddings\n",
    "        x_pos = self.position_embedding(x_class)\n",
    "        print(\"x_pos.shape: \", x_pos.shape)\n",
    "\n",
    "        # Pass through the Transformer Encoder\n",
    "        x_transformed = self.transformer_encoder(x_pos)\n",
    "        print(\"x_transformed.shape: \", x_transformed.shape)\n",
    "    \n",
    "        # Perform classification\n",
    "        output = self.classification_head(x_transformed)\n",
    "        print(\"output.shape: \", output.shape)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = VisionTransformerTest(patch_size=patch_size_, D=D_, num_layers=num_layers_, num_classes=num_classes_)\n",
    "model_test.to(device)\n",
    "output = model_test(train_images)\n",
    "print(\"Above was the transformation path of the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING COMPONENTS OF vit.py IN IPYNB BEFORE MOVING TO .PY FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1/8897], Loss: 5.5840\n",
      "Epoch [1/1], Step [2/8897], Loss: 5.6454\n",
      "Epoch [1/1], Step [3/8897], Loss: 5.5074\n",
      "Epoch [1/1], Step [4/8897], Loss: 5.6518\n",
      "Epoch [1/1], Step [5/8897], Loss: 5.4641\n",
      "Epoch [1/1], Step [6/8897], Loss: 5.7026\n",
      "Epoch [1/1], Step [7/8897], Loss: 5.4240\n",
      "Epoch [1/1], Step [8/8897], Loss: 5.2112\n",
      "Epoch [1/1], Step [9/8897], Loss: 5.3378\n",
      "Epoch [1/1], Step [10/8897], Loss: 5.6220\n",
      "Epoch [1/1], Step [11/8897], Loss: 5.3727\n",
      "Epoch [1/1], Step [12/8897], Loss: 5.6748\n",
      "Epoch [1/1], Step [13/8897], Loss: 5.5414\n",
      "Epoch [1/1], Step [14/8897], Loss: 5.6996\n",
      "Epoch [1/1], Step [15/8897], Loss: 5.3948\n",
      "Epoch [1/1], Step [16/8897], Loss: 5.6288\n",
      "Epoch [1/1], Step [17/8897], Loss: 5.4083\n",
      "Epoch [1/1], Step [18/8897], Loss: 5.6328\n",
      "Epoch [1/1], Step [19/8897], Loss: 5.3904\n",
      "Epoch [1/1], Step [20/8897], Loss: 5.5149\n",
      "Epoch [1/1], Step [21/8897], Loss: 5.2195\n",
      "Epoch [1/1], Step [22/8897], Loss: 5.6249\n",
      "Epoch [1/1], Step [23/8897], Loss: 5.3603\n",
      "Epoch [1/1], Step [24/8897], Loss: 5.2377\n",
      "Epoch [1/1], Step [25/8897], Loss: 5.7307\n",
      "Epoch [1/1], Step [26/8897], Loss: 5.6794\n",
      "Epoch [1/1], Step [27/8897], Loss: 5.4873\n",
      "Epoch [1/1], Step [28/8897], Loss: 5.6095\n",
      "Epoch [1/1], Step [29/8897], Loss: 5.5209\n",
      "Epoch [1/1], Step [30/8897], Loss: 5.7212\n",
      "Epoch [1/1], Step [31/8897], Loss: 5.5879\n",
      "Epoch [1/1], Step [32/8897], Loss: 5.4970\n",
      "Epoch [1/1], Step [33/8897], Loss: 5.6102\n",
      "Epoch [1/1], Step [34/8897], Loss: 5.4743\n",
      "Epoch [1/1], Step [35/8897], Loss: 5.3775\n",
      "Epoch [1/1], Step [36/8897], Loss: 5.5905\n",
      "Epoch [1/1], Step [37/8897], Loss: 5.3099\n",
      "Epoch [1/1], Step [38/8897], Loss: 5.4406\n",
      "Epoch [1/1], Step [39/8897], Loss: 5.6411\n",
      "Epoch [1/1], Step [40/8897], Loss: 5.4120\n",
      "Epoch [1/1], Step [41/8897], Loss: 5.2404\n",
      "Epoch [1/1], Step [42/8897], Loss: 5.4178\n",
      "Epoch [1/1], Step [43/8897], Loss: 5.3439\n",
      "Epoch [1/1], Step [44/8897], Loss: 5.5684\n",
      "Epoch [1/1], Step [45/8897], Loss: 5.5886\n",
      "Epoch [1/1], Step [46/8897], Loss: 5.2090\n",
      "Epoch [1/1], Step [47/8897], Loss: 5.3843\n",
      "Epoch [1/1], Step [48/8897], Loss: 5.3330\n",
      "Epoch [1/1], Step [49/8897], Loss: 5.5939\n",
      "Epoch [1/1], Step [50/8897], Loss: 5.5384\n",
      "Epoch [1/1], Step [51/8897], Loss: 5.6827\n",
      "Epoch [1/1], Step [52/8897], Loss: 5.5457\n",
      "Epoch [1/1], Step [53/8897], Loss: 5.5891\n",
      "Epoch [1/1], Step [54/8897], Loss: 5.5191\n",
      "Epoch [1/1], Step [55/8897], Loss: 5.5416\n",
      "Epoch [1/1], Step [56/8897], Loss: 5.4268\n",
      "Epoch [1/1], Step [57/8897], Loss: 5.5076\n",
      "Epoch [1/1], Step [58/8897], Loss: 5.5568\n",
      "Epoch [1/1], Step [59/8897], Loss: 5.5470\n",
      "Epoch [1/1], Step [60/8897], Loss: 5.3722\n",
      "Epoch [1/1], Step [61/8897], Loss: 5.8270\n",
      "Epoch [1/1], Step [62/8897], Loss: 5.5672\n",
      "Epoch [1/1], Step [63/8897], Loss: 5.3507\n",
      "Epoch [1/1], Step [64/8897], Loss: 5.4447\n",
      "Epoch [1/1], Step [65/8897], Loss: 5.5401\n",
      "Epoch [1/1], Step [66/8897], Loss: 5.3336\n",
      "Epoch [1/1], Step [67/8897], Loss: 5.6516\n",
      "Epoch [1/1], Step [68/8897], Loss: 5.6154\n",
      "Epoch [1/1], Step [69/8897], Loss: 5.4860\n",
      "Epoch [1/1], Step [70/8897], Loss: 5.5320\n",
      "Epoch [1/1], Step [71/8897], Loss: 5.4568\n",
      "Epoch [1/1], Step [72/8897], Loss: 5.5081\n",
      "Epoch [1/1], Step [73/8897], Loss: 5.2355\n",
      "Epoch [1/1], Step [74/8897], Loss: 5.4267\n",
      "Epoch [1/1], Step [75/8897], Loss: 5.4133\n",
      "Epoch [1/1], Step [76/8897], Loss: 5.3240\n",
      "Epoch [1/1], Step [77/8897], Loss: 5.4990\n",
      "Epoch [1/1], Step [78/8897], Loss: 5.5187\n",
      "Epoch [1/1], Step [79/8897], Loss: 5.7076\n",
      "Epoch [1/1], Step [80/8897], Loss: 5.5346\n",
      "Epoch [1/1], Step [81/8897], Loss: 5.3120\n",
      "Epoch [1/1], Step [82/8897], Loss: 5.6191\n",
      "Epoch [1/1], Step [83/8897], Loss: 5.6876\n",
      "Epoch [1/1], Step [84/8897], Loss: 5.6446\n",
      "Epoch [1/1], Step [85/8897], Loss: 5.5182\n",
      "Epoch [1/1], Step [86/8897], Loss: 5.7953\n",
      "Epoch [1/1], Step [87/8897], Loss: 5.4772\n",
      "Epoch [1/1], Step [88/8897], Loss: 5.3774\n",
      "Epoch [1/1], Step [89/8897], Loss: 5.3316\n",
      "Epoch [1/1], Step [90/8897], Loss: 5.3247\n",
      "Epoch [1/1], Step [91/8897], Loss: 5.2722\n",
      "Epoch [1/1], Step [92/8897], Loss: 5.6014\n",
      "Epoch [1/1], Step [93/8897], Loss: 5.3117\n",
      "Epoch [1/1], Step [94/8897], Loss: 5.7503\n",
      "Epoch [1/1], Step [95/8897], Loss: 5.4402\n",
      "Epoch [1/1], Step [96/8897], Loss: 5.5943\n",
      "Epoch [1/1], Step [97/8897], Loss: 5.5739\n",
      "Epoch [1/1], Step [98/8897], Loss: 5.1836\n",
      "Epoch [1/1], Step [99/8897], Loss: 5.4994\n",
      "Epoch [1/1], Step [100/8897], Loss: 5.3156\n",
      "Epoch [1/1], Step [101/8897], Loss: 5.2243\n",
      "Epoch [1/1], Step [102/8897], Loss: 5.3997\n",
      "Epoch [1/1], Step [103/8897], Loss: 5.4814\n",
      "Epoch [1/1], Step [104/8897], Loss: 5.5312\n",
      "Epoch [1/1], Step [105/8897], Loss: 5.4153\n",
      "Epoch [1/1], Step [106/8897], Loss: 5.5312\n",
      "Epoch [1/1], Step [107/8897], Loss: 5.3810\n",
      "Epoch [1/1], Step [108/8897], Loss: 5.4316\n",
      "Epoch [1/1], Step [109/8897], Loss: 5.4599\n",
      "Epoch [1/1], Step [110/8897], Loss: 5.3682\n",
      "Epoch [1/1], Step [111/8897], Loss: 5.6198\n",
      "Epoch [1/1], Step [112/8897], Loss: 5.1696\n",
      "Epoch [1/1], Step [113/8897], Loss: 5.5701\n",
      "Epoch [1/1], Step [114/8897], Loss: 5.4637\n",
      "Epoch [1/1], Step [115/8897], Loss: 5.5119\n",
      "Epoch [1/1], Step [116/8897], Loss: 5.2686\n",
      "Epoch [1/1], Step [117/8897], Loss: 5.3371\n",
      "Epoch [1/1], Step [118/8897], Loss: 5.2341\n",
      "Epoch [1/1], Step [119/8897], Loss: 5.3597\n",
      "Epoch [1/1], Step [120/8897], Loss: 5.7982\n",
      "Epoch [1/1], Step [121/8897], Loss: 5.4951\n",
      "Epoch [1/1], Step [122/8897], Loss: 5.6352\n",
      "Epoch [1/1], Step [123/8897], Loss: 5.4190\n",
      "Epoch [1/1], Step [124/8897], Loss: 5.3156\n",
      "Epoch [1/1], Step [125/8897], Loss: 5.2765\n",
      "Epoch [1/1], Step [126/8897], Loss: 5.7493\n",
      "Epoch [1/1], Step [127/8897], Loss: 5.4639\n",
      "Epoch [1/1], Step [128/8897], Loss: 5.6158\n",
      "Epoch [1/1], Step [129/8897], Loss: 5.2973\n",
      "Epoch [1/1], Step [130/8897], Loss: 5.4169\n",
      "Epoch [1/1], Step [131/8897], Loss: 5.4525\n",
      "Epoch [1/1], Step [132/8897], Loss: 5.3697\n",
      "Epoch [1/1], Step [133/8897], Loss: 5.4510\n",
      "Epoch [1/1], Step [134/8897], Loss: 5.5543\n",
      "Epoch [1/1], Step [135/8897], Loss: 5.5721\n",
      "Epoch [1/1], Step [136/8897], Loss: 5.8521\n",
      "Epoch [1/1], Step [137/8897], Loss: 5.4135\n",
      "Epoch [1/1], Step [138/8897], Loss: 5.5534\n",
      "Epoch [1/1], Step [139/8897], Loss: 5.6566\n",
      "Epoch [1/1], Step [140/8897], Loss: 5.2738\n",
      "Epoch [1/1], Step [141/8897], Loss: 5.5293\n",
      "Epoch [1/1], Step [142/8897], Loss: 5.6730\n",
      "Epoch [1/1], Step [143/8897], Loss: 5.3989\n",
      "Epoch [1/1], Step [144/8897], Loss: 5.5965\n",
      "Epoch [1/1], Step [145/8897], Loss: 5.4580\n",
      "Epoch [1/1], Step [146/8897], Loss: 5.4465\n",
      "Epoch [1/1], Step [147/8897], Loss: 5.4317\n",
      "Epoch [1/1], Step [148/8897], Loss: 5.7113\n",
      "Epoch [1/1], Step [149/8897], Loss: 5.5835\n",
      "Epoch [1/1], Step [150/8897], Loss: 5.2812\n",
      "Epoch [1/1], Step [151/8897], Loss: 5.4768\n",
      "Epoch [1/1], Step [152/8897], Loss: 5.5744\n",
      "Epoch [1/1], Step [153/8897], Loss: 5.3236\n",
      "Epoch [1/1], Step [154/8897], Loss: 5.6454\n",
      "Epoch [1/1], Step [155/8897], Loss: 5.4929\n",
      "Epoch [1/1], Step [156/8897], Loss: 5.5093\n",
      "Epoch [1/1], Step [157/8897], Loss: 5.3632\n",
      "Epoch [1/1], Step [158/8897], Loss: 5.4918\n",
      "Epoch [1/1], Step [159/8897], Loss: 5.6598\n",
      "Epoch [1/1], Step [160/8897], Loss: 5.3556\n",
      "Epoch [1/1], Step [161/8897], Loss: 5.5868\n",
      "Epoch [1/1], Step [162/8897], Loss: 5.3965\n",
      "Epoch [1/1], Step [163/8897], Loss: 5.5402\n",
      "Epoch [1/1], Step [164/8897], Loss: 5.3952\n",
      "Epoch [1/1], Step [165/8897], Loss: 5.3844\n",
      "Epoch [1/1], Step [166/8897], Loss: 5.1275\n",
      "Epoch [1/1], Step [167/8897], Loss: 5.5179\n",
      "Epoch [1/1], Step [168/8897], Loss: 5.3852\n",
      "Epoch [1/1], Step [169/8897], Loss: 5.6524\n",
      "Epoch [1/1], Step [170/8897], Loss: 5.5772\n",
      "Epoch [1/1], Step [171/8897], Loss: 5.5425\n",
      "Epoch [1/1], Step [172/8897], Loss: 5.6221\n",
      "Epoch [1/1], Step [173/8897], Loss: 5.4590\n",
      "Epoch [1/1], Step [174/8897], Loss: 5.4395\n",
      "Epoch [1/1], Step [175/8897], Loss: 5.3267\n",
      "Epoch [1/1], Step [176/8897], Loss: 5.4135\n",
      "Epoch [1/1], Step [177/8897], Loss: 5.3342\n",
      "Epoch [1/1], Step [178/8897], Loss: 5.5442\n",
      "Epoch [1/1], Step [179/8897], Loss: 5.2322\n",
      "Epoch [1/1], Step [180/8897], Loss: 5.3505\n",
      "Epoch [1/1], Step [181/8897], Loss: 5.5270\n",
      "Epoch [1/1], Step [182/8897], Loss: 5.6417\n",
      "Epoch [1/1], Step [183/8897], Loss: 5.4874\n",
      "Epoch [1/1], Step [184/8897], Loss: 5.3793\n",
      "Epoch [1/1], Step [185/8897], Loss: 5.2872\n",
      "Epoch [1/1], Step [186/8897], Loss: 5.6135\n",
      "Epoch [1/1], Step [187/8897], Loss: 5.5299\n",
      "Epoch [1/1], Step [188/8897], Loss: 5.5038\n",
      "Epoch [1/1], Step [189/8897], Loss: 5.4377\n",
      "Epoch [1/1], Step [190/8897], Loss: 5.3763\n",
      "Epoch [1/1], Step [191/8897], Loss: 5.4951\n",
      "Epoch [1/1], Step [192/8897], Loss: 5.5413\n",
      "Epoch [1/1], Step [193/8897], Loss: 5.4230\n",
      "Epoch [1/1], Step [194/8897], Loss: 5.6328\n",
      "Epoch [1/1], Step [195/8897], Loss: 5.7124\n",
      "Epoch [1/1], Step [196/8897], Loss: 5.4512\n",
      "Epoch [1/1], Step [197/8897], Loss: 5.3574\n",
      "Epoch [1/1], Step [198/8897], Loss: 5.3546\n",
      "Epoch [1/1], Step [199/8897], Loss: 5.1821\n",
      "Epoch [1/1], Step [200/8897], Loss: 5.3589\n",
      "Epoch [1/1], Step [201/8897], Loss: 5.5320\n",
      "Epoch [1/1], Step [202/8897], Loss: 5.4896\n",
      "Epoch [1/1], Step [203/8897], Loss: 5.5863\n",
      "Epoch [1/1], Step [204/8897], Loss: 5.6390\n",
      "Epoch [1/1], Step [205/8897], Loss: 5.4349\n",
      "Epoch [1/1], Step [206/8897], Loss: 5.5446\n",
      "Epoch [1/1], Step [207/8897], Loss: 5.4643\n",
      "Epoch [1/1], Step [208/8897], Loss: 5.4450\n",
      "Epoch [1/1], Step [209/8897], Loss: 5.6285\n",
      "Epoch [1/1], Step [210/8897], Loss: 5.4875\n",
      "Epoch [1/1], Step [211/8897], Loss: 5.6111\n",
      "Epoch [1/1], Step [212/8897], Loss: 5.1990\n",
      "Epoch [1/1], Step [213/8897], Loss: 5.5076\n",
      "Epoch [1/1], Step [214/8897], Loss: 5.5768\n",
      "Epoch [1/1], Step [215/8897], Loss: 5.6868\n",
      "Epoch [1/1], Step [216/8897], Loss: 5.5810\n",
      "Epoch [1/1], Step [217/8897], Loss: 5.3334\n",
      "Epoch [1/1], Step [218/8897], Loss: 5.4824\n",
      "Epoch [1/1], Step [219/8897], Loss: 5.6806\n",
      "Epoch [1/1], Step [220/8897], Loss: 5.1516\n",
      "Epoch [1/1], Step [221/8897], Loss: 5.4889\n",
      "Epoch [1/1], Step [222/8897], Loss: 5.5313\n",
      "Epoch [1/1], Step [223/8897], Loss: 5.3876\n",
      "Epoch [1/1], Step [224/8897], Loss: 5.5067\n",
      "Epoch [1/1], Step [225/8897], Loss: 5.5181\n",
      "Epoch [1/1], Step [226/8897], Loss: 5.3175\n",
      "Epoch [1/1], Step [227/8897], Loss: 5.3137\n",
      "Epoch [1/1], Step [228/8897], Loss: 5.4179\n",
      "Epoch [1/1], Step [229/8897], Loss: 5.6161\n",
      "Epoch [1/1], Step [230/8897], Loss: 5.4982\n",
      "Epoch [1/1], Step [231/8897], Loss: 5.8299\n",
      "Epoch [1/1], Step [232/8897], Loss: 5.4241\n",
      "Epoch [1/1], Step [233/8897], Loss: 5.5673\n",
      "Epoch [1/1], Step [234/8897], Loss: 5.3016\n",
      "Epoch [1/1], Step [235/8897], Loss: 5.3825\n",
      "Epoch [1/1], Step [236/8897], Loss: 5.6460\n",
      "Epoch [1/1], Step [237/8897], Loss: 5.3014\n",
      "Epoch [1/1], Step [238/8897], Loss: 5.5904\n",
      "Epoch [1/1], Step [239/8897], Loss: 5.5762\n",
      "Epoch [1/1], Step [240/8897], Loss: 5.1651\n",
      "Epoch [1/1], Step [241/8897], Loss: 5.1757\n",
      "Epoch [1/1], Step [242/8897], Loss: 5.2274\n",
      "Epoch [1/1], Step [243/8897], Loss: 5.3132\n",
      "Epoch [1/1], Step [244/8897], Loss: 5.3959\n",
      "Epoch [1/1], Step [245/8897], Loss: 5.1072\n",
      "Epoch [1/1], Step [246/8897], Loss: 5.4432\n",
      "Epoch [1/1], Step [247/8897], Loss: 5.4664\n",
      "Epoch [1/1], Step [248/8897], Loss: 5.7827\n",
      "Epoch [1/1], Step [249/8897], Loss: 5.2923\n",
      "Epoch [1/1], Step [250/8897], Loss: 5.6967\n",
      "Epoch [1/1], Step [251/8897], Loss: 5.5813\n",
      "Epoch [1/1], Step [252/8897], Loss: 5.3246\n",
      "Epoch [1/1], Step [253/8897], Loss: 5.5133\n",
      "Epoch [1/1], Step [254/8897], Loss: 5.1027\n",
      "Epoch [1/1], Step [255/8897], Loss: 5.3294\n",
      "Epoch [1/1], Step [256/8897], Loss: 5.1474\n",
      "Epoch [1/1], Step [257/8897], Loss: 5.2993\n",
      "Epoch [1/1], Step [258/8897], Loss: 5.4576\n",
      "Epoch [1/1], Step [259/8897], Loss: 5.4019\n",
      "Epoch [1/1], Step [260/8897], Loss: 5.5208\n",
      "Epoch [1/1], Step [261/8897], Loss: 5.5182\n",
      "Epoch [1/1], Step [262/8897], Loss: 5.9390\n",
      "Epoch [1/1], Step [263/8897], Loss: 5.3390\n",
      "Epoch [1/1], Step [264/8897], Loss: 5.5060\n",
      "Epoch [1/1], Step [265/8897], Loss: 5.2915\n",
      "Epoch [1/1], Step [266/8897], Loss: 5.5222\n",
      "Epoch [1/1], Step [267/8897], Loss: 5.5970\n",
      "Epoch [1/1], Step [268/8897], Loss: 5.4897\n",
      "Epoch [1/1], Step [269/8897], Loss: 5.3675\n",
      "Epoch [1/1], Step [270/8897], Loss: 5.3047\n",
      "Epoch [1/1], Step [271/8897], Loss: 5.5207\n",
      "Epoch [1/1], Step [272/8897], Loss: 5.4973\n",
      "Epoch [1/1], Step [273/8897], Loss: 5.6235\n",
      "Epoch [1/1], Step [274/8897], Loss: 5.4736\n",
      "Epoch [1/1], Step [275/8897], Loss: 5.4760\n",
      "Epoch [1/1], Step [276/8897], Loss: 5.5329\n",
      "Epoch [1/1], Step [277/8897], Loss: 5.4055\n",
      "Epoch [1/1], Step [278/8897], Loss: 5.5133\n",
      "Epoch [1/1], Step [279/8897], Loss: 5.5842\n",
      "Epoch [1/1], Step [280/8897], Loss: 5.5234\n",
      "Epoch [1/1], Step [281/8897], Loss: 5.5037\n",
      "Epoch [1/1], Step [282/8897], Loss: 5.4565\n",
      "Epoch [1/1], Step [283/8897], Loss: 5.2786\n",
      "Epoch [1/1], Step [284/8897], Loss: 5.5490\n",
      "Epoch [1/1], Step [285/8897], Loss: 5.4049\n",
      "Epoch [1/1], Step [286/8897], Loss: 5.5602\n",
      "Epoch [1/1], Step [287/8897], Loss: 5.2820\n",
      "Epoch [1/1], Step [288/8897], Loss: 5.5278\n",
      "Epoch [1/1], Step [289/8897], Loss: 5.6616\n",
      "Epoch [1/1], Step [290/8897], Loss: 5.3764\n",
      "Epoch [1/1], Step [291/8897], Loss: 5.6427\n",
      "Epoch [1/1], Step [292/8897], Loss: 5.1160\n",
      "Epoch [1/1], Step [293/8897], Loss: 5.7775\n",
      "Epoch [1/1], Step [294/8897], Loss: 5.6910\n",
      "Epoch [1/1], Step [295/8897], Loss: 5.4749\n",
      "Epoch [1/1], Step [296/8897], Loss: 5.5940\n",
      "Epoch [1/1], Step [297/8897], Loss: 5.3756\n",
      "Epoch [1/1], Step [298/8897], Loss: 5.5857\n",
      "Epoch [1/1], Step [299/8897], Loss: 5.5702\n",
      "Epoch [1/1], Step [300/8897], Loss: 5.3703\n",
      "Epoch [1/1], Step [301/8897], Loss: 5.4459\n",
      "Epoch [1/1], Step [302/8897], Loss: 5.7874\n",
      "Epoch [1/1], Step [303/8897], Loss: 5.5101\n",
      "Epoch [1/1], Step [304/8897], Loss: 5.2511\n",
      "Epoch [1/1], Step [305/8897], Loss: 5.6316\n",
      "Epoch [1/1], Step [306/8897], Loss: 5.3106\n",
      "Epoch [1/1], Step [307/8897], Loss: 5.5075\n",
      "Epoch [1/1], Step [308/8897], Loss: 5.2606\n",
      "Epoch [1/1], Step [309/8897], Loss: 5.4455\n",
      "Epoch [1/1], Step [310/8897], Loss: 5.3697\n",
      "Epoch [1/1], Step [311/8897], Loss: 5.3796\n",
      "Epoch [1/1], Step [312/8897], Loss: 5.4470\n",
      "Epoch [1/1], Step [313/8897], Loss: 5.6714\n",
      "Epoch [1/1], Step [314/8897], Loss: 5.4493\n",
      "Epoch [1/1], Step [315/8897], Loss: 5.2742\n",
      "Epoch [1/1], Step [316/8897], Loss: 5.3673\n",
      "Epoch [1/1], Step [317/8897], Loss: 5.3047\n",
      "Epoch [1/1], Step [318/8897], Loss: 5.3295\n",
      "Epoch [1/1], Step [319/8897], Loss: 5.4063\n",
      "Epoch [1/1], Step [320/8897], Loss: 5.3083\n",
      "Epoch [1/1], Step [321/8897], Loss: 5.5991\n",
      "Epoch [1/1], Step [322/8897], Loss: 5.5926\n",
      "Epoch [1/1], Step [323/8897], Loss: 5.4953\n",
      "Epoch [1/1], Step [324/8897], Loss: 5.6487\n",
      "Epoch [1/1], Step [325/8897], Loss: 5.4874\n",
      "Epoch [1/1], Step [326/8897], Loss: 5.4751\n",
      "Epoch [1/1], Step [327/8897], Loss: 5.5270\n",
      "Epoch [1/1], Step [328/8897], Loss: 5.3061\n",
      "Epoch [1/1], Step [329/8897], Loss: 5.5682\n",
      "Epoch [1/1], Step [330/8897], Loss: 5.5109\n",
      "Epoch [1/1], Step [331/8897], Loss: 5.3129\n",
      "Epoch [1/1], Step [332/8897], Loss: 5.5943\n",
      "Epoch [1/1], Step [333/8897], Loss: 5.4859\n",
      "Epoch [1/1], Step [334/8897], Loss: 5.5165\n",
      "Epoch [1/1], Step [335/8897], Loss: 5.7739\n",
      "Epoch [1/1], Step [336/8897], Loss: 5.5407\n",
      "Epoch [1/1], Step [337/8897], Loss: 5.6156\n",
      "Epoch [1/1], Step [338/8897], Loss: 5.3964\n",
      "Epoch [1/1], Step [339/8897], Loss: 5.5038\n",
      "Epoch [1/1], Step [340/8897], Loss: 5.6243\n",
      "Epoch [1/1], Step [341/8897], Loss: 5.3125\n",
      "Epoch [1/1], Step [342/8897], Loss: 5.3972\n",
      "Epoch [1/1], Step [343/8897], Loss: 5.6079\n",
      "Epoch [1/1], Step [344/8897], Loss: 5.5098\n",
      "Epoch [1/1], Step [345/8897], Loss: 5.6310\n",
      "Epoch [1/1], Step [346/8897], Loss: 5.5835\n",
      "Epoch [1/1], Step [347/8897], Loss: 5.4367\n",
      "Epoch [1/1], Step [348/8897], Loss: 5.4986\n",
      "Epoch [1/1], Step [349/8897], Loss: 5.5817\n",
      "Epoch [1/1], Step [350/8897], Loss: 5.5415\n",
      "Epoch [1/1], Step [351/8897], Loss: 5.3540\n",
      "Epoch [1/1], Step [352/8897], Loss: 5.4457\n",
      "Epoch [1/1], Step [353/8897], Loss: 5.5929\n",
      "Epoch [1/1], Step [354/8897], Loss: 5.2818\n",
      "Epoch [1/1], Step [355/8897], Loss: 5.5775\n",
      "Epoch [1/1], Step [356/8897], Loss: 5.4345\n",
      "Epoch [1/1], Step [357/8897], Loss: 5.5058\n",
      "Epoch [1/1], Step [358/8897], Loss: 5.5157\n",
      "Epoch [1/1], Step [359/8897], Loss: 5.5028\n",
      "Epoch [1/1], Step [360/8897], Loss: 5.5004\n",
      "Epoch [1/1], Step [361/8897], Loss: 5.5775\n",
      "Epoch [1/1], Step [362/8897], Loss: 5.4869\n",
      "Epoch [1/1], Step [363/8897], Loss: 5.9155\n",
      "Epoch [1/1], Step [364/8897], Loss: 5.5015\n",
      "Epoch [1/1], Step [365/8897], Loss: 5.3339\n",
      "Epoch [1/1], Step [366/8897], Loss: 5.4674\n",
      "Epoch [1/1], Step [367/8897], Loss: 5.3185\n",
      "Epoch [1/1], Step [368/8897], Loss: 5.5610\n",
      "Epoch [1/1], Step [369/8897], Loss: 5.2825\n",
      "Epoch [1/1], Step [370/8897], Loss: 5.3428\n",
      "Epoch [1/1], Step [371/8897], Loss: 5.6679\n",
      "Epoch [1/1], Step [372/8897], Loss: 5.4418\n",
      "Epoch [1/1], Step [373/8897], Loss: 5.7514\n",
      "Epoch [1/1], Step [374/8897], Loss: 5.2881\n",
      "Epoch [1/1], Step [375/8897], Loss: 5.2884\n",
      "Epoch [1/1], Step [376/8897], Loss: 5.4898\n",
      "Epoch [1/1], Step [377/8897], Loss: 5.3117\n",
      "Epoch [1/1], Step [378/8897], Loss: 5.3655\n",
      "Epoch [1/1], Step [379/8897], Loss: 5.5641\n",
      "Epoch [1/1], Step [380/8897], Loss: 5.4712\n",
      "Epoch [1/1], Step [381/8897], Loss: 5.8005\n",
      "Epoch [1/1], Step [382/8897], Loss: 5.4630\n",
      "Epoch [1/1], Step [383/8897], Loss: 5.5586\n",
      "Epoch [1/1], Step [384/8897], Loss: 5.5131\n",
      "Epoch [1/1], Step [385/8897], Loss: 5.2613\n",
      "Epoch [1/1], Step [386/8897], Loss: 5.6915\n",
      "Epoch [1/1], Step [387/8897], Loss: 5.1516\n",
      "Epoch [1/1], Step [388/8897], Loss: 5.3174\n",
      "Epoch [1/1], Step [389/8897], Loss: 5.4790\n",
      "Epoch [1/1], Step [390/8897], Loss: 5.2837\n",
      "Epoch [1/1], Step [391/8897], Loss: 5.3135\n",
      "Epoch [1/1], Step [392/8897], Loss: 5.4626\n",
      "Epoch [1/1], Step [393/8897], Loss: 5.5388\n",
      "Epoch [1/1], Step [394/8897], Loss: 5.5013\n",
      "Epoch [1/1], Step [395/8897], Loss: 5.6296\n",
      "Epoch [1/1], Step [396/8897], Loss: 5.5347\n",
      "Epoch [1/1], Step [397/8897], Loss: 5.3418\n",
      "Epoch [1/1], Step [398/8897], Loss: 5.5179\n",
      "Epoch [1/1], Step [399/8897], Loss: 5.5826\n",
      "Epoch [1/1], Step [400/8897], Loss: 5.5527\n",
      "Epoch [1/1], Step [401/8897], Loss: 5.3728\n",
      "Epoch [1/1], Step [402/8897], Loss: 5.2600\n",
      "Epoch [1/1], Step [403/8897], Loss: 5.4763\n",
      "Epoch [1/1], Step [404/8897], Loss: 5.4993\n",
      "Epoch [1/1], Step [405/8897], Loss: 5.3051\n",
      "Epoch [1/1], Step [406/8897], Loss: 5.5747\n",
      "Epoch [1/1], Step [407/8897], Loss: 5.5759\n",
      "Epoch [1/1], Step [408/8897], Loss: 5.2700\n",
      "Epoch [1/1], Step [409/8897], Loss: 5.5423\n",
      "Epoch [1/1], Step [410/8897], Loss: 5.4278\n",
      "Epoch [1/1], Step [411/8897], Loss: 5.4518\n",
      "Epoch [1/1], Step [412/8897], Loss: 5.4341\n",
      "Epoch [1/1], Step [413/8897], Loss: 5.5655\n",
      "Epoch [1/1], Step [414/8897], Loss: 5.2716\n",
      "Epoch [1/1], Step [415/8897], Loss: 5.7176\n",
      "Epoch [1/1], Step [416/8897], Loss: 5.4066\n",
      "Epoch [1/1], Step [417/8897], Loss: 5.4304\n",
      "Epoch [1/1], Step [418/8897], Loss: 5.3360\n",
      "Epoch [1/1], Step [419/8897], Loss: 5.3790\n",
      "Epoch [1/1], Step [420/8897], Loss: 5.0129\n",
      "Epoch [1/1], Step [421/8897], Loss: 5.5104\n",
      "Epoch [1/1], Step [422/8897], Loss: 5.5325\n",
      "Epoch [1/1], Step [423/8897], Loss: 5.4869\n",
      "Epoch [1/1], Step [424/8897], Loss: 5.4227\n",
      "Epoch [1/1], Step [425/8897], Loss: 5.4498\n",
      "Epoch [1/1], Step [426/8897], Loss: 5.5147\n",
      "Epoch [1/1], Step [427/8897], Loss: 5.4583\n",
      "Epoch [1/1], Step [428/8897], Loss: 5.5554\n",
      "Epoch [1/1], Step [429/8897], Loss: 5.4524\n",
      "Epoch [1/1], Step [430/8897], Loss: 5.4355\n",
      "Epoch [1/1], Step [431/8897], Loss: 5.3764\n",
      "Epoch [1/1], Step [432/8897], Loss: 5.6004\n",
      "Epoch [1/1], Step [433/8897], Loss: 5.3050\n",
      "Epoch [1/1], Step [434/8897], Loss: 5.8269\n",
      "Epoch [1/1], Step [435/8897], Loss: 5.5650\n",
      "Epoch [1/1], Step [436/8897], Loss: 5.3101\n",
      "Epoch [1/1], Step [437/8897], Loss: 5.5150\n",
      "Epoch [1/1], Step [438/8897], Loss: 5.3690\n",
      "Epoch [1/1], Step [439/8897], Loss: 5.3707\n",
      "Epoch [1/1], Step [440/8897], Loss: 5.3815\n",
      "Epoch [1/1], Step [441/8897], Loss: 5.5904\n",
      "Epoch [1/1], Step [442/8897], Loss: 5.6934\n",
      "Epoch [1/1], Step [443/8897], Loss: 5.4640\n",
      "Epoch [1/1], Step [444/8897], Loss: 5.1922\n",
      "Epoch [1/1], Step [445/8897], Loss: 5.5513\n",
      "Epoch [1/1], Step [446/8897], Loss: 5.5233\n",
      "Epoch [1/1], Step [447/8897], Loss: 5.5756\n",
      "Epoch [1/1], Step [448/8897], Loss: 5.6189\n",
      "Epoch [1/1], Step [449/8897], Loss: 5.5738\n",
      "Epoch [1/1], Step [450/8897], Loss: 5.4527\n",
      "Epoch [1/1], Step [451/8897], Loss: 5.2654\n",
      "Epoch [1/1], Step [452/8897], Loss: 5.6093\n",
      "Epoch [1/1], Step [453/8897], Loss: 5.6859\n",
      "Epoch [1/1], Step [454/8897], Loss: 5.4487\n",
      "Epoch [1/1], Step [455/8897], Loss: 5.4031\n",
      "Epoch [1/1], Step [456/8897], Loss: 5.3299\n",
      "Epoch [1/1], Step [457/8897], Loss: 5.4976\n",
      "Epoch [1/1], Step [458/8897], Loss: 5.2941\n",
      "Epoch [1/1], Step [459/8897], Loss: 5.3884\n",
      "Epoch [1/1], Step [460/8897], Loss: 5.5579\n",
      "Epoch [1/1], Step [461/8897], Loss: 5.1847\n",
      "Epoch [1/1], Step [462/8897], Loss: 5.2613\n",
      "Epoch [1/1], Step [463/8897], Loss: 5.2353\n",
      "Epoch [1/1], Step [464/8897], Loss: 5.5454\n",
      "Epoch [1/1], Step [465/8897], Loss: 5.5991\n",
      "Epoch [1/1], Step [466/8897], Loss: 5.5094\n",
      "Epoch [1/1], Step [467/8897], Loss: 5.3932\n",
      "Epoch [1/1], Step [468/8897], Loss: 5.6906\n",
      "Epoch [1/1], Step [469/8897], Loss: 5.6695\n",
      "Epoch [1/1], Step [470/8897], Loss: 5.4189\n",
      "Epoch [1/1], Step [471/8897], Loss: 5.3729\n",
      "Epoch [1/1], Step [472/8897], Loss: 5.4686\n",
      "Epoch [1/1], Step [473/8897], Loss: 5.5482\n",
      "Epoch [1/1], Step [474/8897], Loss: 5.3764\n",
      "Epoch [1/1], Step [475/8897], Loss: 5.5580\n",
      "Epoch [1/1], Step [476/8897], Loss: 5.3511\n",
      "Epoch [1/1], Step [477/8897], Loss: 5.3960\n",
      "Epoch [1/1], Step [478/8897], Loss: 5.4230\n",
      "Epoch [1/1], Step [479/8897], Loss: 5.3295\n",
      "Epoch [1/1], Step [480/8897], Loss: 5.2887\n",
      "Epoch [1/1], Step [481/8897], Loss: 5.5460\n",
      "Epoch [1/1], Step [482/8897], Loss: 5.2466\n",
      "Epoch [1/1], Step [483/8897], Loss: 5.4586\n",
      "Epoch [1/1], Step [484/8897], Loss: 5.7476\n",
      "Epoch [1/1], Step [485/8897], Loss: 5.2400\n",
      "Epoch [1/1], Step [486/8897], Loss: 5.4480\n",
      "Epoch [1/1], Step [487/8897], Loss: 5.5171\n",
      "Epoch [1/1], Step [488/8897], Loss: 5.2760\n",
      "Epoch [1/1], Step [489/8897], Loss: 5.4445\n",
      "Epoch [1/1], Step [490/8897], Loss: 5.2556\n",
      "Epoch [1/1], Step [491/8897], Loss: 5.4001\n",
      "Epoch [1/1], Step [492/8897], Loss: 5.3988\n",
      "Epoch [1/1], Step [493/8897], Loss: 5.3163\n",
      "Epoch [1/1], Step [494/8897], Loss: 5.4804\n",
      "Epoch [1/1], Step [495/8897], Loss: 5.5056\n",
      "Epoch [1/1], Step [496/8897], Loss: 5.5431\n",
      "Epoch [1/1], Step [497/8897], Loss: 5.2905\n",
      "Epoch [1/1], Step [498/8897], Loss: 5.5092\n",
      "Epoch [1/1], Step [499/8897], Loss: 5.4437\n",
      "Epoch [1/1], Step [500/8897], Loss: 5.3758\n",
      "Epoch [1/1], Step [501/8897], Loss: 5.3551\n",
      "Epoch [1/1], Step [502/8897], Loss: 5.6505\n",
      "Epoch [1/1], Step [503/8897], Loss: 5.4317\n",
      "Epoch [1/1], Step [504/8897], Loss: 5.6381\n",
      "Epoch [1/1], Step [505/8897], Loss: 5.4869\n",
      "Epoch [1/1], Step [506/8897], Loss: 5.3725\n",
      "Epoch [1/1], Step [507/8897], Loss: 5.4290\n",
      "Epoch [1/1], Step [508/8897], Loss: 5.4988\n",
      "Epoch [1/1], Step [509/8897], Loss: 5.4079\n",
      "Epoch [1/1], Step [510/8897], Loss: 5.2333\n",
      "Epoch [1/1], Step [511/8897], Loss: 5.3386\n",
      "Epoch [1/1], Step [512/8897], Loss: 5.2887\n",
      "Epoch [1/1], Step [513/8897], Loss: 5.5672\n",
      "Epoch [1/1], Step [514/8897], Loss: 5.4853\n",
      "Epoch [1/1], Step [515/8897], Loss: 5.7063\n",
      "Epoch [1/1], Step [516/8897], Loss: 5.8347\n",
      "Epoch [1/1], Step [517/8897], Loss: 5.5922\n",
      "Epoch [1/1], Step [518/8897], Loss: 5.6677\n",
      "Epoch [1/1], Step [519/8897], Loss: 5.4510\n",
      "Epoch [1/1], Step [520/8897], Loss: 5.4727\n",
      "Epoch [1/1], Step [521/8897], Loss: 5.5287\n",
      "Epoch [1/1], Step [522/8897], Loss: 5.4949\n",
      "Epoch [1/1], Step [523/8897], Loss: 5.3785\n",
      "Epoch [1/1], Step [524/8897], Loss: 5.6346\n",
      "Epoch [1/1], Step [525/8897], Loss: 5.7224\n",
      "Epoch [1/1], Step [526/8897], Loss: 5.4496\n",
      "Epoch [1/1], Step [527/8897], Loss: 5.5831\n",
      "Epoch [1/1], Step [528/8897], Loss: 5.4375\n",
      "Epoch [1/1], Step [529/8897], Loss: 5.1975\n",
      "Epoch [1/1], Step [530/8897], Loss: 5.6318\n",
      "Epoch [1/1], Step [531/8897], Loss: 5.4451\n",
      "Epoch [1/1], Step [532/8897], Loss: 5.6523\n",
      "Epoch [1/1], Step [533/8897], Loss: 5.3945\n",
      "Epoch [1/1], Step [534/8897], Loss: 5.7528\n",
      "Epoch [1/1], Step [535/8897], Loss: 5.4898\n",
      "Epoch [1/1], Step [536/8897], Loss: 5.4017\n",
      "Epoch [1/1], Step [537/8897], Loss: 5.2381\n",
      "Epoch [1/1], Step [538/8897], Loss: 5.4938\n",
      "Epoch [1/1], Step [539/8897], Loss: 5.3960\n",
      "Epoch [1/1], Step [540/8897], Loss: 5.8050\n",
      "Epoch [1/1], Step [541/8897], Loss: 5.5900\n",
      "Epoch [1/1], Step [542/8897], Loss: 5.4815\n",
      "Epoch [1/1], Step [543/8897], Loss: 5.6191\n",
      "Epoch [1/1], Step [544/8897], Loss: 5.4894\n",
      "Epoch [1/1], Step [545/8897], Loss: 5.4136\n",
      "Epoch [1/1], Step [546/8897], Loss: 5.2855\n",
      "Epoch [1/1], Step [547/8897], Loss: 5.3677\n",
      "Epoch [1/1], Step [548/8897], Loss: 5.3959\n",
      "Epoch [1/1], Step [549/8897], Loss: 5.4434\n",
      "Epoch [1/1], Step [550/8897], Loss: 5.5021\n",
      "Epoch [1/1], Step [551/8897], Loss: 5.2279\n",
      "Epoch [1/1], Step [552/8897], Loss: 5.4710\n",
      "Epoch [1/1], Step [553/8897], Loss: 5.3213\n",
      "Epoch [1/1], Step [554/8897], Loss: 5.5654\n",
      "Epoch [1/1], Step [555/8897], Loss: 5.2964\n",
      "Epoch [1/1], Step [556/8897], Loss: 5.4784\n",
      "Epoch [1/1], Step [557/8897], Loss: 5.4060\n",
      "Epoch [1/1], Step [558/8897], Loss: 5.4215\n",
      "Epoch [1/1], Step [559/8897], Loss: 5.6875\n",
      "Epoch [1/1], Step [560/8897], Loss: 5.3760\n",
      "Epoch [1/1], Step [561/8897], Loss: 5.2494\n",
      "Epoch [1/1], Step [562/8897], Loss: 5.3619\n",
      "Epoch [1/1], Step [563/8897], Loss: 5.3826\n",
      "Epoch [1/1], Step [564/8897], Loss: 5.5738\n",
      "Epoch [1/1], Step [565/8897], Loss: 5.6668\n",
      "Epoch [1/1], Step [566/8897], Loss: 5.5510\n",
      "Epoch [1/1], Step [567/8897], Loss: 5.5227\n",
      "Epoch [1/1], Step [568/8897], Loss: 5.4379\n",
      "Epoch [1/1], Step [569/8897], Loss: 5.5536\n",
      "Epoch [1/1], Step [570/8897], Loss: 5.6038\n",
      "Epoch [1/1], Step [571/8897], Loss: 5.3734\n",
      "Epoch [1/1], Step [572/8897], Loss: 5.6111\n",
      "Epoch [1/1], Step [573/8897], Loss: 5.6706\n",
      "Epoch [1/1], Step [574/8897], Loss: 5.6187\n",
      "Epoch [1/1], Step [575/8897], Loss: 5.2541\n",
      "Epoch [1/1], Step [576/8897], Loss: 5.5913\n",
      "Epoch [1/1], Step [577/8897], Loss: 5.5729\n",
      "Epoch [1/1], Step [578/8897], Loss: 5.7768\n",
      "Epoch [1/1], Step [579/8897], Loss: 5.7013\n",
      "Epoch [1/1], Step [580/8897], Loss: 5.5325\n",
      "Epoch [1/1], Step [581/8897], Loss: 5.2950\n",
      "Epoch [1/1], Step [582/8897], Loss: 5.5694\n",
      "Epoch [1/1], Step [583/8897], Loss: 5.4402\n",
      "Epoch [1/1], Step [584/8897], Loss: 5.3878\n",
      "Epoch [1/1], Step [585/8897], Loss: 5.1340\n",
      "Epoch [1/1], Step [586/8897], Loss: 5.4912\n",
      "Epoch [1/1], Step [587/8897], Loss: 5.4428\n",
      "Epoch [1/1], Step [588/8897], Loss: 5.4597\n",
      "Epoch [1/1], Step [589/8897], Loss: 5.2192\n",
      "Epoch [1/1], Step [590/8897], Loss: 5.5226\n",
      "Epoch [1/1], Step [591/8897], Loss: 5.5760\n",
      "Epoch [1/1], Step [592/8897], Loss: 5.4457\n",
      "Epoch [1/1], Step [593/8897], Loss: 5.8055\n",
      "Epoch [1/1], Step [594/8897], Loss: 5.4101\n",
      "Epoch [1/1], Step [595/8897], Loss: 5.4927\n",
      "Epoch [1/1], Step [596/8897], Loss: 5.4882\n",
      "Epoch [1/1], Step [597/8897], Loss: 5.4684\n",
      "Epoch [1/1], Step [598/8897], Loss: 5.5193\n",
      "Epoch [1/1], Step [599/8897], Loss: 5.5634\n",
      "Epoch [1/1], Step [600/8897], Loss: 5.5956\n",
      "Epoch [1/1], Step [601/8897], Loss: 5.3486\n",
      "Epoch [1/1], Step [602/8897], Loss: 5.5891\n",
      "Epoch [1/1], Step [603/8897], Loss: 5.4546\n",
      "Epoch [1/1], Step [604/8897], Loss: 5.3351\n",
      "Epoch [1/1], Step [605/8897], Loss: 5.6193\n",
      "Epoch [1/1], Step [606/8897], Loss: 5.4594\n",
      "Epoch [1/1], Step [607/8897], Loss: 5.2872\n",
      "Epoch [1/1], Step [608/8897], Loss: 5.3906\n",
      "Epoch [1/1], Step [609/8897], Loss: 5.4852\n",
      "Epoch [1/1], Step [610/8897], Loss: 5.6279\n",
      "Epoch [1/1], Step [611/8897], Loss: 5.6023\n",
      "Epoch [1/1], Step [612/8897], Loss: 5.4821\n",
      "Epoch [1/1], Step [613/8897], Loss: 5.5409\n",
      "Epoch [1/1], Step [614/8897], Loss: 5.1934\n",
      "Epoch [1/1], Step [615/8897], Loss: 5.4754\n",
      "Epoch [1/1], Step [616/8897], Loss: 5.7537\n",
      "Epoch [1/1], Step [617/8897], Loss: 5.6948\n",
      "Epoch [1/1], Step [618/8897], Loss: 5.5047\n",
      "Epoch [1/1], Step [619/8897], Loss: 5.4370\n",
      "Epoch [1/1], Step [620/8897], Loss: 5.4583\n",
      "Epoch [1/1], Step [621/8897], Loss: 5.5325\n",
      "Epoch [1/1], Step [622/8897], Loss: 5.3568\n",
      "Epoch [1/1], Step [623/8897], Loss: 5.5093\n",
      "Epoch [1/1], Step [624/8897], Loss: 5.2810\n",
      "Epoch [1/1], Step [625/8897], Loss: 5.4785\n",
      "Epoch [1/1], Step [626/8897], Loss: 5.2467\n",
      "Epoch [1/1], Step [627/8897], Loss: 5.5590\n",
      "Epoch [1/1], Step [628/8897], Loss: 5.6447\n",
      "Epoch [1/1], Step [629/8897], Loss: 5.5344\n",
      "Epoch [1/1], Step [630/8897], Loss: 5.5729\n",
      "Epoch [1/1], Step [631/8897], Loss: 5.4180\n",
      "Epoch [1/1], Step [632/8897], Loss: 5.4189\n",
      "Epoch [1/1], Step [633/8897], Loss: 5.5545\n",
      "Epoch [1/1], Step [634/8897], Loss: 5.4330\n",
      "Epoch [1/1], Step [635/8897], Loss: 5.4364\n",
      "Epoch [1/1], Step [636/8897], Loss: 5.5705\n",
      "Epoch [1/1], Step [637/8897], Loss: 5.5555\n",
      "Epoch [1/1], Step [638/8897], Loss: 5.5028\n",
      "Epoch [1/1], Step [639/8897], Loss: 5.2825\n",
      "Epoch [1/1], Step [640/8897], Loss: 5.5690\n",
      "Epoch [1/1], Step [641/8897], Loss: 5.4479\n",
      "Epoch [1/1], Step [642/8897], Loss: 5.8481\n",
      "Epoch [1/1], Step [643/8897], Loss: 5.3394\n",
      "Epoch [1/1], Step [644/8897], Loss: 5.4599\n",
      "Epoch [1/1], Step [645/8897], Loss: 5.5029\n",
      "Epoch [1/1], Step [646/8897], Loss: 5.4505\n",
      "Epoch [1/1], Step [647/8897], Loss: 5.3943\n",
      "Epoch [1/1], Step [648/8897], Loss: 5.4839\n",
      "Epoch [1/1], Step [649/8897], Loss: 5.5004\n",
      "Epoch [1/1], Step [650/8897], Loss: 5.6963\n",
      "Epoch [1/1], Step [651/8897], Loss: 5.3986\n",
      "Epoch [1/1], Step [652/8897], Loss: 5.6794\n",
      "Epoch [1/1], Step [653/8897], Loss: 5.3359\n",
      "Epoch [1/1], Step [654/8897], Loss: 5.1851\n",
      "Epoch [1/1], Step [655/8897], Loss: 5.6305\n",
      "Epoch [1/1], Step [656/8897], Loss: 5.4493\n",
      "Epoch [1/1], Step [657/8897], Loss: 5.7377\n",
      "Epoch [1/1], Step [658/8897], Loss: 5.6670\n",
      "Epoch [1/1], Step [659/8897], Loss: 5.2422\n",
      "Epoch [1/1], Step [660/8897], Loss: 5.2489\n",
      "Epoch [1/1], Step [661/8897], Loss: 5.3980\n",
      "Epoch [1/1], Step [662/8897], Loss: 5.2630\n",
      "Epoch [1/1], Step [663/8897], Loss: 5.4451\n",
      "Epoch [1/1], Step [664/8897], Loss: 5.3842\n",
      "Epoch [1/1], Step [665/8897], Loss: 5.3698\n",
      "Epoch [1/1], Step [666/8897], Loss: 5.3635\n",
      "Epoch [1/1], Step [667/8897], Loss: 5.4789\n",
      "Epoch [1/1], Step [668/8897], Loss: 5.6691\n",
      "Epoch [1/1], Step [669/8897], Loss: 5.3093\n",
      "Epoch [1/1], Step [670/8897], Loss: 5.4914\n",
      "Epoch [1/1], Step [671/8897], Loss: 5.6568\n",
      "Epoch [1/1], Step [672/8897], Loss: 5.7194\n",
      "Epoch [1/1], Step [673/8897], Loss: 5.5851\n",
      "Epoch [1/1], Step [674/8897], Loss: 5.5027\n",
      "Epoch [1/1], Step [675/8897], Loss: 5.2843\n",
      "Epoch [1/1], Step [676/8897], Loss: 5.3433\n",
      "Epoch [1/1], Step [677/8897], Loss: 5.3278\n",
      "Epoch [1/1], Step [678/8897], Loss: 5.6156\n",
      "Epoch [1/1], Step [679/8897], Loss: 5.3123\n",
      "Epoch [1/1], Step [680/8897], Loss: 5.3911\n",
      "Epoch [1/1], Step [681/8897], Loss: 5.7440\n",
      "Epoch [1/1], Step [682/8897], Loss: 5.7278\n",
      "Epoch [1/1], Step [683/8897], Loss: 5.5964\n",
      "Epoch [1/1], Step [684/8897], Loss: 5.5440\n",
      "Epoch [1/1], Step [685/8897], Loss: 5.4284\n",
      "Epoch [1/1], Step [686/8897], Loss: 5.5518\n",
      "Epoch [1/1], Step [687/8897], Loss: 5.7409\n",
      "Epoch [1/1], Step [688/8897], Loss: 5.2614\n",
      "Epoch [1/1], Step [689/8897], Loss: 5.4485\n",
      "Epoch [1/1], Step [690/8897], Loss: 5.4285\n",
      "Epoch [1/1], Step [691/8897], Loss: 5.3267\n",
      "Epoch [1/1], Step [692/8897], Loss: 5.3605\n",
      "Epoch [1/1], Step [693/8897], Loss: 5.5744\n",
      "Epoch [1/1], Step [694/8897], Loss: 5.6240\n",
      "Epoch [1/1], Step [695/8897], Loss: 5.4638\n",
      "Epoch [1/1], Step [696/8897], Loss: 5.5137\n",
      "Epoch [1/1], Step [697/8897], Loss: 5.4458\n",
      "Epoch [1/1], Step [698/8897], Loss: 5.7938\n",
      "Epoch [1/1], Step [699/8897], Loss: 5.3473\n",
      "Epoch [1/1], Step [700/8897], Loss: 5.6800\n",
      "Epoch [1/1], Step [701/8897], Loss: 5.4397\n",
      "Epoch [1/1], Step [702/8897], Loss: 5.5270\n",
      "Epoch [1/1], Step [703/8897], Loss: 5.3339\n",
      "Epoch [1/1], Step [704/8897], Loss: 5.2748\n",
      "Epoch [1/1], Step [705/8897], Loss: 5.3775\n",
      "Epoch [1/1], Step [706/8897], Loss: 5.4592\n",
      "Epoch [1/1], Step [707/8897], Loss: 5.7687\n",
      "Epoch [1/1], Step [708/8897], Loss: 5.5017\n",
      "Epoch [1/1], Step [709/8897], Loss: 5.4798\n",
      "Epoch [1/1], Step [710/8897], Loss: 5.5262\n",
      "Epoch [1/1], Step [711/8897], Loss: 5.3376\n",
      "Epoch [1/1], Step [712/8897], Loss: 5.5790\n",
      "Epoch [1/1], Step [713/8897], Loss: 5.5014\n",
      "Epoch [1/1], Step [714/8897], Loss: 5.6646\n",
      "Epoch [1/1], Step [715/8897], Loss: 5.5691\n",
      "Epoch [1/1], Step [716/8897], Loss: 5.5914\n",
      "Epoch [1/1], Step [717/8897], Loss: 5.5090\n",
      "Epoch [1/1], Step [718/8897], Loss: 5.1776\n",
      "Epoch [1/1], Step [719/8897], Loss: 5.5720\n",
      "Epoch [1/1], Step [720/8897], Loss: 5.5960\n",
      "Epoch [1/1], Step [721/8897], Loss: 5.2122\n",
      "Epoch [1/1], Step [722/8897], Loss: 5.4939\n",
      "Epoch [1/1], Step [723/8897], Loss: 5.5587\n",
      "Epoch [1/1], Step [724/8897], Loss: 5.4556\n",
      "Epoch [1/1], Step [725/8897], Loss: 5.5326\n",
      "Epoch [1/1], Step [726/8897], Loss: 5.5345\n",
      "Epoch [1/1], Step [727/8897], Loss: 5.4177\n",
      "Epoch [1/1], Step [728/8897], Loss: 5.3031\n",
      "Epoch [1/1], Step [729/8897], Loss: 5.6512\n",
      "Epoch [1/1], Step [730/8897], Loss: 5.6684\n",
      "Epoch [1/1], Step [731/8897], Loss: 5.6562\n",
      "Epoch [1/1], Step [732/8897], Loss: 5.4063\n",
      "Epoch [1/1], Step [733/8897], Loss: 5.3621\n",
      "Epoch [1/1], Step [734/8897], Loss: 5.1551\n",
      "Epoch [1/1], Step [735/8897], Loss: 5.3772\n",
      "Epoch [1/1], Step [736/8897], Loss: 5.5291\n",
      "Epoch [1/1], Step [737/8897], Loss: 5.7404\n",
      "Epoch [1/1], Step [738/8897], Loss: 5.5789\n",
      "Epoch [1/1], Step [739/8897], Loss: 5.3584\n",
      "Epoch [1/1], Step [740/8897], Loss: 5.3460\n",
      "Epoch [1/1], Step [741/8897], Loss: 5.1802\n",
      "Epoch [1/1], Step [742/8897], Loss: 5.3605\n",
      "Epoch [1/1], Step [743/8897], Loss: 5.2993\n",
      "Epoch [1/1], Step [744/8897], Loss: 5.5508\n",
      "Epoch [1/1], Step [745/8897], Loss: 5.3537\n",
      "Epoch [1/1], Step [746/8897], Loss: 5.4565\n",
      "Epoch [1/1], Step [747/8897], Loss: 5.4305\n",
      "Epoch [1/1], Step [748/8897], Loss: 5.4849\n",
      "Epoch [1/1], Step [749/8897], Loss: 5.3740\n",
      "Epoch [1/1], Step [750/8897], Loss: 5.6787\n",
      "Epoch [1/1], Step [751/8897], Loss: 5.6156\n",
      "Epoch [1/1], Step [752/8897], Loss: 5.4543\n",
      "Epoch [1/1], Step [753/8897], Loss: 5.5660\n",
      "Epoch [1/1], Step [754/8897], Loss: 5.6144\n",
      "Epoch [1/1], Step [755/8897], Loss: 5.2372\n",
      "Epoch [1/1], Step [756/8897], Loss: 5.6731\n",
      "Epoch [1/1], Step [757/8897], Loss: 5.3842\n",
      "Epoch [1/1], Step [758/8897], Loss: 5.4355\n",
      "Epoch [1/1], Step [759/8897], Loss: 5.6365\n",
      "Epoch [1/1], Step [760/8897], Loss: 5.2038\n",
      "Epoch [1/1], Step [761/8897], Loss: 5.5323\n",
      "Epoch [1/1], Step [762/8897], Loss: 5.3405\n",
      "Epoch [1/1], Step [763/8897], Loss: 5.4166\n",
      "Epoch [1/1], Step [764/8897], Loss: 5.5853\n",
      "Epoch [1/1], Step [765/8897], Loss: 5.5569\n",
      "Epoch [1/1], Step [766/8897], Loss: 5.5074\n",
      "Epoch [1/1], Step [767/8897], Loss: 5.6061\n",
      "Epoch [1/1], Step [768/8897], Loss: 5.7647\n",
      "Epoch [1/1], Step [769/8897], Loss: 5.3751\n",
      "Epoch [1/1], Step [770/8897], Loss: 5.5147\n",
      "Epoch [1/1], Step [771/8897], Loss: 5.7596\n",
      "Epoch [1/1], Step [772/8897], Loss: 5.5353\n",
      "Epoch [1/1], Step [773/8897], Loss: 5.4011\n",
      "Epoch [1/1], Step [774/8897], Loss: 5.4772\n",
      "Epoch [1/1], Step [775/8897], Loss: 5.3487\n",
      "Epoch [1/1], Step [776/8897], Loss: 5.4723\n",
      "Epoch [1/1], Step [777/8897], Loss: 5.1214\n",
      "Epoch [1/1], Step [778/8897], Loss: 5.3655\n",
      "Epoch [1/1], Step [779/8897], Loss: 5.6121\n",
      "Epoch [1/1], Step [780/8897], Loss: 5.4645\n",
      "Epoch [1/1], Step [781/8897], Loss: 5.5773\n",
      "Epoch [1/1], Step [782/8897], Loss: 5.4421\n",
      "Epoch [1/1], Step [783/8897], Loss: 5.5301\n",
      "Epoch [1/1], Step [784/8897], Loss: 5.5159\n",
      "Epoch [1/1], Step [785/8897], Loss: 5.3856\n",
      "Epoch [1/1], Step [786/8897], Loss: 5.5393\n",
      "Epoch [1/1], Step [787/8897], Loss: 5.3896\n",
      "Epoch [1/1], Step [788/8897], Loss: 5.4498\n",
      "Epoch [1/1], Step [789/8897], Loss: 5.4627\n",
      "Epoch [1/1], Step [790/8897], Loss: 5.4961\n",
      "Epoch [1/1], Step [791/8897], Loss: 5.3912\n",
      "Epoch [1/1], Step [792/8897], Loss: 5.5515\n",
      "Epoch [1/1], Step [793/8897], Loss: 5.5097\n",
      "Epoch [1/1], Step [794/8897], Loss: 5.3938\n",
      "Epoch [1/1], Step [795/8897], Loss: 5.4984\n",
      "Epoch [1/1], Step [796/8897], Loss: 5.4004\n",
      "Epoch [1/1], Step [797/8897], Loss: 5.3045\n",
      "Epoch [1/1], Step [798/8897], Loss: 5.6553\n",
      "Epoch [1/1], Step [799/8897], Loss: 5.3996\n",
      "Epoch [1/1], Step [800/8897], Loss: 5.3895\n",
      "Epoch [1/1], Step [801/8897], Loss: 5.4826\n",
      "Epoch [1/1], Step [802/8897], Loss: 5.2938\n",
      "Epoch [1/1], Step [803/8897], Loss: 5.6810\n",
      "Epoch [1/1], Step [804/8897], Loss: 5.5337\n",
      "Epoch [1/1], Step [805/8897], Loss: 5.4545\n",
      "Epoch [1/1], Step [806/8897], Loss: 5.4355\n",
      "Epoch [1/1], Step [807/8897], Loss: 5.5863\n",
      "Epoch [1/1], Step [808/8897], Loss: 5.4191\n",
      "Epoch [1/1], Step [809/8897], Loss: 5.2433\n",
      "Epoch [1/1], Step [810/8897], Loss: 5.3489\n",
      "Epoch [1/1], Step [811/8897], Loss: 5.5086\n",
      "Epoch [1/1], Step [812/8897], Loss: 5.1380\n",
      "Epoch [1/1], Step [813/8897], Loss: 5.5223\n",
      "Epoch [1/1], Step [814/8897], Loss: 5.5110\n",
      "Epoch [1/1], Step [815/8897], Loss: 5.5598\n",
      "Epoch [1/1], Step [816/8897], Loss: 5.4396\n",
      "Epoch [1/1], Step [817/8897], Loss: 5.3681\n",
      "Epoch [1/1], Step [818/8897], Loss: 5.4351\n",
      "Epoch [1/1], Step [819/8897], Loss: 5.4953\n",
      "Epoch [1/1], Step [820/8897], Loss: 5.4237\n",
      "Epoch [1/1], Step [821/8897], Loss: 5.5223\n",
      "Epoch [1/1], Step [822/8897], Loss: 5.4652\n",
      "Epoch [1/1], Step [823/8897], Loss: 5.6392\n",
      "Epoch [1/1], Step [824/8897], Loss: 5.3237\n",
      "Epoch [1/1], Step [825/8897], Loss: 5.4864\n",
      "Epoch [1/1], Step [826/8897], Loss: 5.5738\n",
      "Epoch [1/1], Step [827/8897], Loss: 5.3598\n",
      "Epoch [1/1], Step [828/8897], Loss: 5.4247\n",
      "Epoch [1/1], Step [829/8897], Loss: 5.3732\n",
      "Epoch [1/1], Step [830/8897], Loss: 5.5142\n",
      "Epoch [1/1], Step [831/8897], Loss: 5.4810\n",
      "Epoch [1/1], Step [832/8897], Loss: 5.7885\n",
      "Epoch [1/1], Step [833/8897], Loss: 5.2731\n",
      "Epoch [1/1], Step [834/8897], Loss: 5.4272\n",
      "Epoch [1/1], Step [835/8897], Loss: 5.7043\n",
      "Epoch [1/1], Step [836/8897], Loss: 5.2399\n",
      "Epoch [1/1], Step [837/8897], Loss: 5.1457\n",
      "Epoch [1/1], Step [838/8897], Loss: 5.4510\n",
      "Epoch [1/1], Step [839/8897], Loss: 5.4359\n",
      "Epoch [1/1], Step [840/8897], Loss: 5.5373\n",
      "Epoch [1/1], Step [841/8897], Loss: 5.7223\n",
      "Epoch [1/1], Step [842/8897], Loss: 5.4002\n",
      "Epoch [1/1], Step [843/8897], Loss: 5.4767\n",
      "Epoch [1/1], Step [844/8897], Loss: 5.4108\n",
      "Epoch [1/1], Step [845/8897], Loss: 5.4657\n",
      "Epoch [1/1], Step [846/8897], Loss: 5.5356\n",
      "Epoch [1/1], Step [847/8897], Loss: 5.4634\n",
      "Epoch [1/1], Step [848/8897], Loss: 5.5456\n",
      "Epoch [1/1], Step [849/8897], Loss: 5.3694\n",
      "Epoch [1/1], Step [850/8897], Loss: 5.6840\n",
      "Epoch [1/1], Step [851/8897], Loss: 5.3563\n",
      "Epoch [1/1], Step [852/8897], Loss: 5.6832\n",
      "Epoch [1/1], Step [853/8897], Loss: 5.3709\n",
      "Epoch [1/1], Step [854/8897], Loss: 4.9133\n",
      "Epoch [1/1], Step [855/8897], Loss: 5.5172\n",
      "Epoch [1/1], Step [856/8897], Loss: 5.4911\n",
      "Epoch [1/1], Step [857/8897], Loss: 5.4097\n",
      "Epoch [1/1], Step [858/8897], Loss: 5.3357\n",
      "Epoch [1/1], Step [859/8897], Loss: 5.4514\n",
      "Epoch [1/1], Step [860/8897], Loss: 5.3099\n",
      "Epoch [1/1], Step [861/8897], Loss: 5.2652\n",
      "Epoch [1/1], Step [862/8897], Loss: 5.4085\n",
      "Epoch [1/1], Step [863/8897], Loss: 5.6414\n",
      "Epoch [1/1], Step [864/8897], Loss: 5.3786\n",
      "Epoch [1/1], Step [865/8897], Loss: 5.5341\n",
      "Epoch [1/1], Step [866/8897], Loss: 5.5156\n",
      "Epoch [1/1], Step [867/8897], Loss: 5.6935\n",
      "Epoch [1/1], Step [868/8897], Loss: 5.3418\n",
      "Epoch [1/1], Step [869/8897], Loss: 5.2719\n",
      "Epoch [1/1], Step [870/8897], Loss: 5.4464\n",
      "Epoch [1/1], Step [871/8897], Loss: 5.5575\n",
      "Epoch [1/1], Step [872/8897], Loss: 5.4710\n",
      "Epoch [1/1], Step [873/8897], Loss: 5.3599\n",
      "Epoch [1/1], Step [874/8897], Loss: 5.7226\n",
      "Epoch [1/1], Step [875/8897], Loss: 5.2765\n",
      "Epoch [1/1], Step [876/8897], Loss: 5.3136\n",
      "Epoch [1/1], Step [877/8897], Loss: 5.3436\n",
      "Epoch [1/1], Step [878/8897], Loss: 5.4551\n",
      "Epoch [1/1], Step [879/8897], Loss: 5.3851\n",
      "Epoch [1/1], Step [880/8897], Loss: 5.3794\n",
      "Epoch [1/1], Step [881/8897], Loss: 5.5692\n",
      "Epoch [1/1], Step [882/8897], Loss: 5.2784\n",
      "Epoch [1/1], Step [883/8897], Loss: 5.3563\n",
      "Epoch [1/1], Step [884/8897], Loss: 5.5029\n",
      "Epoch [1/1], Step [885/8897], Loss: 5.4708\n",
      "Epoch [1/1], Step [886/8897], Loss: 5.6070\n",
      "Epoch [1/1], Step [887/8897], Loss: 5.5896\n",
      "Epoch [1/1], Step [888/8897], Loss: 5.4159\n",
      "Epoch [1/1], Step [889/8897], Loss: 5.6005\n",
      "Epoch [1/1], Step [890/8897], Loss: 5.2821\n",
      "Epoch [1/1], Step [891/8897], Loss: 5.3455\n",
      "Epoch [1/1], Step [892/8897], Loss: 5.3316\n",
      "Epoch [1/1], Step [893/8897], Loss: 5.3037\n",
      "Epoch [1/1], Step [894/8897], Loss: 5.3406\n",
      "Epoch [1/1], Step [895/8897], Loss: 5.5037\n",
      "Epoch [1/1], Step [896/8897], Loss: 5.5603\n",
      "Epoch [1/1], Step [897/8897], Loss: 5.4981\n",
      "Epoch [1/1], Step [898/8897], Loss: 5.5001\n",
      "Epoch [1/1], Step [899/8897], Loss: 5.4420\n",
      "Epoch [1/1], Step [900/8897], Loss: 5.5520\n",
      "Epoch [1/1], Step [901/8897], Loss: 5.3913\n",
      "Epoch [1/1], Step [902/8897], Loss: 5.5060\n",
      "Epoch [1/1], Step [903/8897], Loss: 5.4137\n",
      "Epoch [1/1], Step [904/8897], Loss: 5.3017\n",
      "Epoch [1/1], Step [905/8897], Loss: 5.5612\n",
      "Epoch [1/1], Step [906/8897], Loss: 5.6043\n",
      "Epoch [1/1], Step [907/8897], Loss: 5.3390\n",
      "Epoch [1/1], Step [908/8897], Loss: 5.3237\n",
      "Epoch [1/1], Step [909/8897], Loss: 5.5938\n",
      "Epoch [1/1], Step [910/8897], Loss: 5.3719\n",
      "Epoch [1/1], Step [911/8897], Loss: 5.4730\n",
      "Epoch [1/1], Step [912/8897], Loss: 5.4695\n",
      "Epoch [1/1], Step [913/8897], Loss: 5.4488\n",
      "Epoch [1/1], Step [914/8897], Loss: 5.4006\n",
      "Epoch [1/1], Step [915/8897], Loss: 5.3067\n",
      "Epoch [1/1], Step [916/8897], Loss: 5.4535\n",
      "Epoch [1/1], Step [917/8897], Loss: 5.2256\n",
      "Epoch [1/1], Step [918/8897], Loss: 5.4947\n",
      "Epoch [1/1], Step [919/8897], Loss: 5.3864\n",
      "Epoch [1/1], Step [920/8897], Loss: 5.4101\n",
      "Epoch [1/1], Step [921/8897], Loss: 5.3164\n",
      "Epoch [1/1], Step [922/8897], Loss: 5.4994\n",
      "Epoch [1/1], Step [923/8897], Loss: 5.2646\n",
      "Epoch [1/1], Step [924/8897], Loss: 5.8126\n",
      "Epoch [1/1], Step [925/8897], Loss: 5.4940\n",
      "Epoch [1/1], Step [926/8897], Loss: 5.7759\n",
      "Epoch [1/1], Step [927/8897], Loss: 5.4522\n",
      "Epoch [1/1], Step [928/8897], Loss: 5.4973\n",
      "Epoch [1/1], Step [929/8897], Loss: 5.5663\n",
      "Epoch [1/1], Step [930/8897], Loss: 5.4179\n",
      "Epoch [1/1], Step [931/8897], Loss: 5.5187\n",
      "Epoch [1/1], Step [932/8897], Loss: 5.3280\n",
      "Epoch [1/1], Step [933/8897], Loss: 5.4710\n",
      "Epoch [1/1], Step [934/8897], Loss: 5.4609\n",
      "Epoch [1/1], Step [935/8897], Loss: 5.3600\n",
      "Epoch [1/1], Step [936/8897], Loss: 5.3627\n",
      "Epoch [1/1], Step [937/8897], Loss: 5.5228\n",
      "Epoch [1/1], Step [938/8897], Loss: 5.4835\n",
      "Epoch [1/1], Step [939/8897], Loss: 5.6787\n",
      "Epoch [1/1], Step [940/8897], Loss: 5.3425\n",
      "Epoch [1/1], Step [941/8897], Loss: 5.3856\n",
      "Epoch [1/1], Step [942/8897], Loss: 5.4905\n",
      "Epoch [1/1], Step [943/8897], Loss: 5.4651\n",
      "Epoch [1/1], Step [944/8897], Loss: 5.3693\n",
      "Epoch [1/1], Step [945/8897], Loss: 5.6453\n",
      "Epoch [1/1], Step [946/8897], Loss: 5.3317\n",
      "Epoch [1/1], Step [947/8897], Loss: 5.5159\n",
      "Epoch [1/1], Step [948/8897], Loss: 5.5076\n",
      "Epoch [1/1], Step [949/8897], Loss: 5.4595\n",
      "Epoch [1/1], Step [950/8897], Loss: 5.3750\n",
      "Epoch [1/1], Step [951/8897], Loss: 5.5137\n",
      "Epoch [1/1], Step [952/8897], Loss: 5.3135\n",
      "Epoch [1/1], Step [953/8897], Loss: 5.6595\n",
      "Epoch [1/1], Step [954/8897], Loss: 5.4699\n",
      "Epoch [1/1], Step [955/8897], Loss: 5.5548\n",
      "Epoch [1/1], Step [956/8897], Loss: 5.3021\n",
      "Epoch [1/1], Step [957/8897], Loss: 5.2567\n",
      "Epoch [1/1], Step [958/8897], Loss: 5.6406\n",
      "Epoch [1/1], Step [959/8897], Loss: 5.5590\n",
      "Epoch [1/1], Step [960/8897], Loss: 5.6979\n",
      "Epoch [1/1], Step [961/8897], Loss: 5.2312\n",
      "Epoch [1/1], Step [962/8897], Loss: 5.3880\n",
      "Epoch [1/1], Step [963/8897], Loss: 5.2161\n",
      "Epoch [1/1], Step [964/8897], Loss: 5.2754\n",
      "Epoch [1/1], Step [965/8897], Loss: 5.4284\n",
      "Epoch [1/1], Step [966/8897], Loss: 5.3366\n",
      "Epoch [1/1], Step [967/8897], Loss: 5.6135\n",
      "Epoch [1/1], Step [968/8897], Loss: 5.4765\n",
      "Epoch [1/1], Step [969/8897], Loss: 5.2962\n",
      "Epoch [1/1], Step [970/8897], Loss: 5.2551\n",
      "Epoch [1/1], Step [971/8897], Loss: 5.3784\n",
      "Epoch [1/1], Step [972/8897], Loss: 5.5310\n",
      "Epoch [1/1], Step [973/8897], Loss: 5.3184\n",
      "Epoch [1/1], Step [974/8897], Loss: 5.2084\n",
      "Epoch [1/1], Step [975/8897], Loss: 5.3355\n",
      "Epoch [1/1], Step [976/8897], Loss: 5.6174\n",
      "Epoch [1/1], Step [977/8897], Loss: 5.3037\n",
      "Epoch [1/1], Step [978/8897], Loss: 5.0640\n",
      "Epoch [1/1], Step [979/8897], Loss: 5.3339\n",
      "Epoch [1/1], Step [980/8897], Loss: 5.1834\n",
      "Epoch [1/1], Step [981/8897], Loss: 5.4543\n",
      "Epoch [1/1], Step [982/8897], Loss: 5.4382\n",
      "Epoch [1/1], Step [983/8897], Loss: 5.5045\n",
      "Epoch [1/1], Step [984/8897], Loss: 5.6226\n",
      "Epoch [1/1], Step [985/8897], Loss: 5.4387\n",
      "Epoch [1/1], Step [986/8897], Loss: 5.4506\n",
      "Epoch [1/1], Step [987/8897], Loss: 5.5231\n",
      "Epoch [1/1], Step [988/8897], Loss: 5.4621\n",
      "Epoch [1/1], Step [989/8897], Loss: 5.2443\n",
      "Epoch [1/1], Step [990/8897], Loss: 5.5403\n",
      "Epoch [1/1], Step [991/8897], Loss: 5.3502\n",
      "Epoch [1/1], Step [992/8897], Loss: 5.5121\n",
      "Epoch [1/1], Step [993/8897], Loss: 5.4778\n",
      "Epoch [1/1], Step [994/8897], Loss: 5.5278\n",
      "Epoch [1/1], Step [995/8897], Loss: 5.6954\n",
      "Epoch [1/1], Step [996/8897], Loss: 5.3652\n",
      "Epoch [1/1], Step [997/8897], Loss: 5.3990\n",
      "Epoch [1/1], Step [998/8897], Loss: 5.5458\n",
      "Epoch [1/1], Step [999/8897], Loss: 5.4330\n",
      "Epoch [1/1], Step [1000/8897], Loss: 5.5926\n",
      "Epoch [1/1], Step [1001/8897], Loss: 5.6390\n",
      "Epoch [1/1], Step [1002/8897], Loss: 5.1649\n",
      "Epoch [1/1], Step [1003/8897], Loss: 5.3723\n",
      "Epoch [1/1], Step [1004/8897], Loss: 5.4539\n",
      "Epoch [1/1], Step [1005/8897], Loss: 5.3426\n",
      "Epoch [1/1], Step [1006/8897], Loss: 5.3484\n",
      "Epoch [1/1], Step [1007/8897], Loss: 5.5496\n",
      "Epoch [1/1], Step [1008/8897], Loss: 5.6603\n",
      "Epoch [1/1], Step [1009/8897], Loss: 5.5302\n",
      "Epoch [1/1], Step [1010/8897], Loss: 5.7016\n",
      "Epoch [1/1], Step [1011/8897], Loss: 5.6872\n",
      "Epoch [1/1], Step [1012/8897], Loss: 5.1637\n",
      "Epoch [1/1], Step [1013/8897], Loss: 5.5524\n",
      "Epoch [1/1], Step [1014/8897], Loss: 5.2955\n",
      "Epoch [1/1], Step [1015/8897], Loss: 5.4532\n",
      "Epoch [1/1], Step [1016/8897], Loss: 5.3142\n",
      "Epoch [1/1], Step [1017/8897], Loss: 5.5375\n",
      "Epoch [1/1], Step [1018/8897], Loss: 5.4929\n",
      "Epoch [1/1], Step [1019/8897], Loss: 5.5914\n",
      "Epoch [1/1], Step [1020/8897], Loss: 5.4827\n",
      "Epoch [1/1], Step [1021/8897], Loss: 5.5635\n",
      "Epoch [1/1], Step [1022/8897], Loss: 5.4452\n",
      "Epoch [1/1], Step [1023/8897], Loss: 5.4904\n",
      "Epoch [1/1], Step [1024/8897], Loss: 5.2569\n",
      "Epoch [1/1], Step [1025/8897], Loss: 5.3243\n",
      "Epoch [1/1], Step [1026/8897], Loss: 5.4714\n",
      "Epoch [1/1], Step [1027/8897], Loss: 5.5964\n",
      "Epoch [1/1], Step [1028/8897], Loss: 5.4382\n",
      "Epoch [1/1], Step [1029/8897], Loss: 5.5040\n",
      "Epoch [1/1], Step [1030/8897], Loss: 5.5246\n",
      "Epoch [1/1], Step [1031/8897], Loss: 5.4892\n",
      "Epoch [1/1], Step [1032/8897], Loss: 5.1407\n",
      "Epoch [1/1], Step [1033/8897], Loss: 5.4468\n",
      "Epoch [1/1], Step [1034/8897], Loss: 5.5764\n",
      "Epoch [1/1], Step [1035/8897], Loss: 5.4369\n",
      "Epoch [1/1], Step [1036/8897], Loss: 5.3528\n",
      "Epoch [1/1], Step [1037/8897], Loss: 5.6015\n",
      "Epoch [1/1], Step [1038/8897], Loss: 5.6250\n",
      "Epoch [1/1], Step [1039/8897], Loss: 5.6031\n",
      "Epoch [1/1], Step [1040/8897], Loss: 5.5525\n",
      "Epoch [1/1], Step [1041/8897], Loss: 5.6598\n",
      "Epoch [1/1], Step [1042/8897], Loss: 5.5704\n",
      "Epoch [1/1], Step [1043/8897], Loss: 5.4041\n",
      "Epoch [1/1], Step [1044/8897], Loss: 5.2905\n",
      "Epoch [1/1], Step [1045/8897], Loss: 5.4565\n",
      "Epoch [1/1], Step [1046/8897], Loss: 5.3149\n",
      "Epoch [1/1], Step [1047/8897], Loss: 5.2659\n",
      "Epoch [1/1], Step [1048/8897], Loss: 5.4932\n",
      "Epoch [1/1], Step [1049/8897], Loss: 5.7155\n",
      "Epoch [1/1], Step [1050/8897], Loss: 5.3090\n",
      "Epoch [1/1], Step [1051/8897], Loss: 5.5102\n",
      "Epoch [1/1], Step [1052/8897], Loss: 5.2894\n",
      "Epoch [1/1], Step [1053/8897], Loss: 5.4800\n",
      "Epoch [1/1], Step [1054/8897], Loss: 5.6921\n",
      "Epoch [1/1], Step [1055/8897], Loss: 5.4047\n",
      "Epoch [1/1], Step [1056/8897], Loss: 5.6539\n",
      "Epoch [1/1], Step [1057/8897], Loss: 5.5441\n",
      "Epoch [1/1], Step [1058/8897], Loss: 5.4252\n",
      "Epoch [1/1], Step [1059/8897], Loss: 5.2594\n",
      "Epoch [1/1], Step [1060/8897], Loss: 5.3341\n",
      "Epoch [1/1], Step [1061/8897], Loss: 5.2603\n",
      "Epoch [1/1], Step [1062/8897], Loss: 5.5938\n",
      "Epoch [1/1], Step [1063/8897], Loss: 5.2080\n",
      "Epoch [1/1], Step [1064/8897], Loss: 5.4646\n",
      "Epoch [1/1], Step [1065/8897], Loss: 5.3025\n",
      "Epoch [1/1], Step [1066/8897], Loss: 5.3325\n",
      "Epoch [1/1], Step [1067/8897], Loss: 5.5237\n",
      "Epoch [1/1], Step [1068/8897], Loss: 5.2707\n",
      "Epoch [1/1], Step [1069/8897], Loss: 5.6110\n",
      "Epoch [1/1], Step [1070/8897], Loss: 5.3854\n",
      "Epoch [1/1], Step [1071/8897], Loss: 5.4398\n",
      "Epoch [1/1], Step [1072/8897], Loss: 5.4677\n",
      "Epoch [1/1], Step [1073/8897], Loss: 5.5966\n",
      "Epoch [1/1], Step [1074/8897], Loss: 5.4458\n",
      "Epoch [1/1], Step [1075/8897], Loss: 5.4909\n",
      "Epoch [1/1], Step [1076/8897], Loss: 5.4884\n",
      "Epoch [1/1], Step [1077/8897], Loss: 5.4015\n",
      "Epoch [1/1], Step [1078/8897], Loss: 5.3978\n",
      "Epoch [1/1], Step [1079/8897], Loss: 5.4496\n",
      "Epoch [1/1], Step [1080/8897], Loss: 5.6636\n",
      "Epoch [1/1], Step [1081/8897], Loss: 5.6539\n",
      "Epoch [1/1], Step [1082/8897], Loss: 5.3025\n",
      "Epoch [1/1], Step [1083/8897], Loss: 5.3867\n",
      "Epoch [1/1], Step [1084/8897], Loss: 5.3613\n",
      "Epoch [1/1], Step [1085/8897], Loss: 5.3894\n",
      "Epoch [1/1], Step [1086/8897], Loss: 5.2447\n",
      "Epoch [1/1], Step [1087/8897], Loss: 5.3117\n",
      "Epoch [1/1], Step [1088/8897], Loss: 5.2251\n",
      "Epoch [1/1], Step [1089/8897], Loss: 5.4142\n",
      "Epoch [1/1], Step [1090/8897], Loss: 5.6550\n",
      "Epoch [1/1], Step [1091/8897], Loss: 5.0350\n",
      "Epoch [1/1], Step [1092/8897], Loss: 5.4632\n",
      "Epoch [1/1], Step [1093/8897], Loss: 5.5161\n",
      "Epoch [1/1], Step [1094/8897], Loss: 5.4484\n",
      "Epoch [1/1], Step [1095/8897], Loss: 5.3147\n",
      "Epoch [1/1], Step [1096/8897], Loss: 5.3657\n",
      "Epoch [1/1], Step [1097/8897], Loss: 5.3012\n",
      "Epoch [1/1], Step [1098/8897], Loss: 5.4697\n",
      "Epoch [1/1], Step [1099/8897], Loss: 5.3595\n",
      "Epoch [1/1], Step [1100/8897], Loss: 5.5476\n",
      "Epoch [1/1], Step [1101/8897], Loss: 5.2485\n",
      "Epoch [1/1], Step [1102/8897], Loss: 5.1951\n",
      "Epoch [1/1], Step [1103/8897], Loss: 5.2381\n",
      "Epoch [1/1], Step [1104/8897], Loss: 5.4089\n",
      "Epoch [1/1], Step [1105/8897], Loss: 5.2550\n",
      "Epoch [1/1], Step [1106/8897], Loss: 5.4422\n",
      "Epoch [1/1], Step [1107/8897], Loss: 5.4925\n",
      "Epoch [1/1], Step [1108/8897], Loss: 5.5737\n",
      "Epoch [1/1], Step [1109/8897], Loss: 5.4974\n",
      "Epoch [1/1], Step [1110/8897], Loss: 5.3446\n",
      "Epoch [1/1], Step [1111/8897], Loss: 5.6577\n",
      "Epoch [1/1], Step [1112/8897], Loss: 5.5852\n",
      "Epoch [1/1], Step [1113/8897], Loss: 5.5571\n",
      "Epoch [1/1], Step [1114/8897], Loss: 5.3475\n",
      "Epoch [1/1], Step [1115/8897], Loss: 5.4622\n",
      "Epoch [1/1], Step [1116/8897], Loss: 5.2327\n",
      "Epoch [1/1], Step [1117/8897], Loss: 5.5660\n",
      "Epoch [1/1], Step [1118/8897], Loss: 5.6190\n",
      "Epoch [1/1], Step [1119/8897], Loss: 5.4923\n",
      "Epoch [1/1], Step [1120/8897], Loss: 5.4841\n",
      "Epoch [1/1], Step [1121/8897], Loss: 5.5091\n",
      "Epoch [1/1], Step [1122/8897], Loss: 5.6703\n",
      "Epoch [1/1], Step [1123/8897], Loss: 5.3397\n",
      "Epoch [1/1], Step [1124/8897], Loss: 5.1911\n",
      "Epoch [1/1], Step [1125/8897], Loss: 5.5345\n",
      "Epoch [1/1], Step [1126/8897], Loss: 5.3566\n",
      "Epoch [1/1], Step [1127/8897], Loss: 5.5732\n",
      "Epoch [1/1], Step [1128/8897], Loss: 5.4437\n",
      "Epoch [1/1], Step [1129/8897], Loss: 5.4433\n",
      "Epoch [1/1], Step [1130/8897], Loss: 5.4000\n",
      "Epoch [1/1], Step [1131/8897], Loss: 5.4158\n",
      "Epoch [1/1], Step [1132/8897], Loss: 5.4720\n",
      "Epoch [1/1], Step [1133/8897], Loss: 5.4336\n",
      "Epoch [1/1], Step [1134/8897], Loss: 5.4148\n",
      "Epoch [1/1], Step [1135/8897], Loss: 5.6393\n",
      "Epoch [1/1], Step [1136/8897], Loss: 5.5521\n",
      "Epoch [1/1], Step [1137/8897], Loss: 5.3829\n",
      "Epoch [1/1], Step [1138/8897], Loss: 5.6717\n",
      "Epoch [1/1], Step [1139/8897], Loss: 5.3686\n",
      "Epoch [1/1], Step [1140/8897], Loss: 5.4640\n",
      "Epoch [1/1], Step [1141/8897], Loss: 5.6643\n",
      "Epoch [1/1], Step [1142/8897], Loss: 5.3995\n",
      "Epoch [1/1], Step [1143/8897], Loss: 5.2036\n",
      "Epoch [1/1], Step [1144/8897], Loss: 5.2205\n",
      "Epoch [1/1], Step [1145/8897], Loss: 5.2556\n",
      "Epoch [1/1], Step [1146/8897], Loss: 5.5563\n",
      "Epoch [1/1], Step [1147/8897], Loss: 5.4816\n",
      "Epoch [1/1], Step [1148/8897], Loss: 5.4010\n",
      "Epoch [1/1], Step [1149/8897], Loss: 5.8027\n",
      "Epoch [1/1], Step [1150/8897], Loss: 5.3429\n",
      "Epoch [1/1], Step [1151/8897], Loss: 5.5495\n",
      "Epoch [1/1], Step [1152/8897], Loss: 5.5879\n",
      "Epoch [1/1], Step [1153/8897], Loss: 5.1894\n",
      "Epoch [1/1], Step [1154/8897], Loss: 5.4669\n",
      "Epoch [1/1], Step [1155/8897], Loss: 5.7984\n",
      "Epoch [1/1], Step [1156/8897], Loss: 5.5716\n",
      "Epoch [1/1], Step [1157/8897], Loss: 5.6202\n",
      "Epoch [1/1], Step [1158/8897], Loss: 5.4198\n",
      "Epoch [1/1], Step [1159/8897], Loss: 5.6389\n",
      "Epoch [1/1], Step [1160/8897], Loss: 5.4285\n",
      "Epoch [1/1], Step [1161/8897], Loss: 5.3996\n",
      "Epoch [1/1], Step [1162/8897], Loss: 5.3890\n",
      "Epoch [1/1], Step [1163/8897], Loss: 5.4751\n",
      "Epoch [1/1], Step [1164/8897], Loss: 5.2612\n",
      "Epoch [1/1], Step [1165/8897], Loss: 5.4727\n",
      "Epoch [1/1], Step [1166/8897], Loss: 5.5872\n",
      "Epoch [1/1], Step [1167/8897], Loss: 5.6116\n",
      "Epoch [1/1], Step [1168/8897], Loss: 5.3562\n",
      "Epoch [1/1], Step [1169/8897], Loss: 5.3167\n",
      "Epoch [1/1], Step [1170/8897], Loss: 5.4988\n",
      "Epoch [1/1], Step [1171/8897], Loss: 5.5578\n",
      "Epoch [1/1], Step [1172/8897], Loss: 5.1009\n",
      "Epoch [1/1], Step [1173/8897], Loss: 5.3372\n",
      "Epoch [1/1], Step [1174/8897], Loss: 5.4892\n",
      "Epoch [1/1], Step [1175/8897], Loss: 5.3045\n",
      "Epoch [1/1], Step [1176/8897], Loss: 5.4349\n",
      "Epoch [1/1], Step [1177/8897], Loss: 5.3415\n",
      "Epoch [1/1], Step [1178/8897], Loss: 5.6721\n",
      "Epoch [1/1], Step [1179/8897], Loss: 5.4911\n",
      "Epoch [1/1], Step [1180/8897], Loss: 5.4498\n",
      "Epoch [1/1], Step [1181/8897], Loss: 5.4575\n",
      "Epoch [1/1], Step [1182/8897], Loss: 5.4468\n",
      "Epoch [1/1], Step [1183/8897], Loss: 5.3899\n",
      "Epoch [1/1], Step [1184/8897], Loss: 5.4044\n",
      "Epoch [1/1], Step [1185/8897], Loss: 5.4871\n",
      "Epoch [1/1], Step [1186/8897], Loss: 5.3005\n",
      "Epoch [1/1], Step [1187/8897], Loss: 5.6238\n",
      "Epoch [1/1], Step [1188/8897], Loss: 5.4401\n",
      "Epoch [1/1], Step [1189/8897], Loss: 5.2191\n",
      "Epoch [1/1], Step [1190/8897], Loss: 5.7313\n",
      "Epoch [1/1], Step [1191/8897], Loss: 5.7126\n",
      "Epoch [1/1], Step [1192/8897], Loss: 5.3465\n",
      "Epoch [1/1], Step [1193/8897], Loss: 5.4709\n",
      "Epoch [1/1], Step [1194/8897], Loss: 5.3393\n",
      "Epoch [1/1], Step [1195/8897], Loss: 5.2576\n",
      "Epoch [1/1], Step [1196/8897], Loss: 5.4564\n",
      "Epoch [1/1], Step [1197/8897], Loss: 5.4011\n",
      "Epoch [1/1], Step [1198/8897], Loss: 5.2490\n",
      "Epoch [1/1], Step [1199/8897], Loss: 5.4145\n",
      "Epoch [1/1], Step [1200/8897], Loss: 5.4218\n",
      "Epoch [1/1], Step [1201/8897], Loss: 5.5623\n",
      "Epoch [1/1], Step [1202/8897], Loss: 5.2180\n",
      "Epoch [1/1], Step [1203/8897], Loss: 5.5688\n",
      "Epoch [1/1], Step [1204/8897], Loss: 5.6716\n",
      "Epoch [1/1], Step [1205/8897], Loss: 5.5488\n",
      "Epoch [1/1], Step [1206/8897], Loss: 5.3737\n",
      "Epoch [1/1], Step [1207/8897], Loss: 5.4509\n",
      "Epoch [1/1], Step [1208/8897], Loss: 5.4320\n",
      "Epoch [1/1], Step [1209/8897], Loss: 5.3874\n",
      "Epoch [1/1], Step [1210/8897], Loss: 5.3204\n",
      "Epoch [1/1], Step [1211/8897], Loss: 5.4268\n",
      "Epoch [1/1], Step [1212/8897], Loss: 5.3007\n",
      "Epoch [1/1], Step [1213/8897], Loss: 5.4765\n",
      "Epoch [1/1], Step [1214/8897], Loss: 5.5079\n",
      "Epoch [1/1], Step [1215/8897], Loss: 5.6812\n",
      "Epoch [1/1], Step [1216/8897], Loss: 5.5655\n",
      "Epoch [1/1], Step [1217/8897], Loss: 5.5854\n",
      "Epoch [1/1], Step [1218/8897], Loss: 5.5976\n",
      "Epoch [1/1], Step [1219/8897], Loss: 5.6004\n",
      "Epoch [1/1], Step [1220/8897], Loss: 5.6095\n",
      "Epoch [1/1], Step [1221/8897], Loss: 5.4205\n",
      "Epoch [1/1], Step [1222/8897], Loss: 5.3521\n",
      "Epoch [1/1], Step [1223/8897], Loss: 5.6690\n",
      "Epoch [1/1], Step [1224/8897], Loss: 5.6111\n",
      "Epoch [1/1], Step [1225/8897], Loss: 5.3350\n",
      "Epoch [1/1], Step [1226/8897], Loss: 5.3004\n",
      "Epoch [1/1], Step [1227/8897], Loss: 5.5369\n",
      "Epoch [1/1], Step [1228/8897], Loss: 5.3730\n",
      "Epoch [1/1], Step [1229/8897], Loss: 5.2873\n",
      "Epoch [1/1], Step [1230/8897], Loss: 5.3669\n",
      "Epoch [1/1], Step [1231/8897], Loss: 5.5846\n",
      "Epoch [1/1], Step [1232/8897], Loss: 5.2141\n",
      "Epoch [1/1], Step [1233/8897], Loss: 5.1501\n",
      "Epoch [1/1], Step [1234/8897], Loss: 5.4216\n",
      "Epoch [1/1], Step [1235/8897], Loss: 5.4870\n",
      "Epoch [1/1], Step [1236/8897], Loss: 5.4843\n",
      "Epoch [1/1], Step [1237/8897], Loss: 5.4270\n",
      "Epoch [1/1], Step [1238/8897], Loss: 5.5507\n",
      "Epoch [1/1], Step [1239/8897], Loss: 5.5581\n",
      "Epoch [1/1], Step [1240/8897], Loss: 5.3455\n",
      "Epoch [1/1], Step [1241/8897], Loss: 5.4357\n",
      "Epoch [1/1], Step [1242/8897], Loss: 5.2487\n",
      "Epoch [1/1], Step [1243/8897], Loss: 5.4019\n",
      "Epoch [1/1], Step [1244/8897], Loss: 5.1555\n",
      "Epoch [1/1], Step [1245/8897], Loss: 5.4989\n",
      "Epoch [1/1], Step [1246/8897], Loss: 5.2806\n",
      "Epoch [1/1], Step [1247/8897], Loss: 5.4904\n",
      "Epoch [1/1], Step [1248/8897], Loss: 5.0608\n",
      "Epoch [1/1], Step [1249/8897], Loss: 5.4094\n",
      "Epoch [1/1], Step [1250/8897], Loss: 5.3476\n",
      "Epoch [1/1], Step [1251/8897], Loss: 5.2733\n",
      "Epoch [1/1], Step [1252/8897], Loss: 5.1605\n",
      "Epoch [1/1], Step [1253/8897], Loss: 5.2970\n",
      "Epoch [1/1], Step [1254/8897], Loss: 5.2734\n",
      "Epoch [1/1], Step [1255/8897], Loss: 5.3768\n",
      "Epoch [1/1], Step [1256/8897], Loss: 5.4794\n",
      "Epoch [1/1], Step [1257/8897], Loss: 5.4624\n",
      "Epoch [1/1], Step [1258/8897], Loss: 5.5592\n",
      "Epoch [1/1], Step [1259/8897], Loss: 5.6599\n",
      "Epoch [1/1], Step [1260/8897], Loss: 5.3526\n",
      "Epoch [1/1], Step [1261/8897], Loss: 5.3704\n",
      "Epoch [1/1], Step [1262/8897], Loss: 5.2879\n",
      "Epoch [1/1], Step [1263/8897], Loss: 5.5375\n",
      "Epoch [1/1], Step [1264/8897], Loss: 5.8193\n",
      "Epoch [1/1], Step [1265/8897], Loss: 5.6629\n",
      "Epoch [1/1], Step [1266/8897], Loss: 5.6008\n",
      "Epoch [1/1], Step [1267/8897], Loss: 5.4408\n",
      "Epoch [1/1], Step [1268/8897], Loss: 5.5970\n",
      "Epoch [1/1], Step [1269/8897], Loss: 5.5445\n",
      "Epoch [1/1], Step [1270/8897], Loss: 5.6503\n",
      "Epoch [1/1], Step [1271/8897], Loss: 5.4292\n",
      "Epoch [1/1], Step [1272/8897], Loss: 5.2610\n",
      "Epoch [1/1], Step [1273/8897], Loss: 5.5045\n",
      "Epoch [1/1], Step [1274/8897], Loss: 5.8494\n",
      "Epoch [1/1], Step [1275/8897], Loss: 5.3315\n",
      "Epoch [1/1], Step [1276/8897], Loss: 5.4913\n",
      "Epoch [1/1], Step [1277/8897], Loss: 5.2779\n",
      "Epoch [1/1], Step [1278/8897], Loss: 5.3513\n",
      "Epoch [1/1], Step [1279/8897], Loss: 5.3859\n",
      "Epoch [1/1], Step [1280/8897], Loss: 5.4185\n",
      "Epoch [1/1], Step [1281/8897], Loss: 5.2982\n",
      "Epoch [1/1], Step [1282/8897], Loss: 5.4109\n",
      "Epoch [1/1], Step [1283/8897], Loss: 5.2119\n",
      "Epoch [1/1], Step [1284/8897], Loss: 5.3741\n",
      "Epoch [1/1], Step [1285/8897], Loss: 5.4476\n",
      "Epoch [1/1], Step [1286/8897], Loss: 5.4795\n",
      "Epoch [1/1], Step [1287/8897], Loss: 5.5449\n",
      "Epoch [1/1], Step [1288/8897], Loss: 5.5825\n",
      "Epoch [1/1], Step [1289/8897], Loss: 5.4924\n",
      "Epoch [1/1], Step [1290/8897], Loss: 5.4173\n",
      "Epoch [1/1], Step [1291/8897], Loss: 5.5068\n",
      "Epoch [1/1], Step [1292/8897], Loss: 5.5619\n",
      "Epoch [1/1], Step [1293/8897], Loss: 5.5926\n",
      "Epoch [1/1], Step [1294/8897], Loss: 5.2283\n",
      "Epoch [1/1], Step [1295/8897], Loss: 5.4649\n",
      "Epoch [1/1], Step [1296/8897], Loss: 5.2655\n",
      "Epoch [1/1], Step [1297/8897], Loss: 5.6889\n",
      "Epoch [1/1], Step [1298/8897], Loss: 5.2619\n",
      "Epoch [1/1], Step [1299/8897], Loss: 5.2683\n",
      "Epoch [1/1], Step [1300/8897], Loss: 5.3564\n",
      "Epoch [1/1], Step [1301/8897], Loss: 5.2367\n",
      "Epoch [1/1], Step [1302/8897], Loss: 5.5922\n",
      "Epoch [1/1], Step [1303/8897], Loss: 5.1737\n",
      "Epoch [1/1], Step [1304/8897], Loss: 5.2362\n",
      "Epoch [1/1], Step [1305/8897], Loss: 5.2960\n",
      "Epoch [1/1], Step [1306/8897], Loss: 5.1785\n",
      "Epoch [1/1], Step [1307/8897], Loss: 5.7553\n",
      "Epoch [1/1], Step [1308/8897], Loss: 5.5748\n",
      "Epoch [1/1], Step [1309/8897], Loss: 5.6019\n",
      "Epoch [1/1], Step [1310/8897], Loss: 5.7150\n",
      "Epoch [1/1], Step [1311/8897], Loss: 5.6630\n",
      "Epoch [1/1], Step [1312/8897], Loss: 5.3209\n",
      "Epoch [1/1], Step [1313/8897], Loss: 5.6201\n",
      "Epoch [1/1], Step [1314/8897], Loss: 5.2139\n",
      "Epoch [1/1], Step [1315/8897], Loss: 5.4172\n",
      "Epoch [1/1], Step [1316/8897], Loss: 5.3643\n",
      "Epoch [1/1], Step [1317/8897], Loss: 5.5575\n",
      "Epoch [1/1], Step [1318/8897], Loss: 5.6808\n",
      "Epoch [1/1], Step [1319/8897], Loss: 5.4304\n",
      "Epoch [1/1], Step [1320/8897], Loss: 5.3888\n",
      "Epoch [1/1], Step [1321/8897], Loss: 5.2097\n",
      "Epoch [1/1], Step [1322/8897], Loss: 5.3114\n",
      "Epoch [1/1], Step [1323/8897], Loss: 5.4662\n",
      "Epoch [1/1], Step [1324/8897], Loss: 5.3788\n",
      "Epoch [1/1], Step [1325/8897], Loss: 5.1743\n",
      "Epoch [1/1], Step [1326/8897], Loss: 5.5868\n",
      "Epoch [1/1], Step [1327/8897], Loss: 5.6166\n",
      "Epoch [1/1], Step [1328/8897], Loss: 5.1721\n",
      "Epoch [1/1], Step [1329/8897], Loss: 5.5235\n",
      "Epoch [1/1], Step [1330/8897], Loss: 5.5531\n",
      "Epoch [1/1], Step [1331/8897], Loss: 5.6069\n",
      "Epoch [1/1], Step [1332/8897], Loss: 5.7640\n",
      "Epoch [1/1], Step [1333/8897], Loss: 5.2901\n",
      "Epoch [1/1], Step [1334/8897], Loss: 5.5173\n",
      "Epoch [1/1], Step [1335/8897], Loss: 5.2120\n",
      "Epoch [1/1], Step [1336/8897], Loss: 5.3994\n",
      "Epoch [1/1], Step [1337/8897], Loss: 5.5097\n",
      "Epoch [1/1], Step [1338/8897], Loss: 5.2973\n",
      "Epoch [1/1], Step [1339/8897], Loss: 5.4936\n",
      "Epoch [1/1], Step [1340/8897], Loss: 5.4402\n",
      "Epoch [1/1], Step [1341/8897], Loss: 5.1767\n",
      "Epoch [1/1], Step [1342/8897], Loss: 5.3151\n",
      "Epoch [1/1], Step [1343/8897], Loss: 5.4819\n",
      "Epoch [1/1], Step [1344/8897], Loss: 5.5440\n",
      "Epoch [1/1], Step [1345/8897], Loss: 5.3217\n",
      "Epoch [1/1], Step [1346/8897], Loss: 5.3877\n",
      "Epoch [1/1], Step [1347/8897], Loss: 5.4634\n",
      "Epoch [1/1], Step [1348/8897], Loss: 5.6062\n",
      "Epoch [1/1], Step [1349/8897], Loss: 5.4531\n",
      "Epoch [1/1], Step [1350/8897], Loss: 5.5497\n",
      "Epoch [1/1], Step [1351/8897], Loss: 5.3143\n",
      "Epoch [1/1], Step [1352/8897], Loss: 5.3908\n",
      "Epoch [1/1], Step [1353/8897], Loss: 5.3120\n",
      "Epoch [1/1], Step [1354/8897], Loss: 5.3044\n",
      "Epoch [1/1], Step [1355/8897], Loss: 5.5263\n",
      "Epoch [1/1], Step [1356/8897], Loss: 5.4845\n",
      "Epoch [1/1], Step [1357/8897], Loss: 5.6010\n",
      "Epoch [1/1], Step [1358/8897], Loss: 5.5657\n",
      "Epoch [1/1], Step [1359/8897], Loss: 5.5400\n",
      "Epoch [1/1], Step [1360/8897], Loss: 5.4654\n",
      "Epoch [1/1], Step [1361/8897], Loss: 5.1805\n",
      "Epoch [1/1], Step [1362/8897], Loss: 5.6137\n",
      "Epoch [1/1], Step [1363/8897], Loss: 5.3576\n",
      "Epoch [1/1], Step [1364/8897], Loss: 5.3392\n",
      "Epoch [1/1], Step [1365/8897], Loss: 5.3752\n",
      "Epoch [1/1], Step [1366/8897], Loss: 5.6085\n",
      "Epoch [1/1], Step [1367/8897], Loss: 5.7080\n",
      "Epoch [1/1], Step [1368/8897], Loss: 5.3322\n",
      "Epoch [1/1], Step [1369/8897], Loss: 5.4854\n",
      "Epoch [1/1], Step [1370/8897], Loss: 5.4297\n",
      "Epoch [1/1], Step [1371/8897], Loss: 5.5306\n",
      "Epoch [1/1], Step [1372/8897], Loss: 5.3691\n",
      "Epoch [1/1], Step [1373/8897], Loss: 5.1027\n",
      "Epoch [1/1], Step [1374/8897], Loss: 5.4712\n",
      "Epoch [1/1], Step [1375/8897], Loss: 5.2472\n",
      "Epoch [1/1], Step [1376/8897], Loss: 5.5747\n",
      "Epoch [1/1], Step [1377/8897], Loss: 5.4639\n",
      "Epoch [1/1], Step [1378/8897], Loss: 5.5676\n",
      "Epoch [1/1], Step [1379/8897], Loss: 5.3925\n",
      "Epoch [1/1], Step [1380/8897], Loss: 5.3737\n",
      "Epoch [1/1], Step [1381/8897], Loss: 5.2448\n",
      "Epoch [1/1], Step [1382/8897], Loss: 5.2201\n",
      "Epoch [1/1], Step [1383/8897], Loss: 5.2395\n",
      "Epoch [1/1], Step [1384/8897], Loss: 5.3777\n",
      "Epoch [1/1], Step [1385/8897], Loss: 5.6391\n",
      "Epoch [1/1], Step [1386/8897], Loss: 5.3798\n",
      "Epoch [1/1], Step [1387/8897], Loss: 5.4600\n",
      "Epoch [1/1], Step [1388/8897], Loss: 5.2219\n",
      "Epoch [1/1], Step [1389/8897], Loss: 5.1024\n",
      "Epoch [1/1], Step [1390/8897], Loss: 5.5335\n",
      "Epoch [1/1], Step [1391/8897], Loss: 5.5755\n",
      "Epoch [1/1], Step [1392/8897], Loss: 5.7149\n",
      "Epoch [1/1], Step [1393/8897], Loss: 5.5651\n",
      "Epoch [1/1], Step [1394/8897], Loss: 5.6149\n",
      "Epoch [1/1], Step [1395/8897], Loss: 5.4327\n",
      "Epoch [1/1], Step [1396/8897], Loss: 5.3440\n",
      "Epoch [1/1], Step [1397/8897], Loss: 5.3264\n",
      "Epoch [1/1], Step [1398/8897], Loss: 5.5461\n",
      "Epoch [1/1], Step [1399/8897], Loss: 5.4989\n",
      "Epoch [1/1], Step [1400/8897], Loss: 5.3233\n",
      "Epoch [1/1], Step [1401/8897], Loss: 5.6560\n",
      "Epoch [1/1], Step [1402/8897], Loss: 5.2634\n",
      "Epoch [1/1], Step [1403/8897], Loss: 5.1326\n",
      "Epoch [1/1], Step [1404/8897], Loss: 5.3505\n",
      "Epoch [1/1], Step [1405/8897], Loss: 5.4420\n",
      "Epoch [1/1], Step [1406/8897], Loss: 5.5242\n",
      "Epoch [1/1], Step [1407/8897], Loss: 5.4720\n",
      "Epoch [1/1], Step [1408/8897], Loss: 5.5485\n",
      "Epoch [1/1], Step [1409/8897], Loss: 5.3903\n",
      "Epoch [1/1], Step [1410/8897], Loss: 5.3007\n",
      "Epoch [1/1], Step [1411/8897], Loss: 5.6362\n",
      "Epoch [1/1], Step [1412/8897], Loss: 5.5464\n",
      "Epoch [1/1], Step [1413/8897], Loss: 5.6373\n",
      "Epoch [1/1], Step [1414/8897], Loss: 5.5996\n",
      "Epoch [1/1], Step [1415/8897], Loss: 5.4007\n",
      "Epoch [1/1], Step [1416/8897], Loss: 5.3202\n",
      "Epoch [1/1], Step [1417/8897], Loss: 5.5418\n",
      "Epoch [1/1], Step [1418/8897], Loss: 5.2928\n",
      "Epoch [1/1], Step [1419/8897], Loss: 5.4512\n",
      "Epoch [1/1], Step [1420/8897], Loss: 5.4331\n",
      "Epoch [1/1], Step [1421/8897], Loss: 5.1929\n",
      "Epoch [1/1], Step [1422/8897], Loss: 5.5863\n",
      "Epoch [1/1], Step [1423/8897], Loss: 5.5302\n",
      "Epoch [1/1], Step [1424/8897], Loss: 5.6182\n",
      "Epoch [1/1], Step [1425/8897], Loss: 5.4643\n",
      "Epoch [1/1], Step [1426/8897], Loss: 5.3398\n",
      "Epoch [1/1], Step [1427/8897], Loss: 5.1817\n",
      "Epoch [1/1], Step [1428/8897], Loss: 5.2167\n",
      "Epoch [1/1], Step [1429/8897], Loss: 5.6210\n",
      "Epoch [1/1], Step [1430/8897], Loss: 5.3195\n",
      "Epoch [1/1], Step [1431/8897], Loss: 5.2448\n",
      "Epoch [1/1], Step [1432/8897], Loss: 5.5700\n",
      "Epoch [1/1], Step [1433/8897], Loss: 5.3211\n",
      "Epoch [1/1], Step [1434/8897], Loss: 5.7733\n",
      "Epoch [1/1], Step [1435/8897], Loss: 5.8128\n",
      "Epoch [1/1], Step [1436/8897], Loss: 5.5371\n",
      "Epoch [1/1], Step [1437/8897], Loss: 5.4838\n",
      "Epoch [1/1], Step [1438/8897], Loss: 5.6024\n",
      "Epoch [1/1], Step [1439/8897], Loss: 5.5345\n",
      "Epoch [1/1], Step [1440/8897], Loss: 5.2248\n",
      "Epoch [1/1], Step [1441/8897], Loss: 5.3985\n",
      "Epoch [1/1], Step [1442/8897], Loss: 5.6099\n",
      "Epoch [1/1], Step [1443/8897], Loss: 5.4055\n",
      "Epoch [1/1], Step [1444/8897], Loss: 5.3997\n",
      "Epoch [1/1], Step [1445/8897], Loss: 5.2935\n",
      "Epoch [1/1], Step [1446/8897], Loss: 5.4889\n",
      "Epoch [1/1], Step [1447/8897], Loss: 5.3710\n",
      "Epoch [1/1], Step [1448/8897], Loss: 5.4871\n",
      "Epoch [1/1], Step [1449/8897], Loss: 5.5890\n",
      "Epoch [1/1], Step [1450/8897], Loss: 5.2579\n",
      "Epoch [1/1], Step [1451/8897], Loss: 5.2546\n",
      "Epoch [1/1], Step [1452/8897], Loss: 5.4076\n",
      "Epoch [1/1], Step [1453/8897], Loss: 5.2737\n",
      "Epoch [1/1], Step [1454/8897], Loss: 5.5155\n",
      "Epoch [1/1], Step [1455/8897], Loss: 5.4627\n",
      "Epoch [1/1], Step [1456/8897], Loss: 5.5577\n",
      "Epoch [1/1], Step [1457/8897], Loss: 5.3438\n",
      "Epoch [1/1], Step [1458/8897], Loss: 5.4556\n",
      "Epoch [1/1], Step [1459/8897], Loss: 5.3050\n",
      "Epoch [1/1], Step [1460/8897], Loss: 5.5589\n",
      "Epoch [1/1], Step [1461/8897], Loss: 5.4928\n",
      "Epoch [1/1], Step [1462/8897], Loss: 5.6188\n",
      "Epoch [1/1], Step [1463/8897], Loss: 5.6372\n",
      "Epoch [1/1], Step [1464/8897], Loss: 5.3579\n",
      "Epoch [1/1], Step [1465/8897], Loss: 5.3936\n",
      "Epoch [1/1], Step [1466/8897], Loss: 5.3676\n",
      "Epoch [1/1], Step [1467/8897], Loss: 5.3573\n",
      "Epoch [1/1], Step [1468/8897], Loss: 5.3812\n",
      "Epoch [1/1], Step [1469/8897], Loss: 5.4183\n",
      "Epoch [1/1], Step [1470/8897], Loss: 5.4450\n",
      "Epoch [1/1], Step [1471/8897], Loss: 5.4317\n",
      "Epoch [1/1], Step [1472/8897], Loss: 5.3062\n",
      "Epoch [1/1], Step [1473/8897], Loss: 5.3683\n",
      "Epoch [1/1], Step [1474/8897], Loss: 5.3964\n",
      "Epoch [1/1], Step [1475/8897], Loss: 5.3615\n",
      "Epoch [1/1], Step [1476/8897], Loss: 5.7468\n",
      "Epoch [1/1], Step [1477/8897], Loss: 5.4923\n",
      "Epoch [1/1], Step [1478/8897], Loss: 5.3931\n",
      "Epoch [1/1], Step [1479/8897], Loss: 5.3065\n",
      "Epoch [1/1], Step [1480/8897], Loss: 5.4648\n",
      "Epoch [1/1], Step [1481/8897], Loss: 5.7013\n",
      "Epoch [1/1], Step [1482/8897], Loss: 5.3065\n",
      "Epoch [1/1], Step [1483/8897], Loss: 5.3778\n",
      "Epoch [1/1], Step [1484/8897], Loss: 5.2879\n",
      "Epoch [1/1], Step [1485/8897], Loss: 5.4280\n",
      "Epoch [1/1], Step [1486/8897], Loss: 5.5035\n",
      "Epoch [1/1], Step [1487/8897], Loss: 5.2552\n",
      "Epoch [1/1], Step [1488/8897], Loss: 5.4989\n",
      "Epoch [1/1], Step [1489/8897], Loss: 5.2810\n",
      "Epoch [1/1], Step [1490/8897], Loss: 5.5220\n",
      "Epoch [1/1], Step [1491/8897], Loss: 5.8098\n",
      "Epoch [1/1], Step [1492/8897], Loss: 5.3011\n",
      "Epoch [1/1], Step [1493/8897], Loss: 5.4826\n",
      "Epoch [1/1], Step [1494/8897], Loss: 5.5308\n",
      "Epoch [1/1], Step [1495/8897], Loss: 5.4772\n",
      "Epoch [1/1], Step [1496/8897], Loss: 5.4922\n",
      "Epoch [1/1], Step [1497/8897], Loss: 5.5020\n",
      "Epoch [1/1], Step [1498/8897], Loss: 5.6779\n",
      "Epoch [1/1], Step [1499/8897], Loss: 5.4761\n",
      "Epoch [1/1], Step [1500/8897], Loss: 5.4197\n",
      "Epoch [1/1], Step [1501/8897], Loss: 5.1225\n",
      "Epoch [1/1], Step [1502/8897], Loss: 5.4570\n",
      "Epoch [1/1], Step [1503/8897], Loss: 5.3120\n",
      "Epoch [1/1], Step [1504/8897], Loss: 5.6678\n",
      "Epoch [1/1], Step [1505/8897], Loss: 5.4651\n",
      "Epoch [1/1], Step [1506/8897], Loss: 5.5414\n",
      "Epoch [1/1], Step [1507/8897], Loss: 5.2652\n",
      "Epoch [1/1], Step [1508/8897], Loss: 5.4717\n",
      "Epoch [1/1], Step [1509/8897], Loss: 5.7399\n",
      "Epoch [1/1], Step [1510/8897], Loss: 5.4985\n",
      "Epoch [1/1], Step [1511/8897], Loss: 5.5200\n",
      "Epoch [1/1], Step [1512/8897], Loss: 5.5061\n",
      "Epoch [1/1], Step [1513/8897], Loss: 5.5747\n",
      "Epoch [1/1], Step [1514/8897], Loss: 5.3773\n",
      "Epoch [1/1], Step [1515/8897], Loss: 5.3877\n",
      "Epoch [1/1], Step [1516/8897], Loss: 5.3310\n",
      "Epoch [1/1], Step [1517/8897], Loss: 5.5462\n",
      "Epoch [1/1], Step [1518/8897], Loss: 5.3988\n",
      "Epoch [1/1], Step [1519/8897], Loss: 5.2062\n",
      "Epoch [1/1], Step [1520/8897], Loss: 5.5045\n",
      "Epoch [1/1], Step [1521/8897], Loss: 5.5125\n",
      "Epoch [1/1], Step [1522/8897], Loss: 5.4337\n",
      "Epoch [1/1], Step [1523/8897], Loss: 5.2408\n",
      "Epoch [1/1], Step [1524/8897], Loss: 5.5483\n",
      "Epoch [1/1], Step [1525/8897], Loss: 5.4487\n",
      "Epoch [1/1], Step [1526/8897], Loss: 5.4585\n",
      "Epoch [1/1], Step [1527/8897], Loss: 5.4497\n",
      "Epoch [1/1], Step [1528/8897], Loss: 5.4652\n",
      "Epoch [1/1], Step [1529/8897], Loss: 5.2659\n",
      "Epoch [1/1], Step [1530/8897], Loss: 5.5879\n",
      "Epoch [1/1], Step [1531/8897], Loss: 5.5246\n",
      "Epoch [1/1], Step [1532/8897], Loss: 5.4535\n",
      "Epoch [1/1], Step [1533/8897], Loss: 5.6022\n",
      "Epoch [1/1], Step [1534/8897], Loss: 5.5913\n",
      "Epoch [1/1], Step [1535/8897], Loss: 5.5562\n",
      "Epoch [1/1], Step [1536/8897], Loss: 5.3060\n",
      "Epoch [1/1], Step [1537/8897], Loss: 5.5226\n",
      "Epoch [1/1], Step [1538/8897], Loss: 5.5146\n",
      "Epoch [1/1], Step [1539/8897], Loss: 5.4658\n",
      "Epoch [1/1], Step [1540/8897], Loss: 5.5688\n",
      "Epoch [1/1], Step [1541/8897], Loss: 5.4347\n",
      "Epoch [1/1], Step [1542/8897], Loss: 5.3014\n",
      "Epoch [1/1], Step [1543/8897], Loss: 5.4098\n",
      "Epoch [1/1], Step [1544/8897], Loss: 5.3107\n",
      "Epoch [1/1], Step [1545/8897], Loss: 5.3161\n",
      "Epoch [1/1], Step [1546/8897], Loss: 5.4487\n",
      "Epoch [1/1], Step [1547/8897], Loss: 5.3218\n",
      "Epoch [1/1], Step [1548/8897], Loss: 5.1042\n",
      "Epoch [1/1], Step [1549/8897], Loss: 5.2493\n",
      "Epoch [1/1], Step [1550/8897], Loss: 5.3503\n",
      "Epoch [1/1], Step [1551/8897], Loss: 5.4201\n",
      "Epoch [1/1], Step [1552/8897], Loss: 5.4998\n",
      "Epoch [1/1], Step [1553/8897], Loss: 5.4413\n",
      "Epoch [1/1], Step [1554/8897], Loss: 5.5698\n",
      "Epoch [1/1], Step [1555/8897], Loss: 5.2701\n",
      "Epoch [1/1], Step [1556/8897], Loss: 5.1362\n",
      "Epoch [1/1], Step [1557/8897], Loss: 5.5622\n",
      "Epoch [1/1], Step [1558/8897], Loss: 5.1879\n",
      "Epoch [1/1], Step [1559/8897], Loss: 5.3979\n",
      "Epoch [1/1], Step [1560/8897], Loss: 5.6712\n",
      "Epoch [1/1], Step [1561/8897], Loss: 5.1268\n",
      "Epoch [1/1], Step [1562/8897], Loss: 5.5353\n",
      "Epoch [1/1], Step [1563/8897], Loss: 5.4587\n",
      "Epoch [1/1], Step [1564/8897], Loss: 5.7145\n",
      "Epoch [1/1], Step [1565/8897], Loss: 5.3870\n",
      "Epoch [1/1], Step [1566/8897], Loss: 5.2924\n",
      "Epoch [1/1], Step [1567/8897], Loss: 5.2661\n",
      "Epoch [1/1], Step [1568/8897], Loss: 5.4268\n",
      "Epoch [1/1], Step [1569/8897], Loss: 5.2136\n",
      "Epoch [1/1], Step [1570/8897], Loss: 5.6849\n",
      "Epoch [1/1], Step [1571/8897], Loss: 5.4013\n",
      "Epoch [1/1], Step [1572/8897], Loss: 5.1466\n",
      "Epoch [1/1], Step [1573/8897], Loss: 5.4943\n",
      "Epoch [1/1], Step [1574/8897], Loss: 5.3640\n",
      "Epoch [1/1], Step [1575/8897], Loss: 5.5015\n",
      "Epoch [1/1], Step [1576/8897], Loss: 5.4836\n",
      "Epoch [1/1], Step [1577/8897], Loss: 5.4195\n",
      "Epoch [1/1], Step [1578/8897], Loss: 5.6966\n",
      "Epoch [1/1], Step [1579/8897], Loss: 5.4551\n",
      "Epoch [1/1], Step [1580/8897], Loss: 5.4793\n",
      "Epoch [1/1], Step [1581/8897], Loss: 5.4054\n",
      "Epoch [1/1], Step [1582/8897], Loss: 5.5676\n",
      "Epoch [1/1], Step [1583/8897], Loss: 5.3268\n",
      "Epoch [1/1], Step [1584/8897], Loss: 5.1284\n",
      "Epoch [1/1], Step [1585/8897], Loss: 5.6545\n",
      "Epoch [1/1], Step [1586/8897], Loss: 5.2037\n",
      "Epoch [1/1], Step [1587/8897], Loss: 5.3521\n",
      "Epoch [1/1], Step [1588/8897], Loss: 5.3777\n",
      "Epoch [1/1], Step [1589/8897], Loss: 5.5225\n",
      "Epoch [1/1], Step [1590/8897], Loss: 5.4105\n",
      "Epoch [1/1], Step [1591/8897], Loss: 5.4370\n",
      "Epoch [1/1], Step [1592/8897], Loss: 5.1162\n",
      "Epoch [1/1], Step [1593/8897], Loss: 5.5720\n",
      "Epoch [1/1], Step [1594/8897], Loss: 5.5016\n",
      "Epoch [1/1], Step [1595/8897], Loss: 5.4336\n",
      "Epoch [1/1], Step [1596/8897], Loss: 5.5247\n",
      "Epoch [1/1], Step [1597/8897], Loss: 5.4567\n",
      "Epoch [1/1], Step [1598/8897], Loss: 5.5984\n",
      "Epoch [1/1], Step [1599/8897], Loss: 5.5881\n",
      "Epoch [1/1], Step [1600/8897], Loss: 5.3869\n",
      "Epoch [1/1], Step [1601/8897], Loss: 5.3496\n",
      "Epoch [1/1], Step [1602/8897], Loss: 5.3459\n",
      "Epoch [1/1], Step [1603/8897], Loss: 5.6869\n",
      "Epoch [1/1], Step [1604/8897], Loss: 5.3846\n",
      "Epoch [1/1], Step [1605/8897], Loss: 5.5689\n",
      "Epoch [1/1], Step [1606/8897], Loss: 5.5888\n",
      "Epoch [1/1], Step [1607/8897], Loss: 5.2815\n",
      "Epoch [1/1], Step [1608/8897], Loss: 5.4320\n",
      "Epoch [1/1], Step [1609/8897], Loss: 5.3593\n",
      "Epoch [1/1], Step [1610/8897], Loss: 5.4055\n",
      "Epoch [1/1], Step [1611/8897], Loss: 5.6356\n",
      "Epoch [1/1], Step [1612/8897], Loss: 5.6457\n",
      "Epoch [1/1], Step [1613/8897], Loss: 5.3436\n",
      "Epoch [1/1], Step [1614/8897], Loss: 5.2712\n",
      "Epoch [1/1], Step [1615/8897], Loss: 5.2677\n",
      "Epoch [1/1], Step [1616/8897], Loss: 5.5322\n",
      "Epoch [1/1], Step [1617/8897], Loss: 5.7419\n",
      "Epoch [1/1], Step [1618/8897], Loss: 5.3116\n",
      "Epoch [1/1], Step [1619/8897], Loss: 5.2695\n",
      "Epoch [1/1], Step [1620/8897], Loss: 5.4354\n",
      "Epoch [1/1], Step [1621/8897], Loss: 5.3598\n",
      "Epoch [1/1], Step [1622/8897], Loss: 5.2266\n",
      "Epoch [1/1], Step [1623/8897], Loss: 5.5673\n",
      "Epoch [1/1], Step [1624/8897], Loss: 5.6105\n",
      "Epoch [1/1], Step [1625/8897], Loss: 5.5946\n",
      "Epoch [1/1], Step [1626/8897], Loss: 5.4789\n",
      "Epoch [1/1], Step [1627/8897], Loss: 5.4671\n",
      "Epoch [1/1], Step [1628/8897], Loss: 5.3400\n",
      "Epoch [1/1], Step [1629/8897], Loss: 5.5074\n",
      "Epoch [1/1], Step [1630/8897], Loss: 5.6179\n",
      "Epoch [1/1], Step [1631/8897], Loss: 5.7043\n",
      "Epoch [1/1], Step [1632/8897], Loss: 5.3635\n",
      "Epoch [1/1], Step [1633/8897], Loss: 5.5799\n",
      "Epoch [1/1], Step [1634/8897], Loss: 5.3859\n",
      "Epoch [1/1], Step [1635/8897], Loss: 5.5603\n",
      "Epoch [1/1], Step [1636/8897], Loss: 5.7645\n",
      "Epoch [1/1], Step [1637/8897], Loss: 5.3793\n",
      "Epoch [1/1], Step [1638/8897], Loss: 5.7184\n",
      "Epoch [1/1], Step [1639/8897], Loss: 5.6006\n",
      "Epoch [1/1], Step [1640/8897], Loss: 5.5956\n",
      "Epoch [1/1], Step [1641/8897], Loss: 5.3349\n",
      "Epoch [1/1], Step [1642/8897], Loss: 5.5035\n",
      "Epoch [1/1], Step [1643/8897], Loss: 5.4790\n",
      "Epoch [1/1], Step [1644/8897], Loss: 5.5431\n",
      "Epoch [1/1], Step [1645/8897], Loss: 5.3876\n",
      "Epoch [1/1], Step [1646/8897], Loss: 5.1782\n",
      "Epoch [1/1], Step [1647/8897], Loss: 5.3392\n",
      "Epoch [1/1], Step [1648/8897], Loss: 5.6679\n",
      "Epoch [1/1], Step [1649/8897], Loss: 5.5374\n",
      "Epoch [1/1], Step [1650/8897], Loss: 5.2567\n",
      "Epoch [1/1], Step [1651/8897], Loss: 5.5358\n",
      "Epoch [1/1], Step [1652/8897], Loss: 5.3122\n",
      "Epoch [1/1], Step [1653/8897], Loss: 5.2620\n",
      "Epoch [1/1], Step [1654/8897], Loss: 5.4805\n",
      "Epoch [1/1], Step [1655/8897], Loss: 5.3984\n",
      "Epoch [1/1], Step [1656/8897], Loss: 5.3113\n",
      "Epoch [1/1], Step [1657/8897], Loss: 5.4126\n",
      "Epoch [1/1], Step [1658/8897], Loss: 5.7485\n",
      "Epoch [1/1], Step [1659/8897], Loss: 5.6647\n",
      "Epoch [1/1], Step [1660/8897], Loss: 5.5078\n",
      "Epoch [1/1], Step [1661/8897], Loss: 5.4523\n",
      "Epoch [1/1], Step [1662/8897], Loss: 5.4494\n",
      "Epoch [1/1], Step [1663/8897], Loss: 5.5077\n",
      "Epoch [1/1], Step [1664/8897], Loss: 5.4144\n",
      "Epoch [1/1], Step [1665/8897], Loss: 5.4433\n",
      "Epoch [1/1], Step [1666/8897], Loss: 5.3854\n",
      "Epoch [1/1], Step [1667/8897], Loss: 5.4836\n",
      "Epoch [1/1], Step [1668/8897], Loss: 5.4544\n",
      "Epoch [1/1], Step [1669/8897], Loss: 5.3776\n",
      "Epoch [1/1], Step [1670/8897], Loss: 5.4242\n",
      "Epoch [1/1], Step [1671/8897], Loss: 5.3070\n",
      "Epoch [1/1], Step [1672/8897], Loss: 5.3255\n",
      "Epoch [1/1], Step [1673/8897], Loss: 5.2604\n",
      "Epoch [1/1], Step [1674/8897], Loss: 5.2892\n",
      "Epoch [1/1], Step [1675/8897], Loss: 5.5056\n",
      "Epoch [1/1], Step [1676/8897], Loss: 5.3379\n",
      "Epoch [1/1], Step [1677/8897], Loss: 5.3098\n",
      "Epoch [1/1], Step [1678/8897], Loss: 5.5494\n",
      "Epoch [1/1], Step [1679/8897], Loss: 5.3057\n",
      "Epoch [1/1], Step [1680/8897], Loss: 5.5327\n",
      "Epoch [1/1], Step [1681/8897], Loss: 5.3990\n",
      "Epoch [1/1], Step [1682/8897], Loss: 5.3276\n",
      "Epoch [1/1], Step [1683/8897], Loss: 5.3662\n",
      "Epoch [1/1], Step [1684/8897], Loss: 5.8238\n",
      "Epoch [1/1], Step [1685/8897], Loss: 5.3755\n",
      "Epoch [1/1], Step [1686/8897], Loss: 5.4546\n",
      "Epoch [1/1], Step [1687/8897], Loss: 5.3177\n",
      "Epoch [1/1], Step [1688/8897], Loss: 5.5828\n",
      "Epoch [1/1], Step [1689/8897], Loss: 5.3827\n",
      "Epoch [1/1], Step [1690/8897], Loss: 5.6408\n",
      "Epoch [1/1], Step [1691/8897], Loss: 5.6231\n",
      "Epoch [1/1], Step [1692/8897], Loss: 5.3057\n",
      "Epoch [1/1], Step [1693/8897], Loss: 5.4304\n",
      "Epoch [1/1], Step [1694/8897], Loss: 5.2672\n",
      "Epoch [1/1], Step [1695/8897], Loss: 5.5422\n",
      "Epoch [1/1], Step [1696/8897], Loss: 5.5733\n",
      "Epoch [1/1], Step [1697/8897], Loss: 5.5904\n",
      "Epoch [1/1], Step [1698/8897], Loss: 5.4958\n",
      "Epoch [1/1], Step [1699/8897], Loss: 5.4619\n",
      "Epoch [1/1], Step [1700/8897], Loss: 5.5817\n",
      "Epoch [1/1], Step [1701/8897], Loss: 5.4472\n",
      "Epoch [1/1], Step [1702/8897], Loss: 5.2767\n",
      "Epoch [1/1], Step [1703/8897], Loss: 5.5449\n",
      "Epoch [1/1], Step [1704/8897], Loss: 5.4091\n",
      "Epoch [1/1], Step [1705/8897], Loss: 5.7518\n",
      "Epoch [1/1], Step [1706/8897], Loss: 5.3138\n",
      "Epoch [1/1], Step [1707/8897], Loss: 5.3612\n",
      "Epoch [1/1], Step [1708/8897], Loss: 5.5998\n",
      "Epoch [1/1], Step [1709/8897], Loss: 5.5341\n",
      "Epoch [1/1], Step [1710/8897], Loss: 5.1806\n",
      "Epoch [1/1], Step [1711/8897], Loss: 5.3442\n",
      "Epoch [1/1], Step [1712/8897], Loss: 5.4404\n",
      "Epoch [1/1], Step [1713/8897], Loss: 5.2189\n",
      "Epoch [1/1], Step [1714/8897], Loss: 5.5454\n",
      "Epoch [1/1], Step [1715/8897], Loss: 5.3602\n",
      "Epoch [1/1], Step [1716/8897], Loss: 5.4741\n",
      "Epoch [1/1], Step [1717/8897], Loss: 5.3881\n",
      "Epoch [1/1], Step [1718/8897], Loss: 5.2637\n",
      "Epoch [1/1], Step [1719/8897], Loss: 5.5128\n",
      "Epoch [1/1], Step [1720/8897], Loss: 5.5063\n",
      "Epoch [1/1], Step [1721/8897], Loss: 5.1200\n",
      "Epoch [1/1], Step [1722/8897], Loss: 5.5223\n",
      "Epoch [1/1], Step [1723/8897], Loss: 5.3353\n",
      "Epoch [1/1], Step [1724/8897], Loss: 5.2211\n",
      "Epoch [1/1], Step [1725/8897], Loss: 5.4924\n",
      "Epoch [1/1], Step [1726/8897], Loss: 5.2565\n",
      "Epoch [1/1], Step [1727/8897], Loss: 5.5145\n",
      "Epoch [1/1], Step [1728/8897], Loss: 5.2875\n",
      "Epoch [1/1], Step [1729/8897], Loss: 5.6814\n",
      "Epoch [1/1], Step [1730/8897], Loss: 5.4203\n",
      "Epoch [1/1], Step [1731/8897], Loss: 5.6131\n",
      "Epoch [1/1], Step [1732/8897], Loss: 5.6749\n",
      "Epoch [1/1], Step [1733/8897], Loss: 5.1641\n",
      "Epoch [1/1], Step [1734/8897], Loss: 5.6388\n",
      "Epoch [1/1], Step [1735/8897], Loss: 5.3758\n",
      "Epoch [1/1], Step [1736/8897], Loss: 5.2379\n",
      "Epoch [1/1], Step [1737/8897], Loss: 5.3579\n",
      "Epoch [1/1], Step [1738/8897], Loss: 5.2554\n",
      "Epoch [1/1], Step [1739/8897], Loss: 5.4895\n",
      "Epoch [1/1], Step [1740/8897], Loss: 5.5217\n",
      "Epoch [1/1], Step [1741/8897], Loss: 5.3263\n",
      "Epoch [1/1], Step [1742/8897], Loss: 5.4603\n",
      "Epoch [1/1], Step [1743/8897], Loss: 5.4289\n",
      "Epoch [1/1], Step [1744/8897], Loss: 5.2962\n",
      "Epoch [1/1], Step [1745/8897], Loss: 5.5292\n",
      "Epoch [1/1], Step [1746/8897], Loss: 5.4176\n",
      "Epoch [1/1], Step [1747/8897], Loss: 5.0693\n",
      "Epoch [1/1], Step [1748/8897], Loss: 5.4732\n",
      "Epoch [1/1], Step [1749/8897], Loss: 5.3275\n",
      "Epoch [1/1], Step [1750/8897], Loss: 5.4379\n",
      "Epoch [1/1], Step [1751/8897], Loss: 5.3711\n",
      "Epoch [1/1], Step [1752/8897], Loss: 5.6700\n",
      "Epoch [1/1], Step [1753/8897], Loss: 5.1088\n",
      "Epoch [1/1], Step [1754/8897], Loss: 5.4651\n",
      "Epoch [1/1], Step [1755/8897], Loss: 5.4538\n",
      "Epoch [1/1], Step [1756/8897], Loss: 5.3895\n",
      "Epoch [1/1], Step [1757/8897], Loss: 5.7184\n",
      "Epoch [1/1], Step [1758/8897], Loss: 5.6478\n",
      "Epoch [1/1], Step [1759/8897], Loss: 5.3998\n",
      "Epoch [1/1], Step [1760/8897], Loss: 5.6657\n",
      "Epoch [1/1], Step [1761/8897], Loss: 5.2693\n",
      "Epoch [1/1], Step [1762/8897], Loss: 5.2430\n",
      "Epoch [1/1], Step [1763/8897], Loss: 5.5739\n",
      "Epoch [1/1], Step [1764/8897], Loss: 5.5001\n",
      "Epoch [1/1], Step [1765/8897], Loss: 5.4373\n",
      "Epoch [1/1], Step [1766/8897], Loss: 5.4107\n",
      "Epoch [1/1], Step [1767/8897], Loss: 5.3923\n",
      "Epoch [1/1], Step [1768/8897], Loss: 5.5483\n",
      "Epoch [1/1], Step [1769/8897], Loss: 5.4298\n",
      "Epoch [1/1], Step [1770/8897], Loss: 5.6519\n",
      "Epoch [1/1], Step [1771/8897], Loss: 5.6368\n",
      "Epoch [1/1], Step [1772/8897], Loss: 5.2999\n",
      "Epoch [1/1], Step [1773/8897], Loss: 5.1621\n",
      "Epoch [1/1], Step [1774/8897], Loss: 5.4964\n",
      "Epoch [1/1], Step [1775/8897], Loss: 5.2881\n",
      "Epoch [1/1], Step [1776/8897], Loss: 5.3669\n",
      "Epoch [1/1], Step [1777/8897], Loss: 5.4079\n",
      "Epoch [1/1], Step [1778/8897], Loss: 5.3248\n",
      "Epoch [1/1], Step [1779/8897], Loss: 5.4039\n",
      "Epoch [1/1], Step [1780/8897], Loss: 5.2270\n",
      "Epoch [1/1], Step [1781/8897], Loss: 5.4300\n",
      "Epoch [1/1], Step [1782/8897], Loss: 5.3651\n",
      "Epoch [1/1], Step [1783/8897], Loss: 5.3889\n",
      "Epoch [1/1], Step [1784/8897], Loss: 5.4179\n",
      "Epoch [1/1], Step [1785/8897], Loss: 5.4165\n",
      "Epoch [1/1], Step [1786/8897], Loss: 5.6503\n",
      "Epoch [1/1], Step [1787/8897], Loss: 5.5945\n",
      "Epoch [1/1], Step [1788/8897], Loss: 5.4508\n",
      "Epoch [1/1], Step [1789/8897], Loss: 5.4910\n",
      "Epoch [1/1], Step [1790/8897], Loss: 5.4117\n",
      "Epoch [1/1], Step [1791/8897], Loss: 5.4225\n",
      "Epoch [1/1], Step [1792/8897], Loss: 5.4269\n",
      "Epoch [1/1], Step [1793/8897], Loss: 5.3411\n",
      "Epoch [1/1], Step [1794/8897], Loss: 5.4107\n",
      "Epoch [1/1], Step [1795/8897], Loss: 5.6721\n",
      "Epoch [1/1], Step [1796/8897], Loss: 5.4769\n",
      "Epoch [1/1], Step [1797/8897], Loss: 5.5079\n",
      "Epoch [1/1], Step [1798/8897], Loss: 5.5166\n",
      "Epoch [1/1], Step [1799/8897], Loss: 5.2818\n",
      "Epoch [1/1], Step [1800/8897], Loss: 5.3830\n",
      "Epoch [1/1], Step [1801/8897], Loss: 5.4501\n",
      "Epoch [1/1], Step [1802/8897], Loss: 5.5103\n",
      "Epoch [1/1], Step [1803/8897], Loss: 5.4326\n",
      "Epoch [1/1], Step [1804/8897], Loss: 5.4485\n",
      "Epoch [1/1], Step [1805/8897], Loss: 5.3683\n",
      "Epoch [1/1], Step [1806/8897], Loss: 5.4921\n",
      "Epoch [1/1], Step [1807/8897], Loss: 5.5415\n",
      "Epoch [1/1], Step [1808/8897], Loss: 5.3733\n",
      "Epoch [1/1], Step [1809/8897], Loss: 5.5110\n",
      "Epoch [1/1], Step [1810/8897], Loss: 5.3792\n",
      "Epoch [1/1], Step [1811/8897], Loss: 5.2954\n",
      "Epoch [1/1], Step [1812/8897], Loss: 5.4488\n",
      "Epoch [1/1], Step [1813/8897], Loss: 5.5603\n",
      "Epoch [1/1], Step [1814/8897], Loss: 5.5271\n",
      "Epoch [1/1], Step [1815/8897], Loss: 5.5126\n",
      "Epoch [1/1], Step [1816/8897], Loss: 5.5988\n",
      "Epoch [1/1], Step [1817/8897], Loss: 5.3983\n",
      "Epoch [1/1], Step [1818/8897], Loss: 5.3829\n",
      "Epoch [1/1], Step [1819/8897], Loss: 5.6636\n",
      "Epoch [1/1], Step [1820/8897], Loss: 5.4642\n",
      "Epoch [1/1], Step [1821/8897], Loss: 5.6626\n",
      "Epoch [1/1], Step [1822/8897], Loss: 5.3822\n",
      "Epoch [1/1], Step [1823/8897], Loss: 5.5995\n",
      "Epoch [1/1], Step [1824/8897], Loss: 5.4462\n",
      "Epoch [1/1], Step [1825/8897], Loss: 5.5397\n",
      "Epoch [1/1], Step [1826/8897], Loss: 5.3789\n",
      "Epoch [1/1], Step [1827/8897], Loss: 5.4642\n",
      "Epoch [1/1], Step [1828/8897], Loss: 5.4680\n",
      "Epoch [1/1], Step [1829/8897], Loss: 5.2848\n",
      "Epoch [1/1], Step [1830/8897], Loss: 5.8711\n",
      "Epoch [1/1], Step [1831/8897], Loss: 5.4092\n",
      "Epoch [1/1], Step [1832/8897], Loss: 5.5374\n",
      "Epoch [1/1], Step [1833/8897], Loss: 5.3661\n",
      "Epoch [1/1], Step [1834/8897], Loss: 5.5488\n",
      "Epoch [1/1], Step [1835/8897], Loss: 5.4469\n",
      "Epoch [1/1], Step [1836/8897], Loss: 5.6407\n",
      "Epoch [1/1], Step [1837/8897], Loss: 5.3160\n",
      "Epoch [1/1], Step [1838/8897], Loss: 5.3575\n",
      "Epoch [1/1], Step [1839/8897], Loss: 5.4399\n",
      "Epoch [1/1], Step [1840/8897], Loss: 5.3927\n",
      "Epoch [1/1], Step [1841/8897], Loss: 5.2439\n",
      "Epoch [1/1], Step [1842/8897], Loss: 5.1681\n",
      "Epoch [1/1], Step [1843/8897], Loss: 5.3506\n",
      "Epoch [1/1], Step [1844/8897], Loss: 5.3467\n",
      "Epoch [1/1], Step [1845/8897], Loss: 5.5575\n",
      "Epoch [1/1], Step [1846/8897], Loss: 5.4975\n",
      "Epoch [1/1], Step [1847/8897], Loss: 5.2901\n",
      "Epoch [1/1], Step [1848/8897], Loss: 5.5124\n",
      "Epoch [1/1], Step [1849/8897], Loss: 5.4656\n",
      "Epoch [1/1], Step [1850/8897], Loss: 5.5030\n",
      "Epoch [1/1], Step [1851/8897], Loss: 5.5156\n",
      "Epoch [1/1], Step [1852/8897], Loss: 5.6228\n",
      "Epoch [1/1], Step [1853/8897], Loss: 5.5418\n",
      "Epoch [1/1], Step [1854/8897], Loss: 5.4225\n",
      "Epoch [1/1], Step [1855/8897], Loss: 5.4215\n",
      "Epoch [1/1], Step [1856/8897], Loss: 5.3695\n",
      "Epoch [1/1], Step [1857/8897], Loss: 5.3630\n",
      "Epoch [1/1], Step [1858/8897], Loss: 5.3626\n",
      "Epoch [1/1], Step [1859/8897], Loss: 5.1779\n",
      "Epoch [1/1], Step [1860/8897], Loss: 5.2724\n",
      "Epoch [1/1], Step [1861/8897], Loss: 5.4807\n",
      "Epoch [1/1], Step [1862/8897], Loss: 5.0523\n",
      "Epoch [1/1], Step [1863/8897], Loss: 5.4712\n",
      "Epoch [1/1], Step [1864/8897], Loss: 5.3696\n",
      "Epoch [1/1], Step [1865/8897], Loss: 5.4676\n",
      "Epoch [1/1], Step [1866/8897], Loss: 5.3814\n",
      "Epoch [1/1], Step [1867/8897], Loss: 5.3815\n",
      "Epoch [1/1], Step [1868/8897], Loss: 5.4055\n",
      "Epoch [1/1], Step [1869/8897], Loss: 5.1040\n",
      "Epoch [1/1], Step [1870/8897], Loss: 5.2627\n",
      "Epoch [1/1], Step [1871/8897], Loss: 5.3607\n",
      "Epoch [1/1], Step [1872/8897], Loss: 5.4789\n",
      "Epoch [1/1], Step [1873/8897], Loss: 5.5628\n",
      "Epoch [1/1], Step [1874/8897], Loss: 5.4657\n",
      "Epoch [1/1], Step [1875/8897], Loss: 5.1844\n",
      "Epoch [1/1], Step [1876/8897], Loss: 5.5486\n",
      "Epoch [1/1], Step [1877/8897], Loss: 5.3501\n",
      "Epoch [1/1], Step [1878/8897], Loss: 5.5581\n",
      "Epoch [1/1], Step [1879/8897], Loss: 5.5277\n",
      "Epoch [1/1], Step [1880/8897], Loss: 5.6841\n",
      "Epoch [1/1], Step [1881/8897], Loss: 5.4696\n",
      "Epoch [1/1], Step [1882/8897], Loss: 5.4610\n",
      "Epoch [1/1], Step [1883/8897], Loss: 5.4563\n",
      "Epoch [1/1], Step [1884/8897], Loss: 5.7466\n",
      "Epoch [1/1], Step [1885/8897], Loss: 5.3686\n",
      "Epoch [1/1], Step [1886/8897], Loss: 5.1908\n",
      "Epoch [1/1], Step [1887/8897], Loss: 5.4391\n",
      "Epoch [1/1], Step [1888/8897], Loss: 5.4471\n",
      "Epoch [1/1], Step [1889/8897], Loss: 5.4965\n",
      "Epoch [1/1], Step [1890/8897], Loss: 5.4701\n",
      "Epoch [1/1], Step [1891/8897], Loss: 5.6130\n",
      "Epoch [1/1], Step [1892/8897], Loss: 5.1881\n",
      "Epoch [1/1], Step [1893/8897], Loss: 5.4859\n",
      "Epoch [1/1], Step [1894/8897], Loss: 5.3750\n",
      "Epoch [1/1], Step [1895/8897], Loss: 5.3645\n",
      "Epoch [1/1], Step [1896/8897], Loss: 5.4362\n",
      "Epoch [1/1], Step [1897/8897], Loss: 5.6051\n",
      "Epoch [1/1], Step [1898/8897], Loss: 5.4806\n",
      "Epoch [1/1], Step [1899/8897], Loss: 5.2443\n",
      "Epoch [1/1], Step [1900/8897], Loss: 5.4870\n",
      "Epoch [1/1], Step [1901/8897], Loss: 5.5742\n",
      "Epoch [1/1], Step [1902/8897], Loss: 5.6506\n",
      "Epoch [1/1], Step [1903/8897], Loss: 5.3475\n",
      "Epoch [1/1], Step [1904/8897], Loss: 5.1612\n",
      "Epoch [1/1], Step [1905/8897], Loss: 5.4479\n",
      "Epoch [1/1], Step [1906/8897], Loss: 5.5997\n",
      "Epoch [1/1], Step [1907/8897], Loss: 5.1949\n",
      "Epoch [1/1], Step [1908/8897], Loss: 5.4744\n",
      "Epoch [1/1], Step [1909/8897], Loss: 5.5132\n",
      "Epoch [1/1], Step [1910/8897], Loss: 5.2827\n",
      "Epoch [1/1], Step [1911/8897], Loss: 5.4929\n",
      "Epoch [1/1], Step [1912/8897], Loss: 5.2847\n",
      "Epoch [1/1], Step [1913/8897], Loss: 5.4872\n",
      "Epoch [1/1], Step [1914/8897], Loss: 5.6108\n",
      "Epoch [1/1], Step [1915/8897], Loss: 5.3342\n",
      "Epoch [1/1], Step [1916/8897], Loss: 5.4069\n",
      "Epoch [1/1], Step [1917/8897], Loss: 5.4100\n",
      "Epoch [1/1], Step [1918/8897], Loss: 5.5715\n",
      "Epoch [1/1], Step [1919/8897], Loss: 5.3170\n",
      "Epoch [1/1], Step [1920/8897], Loss: 5.3347\n",
      "Epoch [1/1], Step [1921/8897], Loss: 5.5195\n",
      "Epoch [1/1], Step [1922/8897], Loss: 5.5358\n",
      "Epoch [1/1], Step [1923/8897], Loss: 5.3800\n",
      "Epoch [1/1], Step [1924/8897], Loss: 5.5874\n",
      "Epoch [1/1], Step [1925/8897], Loss: 5.5175\n",
      "Epoch [1/1], Step [1926/8897], Loss: 5.4478\n",
      "Epoch [1/1], Step [1927/8897], Loss: 5.3678\n",
      "Epoch [1/1], Step [1928/8897], Loss: 5.6083\n",
      "Epoch [1/1], Step [1929/8897], Loss: 5.3459\n",
      "Epoch [1/1], Step [1930/8897], Loss: 5.4377\n",
      "Epoch [1/1], Step [1931/8897], Loss: 5.4082\n",
      "Epoch [1/1], Step [1932/8897], Loss: 5.1831\n",
      "Epoch [1/1], Step [1933/8897], Loss: 5.4244\n",
      "Epoch [1/1], Step [1934/8897], Loss: 5.4809\n",
      "Epoch [1/1], Step [1935/8897], Loss: 5.3292\n",
      "Epoch [1/1], Step [1936/8897], Loss: 5.6236\n",
      "Epoch [1/1], Step [1937/8897], Loss: 5.4275\n",
      "Epoch [1/1], Step [1938/8897], Loss: 5.5904\n",
      "Epoch [1/1], Step [1939/8897], Loss: 5.2463\n",
      "Epoch [1/1], Step [1940/8897], Loss: 5.5643\n",
      "Epoch [1/1], Step [1941/8897], Loss: 5.3751\n",
      "Epoch [1/1], Step [1942/8897], Loss: 5.3245\n",
      "Epoch [1/1], Step [1943/8897], Loss: 5.4993\n",
      "Epoch [1/1], Step [1944/8897], Loss: 5.5521\n",
      "Epoch [1/1], Step [1945/8897], Loss: 5.4324\n",
      "Epoch [1/1], Step [1946/8897], Loss: 5.3546\n",
      "Epoch [1/1], Step [1947/8897], Loss: 5.5126\n",
      "Epoch [1/1], Step [1948/8897], Loss: 5.3184\n",
      "Epoch [1/1], Step [1949/8897], Loss: 5.4774\n",
      "Epoch [1/1], Step [1950/8897], Loss: 5.2007\n",
      "Epoch [1/1], Step [1951/8897], Loss: 5.4140\n",
      "Epoch [1/1], Step [1952/8897], Loss: 5.2164\n",
      "Epoch [1/1], Step [1953/8897], Loss: 5.6110\n",
      "Epoch [1/1], Step [1954/8897], Loss: 5.5580\n",
      "Epoch [1/1], Step [1955/8897], Loss: 5.3171\n",
      "Epoch [1/1], Step [1956/8897], Loss: 5.3033\n",
      "Epoch [1/1], Step [1957/8897], Loss: 5.3394\n",
      "Epoch [1/1], Step [1958/8897], Loss: 5.7163\n",
      "Epoch [1/1], Step [1959/8897], Loss: 5.4583\n",
      "Epoch [1/1], Step [1960/8897], Loss: 5.2583\n",
      "Epoch [1/1], Step [1961/8897], Loss: 5.4515\n",
      "Epoch [1/1], Step [1962/8897], Loss: 5.4527\n",
      "Epoch [1/1], Step [1963/8897], Loss: 5.4961\n",
      "Epoch [1/1], Step [1964/8897], Loss: 5.3956\n",
      "Epoch [1/1], Step [1965/8897], Loss: 5.4041\n",
      "Epoch [1/1], Step [1966/8897], Loss: 5.5058\n",
      "Epoch [1/1], Step [1967/8897], Loss: 5.5582\n",
      "Epoch [1/1], Step [1968/8897], Loss: 5.4831\n",
      "Epoch [1/1], Step [1969/8897], Loss: 5.5560\n",
      "Epoch [1/1], Step [1970/8897], Loss: 5.2369\n",
      "Epoch [1/1], Step [1971/8897], Loss: 5.1833\n",
      "Epoch [1/1], Step [1972/8897], Loss: 5.1866\n",
      "Epoch [1/1], Step [1973/8897], Loss: 5.5801\n",
      "Epoch [1/1], Step [1974/8897], Loss: 5.2738\n",
      "Epoch [1/1], Step [1975/8897], Loss: 5.3067\n",
      "Epoch [1/1], Step [1976/8897], Loss: 5.6441\n",
      "Epoch [1/1], Step [1977/8897], Loss: 5.5069\n",
      "Epoch [1/1], Step [1978/8897], Loss: 5.4504\n",
      "Epoch [1/1], Step [1979/8897], Loss: 5.3731\n",
      "Epoch [1/1], Step [1980/8897], Loss: 5.5844\n",
      "Epoch [1/1], Step [1981/8897], Loss: 5.4049\n",
      "Epoch [1/1], Step [1982/8897], Loss: 5.3745\n",
      "Epoch [1/1], Step [1983/8897], Loss: 5.2497\n",
      "Epoch [1/1], Step [1984/8897], Loss: 5.6390\n",
      "Epoch [1/1], Step [1985/8897], Loss: 5.4560\n",
      "Epoch [1/1], Step [1986/8897], Loss: 5.4033\n",
      "Epoch [1/1], Step [1987/8897], Loss: 5.5764\n",
      "Epoch [1/1], Step [1988/8897], Loss: 5.4237\n",
      "Epoch [1/1], Step [1989/8897], Loss: 5.5888\n",
      "Epoch [1/1], Step [1990/8897], Loss: 5.4229\n",
      "Epoch [1/1], Step [1991/8897], Loss: 5.3689\n",
      "Epoch [1/1], Step [1992/8897], Loss: 5.5820\n",
      "Epoch [1/1], Step [1993/8897], Loss: 5.4333\n",
      "Epoch [1/1], Step [1994/8897], Loss: 5.4904\n",
      "Epoch [1/1], Step [1995/8897], Loss: 5.5728\n",
      "Epoch [1/1], Step [1996/8897], Loss: 5.4609\n",
      "Epoch [1/1], Step [1997/8897], Loss: 5.4633\n",
      "Epoch [1/1], Step [1998/8897], Loss: 5.3377\n",
      "Epoch [1/1], Step [1999/8897], Loss: 5.5442\n",
      "Epoch [1/1], Step [2000/8897], Loss: 5.4342\n",
      "Epoch [1/1], Step [2001/8897], Loss: 5.5031\n",
      "Epoch [1/1], Step [2002/8897], Loss: 5.2310\n",
      "Epoch [1/1], Step [2003/8897], Loss: 5.2433\n",
      "Epoch [1/1], Step [2004/8897], Loss: 5.4629\n",
      "Epoch [1/1], Step [2005/8897], Loss: 5.4573\n",
      "Epoch [1/1], Step [2006/8897], Loss: 5.5680\n",
      "Epoch [1/1], Step [2007/8897], Loss: 5.3199\n",
      "Epoch [1/1], Step [2008/8897], Loss: 5.5797\n",
      "Epoch [1/1], Step [2009/8897], Loss: 5.2694\n",
      "Epoch [1/1], Step [2010/8897], Loss: 5.7221\n",
      "Epoch [1/1], Step [2011/8897], Loss: 5.3168\n",
      "Epoch [1/1], Step [2012/8897], Loss: 5.1673\n",
      "Epoch [1/1], Step [2013/8897], Loss: 5.2534\n",
      "Epoch [1/1], Step [2014/8897], Loss: 5.7238\n",
      "Epoch [1/1], Step [2015/8897], Loss: 5.6295\n",
      "Epoch [1/1], Step [2016/8897], Loss: 5.1652\n",
      "Epoch [1/1], Step [2017/8897], Loss: 5.5076\n",
      "Epoch [1/1], Step [2018/8897], Loss: 5.3858\n",
      "Epoch [1/1], Step [2019/8897], Loss: 5.5450\n",
      "Epoch [1/1], Step [2020/8897], Loss: 5.3902\n",
      "Epoch [1/1], Step [2021/8897], Loss: 5.3005\n",
      "Epoch [1/1], Step [2022/8897], Loss: 5.4076\n",
      "Epoch [1/1], Step [2023/8897], Loss: 5.4723\n",
      "Epoch [1/1], Step [2024/8897], Loss: 5.5841\n",
      "Epoch [1/1], Step [2025/8897], Loss: 5.3928\n",
      "Epoch [1/1], Step [2026/8897], Loss: 5.3225\n",
      "Epoch [1/1], Step [2027/8897], Loss: 5.5042\n",
      "Epoch [1/1], Step [2028/8897], Loss: 5.4758\n",
      "Epoch [1/1], Step [2029/8897], Loss: 5.5045\n",
      "Epoch [1/1], Step [2030/8897], Loss: 5.2137\n",
      "Epoch [1/1], Step [2031/8897], Loss: 5.1365\n",
      "Epoch [1/1], Step [2032/8897], Loss: 5.4313\n",
      "Epoch [1/1], Step [2033/8897], Loss: 5.2409\n",
      "Epoch [1/1], Step [2034/8897], Loss: 5.2498\n",
      "Epoch [1/1], Step [2035/8897], Loss: 5.3221\n",
      "Epoch [1/1], Step [2036/8897], Loss: 5.3712\n",
      "Epoch [1/1], Step [2037/8897], Loss: 5.4562\n",
      "Epoch [1/1], Step [2038/8897], Loss: 5.3259\n",
      "Epoch [1/1], Step [2039/8897], Loss: 5.6449\n",
      "Epoch [1/1], Step [2040/8897], Loss: 5.3157\n",
      "Epoch [1/1], Step [2041/8897], Loss: 5.3565\n",
      "Epoch [1/1], Step [2042/8897], Loss: 5.2409\n",
      "Epoch [1/1], Step [2043/8897], Loss: 5.4468\n",
      "Epoch [1/1], Step [2044/8897], Loss: 5.3499\n",
      "Epoch [1/1], Step [2045/8897], Loss: 5.5964\n",
      "Epoch [1/1], Step [2046/8897], Loss: 5.3365\n",
      "Epoch [1/1], Step [2047/8897], Loss: 5.5568\n",
      "Epoch [1/1], Step [2048/8897], Loss: 5.4892\n",
      "Epoch [1/1], Step [2049/8897], Loss: 5.2082\n",
      "Epoch [1/1], Step [2050/8897], Loss: 5.2062\n",
      "Epoch [1/1], Step [2051/8897], Loss: 5.3178\n",
      "Epoch [1/1], Step [2052/8897], Loss: 5.6088\n",
      "Epoch [1/1], Step [2053/8897], Loss: 5.3961\n",
      "Epoch [1/1], Step [2054/8897], Loss: 5.5511\n",
      "Epoch [1/1], Step [2055/8897], Loss: 5.5793\n",
      "Epoch [1/1], Step [2056/8897], Loss: 5.2743\n",
      "Epoch [1/1], Step [2057/8897], Loss: 5.7420\n",
      "Epoch [1/1], Step [2058/8897], Loss: 5.7543\n",
      "Epoch [1/1], Step [2059/8897], Loss: 5.3013\n",
      "Epoch [1/1], Step [2060/8897], Loss: 5.5312\n",
      "Epoch [1/1], Step [2061/8897], Loss: 5.4756\n",
      "Epoch [1/1], Step [2062/8897], Loss: 5.4807\n",
      "Epoch [1/1], Step [2063/8897], Loss: 5.4383\n",
      "Epoch [1/1], Step [2064/8897], Loss: 5.4485\n",
      "Epoch [1/1], Step [2065/8897], Loss: 5.6592\n",
      "Epoch [1/1], Step [2066/8897], Loss: 5.4181\n",
      "Epoch [1/1], Step [2067/8897], Loss: 5.3331\n",
      "Epoch [1/1], Step [2068/8897], Loss: 5.2865\n",
      "Epoch [1/1], Step [2069/8897], Loss: 5.3942\n",
      "Epoch [1/1], Step [2070/8897], Loss: 5.6207\n",
      "Epoch [1/1], Step [2071/8897], Loss: 5.5440\n",
      "Epoch [1/1], Step [2072/8897], Loss: 5.7514\n",
      "Epoch [1/1], Step [2073/8897], Loss: 5.6658\n",
      "Epoch [1/1], Step [2074/8897], Loss: 5.5567\n",
      "Epoch [1/1], Step [2075/8897], Loss: 5.5920\n",
      "Epoch [1/1], Step [2076/8897], Loss: 5.4788\n",
      "Epoch [1/1], Step [2077/8897], Loss: 5.4535\n",
      "Epoch [1/1], Step [2078/8897], Loss: 5.3338\n",
      "Epoch [1/1], Step [2079/8897], Loss: 5.4901\n",
      "Epoch [1/1], Step [2080/8897], Loss: 5.6666\n",
      "Epoch [1/1], Step [2081/8897], Loss: 5.5620\n",
      "Epoch [1/1], Step [2082/8897], Loss: 5.3787\n",
      "Epoch [1/1], Step [2083/8897], Loss: 5.6177\n",
      "Epoch [1/1], Step [2084/8897], Loss: 5.4233\n",
      "Epoch [1/1], Step [2085/8897], Loss: 5.6605\n",
      "Epoch [1/1], Step [2086/8897], Loss: 5.5262\n",
      "Epoch [1/1], Step [2087/8897], Loss: 5.3782\n",
      "Epoch [1/1], Step [2088/8897], Loss: 5.1953\n",
      "Epoch [1/1], Step [2089/8897], Loss: 5.4619\n",
      "Epoch [1/1], Step [2090/8897], Loss: 5.4083\n",
      "Epoch [1/1], Step [2091/8897], Loss: 5.3407\n",
      "Epoch [1/1], Step [2092/8897], Loss: 5.5568\n",
      "Epoch [1/1], Step [2093/8897], Loss: 5.5173\n",
      "Epoch [1/1], Step [2094/8897], Loss: 5.4527\n",
      "Epoch [1/1], Step [2095/8897], Loss: 5.3128\n",
      "Epoch [1/1], Step [2096/8897], Loss: 5.9795\n",
      "Epoch [1/1], Step [2097/8897], Loss: 5.1942\n",
      "Epoch [1/1], Step [2098/8897], Loss: 5.3835\n",
      "Epoch [1/1], Step [2099/8897], Loss: 5.4163\n",
      "Epoch [1/1], Step [2100/8897], Loss: 5.6415\n",
      "Epoch [1/1], Step [2101/8897], Loss: 5.5214\n",
      "Epoch [1/1], Step [2102/8897], Loss: 5.4835\n",
      "Epoch [1/1], Step [2103/8897], Loss: 5.5791\n",
      "Epoch [1/1], Step [2104/8897], Loss: 5.4917\n",
      "Epoch [1/1], Step [2105/8897], Loss: 5.2253\n",
      "Epoch [1/1], Step [2106/8897], Loss: 5.5761\n",
      "Epoch [1/1], Step [2107/8897], Loss: 5.4082\n",
      "Epoch [1/1], Step [2108/8897], Loss: 5.4038\n",
      "Epoch [1/1], Step [2109/8897], Loss: 5.5866\n",
      "Epoch [1/1], Step [2110/8897], Loss: 5.3753\n",
      "Epoch [1/1], Step [2111/8897], Loss: 5.1508\n",
      "Epoch [1/1], Step [2112/8897], Loss: 5.1379\n",
      "Epoch [1/1], Step [2113/8897], Loss: 5.5231\n",
      "Epoch [1/1], Step [2114/8897], Loss: 5.3475\n",
      "Epoch [1/1], Step [2115/8897], Loss: 5.3821\n",
      "Epoch [1/1], Step [2116/8897], Loss: 5.4467\n",
      "Epoch [1/1], Step [2117/8897], Loss: 5.7181\n",
      "Epoch [1/1], Step [2118/8897], Loss: 5.4918\n",
      "Epoch [1/1], Step [2119/8897], Loss: 5.3114\n",
      "Epoch [1/1], Step [2120/8897], Loss: 5.3420\n",
      "Epoch [1/1], Step [2121/8897], Loss: 5.4381\n",
      "Epoch [1/1], Step [2122/8897], Loss: 5.3000\n",
      "Epoch [1/1], Step [2123/8897], Loss: 5.4812\n",
      "Epoch [1/1], Step [2124/8897], Loss: 5.4228\n",
      "Epoch [1/1], Step [2125/8897], Loss: 5.5941\n",
      "Epoch [1/1], Step [2126/8897], Loss: 5.4264\n",
      "Epoch [1/1], Step [2127/8897], Loss: 5.5547\n",
      "Epoch [1/1], Step [2128/8897], Loss: 5.5340\n",
      "Epoch [1/1], Step [2129/8897], Loss: 5.3824\n",
      "Epoch [1/1], Step [2130/8897], Loss: 5.4747\n",
      "Epoch [1/1], Step [2131/8897], Loss: 5.4159\n",
      "Epoch [1/1], Step [2132/8897], Loss: 5.2232\n",
      "Epoch [1/1], Step [2133/8897], Loss: 5.4675\n",
      "Epoch [1/1], Step [2134/8897], Loss: 5.3906\n",
      "Epoch [1/1], Step [2135/8897], Loss: 5.6025\n",
      "Epoch [1/1], Step [2136/8897], Loss: 5.3206\n",
      "Epoch [1/1], Step [2137/8897], Loss: 5.3079\n",
      "Epoch [1/1], Step [2138/8897], Loss: 5.5332\n",
      "Epoch [1/1], Step [2139/8897], Loss: 5.5758\n",
      "Epoch [1/1], Step [2140/8897], Loss: 5.3935\n",
      "Epoch [1/1], Step [2141/8897], Loss: 5.5465\n",
      "Epoch [1/1], Step [2142/8897], Loss: 5.3542\n",
      "Epoch [1/1], Step [2143/8897], Loss: 5.5986\n",
      "Epoch [1/1], Step [2144/8897], Loss: 5.5554\n",
      "Epoch [1/1], Step [2145/8897], Loss: 5.6029\n",
      "Epoch [1/1], Step [2146/8897], Loss: 5.5604\n",
      "Epoch [1/1], Step [2147/8897], Loss: 5.3069\n",
      "Epoch [1/1], Step [2148/8897], Loss: 5.4803\n",
      "Epoch [1/1], Step [2149/8897], Loss: 5.2387\n",
      "Epoch [1/1], Step [2150/8897], Loss: 5.4235\n",
      "Epoch [1/1], Step [2151/8897], Loss: 5.2920\n",
      "Epoch [1/1], Step [2152/8897], Loss: 5.3558\n",
      "Epoch [1/1], Step [2153/8897], Loss: 5.5384\n",
      "Epoch [1/1], Step [2154/8897], Loss: 5.3652\n",
      "Epoch [1/1], Step [2155/8897], Loss: 5.5076\n",
      "Epoch [1/1], Step [2156/8897], Loss: 5.4967\n",
      "Epoch [1/1], Step [2157/8897], Loss: 5.5979\n",
      "Epoch [1/1], Step [2158/8897], Loss: 5.6227\n",
      "Epoch [1/1], Step [2159/8897], Loss: 5.3437\n",
      "Epoch [1/1], Step [2160/8897], Loss: 5.4199\n",
      "Epoch [1/1], Step [2161/8897], Loss: 5.4744\n",
      "Epoch [1/1], Step [2162/8897], Loss: 5.2936\n",
      "Epoch [1/1], Step [2163/8897], Loss: 5.2999\n",
      "Epoch [1/1], Step [2164/8897], Loss: 5.3486\n",
      "Epoch [1/1], Step [2165/8897], Loss: 5.5317\n",
      "Epoch [1/1], Step [2166/8897], Loss: 5.2903\n",
      "Epoch [1/1], Step [2167/8897], Loss: 5.4308\n",
      "Epoch [1/1], Step [2168/8897], Loss: 5.4161\n",
      "Epoch [1/1], Step [2169/8897], Loss: 5.3063\n",
      "Epoch [1/1], Step [2170/8897], Loss: 5.3515\n",
      "Epoch [1/1], Step [2171/8897], Loss: 5.2528\n",
      "Epoch [1/1], Step [2172/8897], Loss: 5.6834\n",
      "Epoch [1/1], Step [2173/8897], Loss: 5.4185\n",
      "Epoch [1/1], Step [2174/8897], Loss: 5.4969\n",
      "Epoch [1/1], Step [2175/8897], Loss: 5.6041\n",
      "Epoch [1/1], Step [2176/8897], Loss: 5.0133\n",
      "Epoch [1/1], Step [2177/8897], Loss: 5.5744\n",
      "Epoch [1/1], Step [2178/8897], Loss: 5.2282\n",
      "Epoch [1/1], Step [2179/8897], Loss: 5.5782\n",
      "Epoch [1/1], Step [2180/8897], Loss: 5.4189\n",
      "Epoch [1/1], Step [2181/8897], Loss: 5.6036\n",
      "Epoch [1/1], Step [2182/8897], Loss: 5.2741\n",
      "Epoch [1/1], Step [2183/8897], Loss: 5.4438\n",
      "Epoch [1/1], Step [2184/8897], Loss: 5.3244\n",
      "Epoch [1/1], Step [2185/8897], Loss: 5.4180\n",
      "Epoch [1/1], Step [2186/8897], Loss: 5.3347\n",
      "Epoch [1/1], Step [2187/8897], Loss: 5.3169\n",
      "Epoch [1/1], Step [2188/8897], Loss: 5.2346\n",
      "Epoch [1/1], Step [2189/8897], Loss: 5.3334\n",
      "Epoch [1/1], Step [2190/8897], Loss: 5.1407\n",
      "Epoch [1/1], Step [2191/8897], Loss: 5.4137\n",
      "Epoch [1/1], Step [2192/8897], Loss: 5.7652\n",
      "Epoch [1/1], Step [2193/8897], Loss: 5.3643\n",
      "Epoch [1/1], Step [2194/8897], Loss: 5.2769\n",
      "Epoch [1/1], Step [2195/8897], Loss: 5.5269\n",
      "Epoch [1/1], Step [2196/8897], Loss: 5.4832\n",
      "Epoch [1/1], Step [2197/8897], Loss: 5.5259\n",
      "Epoch [1/1], Step [2198/8897], Loss: 5.2945\n",
      "Epoch [1/1], Step [2199/8897], Loss: 5.4348\n",
      "Epoch [1/1], Step [2200/8897], Loss: 5.5722\n",
      "Epoch [1/1], Step [2201/8897], Loss: 5.4261\n",
      "Epoch [1/1], Step [2202/8897], Loss: 5.4379\n",
      "Epoch [1/1], Step [2203/8897], Loss: 5.3572\n",
      "Epoch [1/1], Step [2204/8897], Loss: 5.4039\n",
      "Epoch [1/1], Step [2205/8897], Loss: 5.6359\n",
      "Epoch [1/1], Step [2206/8897], Loss: 5.2473\n",
      "Epoch [1/1], Step [2207/8897], Loss: 5.2951\n",
      "Epoch [1/1], Step [2208/8897], Loss: 5.2945\n",
      "Epoch [1/1], Step [2209/8897], Loss: 5.3949\n",
      "Epoch [1/1], Step [2210/8897], Loss: 5.2294\n",
      "Epoch [1/1], Step [2211/8897], Loss: 5.3873\n",
      "Epoch [1/1], Step [2212/8897], Loss: 5.2844\n",
      "Epoch [1/1], Step [2213/8897], Loss: 5.4315\n",
      "Epoch [1/1], Step [2214/8897], Loss: 5.5892\n",
      "Epoch [1/1], Step [2215/8897], Loss: 5.2416\n",
      "Epoch [1/1], Step [2216/8897], Loss: 5.5853\n",
      "Epoch [1/1], Step [2217/8897], Loss: 5.1921\n",
      "Epoch [1/1], Step [2218/8897], Loss: 5.4913\n",
      "Epoch [1/1], Step [2219/8897], Loss: 5.7203\n",
      "Epoch [1/1], Step [2220/8897], Loss: 5.4483\n",
      "Epoch [1/1], Step [2221/8897], Loss: 5.1523\n",
      "Epoch [1/1], Step [2222/8897], Loss: 5.3741\n",
      "Epoch [1/1], Step [2223/8897], Loss: 5.5626\n",
      "Epoch [1/1], Step [2224/8897], Loss: 5.5056\n",
      "Epoch [1/1], Step [2225/8897], Loss: 5.3009\n",
      "Epoch [1/1], Step [2226/8897], Loss: 5.4472\n",
      "Epoch [1/1], Step [2227/8897], Loss: 5.1810\n",
      "Epoch [1/1], Step [2228/8897], Loss: 5.2831\n",
      "Epoch [1/1], Step [2229/8897], Loss: 5.2736\n",
      "Epoch [1/1], Step [2230/8897], Loss: 5.5090\n",
      "Epoch [1/1], Step [2231/8897], Loss: 5.0889\n",
      "Epoch [1/1], Step [2232/8897], Loss: 5.3142\n",
      "Epoch [1/1], Step [2233/8897], Loss: 5.6126\n",
      "Epoch [1/1], Step [2234/8897], Loss: 5.1747\n",
      "Epoch [1/1], Step [2235/8897], Loss: 5.3611\n",
      "Epoch [1/1], Step [2236/8897], Loss: 5.6611\n",
      "Epoch [1/1], Step [2237/8897], Loss: 5.4898\n",
      "Epoch [1/1], Step [2238/8897], Loss: 5.5728\n",
      "Epoch [1/1], Step [2239/8897], Loss: 5.4367\n",
      "Epoch [1/1], Step [2240/8897], Loss: 5.2617\n",
      "Epoch [1/1], Step [2241/8897], Loss: 5.6114\n",
      "Epoch [1/1], Step [2242/8897], Loss: 5.5913\n",
      "Epoch [1/1], Step [2243/8897], Loss: 5.6774\n",
      "Epoch [1/1], Step [2244/8897], Loss: 5.3119\n",
      "Epoch [1/1], Step [2245/8897], Loss: 5.3024\n",
      "Epoch [1/1], Step [2246/8897], Loss: 5.1868\n",
      "Epoch [1/1], Step [2247/8897], Loss: 5.5112\n",
      "Epoch [1/1], Step [2248/8897], Loss: 4.9948\n",
      "Epoch [1/1], Step [2249/8897], Loss: 5.6440\n",
      "Epoch [1/1], Step [2250/8897], Loss: 5.5022\n",
      "Epoch [1/1], Step [2251/8897], Loss: 5.6921\n",
      "Epoch [1/1], Step [2252/8897], Loss: 5.5942\n",
      "Epoch [1/1], Step [2253/8897], Loss: 5.3396\n",
      "Epoch [1/1], Step [2254/8897], Loss: 5.3428\n",
      "Epoch [1/1], Step [2255/8897], Loss: 5.6349\n",
      "Epoch [1/1], Step [2256/8897], Loss: 5.3342\n",
      "Epoch [1/1], Step [2257/8897], Loss: 5.6259\n",
      "Epoch [1/1], Step [2258/8897], Loss: 5.1685\n",
      "Epoch [1/1], Step [2259/8897], Loss: 5.4984\n",
      "Epoch [1/1], Step [2260/8897], Loss: 5.2483\n",
      "Epoch [1/1], Step [2261/8897], Loss: 5.4503\n",
      "Epoch [1/1], Step [2262/8897], Loss: 5.4233\n",
      "Epoch [1/1], Step [2263/8897], Loss: 5.3117\n",
      "Epoch [1/1], Step [2264/8897], Loss: 5.6913\n",
      "Epoch [1/1], Step [2265/8897], Loss: 5.3887\n",
      "Epoch [1/1], Step [2266/8897], Loss: 5.5318\n",
      "Epoch [1/1], Step [2267/8897], Loss: 5.4157\n",
      "Epoch [1/1], Step [2268/8897], Loss: 5.4345\n",
      "Epoch [1/1], Step [2269/8897], Loss: 5.4068\n",
      "Epoch [1/1], Step [2270/8897], Loss: 5.6791\n",
      "Epoch [1/1], Step [2271/8897], Loss: 5.3514\n",
      "Epoch [1/1], Step [2272/8897], Loss: 5.2208\n",
      "Epoch [1/1], Step [2273/8897], Loss: 5.6215\n",
      "Epoch [1/1], Step [2274/8897], Loss: 5.3629\n",
      "Epoch [1/1], Step [2275/8897], Loss: 5.3740\n",
      "Epoch [1/1], Step [2276/8897], Loss: 5.2180\n",
      "Epoch [1/1], Step [2277/8897], Loss: 5.3655\n",
      "Epoch [1/1], Step [2278/8897], Loss: 5.5038\n",
      "Epoch [1/1], Step [2279/8897], Loss: 5.3350\n",
      "Epoch [1/1], Step [2280/8897], Loss: 5.3787\n",
      "Epoch [1/1], Step [2281/8897], Loss: 5.5219\n",
      "Epoch [1/1], Step [2282/8897], Loss: 5.3003\n",
      "Epoch [1/1], Step [2283/8897], Loss: 5.2993\n",
      "Epoch [1/1], Step [2284/8897], Loss: 5.2550\n",
      "Epoch [1/1], Step [2285/8897], Loss: 5.3319\n",
      "Epoch [1/1], Step [2286/8897], Loss: 5.4212\n",
      "Epoch [1/1], Step [2287/8897], Loss: 5.5294\n",
      "Epoch [1/1], Step [2288/8897], Loss: 5.3102\n",
      "Epoch [1/1], Step [2289/8897], Loss: 5.1963\n",
      "Epoch [1/1], Step [2290/8897], Loss: 5.5079\n",
      "Epoch [1/1], Step [2291/8897], Loss: 5.3922\n",
      "Epoch [1/1], Step [2292/8897], Loss: 5.5604\n",
      "Epoch [1/1], Step [2293/8897], Loss: 5.3308\n",
      "Epoch [1/1], Step [2294/8897], Loss: 5.4828\n",
      "Epoch [1/1], Step [2295/8897], Loss: 5.5601\n",
      "Epoch [1/1], Step [2296/8897], Loss: 5.2368\n",
      "Epoch [1/1], Step [2297/8897], Loss: 5.6740\n",
      "Epoch [1/1], Step [2298/8897], Loss: 5.3640\n",
      "Epoch [1/1], Step [2299/8897], Loss: 5.3513\n",
      "Epoch [1/1], Step [2300/8897], Loss: 5.4273\n",
      "Epoch [1/1], Step [2301/8897], Loss: 5.4029\n",
      "Epoch [1/1], Step [2302/8897], Loss: 5.2273\n",
      "Epoch [1/1], Step [2303/8897], Loss: 5.4885\n",
      "Epoch [1/1], Step [2304/8897], Loss: 5.7828\n",
      "Epoch [1/1], Step [2305/8897], Loss: 5.2882\n",
      "Epoch [1/1], Step [2306/8897], Loss: 5.3606\n",
      "Epoch [1/1], Step [2307/8897], Loss: 5.5959\n",
      "Epoch [1/1], Step [2308/8897], Loss: 5.5391\n",
      "Epoch [1/1], Step [2309/8897], Loss: 5.5586\n",
      "Epoch [1/1], Step [2310/8897], Loss: 5.6783\n",
      "Epoch [1/1], Step [2311/8897], Loss: 5.5416\n",
      "Epoch [1/1], Step [2312/8897], Loss: 5.2040\n",
      "Epoch [1/1], Step [2313/8897], Loss: 5.3893\n",
      "Epoch [1/1], Step [2314/8897], Loss: 5.3134\n",
      "Epoch [1/1], Step [2315/8897], Loss: 5.4615\n",
      "Epoch [1/1], Step [2316/8897], Loss: 5.3216\n",
      "Epoch [1/1], Step [2317/8897], Loss: 5.5597\n",
      "Epoch [1/1], Step [2318/8897], Loss: 5.4822\n",
      "Epoch [1/1], Step [2319/8897], Loss: 5.4021\n",
      "Epoch [1/1], Step [2320/8897], Loss: 5.3239\n",
      "Epoch [1/1], Step [2321/8897], Loss: 5.1603\n",
      "Epoch [1/1], Step [2322/8897], Loss: 5.1918\n",
      "Epoch [1/1], Step [2323/8897], Loss: 5.3603\n",
      "Epoch [1/1], Step [2324/8897], Loss: 5.4474\n",
      "Epoch [1/1], Step [2325/8897], Loss: 5.6391\n",
      "Epoch [1/1], Step [2326/8897], Loss: 5.6561\n",
      "Epoch [1/1], Step [2327/8897], Loss: 5.2159\n",
      "Epoch [1/1], Step [2328/8897], Loss: 5.4652\n",
      "Epoch [1/1], Step [2329/8897], Loss: 5.4522\n",
      "Epoch [1/1], Step [2330/8897], Loss: 5.5481\n",
      "Epoch [1/1], Step [2331/8897], Loss: 5.4209\n",
      "Epoch [1/1], Step [2332/8897], Loss: 5.4232\n",
      "Epoch [1/1], Step [2333/8897], Loss: 5.2676\n",
      "Epoch [1/1], Step [2334/8897], Loss: 5.1083\n",
      "Epoch [1/1], Step [2335/8897], Loss: 5.4897\n",
      "Epoch [1/1], Step [2336/8897], Loss: 5.5964\n",
      "Epoch [1/1], Step [2337/8897], Loss: 5.2417\n",
      "Epoch [1/1], Step [2338/8897], Loss: 5.1888\n",
      "Epoch [1/1], Step [2339/8897], Loss: 5.5144\n",
      "Epoch [1/1], Step [2340/8897], Loss: 5.3049\n",
      "Epoch [1/1], Step [2341/8897], Loss: 5.3038\n",
      "Epoch [1/1], Step [2342/8897], Loss: 5.3275\n",
      "Epoch [1/1], Step [2343/8897], Loss: 5.2717\n",
      "Epoch [1/1], Step [2344/8897], Loss: 5.7652\n",
      "Epoch [1/1], Step [2345/8897], Loss: 5.5919\n",
      "Epoch [1/1], Step [2346/8897], Loss: 5.4306\n",
      "Epoch [1/1], Step [2347/8897], Loss: 5.4482\n",
      "Epoch [1/1], Step [2348/8897], Loss: 5.2089\n",
      "Epoch [1/1], Step [2349/8897], Loss: 5.4726\n",
      "Epoch [1/1], Step [2350/8897], Loss: 5.3356\n",
      "Epoch [1/1], Step [2351/8897], Loss: 5.4373\n",
      "Epoch [1/1], Step [2352/8897], Loss: 5.8836\n",
      "Epoch [1/1], Step [2353/8897], Loss: 5.4748\n",
      "Epoch [1/1], Step [2354/8897], Loss: 5.5389\n",
      "Epoch [1/1], Step [2355/8897], Loss: 5.2626\n",
      "Epoch [1/1], Step [2356/8897], Loss: 5.5352\n",
      "Epoch [1/1], Step [2357/8897], Loss: 5.6594\n",
      "Epoch [1/1], Step [2358/8897], Loss: 5.4194\n",
      "Epoch [1/1], Step [2359/8897], Loss: 5.5080\n",
      "Epoch [1/1], Step [2360/8897], Loss: 5.4057\n",
      "Epoch [1/1], Step [2361/8897], Loss: 5.6426\n",
      "Epoch [1/1], Step [2362/8897], Loss: 5.5544\n",
      "Epoch [1/1], Step [2363/8897], Loss: 5.4271\n",
      "Epoch [1/1], Step [2364/8897], Loss: 5.5965\n",
      "Epoch [1/1], Step [2365/8897], Loss: 5.3475\n",
      "Epoch [1/1], Step [2366/8897], Loss: 5.4628\n",
      "Epoch [1/1], Step [2367/8897], Loss: 5.2606\n",
      "Epoch [1/1], Step [2368/8897], Loss: 5.4373\n",
      "Epoch [1/1], Step [2369/8897], Loss: 5.4052\n",
      "Epoch [1/1], Step [2370/8897], Loss: 5.3007\n",
      "Epoch [1/1], Step [2371/8897], Loss: 5.6155\n",
      "Epoch [1/1], Step [2372/8897], Loss: 5.2685\n",
      "Epoch [1/1], Step [2373/8897], Loss: 5.3812\n",
      "Epoch [1/1], Step [2374/8897], Loss: 5.3044\n",
      "Epoch [1/1], Step [2375/8897], Loss: 5.3325\n",
      "Epoch [1/1], Step [2376/8897], Loss: 5.1974\n",
      "Epoch [1/1], Step [2377/8897], Loss: 5.5506\n",
      "Epoch [1/1], Step [2378/8897], Loss: 5.3495\n",
      "Epoch [1/1], Step [2379/8897], Loss: 5.4345\n",
      "Epoch [1/1], Step [2380/8897], Loss: 5.5341\n",
      "Epoch [1/1], Step [2381/8897], Loss: 5.2056\n",
      "Epoch [1/1], Step [2382/8897], Loss: 5.4782\n",
      "Epoch [1/1], Step [2383/8897], Loss: 5.4199\n",
      "Epoch [1/1], Step [2384/8897], Loss: 5.3077\n",
      "Epoch [1/1], Step [2385/8897], Loss: 5.2215\n",
      "Epoch [1/1], Step [2386/8897], Loss: 5.4056\n",
      "Epoch [1/1], Step [2387/8897], Loss: 5.1496\n",
      "Epoch [1/1], Step [2388/8897], Loss: 5.4185\n",
      "Epoch [1/1], Step [2389/8897], Loss: 5.3248\n",
      "Epoch [1/1], Step [2390/8897], Loss: 5.4516\n",
      "Epoch [1/1], Step [2391/8897], Loss: 5.3834\n",
      "Epoch [1/1], Step [2392/8897], Loss: 5.3791\n",
      "Epoch [1/1], Step [2393/8897], Loss: 5.4970\n",
      "Epoch [1/1], Step [2394/8897], Loss: 5.5606\n",
      "Epoch [1/1], Step [2395/8897], Loss: 5.5926\n",
      "Epoch [1/1], Step [2396/8897], Loss: 5.5112\n",
      "Epoch [1/1], Step [2397/8897], Loss: 5.6468\n",
      "Epoch [1/1], Step [2398/8897], Loss: 5.4630\n",
      "Epoch [1/1], Step [2399/8897], Loss: 5.3182\n",
      "Epoch [1/1], Step [2400/8897], Loss: 5.3719\n",
      "Epoch [1/1], Step [2401/8897], Loss: 5.3253\n",
      "Epoch [1/1], Step [2402/8897], Loss: 5.2863\n",
      "Epoch [1/1], Step [2403/8897], Loss: 5.4127\n",
      "Epoch [1/1], Step [2404/8897], Loss: 5.6098\n",
      "Epoch [1/1], Step [2405/8897], Loss: 5.2228\n",
      "Epoch [1/1], Step [2406/8897], Loss: 5.3638\n",
      "Epoch [1/1], Step [2407/8897], Loss: 5.1044\n",
      "Epoch [1/1], Step [2408/8897], Loss: 5.3804\n",
      "Epoch [1/1], Step [2409/8897], Loss: 5.3481\n",
      "Epoch [1/1], Step [2410/8897], Loss: 5.4028\n",
      "Epoch [1/1], Step [2411/8897], Loss: 5.6019\n",
      "Epoch [1/1], Step [2412/8897], Loss: 5.5514\n",
      "Epoch [1/1], Step [2413/8897], Loss: 5.5024\n",
      "Epoch [1/1], Step [2414/8897], Loss: 5.3927\n",
      "Epoch [1/1], Step [2415/8897], Loss: 5.4377\n",
      "Epoch [1/1], Step [2416/8897], Loss: 5.3276\n",
      "Epoch [1/1], Step [2417/8897], Loss: 5.4300\n",
      "Epoch [1/1], Step [2418/8897], Loss: 5.3750\n",
      "Epoch [1/1], Step [2419/8897], Loss: 5.3618\n",
      "Epoch [1/1], Step [2420/8897], Loss: 5.7085\n",
      "Epoch [1/1], Step [2421/8897], Loss: 5.5121\n",
      "Epoch [1/1], Step [2422/8897], Loss: 5.6474\n",
      "Epoch [1/1], Step [2423/8897], Loss: 5.3090\n",
      "Epoch [1/1], Step [2424/8897], Loss: 5.1790\n",
      "Epoch [1/1], Step [2425/8897], Loss: 5.2673\n",
      "Epoch [1/1], Step [2426/8897], Loss: 5.5356\n",
      "Epoch [1/1], Step [2427/8897], Loss: 5.5365\n",
      "Epoch [1/1], Step [2428/8897], Loss: 5.6490\n",
      "Epoch [1/1], Step [2429/8897], Loss: 5.4417\n",
      "Epoch [1/1], Step [2430/8897], Loss: 5.7122\n",
      "Epoch [1/1], Step [2431/8897], Loss: 5.2617\n",
      "Epoch [1/1], Step [2432/8897], Loss: 5.4466\n",
      "Epoch [1/1], Step [2433/8897], Loss: 5.6306\n",
      "Epoch [1/1], Step [2434/8897], Loss: 5.4050\n",
      "Epoch [1/1], Step [2435/8897], Loss: 5.6748\n",
      "Epoch [1/1], Step [2436/8897], Loss: 5.3248\n",
      "Epoch [1/1], Step [2437/8897], Loss: 5.3972\n",
      "Epoch [1/1], Step [2438/8897], Loss: 5.4282\n",
      "Epoch [1/1], Step [2439/8897], Loss: 5.4414\n",
      "Epoch [1/1], Step [2440/8897], Loss: 5.5069\n",
      "Epoch [1/1], Step [2441/8897], Loss: 5.2611\n",
      "Epoch [1/1], Step [2442/8897], Loss: 5.3274\n",
      "Epoch [1/1], Step [2443/8897], Loss: 5.5349\n",
      "Epoch [1/1], Step [2444/8897], Loss: 5.2110\n",
      "Epoch [1/1], Step [2445/8897], Loss: 5.4502\n",
      "Epoch [1/1], Step [2446/8897], Loss: 5.1357\n",
      "Epoch [1/1], Step [2447/8897], Loss: 5.4971\n",
      "Epoch [1/1], Step [2448/8897], Loss: 5.3631\n",
      "Epoch [1/1], Step [2449/8897], Loss: 5.8416\n",
      "Epoch [1/1], Step [2450/8897], Loss: 5.4992\n",
      "Epoch [1/1], Step [2451/8897], Loss: 5.4374\n",
      "Epoch [1/1], Step [2452/8897], Loss: 5.3798\n",
      "Epoch [1/1], Step [2453/8897], Loss: 5.4237\n",
      "Epoch [1/1], Step [2454/8897], Loss: 5.6345\n",
      "Epoch [1/1], Step [2455/8897], Loss: 5.3892\n",
      "Epoch [1/1], Step [2456/8897], Loss: 5.2350\n",
      "Epoch [1/1], Step [2457/8897], Loss: 5.5353\n",
      "Epoch [1/1], Step [2458/8897], Loss: 5.3854\n",
      "Epoch [1/1], Step [2459/8897], Loss: 5.3538\n",
      "Epoch [1/1], Step [2460/8897], Loss: 5.7419\n",
      "Epoch [1/1], Step [2461/8897], Loss: 5.4488\n",
      "Epoch [1/1], Step [2462/8897], Loss: 5.2792\n",
      "Epoch [1/1], Step [2463/8897], Loss: 5.1647\n",
      "Epoch [1/1], Step [2464/8897], Loss: 5.4511\n",
      "Epoch [1/1], Step [2465/8897], Loss: 5.3546\n",
      "Epoch [1/1], Step [2466/8897], Loss: 5.5126\n",
      "Epoch [1/1], Step [2467/8897], Loss: 5.5599\n",
      "Epoch [1/1], Step [2468/8897], Loss: 5.2904\n",
      "Epoch [1/1], Step [2469/8897], Loss: 5.6243\n",
      "Epoch [1/1], Step [2470/8897], Loss: 5.3746\n",
      "Epoch [1/1], Step [2471/8897], Loss: 5.5279\n",
      "Epoch [1/1], Step [2472/8897], Loss: 5.3050\n",
      "Epoch [1/1], Step [2473/8897], Loss: 5.2972\n",
      "Epoch [1/1], Step [2474/8897], Loss: 5.5776\n",
      "Epoch [1/1], Step [2475/8897], Loss: 5.4657\n",
      "Epoch [1/1], Step [2476/8897], Loss: 5.1337\n",
      "Epoch [1/1], Step [2477/8897], Loss: 5.2891\n",
      "Epoch [1/1], Step [2478/8897], Loss: 5.3377\n",
      "Epoch [1/1], Step [2479/8897], Loss: 5.4436\n",
      "Epoch [1/1], Step [2480/8897], Loss: 5.7195\n",
      "Epoch [1/1], Step [2481/8897], Loss: 5.1855\n",
      "Epoch [1/1], Step [2482/8897], Loss: 5.3946\n",
      "Epoch [1/1], Step [2483/8897], Loss: 5.4306\n",
      "Epoch [1/1], Step [2484/8897], Loss: 5.4239\n",
      "Epoch [1/1], Step [2485/8897], Loss: 5.5934\n",
      "Epoch [1/1], Step [2486/8897], Loss: 5.3441\n",
      "Epoch [1/1], Step [2487/8897], Loss: 5.1519\n",
      "Epoch [1/1], Step [2488/8897], Loss: 5.5381\n",
      "Epoch [1/1], Step [2489/8897], Loss: 5.2168\n",
      "Epoch [1/1], Step [2490/8897], Loss: 5.5436\n",
      "Epoch [1/1], Step [2491/8897], Loss: 5.2771\n",
      "Epoch [1/1], Step [2492/8897], Loss: 5.6523\n",
      "Epoch [1/1], Step [2493/8897], Loss: 5.5158\n",
      "Epoch [1/1], Step [2494/8897], Loss: 5.6561\n",
      "Epoch [1/1], Step [2495/8897], Loss: 5.2268\n",
      "Epoch [1/1], Step [2496/8897], Loss: 5.5824\n",
      "Epoch [1/1], Step [2497/8897], Loss: 5.5297\n",
      "Epoch [1/1], Step [2498/8897], Loss: 5.3249\n",
      "Epoch [1/1], Step [2499/8897], Loss: 5.4172\n",
      "Epoch [1/1], Step [2500/8897], Loss: 5.5205\n",
      "Epoch [1/1], Step [2501/8897], Loss: 5.4119\n",
      "Epoch [1/1], Step [2502/8897], Loss: 5.1292\n",
      "Epoch [1/1], Step [2503/8897], Loss: 5.2175\n",
      "Epoch [1/1], Step [2504/8897], Loss: 5.4058\n",
      "Epoch [1/1], Step [2505/8897], Loss: 5.5387\n",
      "Epoch [1/1], Step [2506/8897], Loss: 5.6159\n",
      "Epoch [1/1], Step [2507/8897], Loss: 5.4471\n",
      "Epoch [1/1], Step [2508/8897], Loss: 5.5769\n",
      "Epoch [1/1], Step [2509/8897], Loss: 5.2884\n",
      "Epoch [1/1], Step [2510/8897], Loss: 5.4584\n",
      "Epoch [1/1], Step [2511/8897], Loss: 5.2431\n",
      "Epoch [1/1], Step [2512/8897], Loss: 5.3225\n",
      "Epoch [1/1], Step [2513/8897], Loss: 5.5893\n",
      "Epoch [1/1], Step [2514/8897], Loss: 5.2097\n",
      "Epoch [1/1], Step [2515/8897], Loss: 5.6049\n",
      "Epoch [1/1], Step [2516/8897], Loss: 5.6433\n",
      "Epoch [1/1], Step [2517/8897], Loss: 5.5605\n",
      "Epoch [1/1], Step [2518/8897], Loss: 5.4129\n",
      "Epoch [1/1], Step [2519/8897], Loss: 5.4334\n",
      "Epoch [1/1], Step [2520/8897], Loss: 5.4147\n",
      "Epoch [1/1], Step [2521/8897], Loss: 5.3959\n",
      "Epoch [1/1], Step [2522/8897], Loss: 5.4934\n",
      "Epoch [1/1], Step [2523/8897], Loss: 5.3912\n",
      "Epoch [1/1], Step [2524/8897], Loss: 5.3560\n",
      "Epoch [1/1], Step [2525/8897], Loss: 5.4873\n",
      "Epoch [1/1], Step [2526/8897], Loss: 5.4797\n",
      "Epoch [1/1], Step [2527/8897], Loss: 5.3896\n",
      "Epoch [1/1], Step [2528/8897], Loss: 5.3205\n",
      "Epoch [1/1], Step [2529/8897], Loss: 5.7253\n",
      "Epoch [1/1], Step [2530/8897], Loss: 5.3401\n",
      "Epoch [1/1], Step [2531/8897], Loss: 5.4289\n",
      "Epoch [1/1], Step [2532/8897], Loss: 5.3148\n",
      "Epoch [1/1], Step [2533/8897], Loss: 5.3484\n",
      "Epoch [1/1], Step [2534/8897], Loss: 5.5667\n",
      "Epoch [1/1], Step [2535/8897], Loss: 5.4551\n",
      "Epoch [1/1], Step [2536/8897], Loss: 5.2943\n",
      "Epoch [1/1], Step [2537/8897], Loss: 5.4678\n",
      "Epoch [1/1], Step [2538/8897], Loss: 5.4593\n",
      "Epoch [1/1], Step [2539/8897], Loss: 5.3595\n",
      "Epoch [1/1], Step [2540/8897], Loss: 5.3943\n",
      "Epoch [1/1], Step [2541/8897], Loss: 5.2572\n",
      "Epoch [1/1], Step [2542/8897], Loss: 5.6730\n",
      "Epoch [1/1], Step [2543/8897], Loss: 5.4948\n",
      "Epoch [1/1], Step [2544/8897], Loss: 5.6268\n",
      "Epoch [1/1], Step [2545/8897], Loss: 5.0034\n",
      "Epoch [1/1], Step [2546/8897], Loss: 5.3634\n",
      "Epoch [1/1], Step [2547/8897], Loss: 5.2410\n",
      "Epoch [1/1], Step [2548/8897], Loss: 5.4112\n",
      "Epoch [1/1], Step [2549/8897], Loss: 5.4434\n",
      "Epoch [1/1], Step [2550/8897], Loss: 5.7118\n",
      "Epoch [1/1], Step [2551/8897], Loss: 5.4076\n",
      "Epoch [1/1], Step [2552/8897], Loss: 5.6579\n",
      "Epoch [1/1], Step [2553/8897], Loss: 5.5339\n",
      "Epoch [1/1], Step [2554/8897], Loss: 5.1789\n",
      "Epoch [1/1], Step [2555/8897], Loss: 5.4797\n",
      "Epoch [1/1], Step [2556/8897], Loss: 5.4391\n",
      "Epoch [1/1], Step [2557/8897], Loss: 5.1566\n",
      "Epoch [1/1], Step [2558/8897], Loss: 5.5067\n",
      "Epoch [1/1], Step [2559/8897], Loss: 5.4563\n",
      "Epoch [1/1], Step [2560/8897], Loss: 5.2146\n",
      "Epoch [1/1], Step [2561/8897], Loss: 5.5592\n",
      "Epoch [1/1], Step [2562/8897], Loss: 5.2246\n",
      "Epoch [1/1], Step [2563/8897], Loss: 5.5476\n",
      "Epoch [1/1], Step [2564/8897], Loss: 5.2924\n",
      "Epoch [1/1], Step [2565/8897], Loss: 5.3382\n",
      "Epoch [1/1], Step [2566/8897], Loss: 5.4446\n",
      "Epoch [1/1], Step [2567/8897], Loss: 5.4901\n",
      "Epoch [1/1], Step [2568/8897], Loss: 5.3232\n",
      "Epoch [1/1], Step [2569/8897], Loss: 5.5538\n",
      "Epoch [1/1], Step [2570/8897], Loss: 5.6412\n",
      "Epoch [1/1], Step [2571/8897], Loss: 5.3570\n",
      "Epoch [1/1], Step [2572/8897], Loss: 5.3362\n",
      "Epoch [1/1], Step [2573/8897], Loss: 5.5021\n",
      "Epoch [1/1], Step [2574/8897], Loss: 5.3954\n",
      "Epoch [1/1], Step [2575/8897], Loss: 5.4729\n",
      "Epoch [1/1], Step [2576/8897], Loss: 5.4319\n",
      "Epoch [1/1], Step [2577/8897], Loss: 5.5576\n",
      "Epoch [1/1], Step [2578/8897], Loss: 5.5480\n",
      "Epoch [1/1], Step [2579/8897], Loss: 5.2293\n",
      "Epoch [1/1], Step [2580/8897], Loss: 5.5238\n",
      "Epoch [1/1], Step [2581/8897], Loss: 5.4102\n",
      "Epoch [1/1], Step [2582/8897], Loss: 4.9939\n",
      "Epoch [1/1], Step [2583/8897], Loss: 5.3246\n",
      "Epoch [1/1], Step [2584/8897], Loss: 5.5214\n",
      "Epoch [1/1], Step [2585/8897], Loss: 5.4438\n",
      "Epoch [1/1], Step [2586/8897], Loss: 5.2550\n",
      "Epoch [1/1], Step [2587/8897], Loss: 5.6959\n",
      "Epoch [1/1], Step [2588/8897], Loss: 5.4055\n",
      "Epoch [1/1], Step [2589/8897], Loss: 5.1044\n",
      "Epoch [1/1], Step [2590/8897], Loss: 5.4368\n",
      "Epoch [1/1], Step [2591/8897], Loss: 5.5151\n",
      "Epoch [1/1], Step [2592/8897], Loss: 5.7038\n",
      "Epoch [1/1], Step [2593/8897], Loss: 5.3565\n",
      "Epoch [1/1], Step [2594/8897], Loss: 5.3792\n",
      "Epoch [1/1], Step [2595/8897], Loss: 5.3432\n",
      "Epoch [1/1], Step [2596/8897], Loss: 5.6112\n",
      "Epoch [1/1], Step [2597/8897], Loss: 5.4578\n",
      "Epoch [1/1], Step [2598/8897], Loss: 5.5198\n",
      "Epoch [1/1], Step [2599/8897], Loss: 5.3614\n",
      "Epoch [1/1], Step [2600/8897], Loss: 5.4829\n",
      "Epoch [1/1], Step [2601/8897], Loss: 5.2305\n",
      "Epoch [1/1], Step [2602/8897], Loss: 5.3076\n",
      "Epoch [1/1], Step [2603/8897], Loss: 5.6239\n",
      "Epoch [1/1], Step [2604/8897], Loss: 5.3871\n",
      "Epoch [1/1], Step [2605/8897], Loss: 5.4853\n",
      "Epoch [1/1], Step [2606/8897], Loss: 5.2689\n",
      "Epoch [1/1], Step [2607/8897], Loss: 5.3489\n",
      "Epoch [1/1], Step [2608/8897], Loss: 5.6139\n",
      "Epoch [1/1], Step [2609/8897], Loss: 5.3953\n",
      "Epoch [1/1], Step [2610/8897], Loss: 5.6982\n",
      "Epoch [1/1], Step [2611/8897], Loss: 5.4669\n",
      "Epoch [1/1], Step [2612/8897], Loss: 5.5821\n",
      "Epoch [1/1], Step [2613/8897], Loss: 5.3073\n",
      "Epoch [1/1], Step [2614/8897], Loss: 5.4843\n",
      "Epoch [1/1], Step [2615/8897], Loss: 5.3262\n",
      "Epoch [1/1], Step [2616/8897], Loss: 5.3253\n",
      "Epoch [1/1], Step [2617/8897], Loss: 5.3365\n",
      "Epoch [1/1], Step [2618/8897], Loss: 5.4310\n",
      "Epoch [1/1], Step [2619/8897], Loss: 5.4254\n",
      "Epoch [1/1], Step [2620/8897], Loss: 5.4534\n",
      "Epoch [1/1], Step [2621/8897], Loss: 5.3170\n",
      "Epoch [1/1], Step [2622/8897], Loss: 5.4537\n",
      "Epoch [1/1], Step [2623/8897], Loss: 5.4093\n",
      "Epoch [1/1], Step [2624/8897], Loss: 5.2730\n",
      "Epoch [1/1], Step [2625/8897], Loss: 5.5266\n",
      "Epoch [1/1], Step [2626/8897], Loss: 5.5143\n",
      "Epoch [1/1], Step [2627/8897], Loss: 5.4751\n",
      "Epoch [1/1], Step [2628/8897], Loss: 5.4055\n",
      "Epoch [1/1], Step [2629/8897], Loss: 5.4103\n",
      "Epoch [1/1], Step [2630/8897], Loss: 5.6011\n",
      "Epoch [1/1], Step [2631/8897], Loss: 5.3456\n",
      "Epoch [1/1], Step [2632/8897], Loss: 5.5607\n",
      "Epoch [1/1], Step [2633/8897], Loss: 5.5606\n",
      "Epoch [1/1], Step [2634/8897], Loss: 5.1306\n",
      "Epoch [1/1], Step [2635/8897], Loss: 5.2560\n",
      "Epoch [1/1], Step [2636/8897], Loss: 5.2695\n",
      "Epoch [1/1], Step [2637/8897], Loss: 5.3968\n",
      "Epoch [1/1], Step [2638/8897], Loss: 5.3254\n",
      "Epoch [1/1], Step [2639/8897], Loss: 5.2547\n",
      "Epoch [1/1], Step [2640/8897], Loss: 5.0608\n",
      "Epoch [1/1], Step [2641/8897], Loss: 5.2822\n",
      "Epoch [1/1], Step [2642/8897], Loss: 5.6126\n",
      "Epoch [1/1], Step [2643/8897], Loss: 5.3657\n",
      "Epoch [1/1], Step [2644/8897], Loss: 5.2020\n",
      "Epoch [1/1], Step [2645/8897], Loss: 5.2325\n",
      "Epoch [1/1], Step [2646/8897], Loss: 5.2948\n",
      "Epoch [1/1], Step [2647/8897], Loss: 5.4003\n",
      "Epoch [1/1], Step [2648/8897], Loss: 5.4040\n",
      "Epoch [1/1], Step [2649/8897], Loss: 5.4964\n",
      "Epoch [1/1], Step [2650/8897], Loss: 5.2358\n",
      "Epoch [1/1], Step [2651/8897], Loss: 5.2743\n",
      "Epoch [1/1], Step [2652/8897], Loss: 5.4044\n",
      "Epoch [1/1], Step [2653/8897], Loss: 5.3335\n",
      "Epoch [1/1], Step [2654/8897], Loss: 5.3080\n",
      "Epoch [1/1], Step [2655/8897], Loss: 5.5111\n",
      "Epoch [1/1], Step [2656/8897], Loss: 5.4682\n",
      "Epoch [1/1], Step [2657/8897], Loss: 5.4220\n",
      "Epoch [1/1], Step [2658/8897], Loss: 5.4750\n",
      "Epoch [1/1], Step [2659/8897], Loss: 5.2108\n",
      "Epoch [1/1], Step [2660/8897], Loss: 5.3912\n",
      "Epoch [1/1], Step [2661/8897], Loss: 5.4614\n",
      "Epoch [1/1], Step [2662/8897], Loss: 5.3718\n",
      "Epoch [1/1], Step [2663/8897], Loss: 5.4756\n",
      "Epoch [1/1], Step [2664/8897], Loss: 5.2367\n",
      "Epoch [1/1], Step [2665/8897], Loss: 5.4395\n",
      "Epoch [1/1], Step [2666/8897], Loss: 5.3952\n",
      "Epoch [1/1], Step [2667/8897], Loss: 5.4443\n",
      "Epoch [1/1], Step [2668/8897], Loss: 5.4175\n",
      "Epoch [1/1], Step [2669/8897], Loss: 5.2316\n",
      "Epoch [1/1], Step [2670/8897], Loss: 5.3987\n",
      "Epoch [1/1], Step [2671/8897], Loss: 5.2139\n",
      "Epoch [1/1], Step [2672/8897], Loss: 5.4046\n",
      "Epoch [1/1], Step [2673/8897], Loss: 5.3675\n",
      "Epoch [1/1], Step [2674/8897], Loss: 5.3956\n",
      "Epoch [1/1], Step [2675/8897], Loss: 5.3779\n",
      "Epoch [1/1], Step [2676/8897], Loss: 5.3640\n",
      "Epoch [1/1], Step [2677/8897], Loss: 5.2348\n",
      "Epoch [1/1], Step [2678/8897], Loss: 5.6308\n",
      "Epoch [1/1], Step [2679/8897], Loss: 5.1822\n",
      "Epoch [1/1], Step [2680/8897], Loss: 5.1795\n",
      "Epoch [1/1], Step [2681/8897], Loss: 5.1468\n",
      "Epoch [1/1], Step [2682/8897], Loss: 5.3852\n",
      "Epoch [1/1], Step [2683/8897], Loss: 5.2119\n",
      "Epoch [1/1], Step [2684/8897], Loss: 5.3360\n",
      "Epoch [1/1], Step [2685/8897], Loss: 5.5456\n",
      "Epoch [1/1], Step [2686/8897], Loss: 5.4679\n",
      "Epoch [1/1], Step [2687/8897], Loss: 5.4351\n",
      "Epoch [1/1], Step [2688/8897], Loss: 5.2789\n",
      "Epoch [1/1], Step [2689/8897], Loss: 5.5193\n",
      "Epoch [1/1], Step [2690/8897], Loss: 5.1952\n",
      "Epoch [1/1], Step [2691/8897], Loss: 5.5995\n",
      "Epoch [1/1], Step [2692/8897], Loss: 5.4293\n",
      "Epoch [1/1], Step [2693/8897], Loss: 5.2759\n",
      "Epoch [1/1], Step [2694/8897], Loss: 5.5437\n",
      "Epoch [1/1], Step [2695/8897], Loss: 5.3564\n",
      "Epoch [1/1], Step [2696/8897], Loss: 5.5262\n",
      "Epoch [1/1], Step [2697/8897], Loss: 5.3971\n",
      "Epoch [1/1], Step [2698/8897], Loss: 5.2688\n",
      "Epoch [1/1], Step [2699/8897], Loss: 5.4277\n",
      "Epoch [1/1], Step [2700/8897], Loss: 5.5086\n",
      "Epoch [1/1], Step [2701/8897], Loss: 5.2966\n",
      "Epoch [1/1], Step [2702/8897], Loss: 5.1582\n",
      "Epoch [1/1], Step [2703/8897], Loss: 5.5595\n",
      "Epoch [1/1], Step [2704/8897], Loss: 5.3187\n",
      "Epoch [1/1], Step [2705/8897], Loss: 5.4961\n",
      "Epoch [1/1], Step [2706/8897], Loss: 5.3483\n",
      "Epoch [1/1], Step [2707/8897], Loss: 5.3566\n",
      "Epoch [1/1], Step [2708/8897], Loss: 5.5032\n",
      "Epoch [1/1], Step [2709/8897], Loss: 5.4039\n",
      "Epoch [1/1], Step [2710/8897], Loss: 5.7510\n",
      "Epoch [1/1], Step [2711/8897], Loss: 5.4926\n",
      "Epoch [1/1], Step [2712/8897], Loss: 5.3478\n",
      "Epoch [1/1], Step [2713/8897], Loss: 5.6758\n",
      "Epoch [1/1], Step [2714/8897], Loss: 5.3229\n",
      "Epoch [1/1], Step [2715/8897], Loss: 5.2357\n",
      "Epoch [1/1], Step [2716/8897], Loss: 5.3702\n",
      "Epoch [1/1], Step [2717/8897], Loss: 5.3242\n",
      "Epoch [1/1], Step [2718/8897], Loss: 5.2342\n",
      "Epoch [1/1], Step [2719/8897], Loss: 5.5790\n",
      "Epoch [1/1], Step [2720/8897], Loss: 5.1454\n",
      "Epoch [1/1], Step [2721/8897], Loss: 5.3709\n",
      "Epoch [1/1], Step [2722/8897], Loss: 5.6793\n",
      "Epoch [1/1], Step [2723/8897], Loss: 5.4322\n",
      "Epoch [1/1], Step [2724/8897], Loss: 5.4272\n",
      "Epoch [1/1], Step [2725/8897], Loss: 5.3707\n",
      "Epoch [1/1], Step [2726/8897], Loss: 5.4234\n",
      "Epoch [1/1], Step [2727/8897], Loss: 5.4704\n",
      "Epoch [1/1], Step [2728/8897], Loss: 5.5136\n",
      "Epoch [1/1], Step [2729/8897], Loss: 5.2891\n",
      "Epoch [1/1], Step [2730/8897], Loss: 5.3229\n",
      "Epoch [1/1], Step [2731/8897], Loss: 5.5729\n",
      "Epoch [1/1], Step [2732/8897], Loss: 5.5015\n",
      "Epoch [1/1], Step [2733/8897], Loss: 5.4711\n",
      "Epoch [1/1], Step [2734/8897], Loss: 5.0147\n",
      "Epoch [1/1], Step [2735/8897], Loss: 5.4199\n",
      "Epoch [1/1], Step [2736/8897], Loss: 5.4253\n",
      "Epoch [1/1], Step [2737/8897], Loss: 5.5959\n",
      "Epoch [1/1], Step [2738/8897], Loss: 5.2749\n",
      "Epoch [1/1], Step [2739/8897], Loss: 5.1194\n",
      "Epoch [1/1], Step [2740/8897], Loss: 5.2911\n",
      "Epoch [1/1], Step [2741/8897], Loss: 5.1541\n",
      "Epoch [1/1], Step [2742/8897], Loss: 5.6753\n",
      "Epoch [1/1], Step [2743/8897], Loss: 5.4987\n",
      "Epoch [1/1], Step [2744/8897], Loss: 5.3920\n",
      "Epoch [1/1], Step [2745/8897], Loss: 5.2530\n",
      "Epoch [1/1], Step [2746/8897], Loss: 5.4622\n",
      "Epoch [1/1], Step [2747/8897], Loss: 5.3901\n",
      "Epoch [1/1], Step [2748/8897], Loss: 5.6487\n",
      "Epoch [1/1], Step [2749/8897], Loss: 5.5397\n",
      "Epoch [1/1], Step [2750/8897], Loss: 5.4973\n",
      "Epoch [1/1], Step [2751/8897], Loss: 5.1730\n",
      "Epoch [1/1], Step [2752/8897], Loss: 5.5475\n",
      "Epoch [1/1], Step [2753/8897], Loss: 5.3492\n",
      "Epoch [1/1], Step [2754/8897], Loss: 5.5431\n",
      "Epoch [1/1], Step [2755/8897], Loss: 5.4645\n",
      "Epoch [1/1], Step [2756/8897], Loss: 5.4972\n",
      "Epoch [1/1], Step [2757/8897], Loss: 5.4724\n",
      "Epoch [1/1], Step [2758/8897], Loss: 5.0117\n",
      "Epoch [1/1], Step [2759/8897], Loss: 5.4392\n",
      "Epoch [1/1], Step [2760/8897], Loss: 5.3025\n",
      "Epoch [1/1], Step [2761/8897], Loss: 5.5063\n",
      "Epoch [1/1], Step [2762/8897], Loss: 5.4077\n",
      "Epoch [1/1], Step [2763/8897], Loss: 5.5182\n",
      "Epoch [1/1], Step [2764/8897], Loss: 5.6555\n",
      "Epoch [1/1], Step [2765/8897], Loss: 5.3200\n",
      "Epoch [1/1], Step [2766/8897], Loss: 5.3946\n",
      "Epoch [1/1], Step [2767/8897], Loss: 5.5779\n",
      "Epoch [1/1], Step [2768/8897], Loss: 5.3818\n",
      "Epoch [1/1], Step [2769/8897], Loss: 5.3598\n",
      "Epoch [1/1], Step [2770/8897], Loss: 5.3963\n",
      "Epoch [1/1], Step [2771/8897], Loss: 5.5521\n",
      "Epoch [1/1], Step [2772/8897], Loss: 5.3144\n",
      "Epoch [1/1], Step [2773/8897], Loss: 5.4202\n",
      "Epoch [1/1], Step [2774/8897], Loss: 5.3332\n",
      "Epoch [1/1], Step [2775/8897], Loss: 5.5108\n",
      "Epoch [1/1], Step [2776/8897], Loss: 5.2862\n",
      "Epoch [1/1], Step [2777/8897], Loss: 5.6472\n",
      "Epoch [1/1], Step [2778/8897], Loss: 5.6644\n",
      "Epoch [1/1], Step [2779/8897], Loss: 5.6487\n",
      "Epoch [1/1], Step [2780/8897], Loss: 5.4280\n",
      "Epoch [1/1], Step [2781/8897], Loss: 5.4897\n",
      "Epoch [1/1], Step [2782/8897], Loss: 5.4371\n",
      "Epoch [1/1], Step [2783/8897], Loss: 5.4368\n",
      "Epoch [1/1], Step [2784/8897], Loss: 5.6248\n",
      "Epoch [1/1], Step [2785/8897], Loss: 5.3369\n",
      "Epoch [1/1], Step [2786/8897], Loss: 5.4160\n",
      "Epoch [1/1], Step [2787/8897], Loss: 5.6353\n",
      "Epoch [1/1], Step [2788/8897], Loss: 5.4838\n",
      "Epoch [1/1], Step [2789/8897], Loss: 5.4496\n",
      "Epoch [1/1], Step [2790/8897], Loss: 5.4700\n",
      "Epoch [1/1], Step [2791/8897], Loss: 5.3054\n",
      "Epoch [1/1], Step [2792/8897], Loss: 5.6969\n",
      "Epoch [1/1], Step [2793/8897], Loss: 5.2252\n",
      "Epoch [1/1], Step [2794/8897], Loss: 5.4599\n",
      "Epoch [1/1], Step [2795/8897], Loss: 5.4312\n",
      "Epoch [1/1], Step [2796/8897], Loss: 5.5638\n",
      "Epoch [1/1], Step [2797/8897], Loss: 5.1809\n",
      "Epoch [1/1], Step [2798/8897], Loss: 5.2152\n",
      "Epoch [1/1], Step [2799/8897], Loss: 5.2439\n",
      "Epoch [1/1], Step [2800/8897], Loss: 5.2963\n",
      "Epoch [1/1], Step [2801/8897], Loss: 5.4940\n",
      "Epoch [1/1], Step [2802/8897], Loss: 5.6981\n",
      "Epoch [1/1], Step [2803/8897], Loss: 5.3384\n",
      "Epoch [1/1], Step [2804/8897], Loss: 5.4077\n",
      "Epoch [1/1], Step [2805/8897], Loss: 5.4955\n",
      "Epoch [1/1], Step [2806/8897], Loss: 5.1650\n",
      "Epoch [1/1], Step [2807/8897], Loss: 5.3273\n",
      "Epoch [1/1], Step [2808/8897], Loss: 5.3322\n",
      "Epoch [1/1], Step [2809/8897], Loss: 5.5054\n",
      "Epoch [1/1], Step [2810/8897], Loss: 5.5206\n",
      "Epoch [1/1], Step [2811/8897], Loss: 5.6091\n",
      "Epoch [1/1], Step [2812/8897], Loss: 5.6977\n",
      "Epoch [1/1], Step [2813/8897], Loss: 5.3083\n",
      "Epoch [1/1], Step [2814/8897], Loss: 5.4491\n",
      "Epoch [1/1], Step [2815/8897], Loss: 5.3415\n",
      "Epoch [1/1], Step [2816/8897], Loss: 5.3563\n",
      "Epoch [1/1], Step [2817/8897], Loss: 5.7053\n",
      "Epoch [1/1], Step [2818/8897], Loss: 5.4543\n",
      "Epoch [1/1], Step [2819/8897], Loss: 5.5368\n",
      "Epoch [1/1], Step [2820/8897], Loss: 5.2543\n",
      "Epoch [1/1], Step [2821/8897], Loss: 5.5937\n",
      "Epoch [1/1], Step [2822/8897], Loss: 5.3833\n",
      "Epoch [1/1], Step [2823/8897], Loss: 5.3904\n",
      "Epoch [1/1], Step [2824/8897], Loss: 5.2921\n",
      "Epoch [1/1], Step [2825/8897], Loss: 5.3869\n",
      "Epoch [1/1], Step [2826/8897], Loss: 5.3826\n",
      "Epoch [1/1], Step [2827/8897], Loss: 5.3397\n",
      "Epoch [1/1], Step [2828/8897], Loss: 5.6021\n",
      "Epoch [1/1], Step [2829/8897], Loss: 5.6159\n",
      "Epoch [1/1], Step [2830/8897], Loss: 5.4214\n",
      "Epoch [1/1], Step [2831/8897], Loss: 5.5819\n",
      "Epoch [1/1], Step [2832/8897], Loss: 5.2886\n",
      "Epoch [1/1], Step [2833/8897], Loss: 5.3772\n",
      "Epoch [1/1], Step [2834/8897], Loss: 5.3869\n",
      "Epoch [1/1], Step [2835/8897], Loss: 5.2347\n",
      "Epoch [1/1], Step [2836/8897], Loss: 5.2669\n",
      "Epoch [1/1], Step [2837/8897], Loss: 5.3388\n",
      "Epoch [1/1], Step [2838/8897], Loss: 5.3033\n",
      "Epoch [1/1], Step [2839/8897], Loss: 5.1513\n",
      "Epoch [1/1], Step [2840/8897], Loss: 5.4429\n",
      "Epoch [1/1], Step [2841/8897], Loss: 5.3638\n",
      "Epoch [1/1], Step [2842/8897], Loss: 5.3025\n",
      "Epoch [1/1], Step [2843/8897], Loss: 5.2456\n",
      "Epoch [1/1], Step [2844/8897], Loss: 5.4214\n",
      "Epoch [1/1], Step [2845/8897], Loss: 5.2138\n",
      "Epoch [1/1], Step [2846/8897], Loss: 5.2732\n",
      "Epoch [1/1], Step [2847/8897], Loss: 5.5778\n",
      "Epoch [1/1], Step [2848/8897], Loss: 5.2572\n",
      "Epoch [1/1], Step [2849/8897], Loss: 5.6083\n",
      "Epoch [1/1], Step [2850/8897], Loss: 5.1293\n",
      "Epoch [1/1], Step [2851/8897], Loss: 5.3198\n",
      "Epoch [1/1], Step [2852/8897], Loss: 5.4721\n",
      "Epoch [1/1], Step [2853/8897], Loss: 5.4448\n",
      "Epoch [1/1], Step [2854/8897], Loss: 5.3426\n",
      "Epoch [1/1], Step [2855/8897], Loss: 5.0833\n",
      "Epoch [1/1], Step [2856/8897], Loss: 5.5406\n",
      "Epoch [1/1], Step [2857/8897], Loss: 5.5594\n",
      "Epoch [1/1], Step [2858/8897], Loss: 5.4632\n",
      "Epoch [1/1], Step [2859/8897], Loss: 5.4696\n",
      "Epoch [1/1], Step [2860/8897], Loss: 5.5939\n",
      "Epoch [1/1], Step [2861/8897], Loss: 5.2225\n",
      "Epoch [1/1], Step [2862/8897], Loss: 5.2024\n",
      "Epoch [1/1], Step [2863/8897], Loss: 5.1955\n",
      "Epoch [1/1], Step [2864/8897], Loss: 5.1527\n",
      "Epoch [1/1], Step [2865/8897], Loss: 5.2591\n",
      "Epoch [1/1], Step [2866/8897], Loss: 5.4160\n",
      "Epoch [1/1], Step [2867/8897], Loss: 5.6123\n",
      "Epoch [1/1], Step [2868/8897], Loss: 5.3776\n",
      "Epoch [1/1], Step [2869/8897], Loss: 5.2484\n",
      "Epoch [1/1], Step [2870/8897], Loss: 5.3446\n",
      "Epoch [1/1], Step [2871/8897], Loss: 5.5270\n",
      "Epoch [1/1], Step [2872/8897], Loss: 5.3656\n",
      "Epoch [1/1], Step [2873/8897], Loss: 5.4005\n",
      "Epoch [1/1], Step [2874/8897], Loss: 5.5534\n",
      "Epoch [1/1], Step [2875/8897], Loss: 5.5823\n",
      "Epoch [1/1], Step [2876/8897], Loss: 5.7045\n",
      "Epoch [1/1], Step [2877/8897], Loss: 5.3403\n",
      "Epoch [1/1], Step [2878/8897], Loss: 5.3894\n",
      "Epoch [1/1], Step [2879/8897], Loss: 5.4595\n",
      "Epoch [1/1], Step [2880/8897], Loss: 5.3909\n",
      "Epoch [1/1], Step [2881/8897], Loss: 5.2831\n",
      "Epoch [1/1], Step [2882/8897], Loss: 5.2841\n",
      "Epoch [1/1], Step [2883/8897], Loss: 5.7938\n",
      "Epoch [1/1], Step [2884/8897], Loss: 5.4947\n",
      "Epoch [1/1], Step [2885/8897], Loss: 5.5110\n",
      "Epoch [1/1], Step [2886/8897], Loss: 5.2943\n",
      "Epoch [1/1], Step [2887/8897], Loss: 5.2161\n",
      "Epoch [1/1], Step [2888/8897], Loss: 5.4744\n",
      "Epoch [1/1], Step [2889/8897], Loss: 5.4700\n",
      "Epoch [1/1], Step [2890/8897], Loss: 5.5553\n",
      "Epoch [1/1], Step [2891/8897], Loss: 5.5209\n",
      "Epoch [1/1], Step [2892/8897], Loss: 5.5888\n",
      "Epoch [1/1], Step [2893/8897], Loss: 5.3329\n",
      "Epoch [1/1], Step [2894/8897], Loss: 5.5432\n",
      "Epoch [1/1], Step [2895/8897], Loss: 5.4107\n",
      "Epoch [1/1], Step [2896/8897], Loss: 5.4991\n",
      "Epoch [1/1], Step [2897/8897], Loss: 5.2248\n",
      "Epoch [1/1], Step [2898/8897], Loss: 5.3887\n",
      "Epoch [1/1], Step [2899/8897], Loss: 5.4879\n",
      "Epoch [1/1], Step [2900/8897], Loss: 5.6771\n",
      "Epoch [1/1], Step [2901/8897], Loss: 5.2787\n",
      "Epoch [1/1], Step [2902/8897], Loss: 5.2612\n",
      "Epoch [1/1], Step [2903/8897], Loss: 5.4794\n",
      "Epoch [1/1], Step [2904/8897], Loss: 5.5360\n",
      "Epoch [1/1], Step [2905/8897], Loss: 5.3278\n",
      "Epoch [1/1], Step [2906/8897], Loss: 5.3051\n",
      "Epoch [1/1], Step [2907/8897], Loss: 5.4092\n",
      "Epoch [1/1], Step [2908/8897], Loss: 5.3105\n",
      "Epoch [1/1], Step [2909/8897], Loss: 5.3029\n",
      "Epoch [1/1], Step [2910/8897], Loss: 5.3837\n",
      "Epoch [1/1], Step [2911/8897], Loss: 5.6167\n",
      "Epoch [1/1], Step [2912/8897], Loss: 5.5165\n",
      "Epoch [1/1], Step [2913/8897], Loss: 5.4350\n",
      "Epoch [1/1], Step [2914/8897], Loss: 5.4510\n",
      "Epoch [1/1], Step [2915/8897], Loss: 5.4591\n",
      "Epoch [1/1], Step [2916/8897], Loss: 5.3289\n",
      "Epoch [1/1], Step [2917/8897], Loss: 5.4790\n",
      "Epoch [1/1], Step [2918/8897], Loss: 5.1645\n",
      "Epoch [1/1], Step [2919/8897], Loss: 5.3261\n",
      "Epoch [1/1], Step [2920/8897], Loss: 5.5853\n",
      "Epoch [1/1], Step [2921/8897], Loss: 5.2163\n",
      "Epoch [1/1], Step [2922/8897], Loss: 5.2597\n",
      "Epoch [1/1], Step [2923/8897], Loss: 5.1595\n",
      "Epoch [1/1], Step [2924/8897], Loss: 5.6061\n",
      "Epoch [1/1], Step [2925/8897], Loss: 5.5314\n",
      "Epoch [1/1], Step [2926/8897], Loss: 5.3035\n",
      "Epoch [1/1], Step [2927/8897], Loss: 5.3840\n",
      "Epoch [1/1], Step [2928/8897], Loss: 5.3985\n",
      "Epoch [1/1], Step [2929/8897], Loss: 5.3029\n",
      "Epoch [1/1], Step [2930/8897], Loss: 5.2800\n",
      "Epoch [1/1], Step [2931/8897], Loss: 5.5286\n",
      "Epoch [1/1], Step [2932/8897], Loss: 5.2629\n",
      "Epoch [1/1], Step [2933/8897], Loss: 5.4597\n",
      "Epoch [1/1], Step [2934/8897], Loss: 5.4692\n",
      "Epoch [1/1], Step [2935/8897], Loss: 5.2516\n",
      "Epoch [1/1], Step [2936/8897], Loss: 5.3966\n",
      "Epoch [1/1], Step [2937/8897], Loss: 5.2789\n",
      "Epoch [1/1], Step [2938/8897], Loss: 5.5353\n",
      "Epoch [1/1], Step [2939/8897], Loss: 5.2235\n",
      "Epoch [1/1], Step [2940/8897], Loss: 5.1921\n",
      "Epoch [1/1], Step [2941/8897], Loss: 5.2319\n",
      "Epoch [1/1], Step [2942/8897], Loss: 5.2543\n",
      "Epoch [1/1], Step [2943/8897], Loss: 5.5976\n",
      "Epoch [1/1], Step [2944/8897], Loss: 5.3101\n",
      "Epoch [1/1], Step [2945/8897], Loss: 5.5608\n",
      "Epoch [1/1], Step [2946/8897], Loss: 5.8159\n",
      "Epoch [1/1], Step [2947/8897], Loss: 5.5024\n",
      "Epoch [1/1], Step [2948/8897], Loss: 5.3442\n",
      "Epoch [1/1], Step [2949/8897], Loss: 5.5075\n",
      "Epoch [1/1], Step [2950/8897], Loss: 5.3614\n",
      "Epoch [1/1], Step [2951/8897], Loss: 5.4059\n",
      "Epoch [1/1], Step [2952/8897], Loss: 5.6574\n",
      "Epoch [1/1], Step [2953/8897], Loss: 5.1422\n",
      "Epoch [1/1], Step [2954/8897], Loss: 5.3688\n",
      "Epoch [1/1], Step [2955/8897], Loss: 5.5098\n",
      "Epoch [1/1], Step [2956/8897], Loss: 5.4841\n",
      "Epoch [1/1], Step [2957/8897], Loss: 5.4721\n",
      "Epoch [1/1], Step [2958/8897], Loss: 5.4300\n",
      "Epoch [1/1], Step [2959/8897], Loss: 5.6607\n",
      "Epoch [1/1], Step [2960/8897], Loss: 5.6214\n",
      "Epoch [1/1], Step [2961/8897], Loss: 5.3089\n",
      "Epoch [1/1], Step [2962/8897], Loss: 5.5347\n",
      "Epoch [1/1], Step [2963/8897], Loss: 5.4141\n",
      "Epoch [1/1], Step [2964/8897], Loss: 5.4242\n",
      "Epoch [1/1], Step [2965/8897], Loss: 5.5104\n",
      "Epoch [1/1], Step [2966/8897], Loss: 5.5150\n",
      "Epoch [1/1], Step [2967/8897], Loss: 5.3419\n",
      "Epoch [1/1], Step [2968/8897], Loss: 5.2487\n",
      "Epoch [1/1], Step [2969/8897], Loss: 5.1471\n",
      "Epoch [1/1], Step [2970/8897], Loss: 5.5406\n",
      "Epoch [1/1], Step [2971/8897], Loss: 5.2921\n",
      "Epoch [1/1], Step [2972/8897], Loss: 5.8314\n",
      "Epoch [1/1], Step [2973/8897], Loss: 5.2011\n",
      "Epoch [1/1], Step [2974/8897], Loss: 5.6591\n",
      "Epoch [1/1], Step [2975/8897], Loss: 5.3848\n",
      "Epoch [1/1], Step [2976/8897], Loss: 5.2748\n",
      "Epoch [1/1], Step [2977/8897], Loss: 5.5469\n",
      "Epoch [1/1], Step [2978/8897], Loss: 5.6002\n",
      "Epoch [1/1], Step [2979/8897], Loss: 5.1505\n",
      "Epoch [1/1], Step [2980/8897], Loss: 5.2045\n",
      "Epoch [1/1], Step [2981/8897], Loss: 5.2462\n",
      "Epoch [1/1], Step [2982/8897], Loss: 5.4456\n",
      "Epoch [1/1], Step [2983/8897], Loss: 5.3986\n",
      "Epoch [1/1], Step [2984/8897], Loss: 5.4797\n",
      "Epoch [1/1], Step [2985/8897], Loss: 5.2827\n",
      "Epoch [1/1], Step [2986/8897], Loss: 5.3686\n",
      "Epoch [1/1], Step [2987/8897], Loss: 5.3238\n",
      "Epoch [1/1], Step [2988/8897], Loss: 5.4816\n",
      "Epoch [1/1], Step [2989/8897], Loss: 5.6577\n",
      "Epoch [1/1], Step [2990/8897], Loss: 5.7619\n",
      "Epoch [1/1], Step [2991/8897], Loss: 5.1915\n",
      "Epoch [1/1], Step [2992/8897], Loss: 5.3808\n",
      "Epoch [1/1], Step [2993/8897], Loss: 5.5177\n",
      "Epoch [1/1], Step [2994/8897], Loss: 5.2937\n",
      "Epoch [1/1], Step [2995/8897], Loss: 5.3781\n",
      "Epoch [1/1], Step [2996/8897], Loss: 5.3852\n",
      "Epoch [1/1], Step [2997/8897], Loss: 5.4473\n",
      "Epoch [1/1], Step [2998/8897], Loss: 5.2614\n",
      "Epoch [1/1], Step [2999/8897], Loss: 5.2181\n",
      "Epoch [1/1], Step [3000/8897], Loss: 5.5968\n",
      "Epoch [1/1], Step [3001/8897], Loss: 5.5624\n",
      "Epoch [1/1], Step [3002/8897], Loss: 5.5021\n",
      "Epoch [1/1], Step [3003/8897], Loss: 5.5432\n",
      "Epoch [1/1], Step [3004/8897], Loss: 5.3528\n",
      "Epoch [1/1], Step [3005/8897], Loss: 5.2715\n",
      "Epoch [1/1], Step [3006/8897], Loss: 5.3152\n",
      "Epoch [1/1], Step [3007/8897], Loss: 5.3290\n",
      "Epoch [1/1], Step [3008/8897], Loss: 5.6323\n",
      "Epoch [1/1], Step [3009/8897], Loss: 5.1186\n",
      "Epoch [1/1], Step [3010/8897], Loss: 5.5276\n",
      "Epoch [1/1], Step [3011/8897], Loss: 5.2748\n",
      "Epoch [1/1], Step [3012/8897], Loss: 5.5043\n",
      "Epoch [1/1], Step [3013/8897], Loss: 5.3891\n",
      "Epoch [1/1], Step [3014/8897], Loss: 5.4606\n",
      "Epoch [1/1], Step [3015/8897], Loss: 5.4643\n",
      "Epoch [1/1], Step [3016/8897], Loss: 5.3415\n",
      "Epoch [1/1], Step [3017/8897], Loss: 5.3751\n",
      "Epoch [1/1], Step [3018/8897], Loss: 5.5228\n",
      "Epoch [1/1], Step [3019/8897], Loss: 5.3979\n",
      "Epoch [1/1], Step [3020/8897], Loss: 5.6785\n",
      "Epoch [1/1], Step [3021/8897], Loss: 5.4533\n",
      "Epoch [1/1], Step [3022/8897], Loss: 5.6477\n",
      "Epoch [1/1], Step [3023/8897], Loss: 5.4066\n",
      "Epoch [1/1], Step [3024/8897], Loss: 5.2994\n",
      "Epoch [1/1], Step [3025/8897], Loss: 5.8147\n",
      "Epoch [1/1], Step [3026/8897], Loss: 5.2589\n",
      "Epoch [1/1], Step [3027/8897], Loss: 5.4276\n",
      "Epoch [1/1], Step [3028/8897], Loss: 5.4992\n",
      "Epoch [1/1], Step [3029/8897], Loss: 5.5393\n",
      "Epoch [1/1], Step [3030/8897], Loss: 5.7557\n",
      "Epoch [1/1], Step [3031/8897], Loss: 5.5185\n",
      "Epoch [1/1], Step [3032/8897], Loss: 5.3737\n",
      "Epoch [1/1], Step [3033/8897], Loss: 5.4206\n",
      "Epoch [1/1], Step [3034/8897], Loss: 5.3675\n",
      "Epoch [1/1], Step [3035/8897], Loss: 5.3148\n",
      "Epoch [1/1], Step [3036/8897], Loss: 5.5330\n",
      "Epoch [1/1], Step [3037/8897], Loss: 5.5280\n",
      "Epoch [1/1], Step [3038/8897], Loss: 5.3257\n",
      "Epoch [1/1], Step [3039/8897], Loss: 5.6671\n",
      "Epoch [1/1], Step [3040/8897], Loss: 5.5257\n",
      "Epoch [1/1], Step [3041/8897], Loss: 5.4569\n",
      "Epoch [1/1], Step [3042/8897], Loss: 5.4848\n",
      "Epoch [1/1], Step [3043/8897], Loss: 5.2120\n",
      "Epoch [1/1], Step [3044/8897], Loss: 5.3052\n",
      "Epoch [1/1], Step [3045/8897], Loss: 5.4349\n",
      "Epoch [1/1], Step [3046/8897], Loss: 5.3122\n",
      "Epoch [1/1], Step [3047/8897], Loss: 5.2732\n",
      "Epoch [1/1], Step [3048/8897], Loss: 5.2408\n",
      "Epoch [1/1], Step [3049/8897], Loss: 5.2426\n",
      "Epoch [1/1], Step [3050/8897], Loss: 5.4257\n",
      "Epoch [1/1], Step [3051/8897], Loss: 5.4386\n",
      "Epoch [1/1], Step [3052/8897], Loss: 5.5270\n",
      "Epoch [1/1], Step [3053/8897], Loss: 5.4608\n",
      "Epoch [1/1], Step [3054/8897], Loss: 5.3076\n",
      "Epoch [1/1], Step [3055/8897], Loss: 5.4864\n",
      "Epoch [1/1], Step [3056/8897], Loss: 5.3821\n",
      "Epoch [1/1], Step [3057/8897], Loss: 5.6741\n",
      "Epoch [1/1], Step [3058/8897], Loss: 5.5104\n",
      "Epoch [1/1], Step [3059/8897], Loss: 5.3840\n",
      "Epoch [1/1], Step [3060/8897], Loss: 5.3456\n",
      "Epoch [1/1], Step [3061/8897], Loss: 5.3190\n",
      "Epoch [1/1], Step [3062/8897], Loss: 5.4112\n",
      "Epoch [1/1], Step [3063/8897], Loss: 5.0137\n",
      "Epoch [1/1], Step [3064/8897], Loss: 5.4702\n",
      "Epoch [1/1], Step [3065/8897], Loss: 5.5401\n",
      "Epoch [1/1], Step [3066/8897], Loss: 5.0190\n",
      "Epoch [1/1], Step [3067/8897], Loss: 5.5639\n",
      "Epoch [1/1], Step [3068/8897], Loss: 5.3440\n",
      "Epoch [1/1], Step [3069/8897], Loss: 5.4452\n",
      "Epoch [1/1], Step [3070/8897], Loss: 5.2187\n",
      "Epoch [1/1], Step [3071/8897], Loss: 5.1231\n",
      "Epoch [1/1], Step [3072/8897], Loss: 5.2191\n",
      "Epoch [1/1], Step [3073/8897], Loss: 5.1972\n",
      "Epoch [1/1], Step [3074/8897], Loss: 5.5253\n",
      "Epoch [1/1], Step [3075/8897], Loss: 5.5497\n",
      "Epoch [1/1], Step [3076/8897], Loss: 5.4776\n",
      "Epoch [1/1], Step [3077/8897], Loss: 5.3758\n",
      "Epoch [1/1], Step [3078/8897], Loss: 5.5777\n",
      "Epoch [1/1], Step [3079/8897], Loss: 5.3778\n",
      "Epoch [1/1], Step [3080/8897], Loss: 5.4586\n",
      "Epoch [1/1], Step [3081/8897], Loss: 5.6128\n",
      "Epoch [1/1], Step [3082/8897], Loss: 5.4906\n",
      "Epoch [1/1], Step [3083/8897], Loss: 5.3526\n",
      "Epoch [1/1], Step [3084/8897], Loss: 5.2625\n",
      "Epoch [1/1], Step [3085/8897], Loss: 5.4618\n",
      "Epoch [1/1], Step [3086/8897], Loss: 5.5100\n",
      "Epoch [1/1], Step [3087/8897], Loss: 5.4647\n",
      "Epoch [1/1], Step [3088/8897], Loss: 5.3800\n",
      "Epoch [1/1], Step [3089/8897], Loss: 5.3449\n",
      "Epoch [1/1], Step [3090/8897], Loss: 5.4242\n",
      "Epoch [1/1], Step [3091/8897], Loss: 5.2798\n",
      "Epoch [1/1], Step [3092/8897], Loss: 5.6700\n",
      "Epoch [1/1], Step [3093/8897], Loss: 5.2774\n",
      "Epoch [1/1], Step [3094/8897], Loss: 5.4917\n",
      "Epoch [1/1], Step [3095/8897], Loss: 5.4518\n",
      "Epoch [1/1], Step [3096/8897], Loss: 5.5770\n",
      "Epoch [1/1], Step [3097/8897], Loss: 5.3966\n",
      "Epoch [1/1], Step [3098/8897], Loss: 5.1552\n",
      "Epoch [1/1], Step [3099/8897], Loss: 5.5436\n",
      "Epoch [1/1], Step [3100/8897], Loss: 5.4288\n",
      "Epoch [1/1], Step [3101/8897], Loss: 5.2719\n",
      "Epoch [1/1], Step [3102/8897], Loss: 5.5973\n",
      "Epoch [1/1], Step [3103/8897], Loss: 5.6590\n",
      "Epoch [1/1], Step [3104/8897], Loss: 5.4111\n",
      "Epoch [1/1], Step [3105/8897], Loss: 5.4231\n",
      "Epoch [1/1], Step [3106/8897], Loss: 5.2969\n",
      "Epoch [1/1], Step [3107/8897], Loss: 5.4503\n",
      "Epoch [1/1], Step [3108/8897], Loss: 5.2880\n",
      "Epoch [1/1], Step [3109/8897], Loss: 5.4788\n",
      "Epoch [1/1], Step [3110/8897], Loss: 5.3458\n",
      "Epoch [1/1], Step [3111/8897], Loss: 5.3029\n",
      "Epoch [1/1], Step [3112/8897], Loss: 5.2583\n",
      "Epoch [1/1], Step [3113/8897], Loss: 5.4158\n",
      "Epoch [1/1], Step [3114/8897], Loss: 5.2674\n",
      "Epoch [1/1], Step [3115/8897], Loss: 5.5179\n",
      "Epoch [1/1], Step [3116/8897], Loss: 5.3817\n",
      "Epoch [1/1], Step [3117/8897], Loss: 5.2808\n",
      "Epoch [1/1], Step [3118/8897], Loss: 5.5792\n",
      "Epoch [1/1], Step [3119/8897], Loss: 5.3933\n",
      "Epoch [1/1], Step [3120/8897], Loss: 5.2775\n",
      "Epoch [1/1], Step [3121/8897], Loss: 5.6082\n",
      "Epoch [1/1], Step [3122/8897], Loss: 5.3615\n",
      "Epoch [1/1], Step [3123/8897], Loss: 5.3566\n",
      "Epoch [1/1], Step [3124/8897], Loss: 5.3526\n",
      "Epoch [1/1], Step [3125/8897], Loss: 5.4303\n",
      "Epoch [1/1], Step [3126/8897], Loss: 5.6016\n",
      "Epoch [1/1], Step [3127/8897], Loss: 5.2395\n",
      "Epoch [1/1], Step [3128/8897], Loss: 5.4000\n",
      "Epoch [1/1], Step [3129/8897], Loss: 5.5726\n",
      "Epoch [1/1], Step [3130/8897], Loss: 5.3333\n",
      "Epoch [1/1], Step [3131/8897], Loss: 5.1220\n",
      "Epoch [1/1], Step [3132/8897], Loss: 5.3646\n",
      "Epoch [1/1], Step [3133/8897], Loss: 5.5151\n",
      "Epoch [1/1], Step [3134/8897], Loss: 5.5031\n",
      "Epoch [1/1], Step [3135/8897], Loss: 5.4141\n",
      "Epoch [1/1], Step [3136/8897], Loss: 5.5455\n",
      "Epoch [1/1], Step [3137/8897], Loss: 5.5189\n",
      "Epoch [1/1], Step [3138/8897], Loss: 5.2995\n",
      "Epoch [1/1], Step [3139/8897], Loss: 5.2854\n",
      "Epoch [1/1], Step [3140/8897], Loss: 5.7816\n",
      "Epoch [1/1], Step [3141/8897], Loss: 5.4004\n",
      "Epoch [1/1], Step [3142/8897], Loss: 5.3845\n",
      "Epoch [1/1], Step [3143/8897], Loss: 5.3928\n",
      "Epoch [1/1], Step [3144/8897], Loss: 5.2450\n",
      "Epoch [1/1], Step [3145/8897], Loss: 5.3234\n",
      "Epoch [1/1], Step [3146/8897], Loss: 5.5204\n",
      "Epoch [1/1], Step [3147/8897], Loss: 5.4313\n",
      "Epoch [1/1], Step [3148/8897], Loss: 5.6742\n",
      "Epoch [1/1], Step [3149/8897], Loss: 5.4573\n",
      "Epoch [1/1], Step [3150/8897], Loss: 5.3833\n",
      "Epoch [1/1], Step [3151/8897], Loss: 5.4928\n",
      "Epoch [1/1], Step [3152/8897], Loss: 5.5426\n",
      "Epoch [1/1], Step [3153/8897], Loss: 5.3646\n",
      "Epoch [1/1], Step [3154/8897], Loss: 5.6583\n",
      "Epoch [1/1], Step [3155/8897], Loss: 5.2781\n",
      "Epoch [1/1], Step [3156/8897], Loss: 5.3770\n",
      "Epoch [1/1], Step [3157/8897], Loss: 5.3424\n",
      "Epoch [1/1], Step [3158/8897], Loss: 5.3098\n",
      "Epoch [1/1], Step [3159/8897], Loss: 5.6088\n",
      "Epoch [1/1], Step [3160/8897], Loss: 5.3016\n",
      "Epoch [1/1], Step [3161/8897], Loss: 5.3112\n",
      "Epoch [1/1], Step [3162/8897], Loss: 5.4528\n",
      "Epoch [1/1], Step [3163/8897], Loss: 5.5242\n",
      "Epoch [1/1], Step [3164/8897], Loss: 5.3848\n",
      "Epoch [1/1], Step [3165/8897], Loss: 5.2042\n",
      "Epoch [1/1], Step [3166/8897], Loss: 5.4989\n",
      "Epoch [1/1], Step [3167/8897], Loss: 5.5311\n",
      "Epoch [1/1], Step [3168/8897], Loss: 5.1732\n",
      "Epoch [1/1], Step [3169/8897], Loss: 5.3100\n",
      "Epoch [1/1], Step [3170/8897], Loss: 5.5156\n",
      "Epoch [1/1], Step [3171/8897], Loss: 5.5243\n",
      "Epoch [1/1], Step [3172/8897], Loss: 5.6006\n",
      "Epoch [1/1], Step [3173/8897], Loss: 5.3255\n",
      "Epoch [1/1], Step [3174/8897], Loss: 5.5783\n",
      "Epoch [1/1], Step [3175/8897], Loss: 5.3730\n",
      "Epoch [1/1], Step [3176/8897], Loss: 5.3678\n",
      "Epoch [1/1], Step [3177/8897], Loss: 5.4227\n",
      "Epoch [1/1], Step [3178/8897], Loss: 5.5628\n",
      "Epoch [1/1], Step [3179/8897], Loss: 5.3951\n",
      "Epoch [1/1], Step [3180/8897], Loss: 5.5753\n",
      "Epoch [1/1], Step [3181/8897], Loss: 5.3514\n",
      "Epoch [1/1], Step [3182/8897], Loss: 5.5112\n",
      "Epoch [1/1], Step [3183/8897], Loss: 5.5757\n",
      "Epoch [1/1], Step [3184/8897], Loss: 5.2664\n",
      "Epoch [1/1], Step [3185/8897], Loss: 5.3930\n",
      "Epoch [1/1], Step [3186/8897], Loss: 5.7122\n",
      "Epoch [1/1], Step [3187/8897], Loss: 5.2947\n",
      "Epoch [1/1], Step [3188/8897], Loss: 5.3350\n",
      "Epoch [1/1], Step [3189/8897], Loss: 5.2305\n",
      "Epoch [1/1], Step [3190/8897], Loss: 5.1613\n",
      "Epoch [1/1], Step [3191/8897], Loss: 5.4592\n",
      "Epoch [1/1], Step [3192/8897], Loss: 5.2441\n",
      "Epoch [1/1], Step [3193/8897], Loss: 5.6081\n",
      "Epoch [1/1], Step [3194/8897], Loss: 5.4335\n",
      "Epoch [1/1], Step [3195/8897], Loss: 5.1744\n",
      "Epoch [1/1], Step [3196/8897], Loss: 5.3116\n",
      "Epoch [1/1], Step [3197/8897], Loss: 5.6187\n",
      "Epoch [1/1], Step [3198/8897], Loss: 5.7451\n",
      "Epoch [1/1], Step [3199/8897], Loss: 5.2653\n",
      "Epoch [1/1], Step [3200/8897], Loss: 5.4378\n",
      "Epoch [1/1], Step [3201/8897], Loss: 5.4773\n",
      "Epoch [1/1], Step [3202/8897], Loss: 5.5153\n",
      "Epoch [1/1], Step [3203/8897], Loss: 5.0893\n",
      "Epoch [1/1], Step [3204/8897], Loss: 5.1687\n",
      "Epoch [1/1], Step [3205/8897], Loss: 5.3995\n",
      "Epoch [1/1], Step [3206/8897], Loss: 5.4579\n",
      "Epoch [1/1], Step [3207/8897], Loss: 5.3281\n",
      "Epoch [1/1], Step [3208/8897], Loss: 5.6116\n",
      "Epoch [1/1], Step [3209/8897], Loss: 5.3638\n",
      "Epoch [1/1], Step [3210/8897], Loss: 5.3161\n",
      "Epoch [1/1], Step [3211/8897], Loss: 5.6220\n",
      "Epoch [1/1], Step [3212/8897], Loss: 5.5335\n",
      "Epoch [1/1], Step [3213/8897], Loss: 5.2837\n",
      "Epoch [1/1], Step [3214/8897], Loss: 5.5275\n",
      "Epoch [1/1], Step [3215/8897], Loss: 5.4360\n",
      "Epoch [1/1], Step [3216/8897], Loss: 5.3761\n",
      "Epoch [1/1], Step [3217/8897], Loss: 5.4497\n",
      "Epoch [1/1], Step [3218/8897], Loss: 5.3296\n",
      "Epoch [1/1], Step [3219/8897], Loss: 5.3557\n",
      "Epoch [1/1], Step [3220/8897], Loss: 5.7957\n",
      "Epoch [1/1], Step [3221/8897], Loss: 5.4728\n",
      "Epoch [1/1], Step [3222/8897], Loss: 5.3154\n",
      "Epoch [1/1], Step [3223/8897], Loss: 5.3140\n",
      "Epoch [1/1], Step [3224/8897], Loss: 5.3694\n",
      "Epoch [1/1], Step [3225/8897], Loss: 5.4954\n",
      "Epoch [1/1], Step [3226/8897], Loss: 5.4625\n",
      "Epoch [1/1], Step [3227/8897], Loss: 5.4840\n",
      "Epoch [1/1], Step [3228/8897], Loss: 5.2601\n",
      "Epoch [1/1], Step [3229/8897], Loss: 5.3322\n",
      "Epoch [1/1], Step [3230/8897], Loss: 5.3098\n",
      "Epoch [1/1], Step [3231/8897], Loss: 5.2058\n",
      "Epoch [1/1], Step [3232/8897], Loss: 5.3301\n",
      "Epoch [1/1], Step [3233/8897], Loss: 5.5148\n",
      "Epoch [1/1], Step [3234/8897], Loss: 5.4027\n",
      "Epoch [1/1], Step [3235/8897], Loss: 5.5148\n",
      "Epoch [1/1], Step [3236/8897], Loss: 5.5292\n",
      "Epoch [1/1], Step [3237/8897], Loss: 5.5966\n",
      "Epoch [1/1], Step [3238/8897], Loss: 5.5804\n",
      "Epoch [1/1], Step [3239/8897], Loss: 5.4712\n",
      "Epoch [1/1], Step [3240/8897], Loss: 5.4741\n",
      "Epoch [1/1], Step [3241/8897], Loss: 5.2656\n",
      "Epoch [1/1], Step [3242/8897], Loss: 5.4754\n",
      "Epoch [1/1], Step [3243/8897], Loss: 5.4322\n",
      "Epoch [1/1], Step [3244/8897], Loss: 5.4833\n",
      "Epoch [1/1], Step [3245/8897], Loss: 5.4870\n",
      "Epoch [1/1], Step [3246/8897], Loss: 5.3083\n",
      "Epoch [1/1], Step [3247/8897], Loss: 5.3439\n",
      "Epoch [1/1], Step [3248/8897], Loss: 5.3591\n",
      "Epoch [1/1], Step [3249/8897], Loss: 5.5878\n",
      "Epoch [1/1], Step [3250/8897], Loss: 5.3058\n",
      "Epoch [1/1], Step [3251/8897], Loss: 5.3563\n",
      "Epoch [1/1], Step [3252/8897], Loss: 5.4089\n",
      "Epoch [1/1], Step [3253/8897], Loss: 5.4278\n",
      "Epoch [1/1], Step [3254/8897], Loss: 5.3833\n",
      "Epoch [1/1], Step [3255/8897], Loss: 5.2591\n",
      "Epoch [1/1], Step [3256/8897], Loss: 5.3685\n",
      "Epoch [1/1], Step [3257/8897], Loss: 5.3822\n",
      "Epoch [1/1], Step [3258/8897], Loss: 5.4036\n",
      "Epoch [1/1], Step [3259/8897], Loss: 5.7062\n",
      "Epoch [1/1], Step [3260/8897], Loss: 5.2317\n",
      "Epoch [1/1], Step [3261/8897], Loss: 5.4203\n",
      "Epoch [1/1], Step [3262/8897], Loss: 5.3450\n",
      "Epoch [1/1], Step [3263/8897], Loss: 5.3969\n",
      "Epoch [1/1], Step [3264/8897], Loss: 5.1791\n",
      "Epoch [1/1], Step [3265/8897], Loss: 5.2142\n",
      "Epoch [1/1], Step [3266/8897], Loss: 5.1567\n",
      "Epoch [1/1], Step [3267/8897], Loss: 5.2984\n",
      "Epoch [1/1], Step [3268/8897], Loss: 5.4675\n",
      "Epoch [1/1], Step [3269/8897], Loss: 5.2862\n",
      "Epoch [1/1], Step [3270/8897], Loss: 5.0093\n",
      "Epoch [1/1], Step [3271/8897], Loss: 5.3722\n",
      "Epoch [1/1], Step [3272/8897], Loss: 5.3725\n",
      "Epoch [1/1], Step [3273/8897], Loss: 5.3943\n",
      "Epoch [1/1], Step [3274/8897], Loss: 5.4744\n",
      "Epoch [1/1], Step [3275/8897], Loss: 5.4510\n",
      "Epoch [1/1], Step [3276/8897], Loss: 5.2791\n",
      "Epoch [1/1], Step [3277/8897], Loss: 5.3659\n",
      "Epoch [1/1], Step [3278/8897], Loss: 5.2663\n",
      "Epoch [1/1], Step [3279/8897], Loss: 5.5610\n",
      "Epoch [1/1], Step [3280/8897], Loss: 5.4239\n",
      "Epoch [1/1], Step [3281/8897], Loss: 5.6665\n",
      "Epoch [1/1], Step [3282/8897], Loss: 5.3516\n",
      "Epoch [1/1], Step [3283/8897], Loss: 5.4178\n",
      "Epoch [1/1], Step [3284/8897], Loss: 5.4676\n",
      "Epoch [1/1], Step [3285/8897], Loss: 5.4402\n",
      "Epoch [1/1], Step [3286/8897], Loss: 5.2991\n",
      "Epoch [1/1], Step [3287/8897], Loss: 5.3631\n",
      "Epoch [1/1], Step [3288/8897], Loss: 5.3465\n",
      "Epoch [1/1], Step [3289/8897], Loss: 5.4600\n",
      "Epoch [1/1], Step [3290/8897], Loss: 5.6730\n",
      "Epoch [1/1], Step [3291/8897], Loss: 5.4494\n",
      "Epoch [1/1], Step [3292/8897], Loss: 5.4400\n",
      "Epoch [1/1], Step [3293/8897], Loss: 5.1988\n",
      "Epoch [1/1], Step [3294/8897], Loss: 5.4252\n",
      "Epoch [1/1], Step [3295/8897], Loss: 5.3091\n",
      "Epoch [1/1], Step [3296/8897], Loss: 5.4904\n",
      "Epoch [1/1], Step [3297/8897], Loss: 5.7385\n",
      "Epoch [1/1], Step [3298/8897], Loss: 5.4616\n",
      "Epoch [1/1], Step [3299/8897], Loss: 5.3598\n",
      "Epoch [1/1], Step [3300/8897], Loss: 5.4849\n",
      "Epoch [1/1], Step [3301/8897], Loss: 5.4998\n",
      "Epoch [1/1], Step [3302/8897], Loss: 5.4225\n",
      "Epoch [1/1], Step [3303/8897], Loss: 5.3028\n",
      "Epoch [1/1], Step [3304/8897], Loss: 5.3449\n",
      "Epoch [1/1], Step [3305/8897], Loss: 5.6020\n",
      "Epoch [1/1], Step [3306/8897], Loss: 5.4214\n",
      "Epoch [1/1], Step [3307/8897], Loss: 5.3080\n",
      "Epoch [1/1], Step [3308/8897], Loss: 5.5695\n",
      "Epoch [1/1], Step [3309/8897], Loss: 5.4628\n",
      "Epoch [1/1], Step [3310/8897], Loss: 5.4829\n",
      "Epoch [1/1], Step [3311/8897], Loss: 5.5572\n",
      "Epoch [1/1], Step [3312/8897], Loss: 5.5730\n",
      "Epoch [1/1], Step [3313/8897], Loss: 5.4755\n",
      "Epoch [1/1], Step [3314/8897], Loss: 5.4117\n",
      "Epoch [1/1], Step [3315/8897], Loss: 5.6958\n",
      "Epoch [1/1], Step [3316/8897], Loss: 5.3892\n",
      "Epoch [1/1], Step [3317/8897], Loss: 5.3267\n",
      "Epoch [1/1], Step [3318/8897], Loss: 5.5584\n",
      "Epoch [1/1], Step [3319/8897], Loss: 5.1487\n",
      "Epoch [1/1], Step [3320/8897], Loss: 5.3068\n",
      "Epoch [1/1], Step [3321/8897], Loss: 5.5211\n",
      "Epoch [1/1], Step [3322/8897], Loss: 5.6679\n",
      "Epoch [1/1], Step [3323/8897], Loss: 5.5149\n",
      "Epoch [1/1], Step [3324/8897], Loss: 5.4415\n",
      "Epoch [1/1], Step [3325/8897], Loss: 5.4923\n",
      "Epoch [1/1], Step [3326/8897], Loss: 5.3846\n",
      "Epoch [1/1], Step [3327/8897], Loss: 5.4343\n",
      "Epoch [1/1], Step [3328/8897], Loss: 5.7241\n",
      "Epoch [1/1], Step [3329/8897], Loss: 5.5259\n",
      "Epoch [1/1], Step [3330/8897], Loss: 5.3132\n",
      "Epoch [1/1], Step [3331/8897], Loss: 5.3224\n",
      "Epoch [1/1], Step [3332/8897], Loss: 5.5357\n",
      "Epoch [1/1], Step [3333/8897], Loss: 5.1260\n",
      "Epoch [1/1], Step [3334/8897], Loss: 5.3591\n",
      "Epoch [1/1], Step [3335/8897], Loss: 5.2326\n",
      "Epoch [1/1], Step [3336/8897], Loss: 5.4777\n",
      "Epoch [1/1], Step [3337/8897], Loss: 5.4423\n",
      "Epoch [1/1], Step [3338/8897], Loss: 5.3862\n",
      "Epoch [1/1], Step [3339/8897], Loss: 5.3021\n",
      "Epoch [1/1], Step [3340/8897], Loss: 5.3321\n",
      "Epoch [1/1], Step [3341/8897], Loss: 5.4938\n",
      "Epoch [1/1], Step [3342/8897], Loss: 5.1395\n",
      "Epoch [1/1], Step [3343/8897], Loss: 5.5113\n",
      "Epoch [1/1], Step [3344/8897], Loss: 5.3429\n",
      "Epoch [1/1], Step [3345/8897], Loss: 5.4890\n",
      "Epoch [1/1], Step [3346/8897], Loss: 5.4158\n",
      "Epoch [1/1], Step [3347/8897], Loss: 5.4608\n",
      "Epoch [1/1], Step [3348/8897], Loss: 5.4557\n",
      "Epoch [1/1], Step [3349/8897], Loss: 5.5172\n",
      "Epoch [1/1], Step [3350/8897], Loss: 5.1083\n",
      "Epoch [1/1], Step [3351/8897], Loss: 5.6720\n",
      "Epoch [1/1], Step [3352/8897], Loss: 5.3938\n",
      "Epoch [1/1], Step [3353/8897], Loss: 5.3605\n",
      "Epoch [1/1], Step [3354/8897], Loss: 5.5637\n",
      "Epoch [1/1], Step [3355/8897], Loss: 5.5056\n",
      "Epoch [1/1], Step [3356/8897], Loss: 5.2192\n",
      "Epoch [1/1], Step [3357/8897], Loss: 5.2507\n",
      "Epoch [1/1], Step [3358/8897], Loss: 5.0895\n",
      "Epoch [1/1], Step [3359/8897], Loss: 5.3227\n",
      "Epoch [1/1], Step [3360/8897], Loss: 5.4409\n",
      "Epoch [1/1], Step [3361/8897], Loss: 5.0880\n",
      "Epoch [1/1], Step [3362/8897], Loss: 5.4028\n",
      "Epoch [1/1], Step [3363/8897], Loss: 5.2082\n",
      "Epoch [1/1], Step [3364/8897], Loss: 5.0474\n",
      "Epoch [1/1], Step [3365/8897], Loss: 5.4887\n",
      "Epoch [1/1], Step [3366/8897], Loss: 5.3930\n",
      "Epoch [1/1], Step [3367/8897], Loss: 5.4686\n",
      "Epoch [1/1], Step [3368/8897], Loss: 5.3049\n",
      "Epoch [1/1], Step [3369/8897], Loss: 5.4686\n",
      "Epoch [1/1], Step [3370/8897], Loss: 5.3602\n",
      "Epoch [1/1], Step [3371/8897], Loss: 5.5574\n",
      "Epoch [1/1], Step [3372/8897], Loss: 5.6787\n",
      "Epoch [1/1], Step [3373/8897], Loss: 5.2024\n",
      "Epoch [1/1], Step [3374/8897], Loss: 5.3313\n",
      "Epoch [1/1], Step [3375/8897], Loss: 5.2772\n",
      "Epoch [1/1], Step [3376/8897], Loss: 5.4176\n",
      "Epoch [1/1], Step [3377/8897], Loss: 5.4506\n",
      "Epoch [1/1], Step [3378/8897], Loss: 5.2509\n",
      "Epoch [1/1], Step [3379/8897], Loss: 5.4290\n",
      "Epoch [1/1], Step [3380/8897], Loss: 5.3899\n",
      "Epoch [1/1], Step [3381/8897], Loss: 5.3388\n",
      "Epoch [1/1], Step [3382/8897], Loss: 5.5863\n",
      "Epoch [1/1], Step [3383/8897], Loss: 5.5741\n",
      "Epoch [1/1], Step [3384/8897], Loss: 5.2820\n",
      "Epoch [1/1], Step [3385/8897], Loss: 5.4075\n",
      "Epoch [1/1], Step [3386/8897], Loss: 5.4554\n",
      "Epoch [1/1], Step [3387/8897], Loss: 5.3526\n",
      "Epoch [1/1], Step [3388/8897], Loss: 5.4764\n",
      "Epoch [1/1], Step [3389/8897], Loss: 5.3775\n",
      "Epoch [1/1], Step [3390/8897], Loss: 5.2566\n",
      "Epoch [1/1], Step [3391/8897], Loss: 5.5812\n",
      "Epoch [1/1], Step [3392/8897], Loss: 5.2889\n",
      "Epoch [1/1], Step [3393/8897], Loss: 5.4038\n",
      "Epoch [1/1], Step [3394/8897], Loss: 5.3591\n",
      "Epoch [1/1], Step [3395/8897], Loss: 5.3097\n",
      "Epoch [1/1], Step [3396/8897], Loss: 5.2890\n",
      "Epoch [1/1], Step [3397/8897], Loss: 5.2399\n",
      "Epoch [1/1], Step [3398/8897], Loss: 5.6312\n",
      "Epoch [1/1], Step [3399/8897], Loss: 5.4352\n",
      "Epoch [1/1], Step [3400/8897], Loss: 5.2442\n",
      "Epoch [1/1], Step [3401/8897], Loss: 5.3304\n",
      "Epoch [1/1], Step [3402/8897], Loss: 5.4491\n",
      "Epoch [1/1], Step [3403/8897], Loss: 5.5071\n",
      "Epoch [1/1], Step [3404/8897], Loss: 5.3900\n",
      "Epoch [1/1], Step [3405/8897], Loss: 5.3058\n",
      "Epoch [1/1], Step [3406/8897], Loss: 5.6898\n",
      "Epoch [1/1], Step [3407/8897], Loss: 5.8684\n",
      "Epoch [1/1], Step [3408/8897], Loss: 5.3237\n",
      "Epoch [1/1], Step [3409/8897], Loss: 5.6115\n",
      "Epoch [1/1], Step [3410/8897], Loss: 5.5837\n",
      "Epoch [1/1], Step [3411/8897], Loss: 5.1646\n",
      "Epoch [1/1], Step [3412/8897], Loss: 5.2805\n",
      "Epoch [1/1], Step [3413/8897], Loss: 5.5153\n",
      "Epoch [1/1], Step [3414/8897], Loss: 5.5760\n",
      "Epoch [1/1], Step [3415/8897], Loss: 5.2803\n",
      "Epoch [1/1], Step [3416/8897], Loss: 5.4202\n",
      "Epoch [1/1], Step [3417/8897], Loss: 5.5989\n",
      "Epoch [1/1], Step [3418/8897], Loss: 5.3403\n",
      "Epoch [1/1], Step [3419/8897], Loss: 5.5310\n",
      "Epoch [1/1], Step [3420/8897], Loss: 5.4959\n",
      "Epoch [1/1], Step [3421/8897], Loss: 5.5435\n",
      "Epoch [1/1], Step [3422/8897], Loss: 5.2739\n",
      "Epoch [1/1], Step [3423/8897], Loss: 5.5747\n",
      "Epoch [1/1], Step [3424/8897], Loss: 5.1847\n",
      "Epoch [1/1], Step [3425/8897], Loss: 5.1699\n",
      "Epoch [1/1], Step [3426/8897], Loss: 5.4221\n",
      "Epoch [1/1], Step [3427/8897], Loss: 5.6294\n",
      "Epoch [1/1], Step [3428/8897], Loss: 5.4227\n",
      "Epoch [1/1], Step [3429/8897], Loss: 5.7167\n",
      "Epoch [1/1], Step [3430/8897], Loss: 5.3536\n",
      "Epoch [1/1], Step [3431/8897], Loss: 5.1669\n",
      "Epoch [1/1], Step [3432/8897], Loss: 5.4164\n",
      "Epoch [1/1], Step [3433/8897], Loss: 5.3583\n",
      "Epoch [1/1], Step [3434/8897], Loss: 5.5984\n",
      "Epoch [1/1], Step [3435/8897], Loss: 5.6128\n",
      "Epoch [1/1], Step [3436/8897], Loss: 5.2663\n",
      "Epoch [1/1], Step [3437/8897], Loss: 5.4264\n",
      "Epoch [1/1], Step [3438/8897], Loss: 5.4353\n",
      "Epoch [1/1], Step [3439/8897], Loss: 5.3756\n",
      "Epoch [1/1], Step [3440/8897], Loss: 5.4763\n",
      "Epoch [1/1], Step [3441/8897], Loss: 5.3829\n",
      "Epoch [1/1], Step [3442/8897], Loss: 5.6133\n",
      "Epoch [1/1], Step [3443/8897], Loss: 5.4610\n",
      "Epoch [1/1], Step [3444/8897], Loss: 5.2606\n",
      "Epoch [1/1], Step [3445/8897], Loss: 5.3747\n",
      "Epoch [1/1], Step [3446/8897], Loss: 5.4336\n",
      "Epoch [1/1], Step [3447/8897], Loss: 5.4611\n",
      "Epoch [1/1], Step [3448/8897], Loss: 5.3772\n",
      "Epoch [1/1], Step [3449/8897], Loss: 5.4799\n",
      "Epoch [1/1], Step [3450/8897], Loss: 5.2691\n",
      "Epoch [1/1], Step [3451/8897], Loss: 5.2969\n",
      "Epoch [1/1], Step [3452/8897], Loss: 5.4372\n",
      "Epoch [1/1], Step [3453/8897], Loss: 5.4857\n",
      "Epoch [1/1], Step [3454/8897], Loss: 5.3431\n",
      "Epoch [1/1], Step [3455/8897], Loss: 5.3064\n",
      "Epoch [1/1], Step [3456/8897], Loss: 5.2079\n",
      "Epoch [1/1], Step [3457/8897], Loss: 5.1338\n",
      "Epoch [1/1], Step [3458/8897], Loss: 5.5835\n",
      "Epoch [1/1], Step [3459/8897], Loss: 5.3328\n",
      "Epoch [1/1], Step [3460/8897], Loss: 5.5619\n",
      "Epoch [1/1], Step [3461/8897], Loss: 5.5725\n",
      "Epoch [1/1], Step [3462/8897], Loss: 5.2027\n",
      "Epoch [1/1], Step [3463/8897], Loss: 5.1051\n",
      "Epoch [1/1], Step [3464/8897], Loss: 5.3872\n",
      "Epoch [1/1], Step [3465/8897], Loss: 5.6036\n",
      "Epoch [1/1], Step [3466/8897], Loss: 5.3868\n",
      "Epoch [1/1], Step [3467/8897], Loss: 5.3749\n",
      "Epoch [1/1], Step [3468/8897], Loss: 5.4340\n",
      "Epoch [1/1], Step [3469/8897], Loss: 5.2448\n",
      "Epoch [1/1], Step [3470/8897], Loss: 5.2199\n",
      "Epoch [1/1], Step [3471/8897], Loss: 5.4738\n",
      "Epoch [1/1], Step [3472/8897], Loss: 5.3275\n",
      "Epoch [1/1], Step [3473/8897], Loss: 5.3187\n",
      "Epoch [1/1], Step [3474/8897], Loss: 5.3193\n",
      "Epoch [1/1], Step [3475/8897], Loss: 5.5558\n",
      "Epoch [1/1], Step [3476/8897], Loss: 5.4821\n",
      "Epoch [1/1], Step [3477/8897], Loss: 5.2090\n",
      "Epoch [1/1], Step [3478/8897], Loss: 5.0822\n",
      "Epoch [1/1], Step [3479/8897], Loss: 5.2260\n",
      "Epoch [1/1], Step [3480/8897], Loss: 5.5692\n",
      "Epoch [1/1], Step [3481/8897], Loss: 5.7550\n",
      "Epoch [1/1], Step [3482/8897], Loss: 5.2963\n",
      "Epoch [1/1], Step [3483/8897], Loss: 5.1445\n",
      "Epoch [1/1], Step [3484/8897], Loss: 5.2720\n",
      "Epoch [1/1], Step [3485/8897], Loss: 5.2953\n",
      "Epoch [1/1], Step [3486/8897], Loss: 5.2607\n",
      "Epoch [1/1], Step [3487/8897], Loss: 5.3134\n",
      "Epoch [1/1], Step [3488/8897], Loss: 5.2200\n",
      "Epoch [1/1], Step [3489/8897], Loss: 5.5043\n",
      "Epoch [1/1], Step [3490/8897], Loss: 5.2628\n",
      "Epoch [1/1], Step [3491/8897], Loss: 5.4888\n",
      "Epoch [1/1], Step [3492/8897], Loss: 5.3727\n",
      "Epoch [1/1], Step [3493/8897], Loss: 5.5312\n",
      "Epoch [1/1], Step [3494/8897], Loss: 5.2500\n",
      "Epoch [1/1], Step [3495/8897], Loss: 5.5241\n",
      "Epoch [1/1], Step [3496/8897], Loss: 5.0513\n",
      "Epoch [1/1], Step [3497/8897], Loss: 5.3162\n",
      "Epoch [1/1], Step [3498/8897], Loss: 5.3941\n",
      "Epoch [1/1], Step [3499/8897], Loss: 5.4862\n",
      "Epoch [1/1], Step [3500/8897], Loss: 5.5579\n",
      "Epoch [1/1], Step [3501/8897], Loss: 5.4054\n",
      "Epoch [1/1], Step [3502/8897], Loss: 5.4051\n",
      "Epoch [1/1], Step [3503/8897], Loss: 5.4720\n",
      "Epoch [1/1], Step [3504/8897], Loss: 5.5277\n",
      "Epoch [1/1], Step [3505/8897], Loss: 5.4135\n",
      "Epoch [1/1], Step [3506/8897], Loss: 5.4086\n",
      "Epoch [1/1], Step [3507/8897], Loss: 5.5704\n",
      "Epoch [1/1], Step [3508/8897], Loss: 5.2073\n",
      "Epoch [1/1], Step [3509/8897], Loss: 5.3477\n",
      "Epoch [1/1], Step [3510/8897], Loss: 5.5997\n",
      "Epoch [1/1], Step [3511/8897], Loss: 5.4321\n",
      "Epoch [1/1], Step [3512/8897], Loss: 5.4895\n",
      "Epoch [1/1], Step [3513/8897], Loss: 5.4164\n",
      "Epoch [1/1], Step [3514/8897], Loss: 5.3552\n",
      "Epoch [1/1], Step [3515/8897], Loss: 5.6244\n",
      "Epoch [1/1], Step [3516/8897], Loss: 4.9981\n",
      "Epoch [1/1], Step [3517/8897], Loss: 5.2687\n",
      "Epoch [1/1], Step [3518/8897], Loss: 5.4903\n",
      "Epoch [1/1], Step [3519/8897], Loss: 5.4508\n",
      "Epoch [1/1], Step [3520/8897], Loss: 5.4420\n",
      "Epoch [1/1], Step [3521/8897], Loss: 5.3355\n",
      "Epoch [1/1], Step [3522/8897], Loss: 5.4310\n",
      "Epoch [1/1], Step [3523/8897], Loss: 5.4163\n",
      "Epoch [1/1], Step [3524/8897], Loss: 5.3798\n",
      "Epoch [1/1], Step [3525/8897], Loss: 5.4187\n",
      "Epoch [1/1], Step [3526/8897], Loss: 5.4839\n",
      "Epoch [1/1], Step [3527/8897], Loss: 5.2514\n",
      "Epoch [1/1], Step [3528/8897], Loss: 5.3679\n",
      "Epoch [1/1], Step [3529/8897], Loss: 5.5341\n",
      "Epoch [1/1], Step [3530/8897], Loss: 5.5523\n",
      "Epoch [1/1], Step [3531/8897], Loss: 5.6718\n",
      "Epoch [1/1], Step [3532/8897], Loss: 5.4604\n",
      "Epoch [1/1], Step [3533/8897], Loss: 5.4998\n",
      "Epoch [1/1], Step [3534/8897], Loss: 5.4099\n",
      "Epoch [1/1], Step [3535/8897], Loss: 5.3790\n",
      "Epoch [1/1], Step [3536/8897], Loss: 5.3996\n",
      "Epoch [1/1], Step [3537/8897], Loss: 5.5483\n",
      "Epoch [1/1], Step [3538/8897], Loss: 5.3559\n",
      "Epoch [1/1], Step [3539/8897], Loss: 5.3377\n",
      "Epoch [1/1], Step [3540/8897], Loss: 5.3414\n",
      "Epoch [1/1], Step [3541/8897], Loss: 5.3044\n",
      "Epoch [1/1], Step [3542/8897], Loss: 5.4707\n",
      "Epoch [1/1], Step [3543/8897], Loss: 5.4530\n",
      "Epoch [1/1], Step [3544/8897], Loss: 5.3584\n",
      "Epoch [1/1], Step [3545/8897], Loss: 5.3728\n",
      "Epoch [1/1], Step [3546/8897], Loss: 5.1901\n",
      "Epoch [1/1], Step [3547/8897], Loss: 5.4480\n",
      "Epoch [1/1], Step [3548/8897], Loss: 5.5976\n",
      "Epoch [1/1], Step [3549/8897], Loss: 5.3582\n",
      "Epoch [1/1], Step [3550/8897], Loss: 5.1769\n",
      "Epoch [1/1], Step [3551/8897], Loss: 5.3332\n",
      "Epoch [1/1], Step [3552/8897], Loss: 5.5530\n",
      "Epoch [1/1], Step [3553/8897], Loss: 5.3100\n",
      "Epoch [1/1], Step [3554/8897], Loss: 5.3465\n",
      "Epoch [1/1], Step [3555/8897], Loss: 5.5744\n",
      "Epoch [1/1], Step [3556/8897], Loss: 5.5494\n",
      "Epoch [1/1], Step [3557/8897], Loss: 5.4226\n",
      "Epoch [1/1], Step [3558/8897], Loss: 5.3547\n",
      "Epoch [1/1], Step [3559/8897], Loss: 5.4324\n",
      "Epoch [1/1], Step [3560/8897], Loss: 5.3359\n",
      "Epoch [1/1], Step [3561/8897], Loss: 5.3058\n",
      "Epoch [1/1], Step [3562/8897], Loss: 5.5737\n",
      "Epoch [1/1], Step [3563/8897], Loss: 5.3065\n",
      "Epoch [1/1], Step [3564/8897], Loss: 5.4352\n",
      "Epoch [1/1], Step [3565/8897], Loss: 5.1904\n",
      "Epoch [1/1], Step [3566/8897], Loss: 5.5046\n",
      "Epoch [1/1], Step [3567/8897], Loss: 5.2919\n",
      "Epoch [1/1], Step [3568/8897], Loss: 5.4974\n",
      "Epoch [1/1], Step [3569/8897], Loss: 5.3529\n",
      "Epoch [1/1], Step [3570/8897], Loss: 5.3675\n",
      "Epoch [1/1], Step [3571/8897], Loss: 5.4734\n",
      "Epoch [1/1], Step [3572/8897], Loss: 5.3535\n",
      "Epoch [1/1], Step [3573/8897], Loss: 5.4030\n",
      "Epoch [1/1], Step [3574/8897], Loss: 5.5471\n",
      "Epoch [1/1], Step [3575/8897], Loss: 5.0810\n",
      "Epoch [1/1], Step [3576/8897], Loss: 5.1315\n",
      "Epoch [1/1], Step [3577/8897], Loss: 5.3741\n",
      "Epoch [1/1], Step [3578/8897], Loss: 5.2559\n",
      "Epoch [1/1], Step [3579/8897], Loss: 5.5971\n",
      "Epoch [1/1], Step [3580/8897], Loss: 5.3296\n",
      "Epoch [1/1], Step [3581/8897], Loss: 5.6640\n",
      "Epoch [1/1], Step [3582/8897], Loss: 5.3792\n",
      "Epoch [1/1], Step [3583/8897], Loss: 5.3189\n",
      "Epoch [1/1], Step [3584/8897], Loss: 5.7050\n",
      "Epoch [1/1], Step [3585/8897], Loss: 5.5136\n",
      "Epoch [1/1], Step [3586/8897], Loss: 5.4368\n",
      "Epoch [1/1], Step [3587/8897], Loss: 5.5417\n",
      "Epoch [1/1], Step [3588/8897], Loss: 5.5580\n",
      "Epoch [1/1], Step [3589/8897], Loss: 5.5286\n",
      "Epoch [1/1], Step [3590/8897], Loss: 5.4594\n",
      "Epoch [1/1], Step [3591/8897], Loss: 5.5575\n",
      "Epoch [1/1], Step [3592/8897], Loss: 5.3543\n",
      "Epoch [1/1], Step [3593/8897], Loss: 5.2761\n",
      "Epoch [1/1], Step [3594/8897], Loss: 5.4689\n",
      "Epoch [1/1], Step [3595/8897], Loss: 5.3848\n",
      "Epoch [1/1], Step [3596/8897], Loss: 5.4135\n",
      "Epoch [1/1], Step [3597/8897], Loss: 5.4231\n",
      "Epoch [1/1], Step [3598/8897], Loss: 5.0686\n",
      "Epoch [1/1], Step [3599/8897], Loss: 5.4719\n",
      "Epoch [1/1], Step [3600/8897], Loss: 5.3060\n",
      "Epoch [1/1], Step [3601/8897], Loss: 5.4955\n",
      "Epoch [1/1], Step [3602/8897], Loss: 5.0114\n",
      "Epoch [1/1], Step [3603/8897], Loss: 5.4324\n",
      "Epoch [1/1], Step [3604/8897], Loss: 5.2957\n",
      "Epoch [1/1], Step [3605/8897], Loss: 5.3219\n",
      "Epoch [1/1], Step [3606/8897], Loss: 5.4497\n",
      "Epoch [1/1], Step [3607/8897], Loss: 5.3445\n",
      "Epoch [1/1], Step [3608/8897], Loss: 5.5370\n",
      "Epoch [1/1], Step [3609/8897], Loss: 5.7215\n",
      "Epoch [1/1], Step [3610/8897], Loss: 5.3566\n",
      "Epoch [1/1], Step [3611/8897], Loss: 5.4451\n",
      "Epoch [1/1], Step [3612/8897], Loss: 5.2535\n",
      "Epoch [1/1], Step [3613/8897], Loss: 5.6130\n",
      "Epoch [1/1], Step [3614/8897], Loss: 5.2136\n",
      "Epoch [1/1], Step [3615/8897], Loss: 5.4982\n",
      "Epoch [1/1], Step [3616/8897], Loss: 5.2624\n",
      "Epoch [1/1], Step [3617/8897], Loss: 5.1310\n",
      "Epoch [1/1], Step [3618/8897], Loss: 5.3169\n",
      "Epoch [1/1], Step [3619/8897], Loss: 5.5651\n",
      "Epoch [1/1], Step [3620/8897], Loss: 5.3646\n",
      "Epoch [1/1], Step [3621/8897], Loss: 5.5579\n",
      "Epoch [1/1], Step [3622/8897], Loss: 5.5244\n",
      "Epoch [1/1], Step [3623/8897], Loss: 5.4911\n",
      "Epoch [1/1], Step [3624/8897], Loss: 5.6486\n",
      "Epoch [1/1], Step [3625/8897], Loss: 5.1248\n",
      "Epoch [1/1], Step [3626/8897], Loss: 5.1855\n",
      "Epoch [1/1], Step [3627/8897], Loss: 5.3527\n",
      "Epoch [1/1], Step [3628/8897], Loss: 5.5237\n",
      "Epoch [1/1], Step [3629/8897], Loss: 5.5289\n",
      "Epoch [1/1], Step [3630/8897], Loss: 5.3504\n",
      "Epoch [1/1], Step [3631/8897], Loss: 5.3996\n",
      "Epoch [1/1], Step [3632/8897], Loss: 5.3958\n",
      "Epoch [1/1], Step [3633/8897], Loss: 5.1624\n",
      "Epoch [1/1], Step [3634/8897], Loss: 5.4533\n",
      "Epoch [1/1], Step [3635/8897], Loss: 5.3968\n",
      "Epoch [1/1], Step [3636/8897], Loss: 5.3050\n",
      "Epoch [1/1], Step [3637/8897], Loss: 5.1097\n",
      "Epoch [1/1], Step [3638/8897], Loss: 5.4654\n",
      "Epoch [1/1], Step [3639/8897], Loss: 5.4195\n",
      "Epoch [1/1], Step [3640/8897], Loss: 5.4887\n",
      "Epoch [1/1], Step [3641/8897], Loss: 5.3281\n",
      "Epoch [1/1], Step [3642/8897], Loss: 5.6505\n",
      "Epoch [1/1], Step [3643/8897], Loss: 5.4047\n",
      "Epoch [1/1], Step [3644/8897], Loss: 5.3746\n",
      "Epoch [1/1], Step [3645/8897], Loss: 5.3895\n",
      "Epoch [1/1], Step [3646/8897], Loss: 5.5837\n",
      "Epoch [1/1], Step [3647/8897], Loss: 5.4640\n",
      "Epoch [1/1], Step [3648/8897], Loss: 5.4118\n",
      "Epoch [1/1], Step [3649/8897], Loss: 5.3963\n",
      "Epoch [1/1], Step [3650/8897], Loss: 5.5692\n",
      "Epoch [1/1], Step [3651/8897], Loss: 5.3345\n",
      "Epoch [1/1], Step [3652/8897], Loss: 5.5960\n",
      "Epoch [1/1], Step [3653/8897], Loss: 5.2434\n",
      "Epoch [1/1], Step [3654/8897], Loss: 5.3119\n",
      "Epoch [1/1], Step [3655/8897], Loss: 5.2936\n",
      "Epoch [1/1], Step [3656/8897], Loss: 5.3383\n",
      "Epoch [1/1], Step [3657/8897], Loss: 5.4802\n",
      "Epoch [1/1], Step [3658/8897], Loss: 5.3811\n",
      "Epoch [1/1], Step [3659/8897], Loss: 5.3250\n",
      "Epoch [1/1], Step [3660/8897], Loss: 5.1672\n",
      "Epoch [1/1], Step [3661/8897], Loss: 5.4650\n",
      "Epoch [1/1], Step [3662/8897], Loss: 5.6527\n",
      "Epoch [1/1], Step [3663/8897], Loss: 5.3794\n",
      "Epoch [1/1], Step [3664/8897], Loss: 5.4029\n",
      "Epoch [1/1], Step [3665/8897], Loss: 5.2862\n",
      "Epoch [1/1], Step [3666/8897], Loss: 5.4759\n",
      "Epoch [1/1], Step [3667/8897], Loss: 5.3724\n",
      "Epoch [1/1], Step [3668/8897], Loss: 5.4198\n",
      "Epoch [1/1], Step [3669/8897], Loss: 5.5730\n",
      "Epoch [1/1], Step [3670/8897], Loss: 5.4596\n",
      "Epoch [1/1], Step [3671/8897], Loss: 5.3815\n",
      "Epoch [1/1], Step [3672/8897], Loss: 5.5175\n",
      "Epoch [1/1], Step [3673/8897], Loss: 5.5541\n",
      "Epoch [1/1], Step [3674/8897], Loss: 5.4831\n",
      "Epoch [1/1], Step [3675/8897], Loss: 5.5572\n",
      "Epoch [1/1], Step [3676/8897], Loss: 5.4318\n",
      "Epoch [1/1], Step [3677/8897], Loss: 5.2215\n",
      "Epoch [1/1], Step [3678/8897], Loss: 5.6507\n",
      "Epoch [1/1], Step [3679/8897], Loss: 5.3692\n",
      "Epoch [1/1], Step [3680/8897], Loss: 5.3273\n",
      "Epoch [1/1], Step [3681/8897], Loss: 5.6869\n",
      "Epoch [1/1], Step [3682/8897], Loss: 5.2632\n",
      "Epoch [1/1], Step [3683/8897], Loss: 5.3784\n",
      "Epoch [1/1], Step [3684/8897], Loss: 5.2467\n",
      "Epoch [1/1], Step [3685/8897], Loss: 5.2672\n",
      "Epoch [1/1], Step [3686/8897], Loss: 5.5106\n",
      "Epoch [1/1], Step [3687/8897], Loss: 5.7857\n",
      "Epoch [1/1], Step [3688/8897], Loss: 5.2290\n",
      "Epoch [1/1], Step [3689/8897], Loss: 5.3540\n",
      "Epoch [1/1], Step [3690/8897], Loss: 5.4798\n",
      "Epoch [1/1], Step [3691/8897], Loss: 5.4884\n",
      "Epoch [1/1], Step [3692/8897], Loss: 5.3590\n",
      "Epoch [1/1], Step [3693/8897], Loss: 5.5271\n",
      "Epoch [1/1], Step [3694/8897], Loss: 5.2696\n",
      "Epoch [1/1], Step [3695/8897], Loss: 5.2839\n",
      "Epoch [1/1], Step [3696/8897], Loss: 5.3167\n",
      "Epoch [1/1], Step [3697/8897], Loss: 5.3206\n",
      "Epoch [1/1], Step [3698/8897], Loss: 5.3223\n",
      "Epoch [1/1], Step [3699/8897], Loss: 5.5223\n",
      "Epoch [1/1], Step [3700/8897], Loss: 5.4742\n",
      "Epoch [1/1], Step [3701/8897], Loss: 5.2197\n",
      "Epoch [1/1], Step [3702/8897], Loss: 5.2142\n",
      "Epoch [1/1], Step [3703/8897], Loss: 5.6217\n",
      "Epoch [1/1], Step [3704/8897], Loss: 5.3411\n",
      "Epoch [1/1], Step [3705/8897], Loss: 5.6172\n",
      "Epoch [1/1], Step [3706/8897], Loss: 5.2161\n",
      "Epoch [1/1], Step [3707/8897], Loss: 5.0828\n",
      "Epoch [1/1], Step [3708/8897], Loss: 5.4037\n",
      "Epoch [1/1], Step [3709/8897], Loss: 5.2522\n",
      "Epoch [1/1], Step [3710/8897], Loss: 5.7487\n",
      "Epoch [1/1], Step [3711/8897], Loss: 5.2270\n",
      "Epoch [1/1], Step [3712/8897], Loss: 5.5139\n",
      "Epoch [1/1], Step [3713/8897], Loss: 5.3719\n",
      "Epoch [1/1], Step [3714/8897], Loss: 5.3686\n",
      "Epoch [1/1], Step [3715/8897], Loss: 5.2800\n",
      "Epoch [1/1], Step [3716/8897], Loss: 5.2037\n",
      "Epoch [1/1], Step [3717/8897], Loss: 5.4141\n",
      "Epoch [1/1], Step [3718/8897], Loss: 5.6001\n",
      "Epoch [1/1], Step [3719/8897], Loss: 5.2013\n",
      "Epoch [1/1], Step [3720/8897], Loss: 5.2539\n",
      "Epoch [1/1], Step [3721/8897], Loss: 5.5233\n",
      "Epoch [1/1], Step [3722/8897], Loss: 5.4811\n",
      "Epoch [1/1], Step [3723/8897], Loss: 5.6362\n",
      "Epoch [1/1], Step [3724/8897], Loss: 5.6735\n",
      "Epoch [1/1], Step [3725/8897], Loss: 5.4155\n",
      "Epoch [1/1], Step [3726/8897], Loss: 5.2094\n",
      "Epoch [1/1], Step [3727/8897], Loss: 5.3558\n",
      "Epoch [1/1], Step [3728/8897], Loss: 5.3086\n",
      "Epoch [1/1], Step [3729/8897], Loss: 5.6163\n",
      "Epoch [1/1], Step [3730/8897], Loss: 5.5527\n",
      "Epoch [1/1], Step [3731/8897], Loss: 5.4136\n",
      "Epoch [1/1], Step [3732/8897], Loss: 5.4754\n",
      "Epoch [1/1], Step [3733/8897], Loss: 5.2509\n",
      "Epoch [1/1], Step [3734/8897], Loss: 5.2218\n",
      "Epoch [1/1], Step [3735/8897], Loss: 5.3331\n",
      "Epoch [1/1], Step [3736/8897], Loss: 5.4720\n",
      "Epoch [1/1], Step [3737/8897], Loss: 5.3821\n",
      "Epoch [1/1], Step [3738/8897], Loss: 5.4676\n",
      "Epoch [1/1], Step [3739/8897], Loss: 5.5737\n",
      "Epoch [1/1], Step [3740/8897], Loss: 5.4311\n",
      "Epoch [1/1], Step [3741/8897], Loss: 5.4685\n",
      "Epoch [1/1], Step [3742/8897], Loss: 5.6748\n",
      "Epoch [1/1], Step [3743/8897], Loss: 5.3980\n",
      "Epoch [1/1], Step [3744/8897], Loss: 5.1750\n",
      "Epoch [1/1], Step [3745/8897], Loss: 5.4633\n",
      "Epoch [1/1], Step [3746/8897], Loss: 5.2640\n",
      "Epoch [1/1], Step [3747/8897], Loss: 5.1508\n",
      "Epoch [1/1], Step [3748/8897], Loss: 5.5482\n",
      "Epoch [1/1], Step [3749/8897], Loss: 5.3367\n",
      "Epoch [1/1], Step [3750/8897], Loss: 5.2055\n",
      "Epoch [1/1], Step [3751/8897], Loss: 5.3620\n",
      "Epoch [1/1], Step [3752/8897], Loss: 5.5007\n",
      "Epoch [1/1], Step [3753/8897], Loss: 5.4201\n",
      "Epoch [1/1], Step [3754/8897], Loss: 5.4345\n",
      "Epoch [1/1], Step [3755/8897], Loss: 5.1435\n",
      "Epoch [1/1], Step [3756/8897], Loss: 5.3771\n",
      "Epoch [1/1], Step [3757/8897], Loss: 5.3477\n",
      "Epoch [1/1], Step [3758/8897], Loss: 5.2548\n",
      "Epoch [1/1], Step [3759/8897], Loss: 5.3150\n",
      "Epoch [1/1], Step [3760/8897], Loss: 5.5201\n",
      "Epoch [1/1], Step [3761/8897], Loss: 5.3322\n",
      "Epoch [1/1], Step [3762/8897], Loss: 5.2747\n",
      "Epoch [1/1], Step [3763/8897], Loss: 5.6989\n",
      "Epoch [1/1], Step [3764/8897], Loss: 5.3502\n",
      "Epoch [1/1], Step [3765/8897], Loss: 5.2147\n",
      "Epoch [1/1], Step [3766/8897], Loss: 5.5321\n",
      "Epoch [1/1], Step [3767/8897], Loss: 5.4164\n",
      "Epoch [1/1], Step [3768/8897], Loss: 5.3321\n",
      "Epoch [1/1], Step [3769/8897], Loss: 5.1693\n",
      "Epoch [1/1], Step [3770/8897], Loss: 5.4432\n",
      "Epoch [1/1], Step [3771/8897], Loss: 5.3588\n",
      "Epoch [1/1], Step [3772/8897], Loss: 5.4325\n",
      "Epoch [1/1], Step [3773/8897], Loss: 5.3788\n",
      "Epoch [1/1], Step [3774/8897], Loss: 5.4143\n",
      "Epoch [1/1], Step [3775/8897], Loss: 5.3442\n",
      "Epoch [1/1], Step [3776/8897], Loss: 5.4200\n",
      "Epoch [1/1], Step [3777/8897], Loss: 5.3449\n",
      "Epoch [1/1], Step [3778/8897], Loss: 5.2907\n",
      "Epoch [1/1], Step [3779/8897], Loss: 5.3428\n",
      "Epoch [1/1], Step [3780/8897], Loss: 5.3824\n",
      "Epoch [1/1], Step [3781/8897], Loss: 5.2466\n",
      "Epoch [1/1], Step [3782/8897], Loss: 5.5315\n",
      "Epoch [1/1], Step [3783/8897], Loss: 5.3626\n",
      "Epoch [1/1], Step [3784/8897], Loss: 5.0650\n",
      "Epoch [1/1], Step [3785/8897], Loss: 5.2300\n",
      "Epoch [1/1], Step [3786/8897], Loss: 5.3689\n",
      "Epoch [1/1], Step [3787/8897], Loss: 5.3416\n",
      "Epoch [1/1], Step [3788/8897], Loss: 5.2585\n",
      "Epoch [1/1], Step [3789/8897], Loss: 5.4591\n",
      "Epoch [1/1], Step [3790/8897], Loss: 5.4072\n",
      "Epoch [1/1], Step [3791/8897], Loss: 5.4445\n",
      "Epoch [1/1], Step [3792/8897], Loss: 5.5043\n",
      "Epoch [1/1], Step [3793/8897], Loss: 5.2974\n",
      "Epoch [1/1], Step [3794/8897], Loss: 5.3705\n",
      "Epoch [1/1], Step [3795/8897], Loss: 5.0157\n",
      "Epoch [1/1], Step [3796/8897], Loss: 5.2318\n",
      "Epoch [1/1], Step [3797/8897], Loss: 5.4268\n",
      "Epoch [1/1], Step [3798/8897], Loss: 5.2284\n",
      "Epoch [1/1], Step [3799/8897], Loss: 5.4196\n",
      "Epoch [1/1], Step [3800/8897], Loss: 5.3456\n",
      "Epoch [1/1], Step [3801/8897], Loss: 5.3170\n",
      "Epoch [1/1], Step [3802/8897], Loss: 5.2279\n",
      "Epoch [1/1], Step [3803/8897], Loss: 5.3379\n",
      "Epoch [1/1], Step [3804/8897], Loss: 5.4121\n",
      "Epoch [1/1], Step [3805/8897], Loss: 5.4295\n",
      "Epoch [1/1], Step [3806/8897], Loss: 5.1348\n",
      "Epoch [1/1], Step [3807/8897], Loss: 5.3129\n",
      "Epoch [1/1], Step [3808/8897], Loss: 5.4224\n",
      "Epoch [1/1], Step [3809/8897], Loss: 5.2752\n",
      "Epoch [1/1], Step [3810/8897], Loss: 5.2925\n",
      "Epoch [1/1], Step [3811/8897], Loss: 5.3667\n",
      "Epoch [1/1], Step [3812/8897], Loss: 5.4477\n",
      "Epoch [1/1], Step [3813/8897], Loss: 5.5567\n",
      "Epoch [1/1], Step [3814/8897], Loss: 5.4026\n",
      "Epoch [1/1], Step [3815/8897], Loss: 5.2738\n",
      "Epoch [1/1], Step [3816/8897], Loss: 5.7015\n",
      "Epoch [1/1], Step [3817/8897], Loss: 5.2782\n",
      "Epoch [1/1], Step [3818/8897], Loss: 5.5221\n",
      "Epoch [1/1], Step [3819/8897], Loss: 5.4422\n",
      "Epoch [1/1], Step [3820/8897], Loss: 5.5120\n",
      "Epoch [1/1], Step [3821/8897], Loss: 5.4824\n",
      "Epoch [1/1], Step [3822/8897], Loss: 5.4966\n",
      "Epoch [1/1], Step [3823/8897], Loss: 5.5344\n",
      "Epoch [1/1], Step [3824/8897], Loss: 5.2168\n",
      "Epoch [1/1], Step [3825/8897], Loss: 5.0848\n",
      "Epoch [1/1], Step [3826/8897], Loss: 5.2590\n",
      "Epoch [1/1], Step [3827/8897], Loss: 5.2694\n",
      "Epoch [1/1], Step [3828/8897], Loss: 5.5463\n",
      "Epoch [1/1], Step [3829/8897], Loss: 5.4370\n",
      "Epoch [1/1], Step [3830/8897], Loss: 5.2440\n",
      "Epoch [1/1], Step [3831/8897], Loss: 5.3999\n",
      "Epoch [1/1], Step [3832/8897], Loss: 5.2598\n",
      "Epoch [1/1], Step [3833/8897], Loss: 5.4094\n",
      "Epoch [1/1], Step [3834/8897], Loss: 5.2612\n",
      "Epoch [1/1], Step [3835/8897], Loss: 5.3374\n",
      "Epoch [1/1], Step [3836/8897], Loss: 5.2701\n",
      "Epoch [1/1], Step [3837/8897], Loss: 5.4822\n",
      "Epoch [1/1], Step [3838/8897], Loss: 5.5373\n",
      "Epoch [1/1], Step [3839/8897], Loss: 5.4663\n",
      "Epoch [1/1], Step [3840/8897], Loss: 5.4756\n",
      "Epoch [1/1], Step [3841/8897], Loss: 5.3224\n",
      "Epoch [1/1], Step [3842/8897], Loss: 5.4333\n",
      "Epoch [1/1], Step [3843/8897], Loss: 5.4847\n",
      "Epoch [1/1], Step [3844/8897], Loss: 5.4357\n",
      "Epoch [1/1], Step [3845/8897], Loss: 5.2028\n",
      "Epoch [1/1], Step [3846/8897], Loss: 5.1317\n",
      "Epoch [1/1], Step [3847/8897], Loss: 5.6819\n",
      "Epoch [1/1], Step [3848/8897], Loss: 5.6752\n",
      "Epoch [1/1], Step [3849/8897], Loss: 5.4692\n",
      "Epoch [1/1], Step [3850/8897], Loss: 5.7319\n",
      "Epoch [1/1], Step [3851/8897], Loss: 5.2497\n",
      "Epoch [1/1], Step [3852/8897], Loss: 5.2118\n",
      "Epoch [1/1], Step [3853/8897], Loss: 5.4984\n",
      "Epoch [1/1], Step [3854/8897], Loss: 5.5131\n",
      "Epoch [1/1], Step [3855/8897], Loss: 5.3969\n",
      "Epoch [1/1], Step [3856/8897], Loss: 5.3508\n",
      "Epoch [1/1], Step [3857/8897], Loss: 5.3117\n",
      "Epoch [1/1], Step [3858/8897], Loss: 5.2374\n",
      "Epoch [1/1], Step [3859/8897], Loss: 5.1820\n",
      "Epoch [1/1], Step [3860/8897], Loss: 5.6032\n",
      "Epoch [1/1], Step [3861/8897], Loss: 5.6471\n",
      "Epoch [1/1], Step [3862/8897], Loss: 5.4710\n",
      "Epoch [1/1], Step [3863/8897], Loss: 5.3352\n",
      "Epoch [1/1], Step [3864/8897], Loss: 5.4649\n",
      "Epoch [1/1], Step [3865/8897], Loss: 5.3853\n",
      "Epoch [1/1], Step [3866/8897], Loss: 5.6424\n",
      "Epoch [1/1], Step [3867/8897], Loss: 5.5442\n",
      "Epoch [1/1], Step [3868/8897], Loss: 5.3688\n",
      "Epoch [1/1], Step [3869/8897], Loss: 5.5117\n",
      "Epoch [1/1], Step [3870/8897], Loss: 5.5227\n",
      "Epoch [1/1], Step [3871/8897], Loss: 5.1150\n",
      "Epoch [1/1], Step [3872/8897], Loss: 5.3396\n",
      "Epoch [1/1], Step [3873/8897], Loss: 5.2891\n",
      "Epoch [1/1], Step [3874/8897], Loss: 5.5299\n",
      "Epoch [1/1], Step [3875/8897], Loss: 5.4771\n",
      "Epoch [1/1], Step [3876/8897], Loss: 5.1273\n",
      "Epoch [1/1], Step [3877/8897], Loss: 5.5238\n",
      "Epoch [1/1], Step [3878/8897], Loss: 5.4143\n",
      "Epoch [1/1], Step [3879/8897], Loss: 5.4066\n",
      "Epoch [1/1], Step [3880/8897], Loss: 5.6709\n",
      "Epoch [1/1], Step [3881/8897], Loss: 5.2337\n",
      "Epoch [1/1], Step [3882/8897], Loss: 5.0668\n",
      "Epoch [1/1], Step [3883/8897], Loss: 5.1237\n",
      "Epoch [1/1], Step [3884/8897], Loss: 5.4423\n",
      "Epoch [1/1], Step [3885/8897], Loss: 5.1399\n",
      "Epoch [1/1], Step [3886/8897], Loss: 5.4901\n",
      "Epoch [1/1], Step [3887/8897], Loss: 5.5905\n",
      "Epoch [1/1], Step [3888/8897], Loss: 5.5191\n",
      "Epoch [1/1], Step [3889/8897], Loss: 5.2600\n",
      "Epoch [1/1], Step [3890/8897], Loss: 5.4857\n",
      "Epoch [1/1], Step [3891/8897], Loss: 5.3186\n",
      "Epoch [1/1], Step [3892/8897], Loss: 5.3483\n",
      "Epoch [1/1], Step [3893/8897], Loss: 5.4345\n",
      "Epoch [1/1], Step [3894/8897], Loss: 5.3756\n",
      "Epoch [1/1], Step [3895/8897], Loss: 5.3203\n",
      "Epoch [1/1], Step [3896/8897], Loss: 5.4271\n",
      "Epoch [1/1], Step [3897/8897], Loss: 5.2204\n",
      "Epoch [1/1], Step [3898/8897], Loss: 5.3302\n",
      "Epoch [1/1], Step [3899/8897], Loss: 5.5667\n",
      "Epoch [1/1], Step [3900/8897], Loss: 5.3542\n",
      "Epoch [1/1], Step [3901/8897], Loss: 5.6342\n",
      "Epoch [1/1], Step [3902/8897], Loss: 5.4431\n",
      "Epoch [1/1], Step [3903/8897], Loss: 5.2895\n",
      "Epoch [1/1], Step [3904/8897], Loss: 5.4322\n",
      "Epoch [1/1], Step [3905/8897], Loss: 5.3427\n",
      "Epoch [1/1], Step [3906/8897], Loss: 5.2369\n",
      "Epoch [1/1], Step [3907/8897], Loss: 5.4239\n",
      "Epoch [1/1], Step [3908/8897], Loss: 5.3734\n",
      "Epoch [1/1], Step [3909/8897], Loss: 5.5811\n",
      "Epoch [1/1], Step [3910/8897], Loss: 5.5675\n",
      "Epoch [1/1], Step [3911/8897], Loss: 5.3172\n",
      "Epoch [1/1], Step [3912/8897], Loss: 5.4717\n",
      "Epoch [1/1], Step [3913/8897], Loss: 5.5782\n",
      "Epoch [1/1], Step [3914/8897], Loss: 5.4119\n",
      "Epoch [1/1], Step [3915/8897], Loss: 5.2693\n",
      "Epoch [1/1], Step [3916/8897], Loss: 5.3537\n",
      "Epoch [1/1], Step [3917/8897], Loss: 5.3068\n",
      "Epoch [1/1], Step [3918/8897], Loss: 5.4408\n",
      "Epoch [1/1], Step [3919/8897], Loss: 5.0815\n",
      "Epoch [1/1], Step [3920/8897], Loss: 5.4527\n",
      "Epoch [1/1], Step [3921/8897], Loss: 5.4077\n",
      "Epoch [1/1], Step [3922/8897], Loss: 5.4300\n",
      "Epoch [1/1], Step [3923/8897], Loss: 5.2655\n",
      "Epoch [1/1], Step [3924/8897], Loss: 5.3967\n",
      "Epoch [1/1], Step [3925/8897], Loss: 5.3276\n",
      "Epoch [1/1], Step [3926/8897], Loss: 5.2303\n",
      "Epoch [1/1], Step [3927/8897], Loss: 5.4603\n",
      "Epoch [1/1], Step [3928/8897], Loss: 5.3788\n",
      "Epoch [1/1], Step [3929/8897], Loss: 5.2680\n",
      "Epoch [1/1], Step [3930/8897], Loss: 5.5051\n",
      "Epoch [1/1], Step [3931/8897], Loss: 4.9958\n",
      "Epoch [1/1], Step [3932/8897], Loss: 5.3940\n",
      "Epoch [1/1], Step [3933/8897], Loss: 5.6204\n",
      "Epoch [1/1], Step [3934/8897], Loss: 5.4979\n",
      "Epoch [1/1], Step [3935/8897], Loss: 5.4120\n",
      "Epoch [1/1], Step [3936/8897], Loss: 5.5119\n",
      "Epoch [1/1], Step [3937/8897], Loss: 5.7968\n",
      "Epoch [1/1], Step [3938/8897], Loss: 5.4974\n",
      "Epoch [1/1], Step [3939/8897], Loss: 5.5478\n",
      "Epoch [1/1], Step [3940/8897], Loss: 5.2861\n",
      "Epoch [1/1], Step [3941/8897], Loss: 5.3469\n",
      "Epoch [1/1], Step [3942/8897], Loss: 5.4404\n",
      "Epoch [1/1], Step [3943/8897], Loss: 5.2027\n",
      "Epoch [1/1], Step [3944/8897], Loss: 5.4663\n",
      "Epoch [1/1], Step [3945/8897], Loss: 5.5556\n",
      "Epoch [1/1], Step [3946/8897], Loss: 5.3568\n",
      "Epoch [1/1], Step [3947/8897], Loss: 5.2263\n",
      "Epoch [1/1], Step [3948/8897], Loss: 5.6608\n",
      "Epoch [1/1], Step [3949/8897], Loss: 5.3912\n",
      "Epoch [1/1], Step [3950/8897], Loss: 5.3214\n",
      "Epoch [1/1], Step [3951/8897], Loss: 5.1751\n",
      "Epoch [1/1], Step [3952/8897], Loss: 5.4408\n",
      "Epoch [1/1], Step [3953/8897], Loss: 5.4099\n",
      "Epoch [1/1], Step [3954/8897], Loss: 5.6494\n",
      "Epoch [1/1], Step [3955/8897], Loss: 5.3076\n",
      "Epoch [1/1], Step [3956/8897], Loss: 5.3467\n",
      "Epoch [1/1], Step [3957/8897], Loss: 5.4582\n",
      "Epoch [1/1], Step [3958/8897], Loss: 5.4025\n",
      "Epoch [1/1], Step [3959/8897], Loss: 5.3621\n",
      "Epoch [1/1], Step [3960/8897], Loss: 5.6257\n",
      "Epoch [1/1], Step [3961/8897], Loss: 5.2724\n",
      "Epoch [1/1], Step [3962/8897], Loss: 5.5996\n",
      "Epoch [1/1], Step [3963/8897], Loss: 5.3760\n",
      "Epoch [1/1], Step [3964/8897], Loss: 5.3035\n",
      "Epoch [1/1], Step [3965/8897], Loss: 5.2389\n",
      "Epoch [1/1], Step [3966/8897], Loss: 5.3136\n",
      "Epoch [1/1], Step [3967/8897], Loss: 5.0018\n",
      "Epoch [1/1], Step [3968/8897], Loss: 5.4043\n",
      "Epoch [1/1], Step [3969/8897], Loss: 5.1755\n",
      "Epoch [1/1], Step [3970/8897], Loss: 5.5478\n",
      "Epoch [1/1], Step [3971/8897], Loss: 5.3090\n",
      "Epoch [1/1], Step [3972/8897], Loss: 5.3865\n",
      "Epoch [1/1], Step [3973/8897], Loss: 5.2669\n",
      "Epoch [1/1], Step [3974/8897], Loss: 5.6770\n",
      "Epoch [1/1], Step [3975/8897], Loss: 5.4104\n",
      "Epoch [1/1], Step [3976/8897], Loss: 5.6607\n",
      "Epoch [1/1], Step [3977/8897], Loss: 5.5562\n",
      "Epoch [1/1], Step [3978/8897], Loss: 5.3126\n",
      "Epoch [1/1], Step [3979/8897], Loss: 5.3430\n",
      "Epoch [1/1], Step [3980/8897], Loss: 5.0832\n",
      "Epoch [1/1], Step [3981/8897], Loss: 5.2342\n",
      "Epoch [1/1], Step [3982/8897], Loss: 5.4898\n",
      "Epoch [1/1], Step [3983/8897], Loss: 5.3556\n",
      "Epoch [1/1], Step [3984/8897], Loss: 5.5155\n",
      "Epoch [1/1], Step [3985/8897], Loss: 5.2258\n",
      "Epoch [1/1], Step [3986/8897], Loss: 5.3007\n",
      "Epoch [1/1], Step [3987/8897], Loss: 5.2320\n",
      "Epoch [1/1], Step [3988/8897], Loss: 5.6503\n",
      "Epoch [1/1], Step [3989/8897], Loss: 5.3323\n",
      "Epoch [1/1], Step [3990/8897], Loss: 5.2774\n",
      "Epoch [1/1], Step [3991/8897], Loss: 5.4362\n",
      "Epoch [1/1], Step [3992/8897], Loss: 5.1962\n",
      "Epoch [1/1], Step [3993/8897], Loss: 5.4932\n",
      "Epoch [1/1], Step [3994/8897], Loss: 5.1650\n",
      "Epoch [1/1], Step [3995/8897], Loss: 5.3696\n",
      "Epoch [1/1], Step [3996/8897], Loss: 5.3886\n",
      "Epoch [1/1], Step [3997/8897], Loss: 5.4356\n",
      "Epoch [1/1], Step [3998/8897], Loss: 5.4751\n",
      "Epoch [1/1], Step [3999/8897], Loss: 5.1395\n",
      "Epoch [1/1], Step [4000/8897], Loss: 5.3704\n",
      "Epoch [1/1], Step [4001/8897], Loss: 5.2832\n",
      "Epoch [1/1], Step [4002/8897], Loss: 5.4550\n",
      "Epoch [1/1], Step [4003/8897], Loss: 5.3488\n",
      "Epoch [1/1], Step [4004/8897], Loss: 5.3588\n",
      "Epoch [1/1], Step [4005/8897], Loss: 5.4020\n",
      "Epoch [1/1], Step [4006/8897], Loss: 5.4783\n",
      "Epoch [1/1], Step [4007/8897], Loss: 5.4837\n",
      "Epoch [1/1], Step [4008/8897], Loss: 5.1792\n",
      "Epoch [1/1], Step [4009/8897], Loss: 5.5556\n",
      "Epoch [1/1], Step [4010/8897], Loss: 5.2674\n",
      "Epoch [1/1], Step [4011/8897], Loss: 5.3202\n",
      "Epoch [1/1], Step [4012/8897], Loss: 5.3244\n",
      "Epoch [1/1], Step [4013/8897], Loss: 5.1564\n",
      "Epoch [1/1], Step [4014/8897], Loss: 5.3072\n",
      "Epoch [1/1], Step [4015/8897], Loss: 5.6496\n",
      "Epoch [1/1], Step [4016/8897], Loss: 5.6271\n",
      "Epoch [1/1], Step [4017/8897], Loss: 5.5621\n",
      "Epoch [1/1], Step [4018/8897], Loss: 5.2084\n",
      "Epoch [1/1], Step [4019/8897], Loss: 5.2719\n",
      "Epoch [1/1], Step [4020/8897], Loss: 5.1812\n",
      "Epoch [1/1], Step [4021/8897], Loss: 5.3716\n",
      "Epoch [1/1], Step [4022/8897], Loss: 5.2421\n",
      "Epoch [1/1], Step [4023/8897], Loss: 5.0021\n",
      "Epoch [1/1], Step [4024/8897], Loss: 5.3041\n",
      "Epoch [1/1], Step [4025/8897], Loss: 5.5342\n",
      "Epoch [1/1], Step [4026/8897], Loss: 5.4453\n",
      "Epoch [1/1], Step [4027/8897], Loss: 5.4301\n",
      "Epoch [1/1], Step [4028/8897], Loss: 5.3891\n",
      "Epoch [1/1], Step [4029/8897], Loss: 5.3578\n",
      "Epoch [1/1], Step [4030/8897], Loss: 5.2837\n",
      "Epoch [1/1], Step [4031/8897], Loss: 5.2011\n",
      "Epoch [1/1], Step [4032/8897], Loss: 5.3363\n",
      "Epoch [1/1], Step [4033/8897], Loss: 5.3868\n",
      "Epoch [1/1], Step [4034/8897], Loss: 5.1606\n",
      "Epoch [1/1], Step [4035/8897], Loss: 5.3997\n",
      "Epoch [1/1], Step [4036/8897], Loss: 5.4512\n",
      "Epoch [1/1], Step [4037/8897], Loss: 5.1193\n",
      "Epoch [1/1], Step [4038/8897], Loss: 5.6538\n",
      "Epoch [1/1], Step [4039/8897], Loss: 5.3260\n",
      "Epoch [1/1], Step [4040/8897], Loss: 5.2055\n",
      "Epoch [1/1], Step [4041/8897], Loss: 5.2385\n",
      "Epoch [1/1], Step [4042/8897], Loss: 5.5820\n",
      "Epoch [1/1], Step [4043/8897], Loss: 5.4115\n",
      "Epoch [1/1], Step [4044/8897], Loss: 5.4359\n",
      "Epoch [1/1], Step [4045/8897], Loss: 5.3818\n",
      "Epoch [1/1], Step [4046/8897], Loss: 5.3069\n",
      "Epoch [1/1], Step [4047/8897], Loss: 5.3953\n",
      "Epoch [1/1], Step [4048/8897], Loss: 5.2827\n",
      "Epoch [1/1], Step [4049/8897], Loss: 5.4248\n",
      "Epoch [1/1], Step [4050/8897], Loss: 5.4354\n",
      "Epoch [1/1], Step [4051/8897], Loss: 5.4073\n",
      "Epoch [1/1], Step [4052/8897], Loss: 5.4725\n",
      "Epoch [1/1], Step [4053/8897], Loss: 5.5645\n",
      "Epoch [1/1], Step [4054/8897], Loss: 5.4451\n",
      "Epoch [1/1], Step [4055/8897], Loss: 5.5030\n",
      "Epoch [1/1], Step [4056/8897], Loss: 5.1990\n",
      "Epoch [1/1], Step [4057/8897], Loss: 5.3594\n",
      "Epoch [1/1], Step [4058/8897], Loss: 5.4196\n",
      "Epoch [1/1], Step [4059/8897], Loss: 5.4577\n",
      "Epoch [1/1], Step [4060/8897], Loss: 5.4641\n",
      "Epoch [1/1], Step [4061/8897], Loss: 5.2347\n",
      "Epoch [1/1], Step [4062/8897], Loss: 5.4031\n",
      "Epoch [1/1], Step [4063/8897], Loss: 5.5286\n",
      "Epoch [1/1], Step [4064/8897], Loss: 5.6026\n",
      "Epoch [1/1], Step [4065/8897], Loss: 5.4347\n",
      "Epoch [1/1], Step [4066/8897], Loss: 5.5633\n",
      "Epoch [1/1], Step [4067/8897], Loss: 5.4334\n",
      "Epoch [1/1], Step [4068/8897], Loss: 5.5363\n",
      "Epoch [1/1], Step [4069/8897], Loss: 5.0947\n",
      "Epoch [1/1], Step [4070/8897], Loss: 5.3541\n",
      "Epoch [1/1], Step [4071/8897], Loss: 5.6308\n",
      "Epoch [1/1], Step [4072/8897], Loss: 5.2830\n",
      "Epoch [1/1], Step [4073/8897], Loss: 5.2078\n",
      "Epoch [1/1], Step [4074/8897], Loss: 5.4273\n",
      "Epoch [1/1], Step [4075/8897], Loss: 5.2406\n",
      "Epoch [1/1], Step [4076/8897], Loss: 5.2493\n",
      "Epoch [1/1], Step [4077/8897], Loss: 5.6405\n",
      "Epoch [1/1], Step [4078/8897], Loss: 5.4931\n",
      "Epoch [1/1], Step [4079/8897], Loss: 5.5736\n",
      "Epoch [1/1], Step [4080/8897], Loss: 5.2749\n",
      "Epoch [1/1], Step [4081/8897], Loss: 5.2924\n",
      "Epoch [1/1], Step [4082/8897], Loss: 5.3490\n",
      "Epoch [1/1], Step [4083/8897], Loss: 5.3915\n",
      "Epoch [1/1], Step [4084/8897], Loss: 5.2576\n",
      "Epoch [1/1], Step [4085/8897], Loss: 5.3822\n",
      "Epoch [1/1], Step [4086/8897], Loss: 5.4839\n",
      "Epoch [1/1], Step [4087/8897], Loss: 5.6993\n",
      "Epoch [1/1], Step [4088/8897], Loss: 5.4222\n",
      "Epoch [1/1], Step [4089/8897], Loss: 5.2193\n",
      "Epoch [1/1], Step [4090/8897], Loss: 5.4541\n",
      "Epoch [1/1], Step [4091/8897], Loss: 5.4940\n",
      "Epoch [1/1], Step [4092/8897], Loss: 5.3883\n",
      "Epoch [1/1], Step [4093/8897], Loss: 5.5424\n",
      "Epoch [1/1], Step [4094/8897], Loss: 5.3279\n",
      "Epoch [1/1], Step [4095/8897], Loss: 5.3321\n",
      "Epoch [1/1], Step [4096/8897], Loss: 5.3323\n",
      "Epoch [1/1], Step [4097/8897], Loss: 5.5050\n",
      "Epoch [1/1], Step [4098/8897], Loss: 5.5936\n",
      "Epoch [1/1], Step [4099/8897], Loss: 5.4260\n",
      "Epoch [1/1], Step [4100/8897], Loss: 5.3550\n",
      "Epoch [1/1], Step [4101/8897], Loss: 5.1925\n",
      "Epoch [1/1], Step [4102/8897], Loss: 5.3625\n",
      "Epoch [1/1], Step [4103/8897], Loss: 5.3284\n",
      "Epoch [1/1], Step [4104/8897], Loss: 5.4414\n",
      "Epoch [1/1], Step [4105/8897], Loss: 5.3165\n",
      "Epoch [1/1], Step [4106/8897], Loss: 5.3531\n",
      "Epoch [1/1], Step [4107/8897], Loss: 5.3541\n",
      "Epoch [1/1], Step [4108/8897], Loss: 5.3004\n",
      "Epoch [1/1], Step [4109/8897], Loss: 5.3475\n",
      "Epoch [1/1], Step [4110/8897], Loss: 5.1758\n",
      "Epoch [1/1], Step [4111/8897], Loss: 5.4080\n",
      "Epoch [1/1], Step [4112/8897], Loss: 5.4747\n",
      "Epoch [1/1], Step [4113/8897], Loss: 5.4340\n",
      "Epoch [1/1], Step [4114/8897], Loss: 5.5401\n",
      "Epoch [1/1], Step [4115/8897], Loss: 5.1467\n",
      "Epoch [1/1], Step [4116/8897], Loss: 5.5341\n",
      "Epoch [1/1], Step [4117/8897], Loss: 5.6171\n",
      "Epoch [1/1], Step [4118/8897], Loss: 5.2066\n",
      "Epoch [1/1], Step [4119/8897], Loss: 5.1188\n",
      "Epoch [1/1], Step [4120/8897], Loss: 5.6114\n",
      "Epoch [1/1], Step [4121/8897], Loss: 5.4739\n",
      "Epoch [1/1], Step [4122/8897], Loss: 5.7536\n",
      "Epoch [1/1], Step [4123/8897], Loss: 5.5897\n",
      "Epoch [1/1], Step [4124/8897], Loss: 5.0633\n",
      "Epoch [1/1], Step [4125/8897], Loss: 5.4397\n",
      "Epoch [1/1], Step [4126/8897], Loss: 5.3417\n",
      "Epoch [1/1], Step [4127/8897], Loss: 5.4965\n",
      "Epoch [1/1], Step [4128/8897], Loss: 5.1688\n",
      "Epoch [1/1], Step [4129/8897], Loss: 5.6258\n",
      "Epoch [1/1], Step [4130/8897], Loss: 5.3434\n",
      "Epoch [1/1], Step [4131/8897], Loss: 5.4113\n",
      "Epoch [1/1], Step [4132/8897], Loss: 5.3307\n",
      "Epoch [1/1], Step [4133/8897], Loss: 5.4978\n",
      "Epoch [1/1], Step [4134/8897], Loss: 5.8402\n",
      "Epoch [1/1], Step [4135/8897], Loss: 5.3702\n",
      "Epoch [1/1], Step [4136/8897], Loss: 5.3049\n",
      "Epoch [1/1], Step [4137/8897], Loss: 5.4003\n",
      "Epoch [1/1], Step [4138/8897], Loss: 5.2024\n",
      "Epoch [1/1], Step [4139/8897], Loss: 5.1818\n",
      "Epoch [1/1], Step [4140/8897], Loss: 5.5016\n",
      "Epoch [1/1], Step [4141/8897], Loss: 5.4237\n",
      "Epoch [1/1], Step [4142/8897], Loss: 5.2235\n",
      "Epoch [1/1], Step [4143/8897], Loss: 5.3216\n",
      "Epoch [1/1], Step [4144/8897], Loss: 5.6963\n",
      "Epoch [1/1], Step [4145/8897], Loss: 5.4559\n",
      "Epoch [1/1], Step [4146/8897], Loss: 5.3350\n",
      "Epoch [1/1], Step [4147/8897], Loss: 5.3111\n",
      "Epoch [1/1], Step [4148/8897], Loss: 5.3895\n",
      "Epoch [1/1], Step [4149/8897], Loss: 5.6059\n",
      "Epoch [1/1], Step [4150/8897], Loss: 5.2491\n",
      "Epoch [1/1], Step [4151/8897], Loss: 5.3054\n",
      "Epoch [1/1], Step [4152/8897], Loss: 5.3581\n",
      "Epoch [1/1], Step [4153/8897], Loss: 5.5808\n",
      "Epoch [1/1], Step [4154/8897], Loss: 5.3204\n",
      "Epoch [1/1], Step [4155/8897], Loss: 5.6871\n",
      "Epoch [1/1], Step [4156/8897], Loss: 5.4113\n",
      "Epoch [1/1], Step [4157/8897], Loss: 5.7197\n",
      "Epoch [1/1], Step [4158/8897], Loss: 5.3409\n",
      "Epoch [1/1], Step [4159/8897], Loss: 5.5114\n",
      "Epoch [1/1], Step [4160/8897], Loss: 5.3880\n",
      "Epoch [1/1], Step [4161/8897], Loss: 5.3840\n",
      "Epoch [1/1], Step [4162/8897], Loss: 5.6198\n",
      "Epoch [1/1], Step [4163/8897], Loss: 5.5215\n",
      "Epoch [1/1], Step [4164/8897], Loss: 5.5431\n",
      "Epoch [1/1], Step [4165/8897], Loss: 5.2877\n",
      "Epoch [1/1], Step [4166/8897], Loss: 5.2860\n",
      "Epoch [1/1], Step [4167/8897], Loss: 5.3279\n",
      "Epoch [1/1], Step [4168/8897], Loss: 5.5520\n",
      "Epoch [1/1], Step [4169/8897], Loss: 5.6030\n",
      "Epoch [1/1], Step [4170/8897], Loss: 5.4009\n",
      "Epoch [1/1], Step [4171/8897], Loss: 5.2987\n",
      "Epoch [1/1], Step [4172/8897], Loss: 5.2921\n",
      "Epoch [1/1], Step [4173/8897], Loss: 5.3481\n",
      "Epoch [1/1], Step [4174/8897], Loss: 5.3859\n",
      "Epoch [1/1], Step [4175/8897], Loss: 5.5159\n",
      "Epoch [1/1], Step [4176/8897], Loss: 5.4273\n",
      "Epoch [1/1], Step [4177/8897], Loss: 5.2166\n",
      "Epoch [1/1], Step [4178/8897], Loss: 5.2309\n",
      "Epoch [1/1], Step [4179/8897], Loss: 5.4040\n",
      "Epoch [1/1], Step [4180/8897], Loss: 5.5995\n",
      "Epoch [1/1], Step [4181/8897], Loss: 5.7325\n",
      "Epoch [1/1], Step [4182/8897], Loss: 4.9577\n",
      "Epoch [1/1], Step [4183/8897], Loss: 5.3313\n",
      "Epoch [1/1], Step [4184/8897], Loss: 5.3225\n",
      "Epoch [1/1], Step [4185/8897], Loss: 5.4216\n",
      "Epoch [1/1], Step [4186/8897], Loss: 5.1399\n",
      "Epoch [1/1], Step [4187/8897], Loss: 5.6134\n",
      "Epoch [1/1], Step [4188/8897], Loss: 5.3495\n",
      "Epoch [1/1], Step [4189/8897], Loss: 5.4506\n",
      "Epoch [1/1], Step [4190/8897], Loss: 5.2743\n",
      "Epoch [1/1], Step [4191/8897], Loss: 5.4749\n",
      "Epoch [1/1], Step [4192/8897], Loss: 5.4888\n",
      "Epoch [1/1], Step [4193/8897], Loss: 5.2206\n",
      "Epoch [1/1], Step [4194/8897], Loss: 5.3573\n",
      "Epoch [1/1], Step [4195/8897], Loss: 5.3830\n",
      "Epoch [1/1], Step [4196/8897], Loss: 5.3903\n",
      "Epoch [1/1], Step [4197/8897], Loss: 5.5135\n",
      "Epoch [1/1], Step [4198/8897], Loss: 5.1105\n",
      "Epoch [1/1], Step [4199/8897], Loss: 5.5500\n",
      "Epoch [1/1], Step [4200/8897], Loss: 5.2018\n",
      "Epoch [1/1], Step [4201/8897], Loss: 5.3466\n",
      "Epoch [1/1], Step [4202/8897], Loss: 5.3184\n",
      "Epoch [1/1], Step [4203/8897], Loss: 5.6724\n",
      "Epoch [1/1], Step [4204/8897], Loss: 5.2651\n",
      "Epoch [1/1], Step [4205/8897], Loss: 5.4363\n",
      "Epoch [1/1], Step [4206/8897], Loss: 5.6819\n",
      "Epoch [1/1], Step [4207/8897], Loss: 5.3522\n",
      "Epoch [1/1], Step [4208/8897], Loss: 5.4697\n",
      "Epoch [1/1], Step [4209/8897], Loss: 5.2843\n",
      "Epoch [1/1], Step [4210/8897], Loss: 5.3320\n",
      "Epoch [1/1], Step [4211/8897], Loss: 5.5021\n",
      "Epoch [1/1], Step [4212/8897], Loss: 5.1358\n",
      "Epoch [1/1], Step [4213/8897], Loss: 5.4837\n",
      "Epoch [1/1], Step [4214/8897], Loss: 5.3433\n",
      "Epoch [1/1], Step [4215/8897], Loss: 5.4751\n",
      "Epoch [1/1], Step [4216/8897], Loss: 5.3128\n",
      "Epoch [1/1], Step [4217/8897], Loss: 5.4134\n",
      "Epoch [1/1], Step [4218/8897], Loss: 5.4495\n",
      "Epoch [1/1], Step [4219/8897], Loss: 5.3256\n",
      "Epoch [1/1], Step [4220/8897], Loss: 5.5616\n",
      "Epoch [1/1], Step [4221/8897], Loss: 5.4072\n",
      "Epoch [1/1], Step [4222/8897], Loss: 5.5385\n",
      "Epoch [1/1], Step [4223/8897], Loss: 5.4941\n",
      "Epoch [1/1], Step [4224/8897], Loss: 5.1802\n",
      "Epoch [1/1], Step [4225/8897], Loss: 5.5380\n",
      "Epoch [1/1], Step [4226/8897], Loss: 5.2562\n",
      "Epoch [1/1], Step [4227/8897], Loss: 5.4444\n",
      "Epoch [1/1], Step [4228/8897], Loss: 5.5333\n",
      "Epoch [1/1], Step [4229/8897], Loss: 5.5295\n",
      "Epoch [1/1], Step [4230/8897], Loss: 5.5763\n",
      "Epoch [1/1], Step [4231/8897], Loss: 5.2868\n",
      "Epoch [1/1], Step [4232/8897], Loss: 5.4298\n",
      "Epoch [1/1], Step [4233/8897], Loss: 5.3162\n",
      "Epoch [1/1], Step [4234/8897], Loss: 5.3970\n",
      "Epoch [1/1], Step [4235/8897], Loss: 5.4235\n",
      "Epoch [1/1], Step [4236/8897], Loss: 5.3216\n",
      "Epoch [1/1], Step [4237/8897], Loss: 5.2554\n",
      "Epoch [1/1], Step [4238/8897], Loss: 5.2569\n",
      "Epoch [1/1], Step [4239/8897], Loss: 5.2686\n",
      "Epoch [1/1], Step [4240/8897], Loss: 5.4560\n",
      "Epoch [1/1], Step [4241/8897], Loss: 5.4637\n",
      "Epoch [1/1], Step [4242/8897], Loss: 5.4285\n",
      "Epoch [1/1], Step [4243/8897], Loss: 5.5262\n",
      "Epoch [1/1], Step [4244/8897], Loss: 5.5963\n",
      "Epoch [1/1], Step [4245/8897], Loss: 5.5893\n",
      "Epoch [1/1], Step [4246/8897], Loss: 5.1980\n",
      "Epoch [1/1], Step [4247/8897], Loss: 5.3319\n",
      "Epoch [1/1], Step [4248/8897], Loss: 5.5163\n",
      "Epoch [1/1], Step [4249/8897], Loss: 5.3644\n",
      "Epoch [1/1], Step [4250/8897], Loss: 5.2998\n",
      "Epoch [1/1], Step [4251/8897], Loss: 5.3324\n",
      "Epoch [1/1], Step [4252/8897], Loss: 5.5549\n",
      "Epoch [1/1], Step [4253/8897], Loss: 5.4404\n",
      "Epoch [1/1], Step [4254/8897], Loss: 5.4597\n",
      "Epoch [1/1], Step [4255/8897], Loss: 5.4337\n",
      "Epoch [1/1], Step [4256/8897], Loss: 5.4658\n",
      "Epoch [1/1], Step [4257/8897], Loss: 5.2773\n",
      "Epoch [1/1], Step [4258/8897], Loss: 5.2581\n",
      "Epoch [1/1], Step [4259/8897], Loss: 5.6999\n",
      "Epoch [1/1], Step [4260/8897], Loss: 5.3597\n",
      "Epoch [1/1], Step [4261/8897], Loss: 5.5978\n",
      "Epoch [1/1], Step [4262/8897], Loss: 5.2444\n",
      "Epoch [1/1], Step [4263/8897], Loss: 5.6041\n",
      "Epoch [1/1], Step [4264/8897], Loss: 5.2827\n",
      "Epoch [1/1], Step [4265/8897], Loss: 5.4093\n",
      "Epoch [1/1], Step [4266/8897], Loss: 5.6479\n",
      "Epoch [1/1], Step [4267/8897], Loss: 5.6699\n",
      "Epoch [1/1], Step [4268/8897], Loss: 5.3159\n",
      "Epoch [1/1], Step [4269/8897], Loss: 5.2924\n",
      "Epoch [1/1], Step [4270/8897], Loss: 5.4507\n",
      "Epoch [1/1], Step [4271/8897], Loss: 5.2082\n",
      "Epoch [1/1], Step [4272/8897], Loss: 5.6988\n",
      "Epoch [1/1], Step [4273/8897], Loss: 5.5294\n",
      "Epoch [1/1], Step [4274/8897], Loss: 5.3530\n",
      "Epoch [1/1], Step [4275/8897], Loss: 5.0081\n",
      "Epoch [1/1], Step [4276/8897], Loss: 5.3729\n",
      "Epoch [1/1], Step [4277/8897], Loss: 5.2389\n",
      "Epoch [1/1], Step [4278/8897], Loss: 5.4114\n",
      "Epoch [1/1], Step [4279/8897], Loss: 5.3722\n",
      "Epoch [1/1], Step [4280/8897], Loss: 5.3779\n",
      "Epoch [1/1], Step [4281/8897], Loss: 5.4813\n",
      "Epoch [1/1], Step [4282/8897], Loss: 5.4035\n",
      "Epoch [1/1], Step [4283/8897], Loss: 5.2835\n",
      "Epoch [1/1], Step [4284/8897], Loss: 5.4232\n",
      "Epoch [1/1], Step [4285/8897], Loss: 5.2156\n",
      "Epoch [1/1], Step [4286/8897], Loss: 5.2116\n",
      "Epoch [1/1], Step [4287/8897], Loss: 5.2752\n",
      "Epoch [1/1], Step [4288/8897], Loss: 5.1597\n",
      "Epoch [1/1], Step [4289/8897], Loss: 5.4747\n",
      "Epoch [1/1], Step [4290/8897], Loss: 5.6264\n",
      "Epoch [1/1], Step [4291/8897], Loss: 5.4862\n",
      "Epoch [1/1], Step [4292/8897], Loss: 5.5920\n",
      "Epoch [1/1], Step [4293/8897], Loss: 5.2658\n",
      "Epoch [1/1], Step [4294/8897], Loss: 5.3117\n",
      "Epoch [1/1], Step [4295/8897], Loss: 5.2547\n",
      "Epoch [1/1], Step [4296/8897], Loss: 5.7011\n",
      "Epoch [1/1], Step [4297/8897], Loss: 5.6224\n",
      "Epoch [1/1], Step [4298/8897], Loss: 5.4994\n",
      "Epoch [1/1], Step [4299/8897], Loss: 5.2988\n",
      "Epoch [1/1], Step [4300/8897], Loss: 5.4676\n",
      "Epoch [1/1], Step [4301/8897], Loss: 5.2447\n",
      "Epoch [1/1], Step [4302/8897], Loss: 5.2802\n",
      "Epoch [1/1], Step [4303/8897], Loss: 5.3708\n",
      "Epoch [1/1], Step [4304/8897], Loss: 5.4415\n",
      "Epoch [1/1], Step [4305/8897], Loss: 5.2790\n",
      "Epoch [1/1], Step [4306/8897], Loss: 5.1816\n",
      "Epoch [1/1], Step [4307/8897], Loss: 5.2944\n",
      "Epoch [1/1], Step [4308/8897], Loss: 5.2002\n",
      "Epoch [1/1], Step [4309/8897], Loss: 5.4127\n",
      "Epoch [1/1], Step [4310/8897], Loss: 5.1600\n",
      "Epoch [1/1], Step [4311/8897], Loss: 5.3900\n",
      "Epoch [1/1], Step [4312/8897], Loss: 5.3189\n",
      "Epoch [1/1], Step [4313/8897], Loss: 5.5813\n",
      "Epoch [1/1], Step [4314/8897], Loss: 5.5106\n",
      "Epoch [1/1], Step [4315/8897], Loss: 5.4273\n",
      "Epoch [1/1], Step [4316/8897], Loss: 5.3706\n",
      "Epoch [1/1], Step [4317/8897], Loss: 5.2642\n",
      "Epoch [1/1], Step [4318/8897], Loss: 5.6238\n",
      "Epoch [1/1], Step [4319/8897], Loss: 5.2132\n",
      "Epoch [1/1], Step [4320/8897], Loss: 5.3802\n",
      "Epoch [1/1], Step [4321/8897], Loss: 5.1763\n",
      "Epoch [1/1], Step [4322/8897], Loss: 5.3885\n",
      "Epoch [1/1], Step [4323/8897], Loss: 5.5609\n",
      "Epoch [1/1], Step [4324/8897], Loss: 5.2300\n",
      "Epoch [1/1], Step [4325/8897], Loss: 5.5105\n",
      "Epoch [1/1], Step [4326/8897], Loss: 5.4027\n",
      "Epoch [1/1], Step [4327/8897], Loss: 5.5333\n",
      "Epoch [1/1], Step [4328/8897], Loss: 5.4390\n",
      "Epoch [1/1], Step [4329/8897], Loss: 5.3666\n",
      "Epoch [1/1], Step [4330/8897], Loss: 5.3843\n",
      "Epoch [1/1], Step [4331/8897], Loss: 5.3138\n",
      "Epoch [1/1], Step [4332/8897], Loss: 5.2334\n",
      "Epoch [1/1], Step [4333/8897], Loss: 5.5354\n",
      "Epoch [1/1], Step [4334/8897], Loss: 5.3018\n",
      "Epoch [1/1], Step [4335/8897], Loss: 5.2437\n",
      "Epoch [1/1], Step [4336/8897], Loss: 5.4503\n",
      "Epoch [1/1], Step [4337/8897], Loss: 5.3655\n",
      "Epoch [1/1], Step [4338/8897], Loss: 5.5036\n",
      "Epoch [1/1], Step [4339/8897], Loss: 5.4330\n",
      "Epoch [1/1], Step [4340/8897], Loss: 5.4374\n",
      "Epoch [1/1], Step [4341/8897], Loss: 5.6688\n",
      "Epoch [1/1], Step [4342/8897], Loss: 5.2787\n",
      "Epoch [1/1], Step [4343/8897], Loss: 5.2949\n",
      "Epoch [1/1], Step [4344/8897], Loss: 5.4682\n",
      "Epoch [1/1], Step [4345/8897], Loss: 5.2070\n",
      "Epoch [1/1], Step [4346/8897], Loss: 5.5118\n",
      "Epoch [1/1], Step [4347/8897], Loss: 5.2813\n",
      "Epoch [1/1], Step [4348/8897], Loss: 5.3078\n",
      "Epoch [1/1], Step [4349/8897], Loss: 5.4075\n",
      "Epoch [1/1], Step [4350/8897], Loss: 5.3469\n",
      "Epoch [1/1], Step [4351/8897], Loss: 5.4190\n",
      "Epoch [1/1], Step [4352/8897], Loss: 5.7648\n",
      "Epoch [1/1], Step [4353/8897], Loss: 5.4334\n",
      "Epoch [1/1], Step [4354/8897], Loss: 5.3219\n",
      "Epoch [1/1], Step [4355/8897], Loss: 5.1550\n",
      "Epoch [1/1], Step [4356/8897], Loss: 5.2297\n",
      "Epoch [1/1], Step [4357/8897], Loss: 5.0728\n",
      "Epoch [1/1], Step [4358/8897], Loss: 5.3930\n",
      "Epoch [1/1], Step [4359/8897], Loss: 5.6377\n",
      "Epoch [1/1], Step [4360/8897], Loss: 5.3917\n",
      "Epoch [1/1], Step [4361/8897], Loss: 5.5197\n",
      "Epoch [1/1], Step [4362/8897], Loss: 5.7574\n",
      "Epoch [1/1], Step [4363/8897], Loss: 5.3532\n",
      "Epoch [1/1], Step [4364/8897], Loss: 5.5364\n",
      "Epoch [1/1], Step [4365/8897], Loss: 5.3337\n",
      "Epoch [1/1], Step [4366/8897], Loss: 5.2979\n",
      "Epoch [1/1], Step [4367/8897], Loss: 5.5704\n",
      "Epoch [1/1], Step [4368/8897], Loss: 5.4197\n",
      "Epoch [1/1], Step [4369/8897], Loss: 5.3838\n",
      "Epoch [1/1], Step [4370/8897], Loss: 5.7305\n",
      "Epoch [1/1], Step [4371/8897], Loss: 5.3875\n",
      "Epoch [1/1], Step [4372/8897], Loss: 5.2594\n",
      "Epoch [1/1], Step [4373/8897], Loss: 5.2155\n",
      "Epoch [1/1], Step [4374/8897], Loss: 5.3396\n",
      "Epoch [1/1], Step [4375/8897], Loss: 5.4144\n",
      "Epoch [1/1], Step [4376/8897], Loss: 5.4339\n",
      "Epoch [1/1], Step [4377/8897], Loss: 5.4461\n",
      "Epoch [1/1], Step [4378/8897], Loss: 5.4241\n",
      "Epoch [1/1], Step [4379/8897], Loss: 5.3325\n",
      "Epoch [1/1], Step [4380/8897], Loss: 5.2938\n",
      "Epoch [1/1], Step [4381/8897], Loss: 5.5526\n",
      "Epoch [1/1], Step [4382/8897], Loss: 5.1206\n",
      "Epoch [1/1], Step [4383/8897], Loss: 5.3851\n",
      "Epoch [1/1], Step [4384/8897], Loss: 5.1427\n",
      "Epoch [1/1], Step [4385/8897], Loss: 5.3244\n",
      "Epoch [1/1], Step [4386/8897], Loss: 5.3558\n",
      "Epoch [1/1], Step [4387/8897], Loss: 5.4113\n",
      "Epoch [1/1], Step [4388/8897], Loss: 5.3112\n",
      "Epoch [1/1], Step [4389/8897], Loss: 5.6230\n",
      "Epoch [1/1], Step [4390/8897], Loss: 5.3135\n",
      "Epoch [1/1], Step [4391/8897], Loss: 5.3761\n",
      "Epoch [1/1], Step [4392/8897], Loss: 5.3052\n",
      "Epoch [1/1], Step [4393/8897], Loss: 5.6156\n",
      "Epoch [1/1], Step [4394/8897], Loss: 5.1575\n",
      "Epoch [1/1], Step [4395/8897], Loss: 5.6411\n",
      "Epoch [1/1], Step [4396/8897], Loss: 5.1788\n",
      "Epoch [1/1], Step [4397/8897], Loss: 5.2866\n",
      "Epoch [1/1], Step [4398/8897], Loss: 5.3446\n",
      "Epoch [1/1], Step [4399/8897], Loss: 5.2752\n",
      "Epoch [1/1], Step [4400/8897], Loss: 5.3183\n",
      "Epoch [1/1], Step [4401/8897], Loss: 5.3669\n",
      "Epoch [1/1], Step [4402/8897], Loss: 5.6431\n",
      "Epoch [1/1], Step [4403/8897], Loss: 5.6133\n",
      "Epoch [1/1], Step [4404/8897], Loss: 5.1701\n",
      "Epoch [1/1], Step [4405/8897], Loss: 5.3622\n",
      "Epoch [1/1], Step [4406/8897], Loss: 5.3248\n",
      "Epoch [1/1], Step [4407/8897], Loss: 5.1649\n",
      "Epoch [1/1], Step [4408/8897], Loss: 5.3509\n",
      "Epoch [1/1], Step [4409/8897], Loss: 5.1632\n",
      "Epoch [1/1], Step [4410/8897], Loss: 5.3975\n",
      "Epoch [1/1], Step [4411/8897], Loss: 5.0843\n",
      "Epoch [1/1], Step [4412/8897], Loss: 5.4920\n",
      "Epoch [1/1], Step [4413/8897], Loss: 5.1207\n",
      "Epoch [1/1], Step [4414/8897], Loss: 5.6202\n",
      "Epoch [1/1], Step [4415/8897], Loss: 5.4516\n",
      "Epoch [1/1], Step [4416/8897], Loss: 5.4402\n",
      "Epoch [1/1], Step [4417/8897], Loss: 5.2830\n",
      "Epoch [1/1], Step [4418/8897], Loss: 5.2607\n",
      "Epoch [1/1], Step [4419/8897], Loss: 5.3390\n",
      "Epoch [1/1], Step [4420/8897], Loss: 5.1820\n",
      "Epoch [1/1], Step [4421/8897], Loss: 5.2762\n",
      "Epoch [1/1], Step [4422/8897], Loss: 5.3883\n",
      "Epoch [1/1], Step [4423/8897], Loss: 5.3959\n",
      "Epoch [1/1], Step [4424/8897], Loss: 5.4325\n",
      "Epoch [1/1], Step [4425/8897], Loss: 5.4474\n",
      "Epoch [1/1], Step [4426/8897], Loss: 5.7177\n",
      "Epoch [1/1], Step [4427/8897], Loss: 5.2477\n",
      "Epoch [1/1], Step [4428/8897], Loss: 5.4321\n",
      "Epoch [1/1], Step [4429/8897], Loss: 5.5321\n",
      "Epoch [1/1], Step [4430/8897], Loss: 5.4283\n",
      "Epoch [1/1], Step [4431/8897], Loss: 5.3147\n",
      "Epoch [1/1], Step [4432/8897], Loss: 5.4132\n",
      "Epoch [1/1], Step [4433/8897], Loss: 5.4838\n",
      "Epoch [1/1], Step [4434/8897], Loss: 5.4096\n",
      "Epoch [1/1], Step [4435/8897], Loss: 5.3828\n",
      "Epoch [1/1], Step [4436/8897], Loss: 5.2208\n",
      "Epoch [1/1], Step [4437/8897], Loss: 5.4171\n",
      "Epoch [1/1], Step [4438/8897], Loss: 5.5395\n",
      "Epoch [1/1], Step [4439/8897], Loss: 5.2561\n",
      "Epoch [1/1], Step [4440/8897], Loss: 5.4358\n",
      "Epoch [1/1], Step [4441/8897], Loss: 5.1994\n",
      "Epoch [1/1], Step [4442/8897], Loss: 5.4883\n",
      "Epoch [1/1], Step [4443/8897], Loss: 5.1025\n",
      "Epoch [1/1], Step [4444/8897], Loss: 5.3104\n",
      "Epoch [1/1], Step [4445/8897], Loss: 5.7979\n",
      "Epoch [1/1], Step [4446/8897], Loss: 5.5476\n",
      "Epoch [1/1], Step [4447/8897], Loss: 5.2420\n",
      "Epoch [1/1], Step [4448/8897], Loss: 5.3058\n",
      "Epoch [1/1], Step [4449/8897], Loss: 5.2999\n",
      "Epoch [1/1], Step [4450/8897], Loss: 5.6001\n",
      "Epoch [1/1], Step [4451/8897], Loss: 5.3497\n",
      "Epoch [1/1], Step [4452/8897], Loss: 5.4213\n",
      "Epoch [1/1], Step [4453/8897], Loss: 5.5859\n",
      "Epoch [1/1], Step [4454/8897], Loss: 5.2886\n",
      "Epoch [1/1], Step [4455/8897], Loss: 5.1957\n",
      "Epoch [1/1], Step [4456/8897], Loss: 5.2836\n",
      "Epoch [1/1], Step [4457/8897], Loss: 5.5515\n",
      "Epoch [1/1], Step [4458/8897], Loss: 5.2994\n",
      "Epoch [1/1], Step [4459/8897], Loss: 5.3555\n",
      "Epoch [1/1], Step [4460/8897], Loss: 5.4133\n",
      "Epoch [1/1], Step [4461/8897], Loss: 5.3832\n",
      "Epoch [1/1], Step [4462/8897], Loss: 5.2843\n",
      "Epoch [1/1], Step [4463/8897], Loss: 5.5723\n",
      "Epoch [1/1], Step [4464/8897], Loss: 5.3013\n",
      "Epoch [1/1], Step [4465/8897], Loss: 5.1864\n",
      "Epoch [1/1], Step [4466/8897], Loss: 5.5538\n",
      "Epoch [1/1], Step [4467/8897], Loss: 5.3124\n",
      "Epoch [1/1], Step [4468/8897], Loss: 5.5051\n",
      "Epoch [1/1], Step [4469/8897], Loss: 5.6085\n",
      "Epoch [1/1], Step [4470/8897], Loss: 5.2934\n",
      "Epoch [1/1], Step [4471/8897], Loss: 5.1731\n",
      "Epoch [1/1], Step [4472/8897], Loss: 5.5411\n",
      "Epoch [1/1], Step [4473/8897], Loss: 5.2516\n",
      "Epoch [1/1], Step [4474/8897], Loss: 5.3765\n",
      "Epoch [1/1], Step [4475/8897], Loss: 5.0275\n",
      "Epoch [1/1], Step [4476/8897], Loss: 5.3706\n",
      "Epoch [1/1], Step [4477/8897], Loss: 5.5435\n",
      "Epoch [1/1], Step [4478/8897], Loss: 5.4413\n",
      "Epoch [1/1], Step [4479/8897], Loss: 5.1750\n",
      "Epoch [1/1], Step [4480/8897], Loss: 5.7636\n",
      "Epoch [1/1], Step [4481/8897], Loss: 5.6357\n",
      "Epoch [1/1], Step [4482/8897], Loss: 5.4648\n",
      "Epoch [1/1], Step [4483/8897], Loss: 5.5498\n",
      "Epoch [1/1], Step [4484/8897], Loss: 5.4605\n",
      "Epoch [1/1], Step [4485/8897], Loss: 5.3136\n",
      "Epoch [1/1], Step [4486/8897], Loss: 5.5440\n",
      "Epoch [1/1], Step [4487/8897], Loss: 5.5166\n",
      "Epoch [1/1], Step [4488/8897], Loss: 5.1411\n",
      "Epoch [1/1], Step [4489/8897], Loss: 5.4406\n",
      "Epoch [1/1], Step [4490/8897], Loss: 5.1738\n",
      "Epoch [1/1], Step [4491/8897], Loss: 5.5958\n",
      "Epoch [1/1], Step [4492/8897], Loss: 5.2534\n",
      "Epoch [1/1], Step [4493/8897], Loss: 5.6068\n",
      "Epoch [1/1], Step [4494/8897], Loss: 5.4427\n",
      "Epoch [1/1], Step [4495/8897], Loss: 5.2915\n",
      "Epoch [1/1], Step [4496/8897], Loss: 5.4051\n",
      "Epoch [1/1], Step [4497/8897], Loss: 5.5525\n",
      "Epoch [1/1], Step [4498/8897], Loss: 5.2174\n",
      "Epoch [1/1], Step [4499/8897], Loss: 5.3740\n",
      "Epoch [1/1], Step [4500/8897], Loss: 5.6281\n",
      "Epoch [1/1], Step [4501/8897], Loss: 5.2917\n",
      "Epoch [1/1], Step [4502/8897], Loss: 5.4079\n",
      "Epoch [1/1], Step [4503/8897], Loss: 5.1813\n",
      "Epoch [1/1], Step [4504/8897], Loss: 5.3732\n",
      "Epoch [1/1], Step [4505/8897], Loss: 5.3181\n",
      "Epoch [1/1], Step [4506/8897], Loss: 5.4607\n",
      "Epoch [1/1], Step [4507/8897], Loss: 5.4620\n",
      "Epoch [1/1], Step [4508/8897], Loss: 5.4113\n",
      "Epoch [1/1], Step [4509/8897], Loss: 5.3420\n",
      "Epoch [1/1], Step [4510/8897], Loss: 5.6322\n",
      "Epoch [1/1], Step [4511/8897], Loss: 5.3675\n",
      "Epoch [1/1], Step [4512/8897], Loss: 5.2172\n",
      "Epoch [1/1], Step [4513/8897], Loss: 5.4207\n",
      "Epoch [1/1], Step [4514/8897], Loss: 5.0452\n",
      "Epoch [1/1], Step [4515/8897], Loss: 5.2304\n",
      "Epoch [1/1], Step [4516/8897], Loss: 5.5219\n",
      "Epoch [1/1], Step [4517/8897], Loss: 5.2170\n",
      "Epoch [1/1], Step [4518/8897], Loss: 5.4496\n",
      "Epoch [1/1], Step [4519/8897], Loss: 5.3342\n",
      "Epoch [1/1], Step [4520/8897], Loss: 5.4021\n",
      "Epoch [1/1], Step [4521/8897], Loss: 5.4215\n",
      "Epoch [1/1], Step [4522/8897], Loss: 5.4595\n",
      "Epoch [1/1], Step [4523/8897], Loss: 5.5039\n",
      "Epoch [1/1], Step [4524/8897], Loss: 5.0108\n",
      "Epoch [1/1], Step [4525/8897], Loss: 5.3469\n",
      "Epoch [1/1], Step [4526/8897], Loss: 5.4625\n",
      "Epoch [1/1], Step [4527/8897], Loss: 5.6859\n",
      "Epoch [1/1], Step [4528/8897], Loss: 5.3844\n",
      "Epoch [1/1], Step [4529/8897], Loss: 5.5127\n",
      "Epoch [1/1], Step [4530/8897], Loss: 5.5219\n",
      "Epoch [1/1], Step [4531/8897], Loss: 5.3930\n",
      "Epoch [1/1], Step [4532/8897], Loss: 5.3812\n",
      "Epoch [1/1], Step [4533/8897], Loss: 5.4404\n",
      "Epoch [1/1], Step [4534/8897], Loss: 5.1858\n",
      "Epoch [1/1], Step [4535/8897], Loss: 5.4854\n",
      "Epoch [1/1], Step [4536/8897], Loss: 5.2325\n",
      "Epoch [1/1], Step [4537/8897], Loss: 5.3772\n",
      "Epoch [1/1], Step [4538/8897], Loss: 5.4304\n",
      "Epoch [1/1], Step [4539/8897], Loss: 5.2461\n",
      "Epoch [1/1], Step [4540/8897], Loss: 5.1770\n",
      "Epoch [1/1], Step [4541/8897], Loss: 5.1610\n",
      "Epoch [1/1], Step [4542/8897], Loss: 5.4626\n",
      "Epoch [1/1], Step [4543/8897], Loss: 5.6629\n",
      "Epoch [1/1], Step [4544/8897], Loss: 5.3170\n",
      "Epoch [1/1], Step [4545/8897], Loss: 5.2449\n",
      "Epoch [1/1], Step [4546/8897], Loss: 5.5821\n",
      "Epoch [1/1], Step [4547/8897], Loss: 5.5378\n",
      "Epoch [1/1], Step [4548/8897], Loss: 5.4426\n",
      "Epoch [1/1], Step [4549/8897], Loss: 5.6576\n",
      "Epoch [1/1], Step [4550/8897], Loss: 5.3299\n",
      "Epoch [1/1], Step [4551/8897], Loss: 5.6387\n",
      "Epoch [1/1], Step [4552/8897], Loss: 5.5963\n",
      "Epoch [1/1], Step [4553/8897], Loss: 5.3687\n",
      "Epoch [1/1], Step [4554/8897], Loss: 5.5424\n",
      "Epoch [1/1], Step [4555/8897], Loss: 5.3678\n",
      "Epoch [1/1], Step [4556/8897], Loss: 5.3738\n",
      "Epoch [1/1], Step [4557/8897], Loss: 5.4353\n",
      "Epoch [1/1], Step [4558/8897], Loss: 5.2823\n",
      "Epoch [1/1], Step [4559/8897], Loss: 5.4147\n",
      "Epoch [1/1], Step [4560/8897], Loss: 5.4483\n",
      "Epoch [1/1], Step [4561/8897], Loss: 5.2564\n",
      "Epoch [1/1], Step [4562/8897], Loss: 5.2716\n",
      "Epoch [1/1], Step [4563/8897], Loss: 5.3781\n",
      "Epoch [1/1], Step [4564/8897], Loss: 5.4703\n",
      "Epoch [1/1], Step [4565/8897], Loss: 5.6208\n",
      "Epoch [1/1], Step [4566/8897], Loss: 5.5230\n",
      "Epoch [1/1], Step [4567/8897], Loss: 5.3691\n",
      "Epoch [1/1], Step [4568/8897], Loss: 5.5594\n",
      "Epoch [1/1], Step [4569/8897], Loss: 5.5727\n",
      "Epoch [1/1], Step [4570/8897], Loss: 5.6183\n",
      "Epoch [1/1], Step [4571/8897], Loss: 5.4839\n",
      "Epoch [1/1], Step [4572/8897], Loss: 5.1984\n",
      "Epoch [1/1], Step [4573/8897], Loss: 5.3758\n",
      "Epoch [1/1], Step [4574/8897], Loss: 5.4044\n",
      "Epoch [1/1], Step [4575/8897], Loss: 5.4330\n",
      "Epoch [1/1], Step [4576/8897], Loss: 5.5076\n",
      "Epoch [1/1], Step [4577/8897], Loss: 5.2963\n",
      "Epoch [1/1], Step [4578/8897], Loss: 5.3220\n",
      "Epoch [1/1], Step [4579/8897], Loss: 5.6228\n",
      "Epoch [1/1], Step [4580/8897], Loss: 5.3901\n",
      "Epoch [1/1], Step [4581/8897], Loss: 5.1328\n",
      "Epoch [1/1], Step [4582/8897], Loss: 5.5417\n",
      "Epoch [1/1], Step [4583/8897], Loss: 5.4128\n",
      "Epoch [1/1], Step [4584/8897], Loss: 5.4599\n",
      "Epoch [1/1], Step [4585/8897], Loss: 5.4869\n",
      "Epoch [1/1], Step [4586/8897], Loss: 5.2106\n",
      "Epoch [1/1], Step [4587/8897], Loss: 5.3898\n",
      "Epoch [1/1], Step [4588/8897], Loss: 5.2575\n",
      "Epoch [1/1], Step [4589/8897], Loss: 5.5110\n",
      "Epoch [1/1], Step [4590/8897], Loss: 5.5416\n",
      "Epoch [1/1], Step [4591/8897], Loss: 5.3336\n",
      "Epoch [1/1], Step [4592/8897], Loss: 5.1554\n",
      "Epoch [1/1], Step [4593/8897], Loss: 5.5151\n",
      "Epoch [1/1], Step [4594/8897], Loss: 5.0054\n",
      "Epoch [1/1], Step [4595/8897], Loss: 5.3547\n",
      "Epoch [1/1], Step [4596/8897], Loss: 5.3926\n",
      "Epoch [1/1], Step [4597/8897], Loss: 5.1936\n",
      "Epoch [1/1], Step [4598/8897], Loss: 5.2773\n",
      "Epoch [1/1], Step [4599/8897], Loss: 5.2484\n",
      "Epoch [1/1], Step [4600/8897], Loss: 5.5559\n",
      "Epoch [1/1], Step [4601/8897], Loss: 5.6279\n",
      "Epoch [1/1], Step [4602/8897], Loss: 5.4243\n",
      "Epoch [1/1], Step [4603/8897], Loss: 5.3429\n",
      "Epoch [1/1], Step [4604/8897], Loss: 5.5882\n",
      "Epoch [1/1], Step [4605/8897], Loss: 5.6953\n",
      "Epoch [1/1], Step [4606/8897], Loss: 5.4598\n",
      "Epoch [1/1], Step [4607/8897], Loss: 5.3317\n",
      "Epoch [1/1], Step [4608/8897], Loss: 5.2543\n",
      "Epoch [1/1], Step [4609/8897], Loss: 5.3342\n",
      "Epoch [1/1], Step [4610/8897], Loss: 5.2181\n",
      "Epoch [1/1], Step [4611/8897], Loss: 5.3082\n",
      "Epoch [1/1], Step [4612/8897], Loss: 5.6928\n",
      "Epoch [1/1], Step [4613/8897], Loss: 5.4625\n",
      "Epoch [1/1], Step [4614/8897], Loss: 5.3879\n",
      "Epoch [1/1], Step [4615/8897], Loss: 5.2968\n",
      "Epoch [1/1], Step [4616/8897], Loss: 5.3907\n",
      "Epoch [1/1], Step [4617/8897], Loss: 5.4820\n",
      "Epoch [1/1], Step [4618/8897], Loss: 5.3052\n",
      "Epoch [1/1], Step [4619/8897], Loss: 5.4471\n",
      "Epoch [1/1], Step [4620/8897], Loss: 5.4391\n",
      "Epoch [1/1], Step [4621/8897], Loss: 5.4443\n",
      "Epoch [1/1], Step [4622/8897], Loss: 5.4568\n",
      "Epoch [1/1], Step [4623/8897], Loss: 5.4704\n",
      "Epoch [1/1], Step [4624/8897], Loss: 5.4063\n",
      "Epoch [1/1], Step [4625/8897], Loss: 5.7397\n",
      "Epoch [1/1], Step [4626/8897], Loss: 5.2814\n",
      "Epoch [1/1], Step [4627/8897], Loss: 5.3473\n",
      "Epoch [1/1], Step [4628/8897], Loss: 5.3708\n",
      "Epoch [1/1], Step [4629/8897], Loss: 5.7327\n",
      "Epoch [1/1], Step [4630/8897], Loss: 5.3771\n",
      "Epoch [1/1], Step [4631/8897], Loss: 5.4077\n",
      "Epoch [1/1], Step [4632/8897], Loss: 5.5338\n",
      "Epoch [1/1], Step [4633/8897], Loss: 5.5240\n",
      "Epoch [1/1], Step [4634/8897], Loss: 5.6808\n",
      "Epoch [1/1], Step [4635/8897], Loss: 5.4555\n",
      "Epoch [1/1], Step [4636/8897], Loss: 5.4630\n",
      "Epoch [1/1], Step [4637/8897], Loss: 5.5131\n",
      "Epoch [1/1], Step [4638/8897], Loss: 5.3746\n",
      "Epoch [1/1], Step [4639/8897], Loss: 5.2569\n",
      "Epoch [1/1], Step [4640/8897], Loss: 5.5268\n",
      "Epoch [1/1], Step [4641/8897], Loss: 5.2355\n",
      "Epoch [1/1], Step [4642/8897], Loss: 5.3134\n",
      "Epoch [1/1], Step [4643/8897], Loss: 5.3605\n",
      "Epoch [1/1], Step [4644/8897], Loss: 5.5600\n",
      "Epoch [1/1], Step [4645/8897], Loss: 5.4060\n",
      "Epoch [1/1], Step [4646/8897], Loss: 5.5804\n",
      "Epoch [1/1], Step [4647/8897], Loss: 5.3864\n",
      "Epoch [1/1], Step [4648/8897], Loss: 5.1297\n",
      "Epoch [1/1], Step [4649/8897], Loss: 5.4164\n",
      "Epoch [1/1], Step [4650/8897], Loss: 5.4126\n",
      "Epoch [1/1], Step [4651/8897], Loss: 5.4034\n",
      "Epoch [1/1], Step [4652/8897], Loss: 5.4102\n",
      "Epoch [1/1], Step [4653/8897], Loss: 5.3022\n",
      "Epoch [1/1], Step [4654/8897], Loss: 5.4589\n",
      "Epoch [1/1], Step [4655/8897], Loss: 5.3624\n",
      "Epoch [1/1], Step [4656/8897], Loss: 5.4434\n",
      "Epoch [1/1], Step [4657/8897], Loss: 5.4660\n",
      "Epoch [1/1], Step [4658/8897], Loss: 5.2334\n",
      "Epoch [1/1], Step [4659/8897], Loss: 5.3771\n",
      "Epoch [1/1], Step [4660/8897], Loss: 5.2861\n",
      "Epoch [1/1], Step [4661/8897], Loss: 5.1649\n",
      "Epoch [1/1], Step [4662/8897], Loss: 5.2557\n",
      "Epoch [1/1], Step [4663/8897], Loss: 5.0986\n",
      "Epoch [1/1], Step [4664/8897], Loss: 5.0526\n",
      "Epoch [1/1], Step [4665/8897], Loss: 5.3582\n",
      "Epoch [1/1], Step [4666/8897], Loss: 5.4667\n",
      "Epoch [1/1], Step [4667/8897], Loss: 5.4061\n",
      "Epoch [1/1], Step [4668/8897], Loss: 5.3435\n",
      "Epoch [1/1], Step [4669/8897], Loss: 5.2684\n",
      "Epoch [1/1], Step [4670/8897], Loss: 5.2841\n",
      "Epoch [1/1], Step [4671/8897], Loss: 5.4074\n",
      "Epoch [1/1], Step [4672/8897], Loss: 5.5667\n",
      "Epoch [1/1], Step [4673/8897], Loss: 5.4650\n",
      "Epoch [1/1], Step [4674/8897], Loss: 5.0398\n",
      "Epoch [1/1], Step [4675/8897], Loss: 5.5694\n",
      "Epoch [1/1], Step [4676/8897], Loss: 5.2893\n",
      "Epoch [1/1], Step [4677/8897], Loss: 5.0447\n",
      "Epoch [1/1], Step [4678/8897], Loss: 5.3861\n",
      "Epoch [1/1], Step [4679/8897], Loss: 5.2149\n",
      "Epoch [1/1], Step [4680/8897], Loss: 5.5149\n",
      "Epoch [1/1], Step [4681/8897], Loss: 5.4127\n",
      "Epoch [1/1], Step [4682/8897], Loss: 5.2227\n",
      "Epoch [1/1], Step [4683/8897], Loss: 5.4272\n",
      "Epoch [1/1], Step [4684/8897], Loss: 5.4957\n",
      "Epoch [1/1], Step [4685/8897], Loss: 5.2489\n",
      "Epoch [1/1], Step [4686/8897], Loss: 5.2692\n",
      "Epoch [1/1], Step [4687/8897], Loss: 5.3143\n",
      "Epoch [1/1], Step [4688/8897], Loss: 5.2526\n",
      "Epoch [1/1], Step [4689/8897], Loss: 5.2419\n",
      "Epoch [1/1], Step [4690/8897], Loss: 5.2906\n",
      "Epoch [1/1], Step [4691/8897], Loss: 5.2717\n",
      "Epoch [1/1], Step [4692/8897], Loss: 5.4392\n",
      "Epoch [1/1], Step [4693/8897], Loss: 5.2766\n",
      "Epoch [1/1], Step [4694/8897], Loss: 5.3263\n",
      "Epoch [1/1], Step [4695/8897], Loss: 5.5403\n",
      "Epoch [1/1], Step [4696/8897], Loss: 5.3002\n",
      "Epoch [1/1], Step [4697/8897], Loss: 5.2345\n",
      "Epoch [1/1], Step [4698/8897], Loss: 5.4848\n",
      "Epoch [1/1], Step [4699/8897], Loss: 5.2754\n",
      "Epoch [1/1], Step [4700/8897], Loss: 5.3284\n",
      "Epoch [1/1], Step [4701/8897], Loss: 5.6106\n",
      "Epoch [1/1], Step [4702/8897], Loss: 5.5435\n",
      "Epoch [1/1], Step [4703/8897], Loss: 5.6632\n",
      "Epoch [1/1], Step [4704/8897], Loss: 5.4543\n",
      "Epoch [1/1], Step [4705/8897], Loss: 5.5139\n",
      "Epoch [1/1], Step [4706/8897], Loss: 5.2691\n",
      "Epoch [1/1], Step [4707/8897], Loss: 5.5659\n",
      "Epoch [1/1], Step [4708/8897], Loss: 5.2082\n",
      "Epoch [1/1], Step [4709/8897], Loss: 5.3857\n",
      "Epoch [1/1], Step [4710/8897], Loss: 5.5763\n",
      "Epoch [1/1], Step [4711/8897], Loss: 5.3480\n",
      "Epoch [1/1], Step [4712/8897], Loss: 5.0707\n",
      "Epoch [1/1], Step [4713/8897], Loss: 5.3499\n",
      "Epoch [1/1], Step [4714/8897], Loss: 5.4721\n",
      "Epoch [1/1], Step [4715/8897], Loss: 5.1732\n",
      "Epoch [1/1], Step [4716/8897], Loss: 5.4583\n",
      "Epoch [1/1], Step [4717/8897], Loss: 5.5195\n",
      "Epoch [1/1], Step [4718/8897], Loss: 5.3091\n",
      "Epoch [1/1], Step [4719/8897], Loss: 5.0948\n",
      "Epoch [1/1], Step [4720/8897], Loss: 5.5203\n",
      "Epoch [1/1], Step [4721/8897], Loss: 5.4773\n",
      "Epoch [1/1], Step [4722/8897], Loss: 5.1001\n",
      "Epoch [1/1], Step [4723/8897], Loss: 5.3470\n",
      "Epoch [1/1], Step [4724/8897], Loss: 5.6090\n",
      "Epoch [1/1], Step [4725/8897], Loss: 5.2777\n",
      "Epoch [1/1], Step [4726/8897], Loss: 5.3564\n",
      "Epoch [1/1], Step [4727/8897], Loss: 5.6021\n",
      "Epoch [1/1], Step [4728/8897], Loss: 5.3176\n",
      "Epoch [1/1], Step [4729/8897], Loss: 5.5185\n",
      "Epoch [1/1], Step [4730/8897], Loss: 5.3585\n",
      "Epoch [1/1], Step [4731/8897], Loss: 5.2110\n",
      "Epoch [1/1], Step [4732/8897], Loss: 5.2517\n",
      "Epoch [1/1], Step [4733/8897], Loss: 5.5632\n",
      "Epoch [1/1], Step [4734/8897], Loss: 5.3957\n",
      "Epoch [1/1], Step [4735/8897], Loss: 5.5023\n",
      "Epoch [1/1], Step [4736/8897], Loss: 4.9840\n",
      "Epoch [1/1], Step [4737/8897], Loss: 5.4438\n",
      "Epoch [1/1], Step [4738/8897], Loss: 5.3281\n",
      "Epoch [1/1], Step [4739/8897], Loss: 5.5957\n",
      "Epoch [1/1], Step [4740/8897], Loss: 5.5601\n",
      "Epoch [1/1], Step [4741/8897], Loss: 5.6392\n",
      "Epoch [1/1], Step [4742/8897], Loss: 5.3311\n",
      "Epoch [1/1], Step [4743/8897], Loss: 5.4930\n",
      "Epoch [1/1], Step [4744/8897], Loss: 5.4956\n",
      "Epoch [1/1], Step [4745/8897], Loss: 5.3525\n",
      "Epoch [1/1], Step [4746/8897], Loss: 5.4972\n",
      "Epoch [1/1], Step [4747/8897], Loss: 5.2743\n",
      "Epoch [1/1], Step [4748/8897], Loss: 5.6850\n",
      "Epoch [1/1], Step [4749/8897], Loss: 5.6195\n",
      "Epoch [1/1], Step [4750/8897], Loss: 5.5257\n",
      "Epoch [1/1], Step [4751/8897], Loss: 5.7081\n",
      "Epoch [1/1], Step [4752/8897], Loss: 5.3422\n",
      "Epoch [1/1], Step [4753/8897], Loss: 5.1885\n",
      "Epoch [1/1], Step [4754/8897], Loss: 5.4348\n",
      "Epoch [1/1], Step [4755/8897], Loss: 5.3469\n",
      "Epoch [1/1], Step [4756/8897], Loss: 5.3865\n",
      "Epoch [1/1], Step [4757/8897], Loss: 5.4870\n",
      "Epoch [1/1], Step [4758/8897], Loss: 5.2378\n",
      "Epoch [1/1], Step [4759/8897], Loss: 5.3089\n",
      "Epoch [1/1], Step [4760/8897], Loss: 4.9583\n",
      "Epoch [1/1], Step [4761/8897], Loss: 5.3194\n",
      "Epoch [1/1], Step [4762/8897], Loss: 5.6172\n",
      "Epoch [1/1], Step [4763/8897], Loss: 5.2774\n",
      "Epoch [1/1], Step [4764/8897], Loss: 5.3712\n",
      "Epoch [1/1], Step [4765/8897], Loss: 5.5224\n",
      "Epoch [1/1], Step [4766/8897], Loss: 5.2652\n",
      "Epoch [1/1], Step [4767/8897], Loss: 5.0905\n",
      "Epoch [1/1], Step [4768/8897], Loss: 5.3354\n",
      "Epoch [1/1], Step [4769/8897], Loss: 5.4073\n",
      "Epoch [1/1], Step [4770/8897], Loss: 5.3768\n",
      "Epoch [1/1], Step [4771/8897], Loss: 5.2955\n",
      "Epoch [1/1], Step [4772/8897], Loss: 5.6237\n",
      "Epoch [1/1], Step [4773/8897], Loss: 5.4362\n",
      "Epoch [1/1], Step [4774/8897], Loss: 5.3125\n",
      "Epoch [1/1], Step [4775/8897], Loss: 5.7862\n",
      "Epoch [1/1], Step [4776/8897], Loss: 5.3396\n",
      "Epoch [1/1], Step [4777/8897], Loss: 5.4340\n",
      "Epoch [1/1], Step [4778/8897], Loss: 5.5727\n",
      "Epoch [1/1], Step [4779/8897], Loss: 5.2842\n",
      "Epoch [1/1], Step [4780/8897], Loss: 5.2569\n",
      "Epoch [1/1], Step [4781/8897], Loss: 5.3462\n",
      "Epoch [1/1], Step [4782/8897], Loss: 5.4425\n",
      "Epoch [1/1], Step [4783/8897], Loss: 5.5036\n",
      "Epoch [1/1], Step [4784/8897], Loss: 5.3419\n",
      "Epoch [1/1], Step [4785/8897], Loss: 5.0757\n",
      "Epoch [1/1], Step [4786/8897], Loss: 5.4433\n",
      "Epoch [1/1], Step [4787/8897], Loss: 5.3737\n",
      "Epoch [1/1], Step [4788/8897], Loss: 5.3086\n",
      "Epoch [1/1], Step [4789/8897], Loss: 5.4500\n",
      "Epoch [1/1], Step [4790/8897], Loss: 5.3062\n",
      "Epoch [1/1], Step [4791/8897], Loss: 5.7386\n",
      "Epoch [1/1], Step [4792/8897], Loss: 5.5687\n",
      "Epoch [1/1], Step [4793/8897], Loss: 5.3920\n",
      "Epoch [1/1], Step [4794/8897], Loss: 5.4475\n",
      "Epoch [1/1], Step [4795/8897], Loss: 5.3569\n",
      "Epoch [1/1], Step [4796/8897], Loss: 5.2830\n",
      "Epoch [1/1], Step [4797/8897], Loss: 5.2570\n",
      "Epoch [1/1], Step [4798/8897], Loss: 5.3080\n",
      "Epoch [1/1], Step [4799/8897], Loss: 5.3696\n",
      "Epoch [1/1], Step [4800/8897], Loss: 5.3020\n",
      "Epoch [1/1], Step [4801/8897], Loss: 5.3856\n",
      "Epoch [1/1], Step [4802/8897], Loss: 5.2663\n",
      "Epoch [1/1], Step [4803/8897], Loss: 5.4964\n",
      "Epoch [1/1], Step [4804/8897], Loss: 5.4347\n",
      "Epoch [1/1], Step [4805/8897], Loss: 5.6247\n",
      "Epoch [1/1], Step [4806/8897], Loss: 5.4446\n",
      "Epoch [1/1], Step [4807/8897], Loss: 5.3413\n",
      "Epoch [1/1], Step [4808/8897], Loss: 5.1340\n",
      "Epoch [1/1], Step [4809/8897], Loss: 5.5755\n",
      "Epoch [1/1], Step [4810/8897], Loss: 5.4923\n",
      "Epoch [1/1], Step [4811/8897], Loss: 5.5093\n",
      "Epoch [1/1], Step [4812/8897], Loss: 5.5551\n",
      "Epoch [1/1], Step [4813/8897], Loss: 5.4101\n",
      "Epoch [1/1], Step [4814/8897], Loss: 5.5010\n",
      "Epoch [1/1], Step [4815/8897], Loss: 5.4445\n",
      "Epoch [1/1], Step [4816/8897], Loss: 5.2041\n",
      "Epoch [1/1], Step [4817/8897], Loss: 5.3967\n",
      "Epoch [1/1], Step [4818/8897], Loss: 5.2290\n",
      "Epoch [1/1], Step [4819/8897], Loss: 5.3342\n",
      "Epoch [1/1], Step [4820/8897], Loss: 5.5100\n",
      "Epoch [1/1], Step [4821/8897], Loss: 5.2715\n",
      "Epoch [1/1], Step [4822/8897], Loss: 5.6685\n",
      "Epoch [1/1], Step [4823/8897], Loss: 5.5056\n",
      "Epoch [1/1], Step [4824/8897], Loss: 5.2303\n",
      "Epoch [1/1], Step [4825/8897], Loss: 5.1918\n",
      "Epoch [1/1], Step [4826/8897], Loss: 5.4126\n",
      "Epoch [1/1], Step [4827/8897], Loss: 5.3351\n",
      "Epoch [1/1], Step [4828/8897], Loss: 5.4267\n",
      "Epoch [1/1], Step [4829/8897], Loss: 5.4816\n",
      "Epoch [1/1], Step [4830/8897], Loss: 5.4162\n",
      "Epoch [1/1], Step [4831/8897], Loss: 5.1749\n",
      "Epoch [1/1], Step [4832/8897], Loss: 5.4474\n",
      "Epoch [1/1], Step [4833/8897], Loss: 5.3492\n",
      "Epoch [1/1], Step [4834/8897], Loss: 5.4167\n",
      "Epoch [1/1], Step [4835/8897], Loss: 5.4908\n",
      "Epoch [1/1], Step [4836/8897], Loss: 5.3011\n",
      "Epoch [1/1], Step [4837/8897], Loss: 5.5844\n",
      "Epoch [1/1], Step [4838/8897], Loss: 5.5882\n",
      "Epoch [1/1], Step [4839/8897], Loss: 5.2521\n",
      "Epoch [1/1], Step [4840/8897], Loss: 5.6622\n",
      "Epoch [1/1], Step [4841/8897], Loss: 5.7964\n",
      "Epoch [1/1], Step [4842/8897], Loss: 5.5647\n",
      "Epoch [1/1], Step [4843/8897], Loss: 5.4436\n",
      "Epoch [1/1], Step [4844/8897], Loss: 5.6094\n",
      "Epoch [1/1], Step [4845/8897], Loss: 5.0754\n",
      "Epoch [1/1], Step [4846/8897], Loss: 5.3060\n",
      "Epoch [1/1], Step [4847/8897], Loss: 5.4461\n",
      "Epoch [1/1], Step [4848/8897], Loss: 5.1651\n",
      "Epoch [1/1], Step [4849/8897], Loss: 5.4899\n",
      "Epoch [1/1], Step [4850/8897], Loss: 5.4599\n",
      "Epoch [1/1], Step [4851/8897], Loss: 5.4028\n",
      "Epoch [1/1], Step [4852/8897], Loss: 5.4558\n",
      "Epoch [1/1], Step [4853/8897], Loss: 5.2949\n",
      "Epoch [1/1], Step [4854/8897], Loss: 5.2997\n",
      "Epoch [1/1], Step [4855/8897], Loss: 5.4459\n",
      "Epoch [1/1], Step [4856/8897], Loss: 5.2546\n",
      "Epoch [1/1], Step [4857/8897], Loss: 5.3919\n",
      "Epoch [1/1], Step [4858/8897], Loss: 5.2744\n",
      "Epoch [1/1], Step [4859/8897], Loss: 5.4381\n",
      "Epoch [1/1], Step [4860/8897], Loss: 5.5563\n",
      "Epoch [1/1], Step [4861/8897], Loss: 5.5847\n",
      "Epoch [1/1], Step [4862/8897], Loss: 5.3688\n",
      "Epoch [1/1], Step [4863/8897], Loss: 5.3889\n",
      "Epoch [1/1], Step [4864/8897], Loss: 5.5835\n",
      "Epoch [1/1], Step [4865/8897], Loss: 5.1003\n",
      "Epoch [1/1], Step [4866/8897], Loss: 5.5019\n",
      "Epoch [1/1], Step [4867/8897], Loss: 5.2648\n",
      "Epoch [1/1], Step [4868/8897], Loss: 5.4722\n",
      "Epoch [1/1], Step [4869/8897], Loss: 5.5147\n",
      "Epoch [1/1], Step [4870/8897], Loss: 5.1477\n",
      "Epoch [1/1], Step [4871/8897], Loss: 5.3146\n",
      "Epoch [1/1], Step [4872/8897], Loss: 5.4626\n",
      "Epoch [1/1], Step [4873/8897], Loss: 5.5160\n",
      "Epoch [1/1], Step [4874/8897], Loss: 5.3315\n",
      "Epoch [1/1], Step [4875/8897], Loss: 5.3468\n",
      "Epoch [1/1], Step [4876/8897], Loss: 5.3172\n",
      "Epoch [1/1], Step [4877/8897], Loss: 5.4246\n",
      "Epoch [1/1], Step [4878/8897], Loss: 5.1989\n",
      "Epoch [1/1], Step [4879/8897], Loss: 5.3154\n",
      "Epoch [1/1], Step [4880/8897], Loss: 5.4089\n",
      "Epoch [1/1], Step [4881/8897], Loss: 5.3173\n",
      "Epoch [1/1], Step [4882/8897], Loss: 5.3926\n",
      "Epoch [1/1], Step [4883/8897], Loss: 5.4470\n",
      "Epoch [1/1], Step [4884/8897], Loss: 5.3188\n",
      "Epoch [1/1], Step [4885/8897], Loss: 5.3590\n",
      "Epoch [1/1], Step [4886/8897], Loss: 5.2951\n",
      "Epoch [1/1], Step [4887/8897], Loss: 5.1894\n",
      "Epoch [1/1], Step [4888/8897], Loss: 5.5298\n",
      "Epoch [1/1], Step [4889/8897], Loss: 5.1498\n",
      "Epoch [1/1], Step [4890/8897], Loss: 5.5932\n",
      "Epoch [1/1], Step [4891/8897], Loss: 5.2886\n",
      "Epoch [1/1], Step [4892/8897], Loss: 5.6065\n",
      "Epoch [1/1], Step [4893/8897], Loss: 5.4904\n",
      "Epoch [1/1], Step [4894/8897], Loss: 5.3285\n",
      "Epoch [1/1], Step [4895/8897], Loss: 5.4136\n",
      "Epoch [1/1], Step [4896/8897], Loss: 5.3657\n",
      "Epoch [1/1], Step [4897/8897], Loss: 5.4196\n",
      "Epoch [1/1], Step [4898/8897], Loss: 5.1493\n",
      "Epoch [1/1], Step [4899/8897], Loss: 5.5641\n",
      "Epoch [1/1], Step [4900/8897], Loss: 5.4292\n",
      "Epoch [1/1], Step [4901/8897], Loss: 5.2996\n",
      "Epoch [1/1], Step [4902/8897], Loss: 5.3154\n",
      "Epoch [1/1], Step [4903/8897], Loss: 5.2492\n",
      "Epoch [1/1], Step [4904/8897], Loss: 5.0244\n",
      "Epoch [1/1], Step [4905/8897], Loss: 5.3350\n",
      "Epoch [1/1], Step [4906/8897], Loss: 5.5884\n",
      "Epoch [1/1], Step [4907/8897], Loss: 5.3037\n",
      "Epoch [1/1], Step [4908/8897], Loss: 5.3218\n",
      "Epoch [1/1], Step [4909/8897], Loss: 5.3844\n",
      "Epoch [1/1], Step [4910/8897], Loss: 5.2313\n",
      "Epoch [1/1], Step [4911/8897], Loss: 5.3905\n",
      "Epoch [1/1], Step [4912/8897], Loss: 5.1955\n",
      "Epoch [1/1], Step [4913/8897], Loss: 5.4780\n",
      "Epoch [1/1], Step [4914/8897], Loss: 5.1706\n",
      "Epoch [1/1], Step [4915/8897], Loss: 5.3908\n",
      "Epoch [1/1], Step [4916/8897], Loss: 5.5512\n",
      "Epoch [1/1], Step [4917/8897], Loss: 5.3985\n",
      "Epoch [1/1], Step [4918/8897], Loss: 5.2004\n",
      "Epoch [1/1], Step [4919/8897], Loss: 5.0418\n",
      "Epoch [1/1], Step [4920/8897], Loss: 5.2021\n",
      "Epoch [1/1], Step [4921/8897], Loss: 5.4011\n",
      "Epoch [1/1], Step [4922/8897], Loss: 5.3833\n",
      "Epoch [1/1], Step [4923/8897], Loss: 5.5509\n",
      "Epoch [1/1], Step [4924/8897], Loss: 5.3768\n",
      "Epoch [1/1], Step [4925/8897], Loss: 5.4398\n",
      "Epoch [1/1], Step [4926/8897], Loss: 5.5677\n",
      "Epoch [1/1], Step [4927/8897], Loss: 5.2002\n",
      "Epoch [1/1], Step [4928/8897], Loss: 5.3315\n",
      "Epoch [1/1], Step [4929/8897], Loss: 5.3575\n",
      "Epoch [1/1], Step [4930/8897], Loss: 5.2492\n",
      "Epoch [1/1], Step [4931/8897], Loss: 5.4822\n",
      "Epoch [1/1], Step [4932/8897], Loss: 5.3244\n",
      "Epoch [1/1], Step [4933/8897], Loss: 5.4822\n",
      "Epoch [1/1], Step [4934/8897], Loss: 5.5402\n",
      "Epoch [1/1], Step [4935/8897], Loss: 5.4820\n",
      "Epoch [1/1], Step [4936/8897], Loss: 5.5384\n",
      "Epoch [1/1], Step [4937/8897], Loss: 5.4007\n",
      "Epoch [1/1], Step [4938/8897], Loss: 5.4899\n",
      "Epoch [1/1], Step [4939/8897], Loss: 5.4581\n",
      "Epoch [1/1], Step [4940/8897], Loss: 5.2967\n",
      "Epoch [1/1], Step [4941/8897], Loss: 5.5479\n",
      "Epoch [1/1], Step [4942/8897], Loss: 5.3989\n",
      "Epoch [1/1], Step [4943/8897], Loss: 5.3125\n",
      "Epoch [1/1], Step [4944/8897], Loss: 5.5134\n",
      "Epoch [1/1], Step [4945/8897], Loss: 5.3695\n",
      "Epoch [1/1], Step [4946/8897], Loss: 5.6249\n",
      "Epoch [1/1], Step [4947/8897], Loss: 5.3365\n",
      "Epoch [1/1], Step [4948/8897], Loss: 5.3646\n",
      "Epoch [1/1], Step [4949/8897], Loss: 5.3496\n",
      "Epoch [1/1], Step [4950/8897], Loss: 5.1005\n",
      "Epoch [1/1], Step [4951/8897], Loss: 5.6679\n",
      "Epoch [1/1], Step [4952/8897], Loss: 5.3662\n",
      "Epoch [1/1], Step [4953/8897], Loss: 5.3565\n",
      "Epoch [1/1], Step [4954/8897], Loss: 5.2342\n",
      "Epoch [1/1], Step [4955/8897], Loss: 5.4002\n",
      "Epoch [1/1], Step [4956/8897], Loss: 5.4217\n",
      "Epoch [1/1], Step [4957/8897], Loss: 5.2765\n",
      "Epoch [1/1], Step [4958/8897], Loss: 5.2507\n",
      "Epoch [1/1], Step [4959/8897], Loss: 5.2264\n",
      "Epoch [1/1], Step [4960/8897], Loss: 5.4652\n",
      "Epoch [1/1], Step [4961/8897], Loss: 5.1774\n",
      "Epoch [1/1], Step [4962/8897], Loss: 5.3630\n",
      "Epoch [1/1], Step [4963/8897], Loss: 5.3285\n",
      "Epoch [1/1], Step [4964/8897], Loss: 5.2773\n",
      "Epoch [1/1], Step [4965/8897], Loss: 5.3482\n",
      "Epoch [1/1], Step [4966/8897], Loss: 5.5803\n",
      "Epoch [1/1], Step [4967/8897], Loss: 5.1282\n",
      "Epoch [1/1], Step [4968/8897], Loss: 5.0569\n",
      "Epoch [1/1], Step [4969/8897], Loss: 5.6084\n",
      "Epoch [1/1], Step [4970/8897], Loss: 5.5287\n",
      "Epoch [1/1], Step [4971/8897], Loss: 5.4544\n",
      "Epoch [1/1], Step [4972/8897], Loss: 5.5056\n",
      "Epoch [1/1], Step [4973/8897], Loss: 5.5859\n",
      "Epoch [1/1], Step [4974/8897], Loss: 5.4492\n",
      "Epoch [1/1], Step [4975/8897], Loss: 5.2590\n",
      "Epoch [1/1], Step [4976/8897], Loss: 5.4477\n",
      "Epoch [1/1], Step [4977/8897], Loss: 5.5422\n",
      "Epoch [1/1], Step [4978/8897], Loss: 5.4436\n",
      "Epoch [1/1], Step [4979/8897], Loss: 5.3640\n",
      "Epoch [1/1], Step [4980/8897], Loss: 5.3557\n",
      "Epoch [1/1], Step [4981/8897], Loss: 5.3756\n",
      "Epoch [1/1], Step [4982/8897], Loss: 5.2283\n",
      "Epoch [1/1], Step [4983/8897], Loss: 5.2364\n",
      "Epoch [1/1], Step [4984/8897], Loss: 5.0996\n",
      "Epoch [1/1], Step [4985/8897], Loss: 5.3917\n",
      "Epoch [1/1], Step [4986/8897], Loss: 5.4328\n",
      "Epoch [1/1], Step [4987/8897], Loss: 5.3285\n",
      "Epoch [1/1], Step [4988/8897], Loss: 5.3179\n",
      "Epoch [1/1], Step [4989/8897], Loss: 5.4028\n",
      "Epoch [1/1], Step [4990/8897], Loss: 5.5278\n",
      "Epoch [1/1], Step [4991/8897], Loss: 5.2726\n",
      "Epoch [1/1], Step [4992/8897], Loss: 5.5842\n",
      "Epoch [1/1], Step [4993/8897], Loss: 5.4192\n",
      "Epoch [1/1], Step [4994/8897], Loss: 5.1451\n",
      "Epoch [1/1], Step [4995/8897], Loss: 5.5859\n",
      "Epoch [1/1], Step [4996/8897], Loss: 5.3252\n",
      "Epoch [1/1], Step [4997/8897], Loss: 5.7377\n",
      "Epoch [1/1], Step [4998/8897], Loss: 5.1700\n",
      "Epoch [1/1], Step [4999/8897], Loss: 5.6973\n",
      "Epoch [1/1], Step [5000/8897], Loss: 5.2347\n",
      "Epoch [1/1], Step [5001/8897], Loss: 5.4792\n",
      "Epoch [1/1], Step [5002/8897], Loss: 5.3292\n",
      "Epoch [1/1], Step [5003/8897], Loss: 5.2968\n",
      "Epoch [1/1], Step [5004/8897], Loss: 5.2473\n",
      "Epoch [1/1], Step [5005/8897], Loss: 5.4744\n",
      "Epoch [1/1], Step [5006/8897], Loss: 5.2920\n",
      "Epoch [1/1], Step [5007/8897], Loss: 5.3626\n",
      "Epoch [1/1], Step [5008/8897], Loss: 5.6980\n",
      "Epoch [1/1], Step [5009/8897], Loss: 5.4728\n",
      "Epoch [1/1], Step [5010/8897], Loss: 5.3945\n",
      "Epoch [1/1], Step [5011/8897], Loss: 5.3327\n",
      "Epoch [1/1], Step [5012/8897], Loss: 5.3410\n",
      "Epoch [1/1], Step [5013/8897], Loss: 5.2813\n",
      "Epoch [1/1], Step [5014/8897], Loss: 5.1573\n",
      "Epoch [1/1], Step [5015/8897], Loss: 5.3576\n",
      "Epoch [1/1], Step [5016/8897], Loss: 5.2305\n",
      "Epoch [1/1], Step [5017/8897], Loss: 5.3893\n",
      "Epoch [1/1], Step [5018/8897], Loss: 5.4035\n",
      "Epoch [1/1], Step [5019/8897], Loss: 5.3105\n",
      "Epoch [1/1], Step [5020/8897], Loss: 5.6516\n",
      "Epoch [1/1], Step [5021/8897], Loss: 5.3413\n",
      "Epoch [1/1], Step [5022/8897], Loss: 5.2179\n",
      "Epoch [1/1], Step [5023/8897], Loss: 5.5102\n",
      "Epoch [1/1], Step [5024/8897], Loss: 5.1266\n",
      "Epoch [1/1], Step [5025/8897], Loss: 5.2926\n",
      "Epoch [1/1], Step [5026/8897], Loss: 5.5904\n",
      "Epoch [1/1], Step [5027/8897], Loss: 5.2336\n",
      "Epoch [1/1], Step [5028/8897], Loss: 5.2790\n",
      "Epoch [1/1], Step [5029/8897], Loss: 5.5351\n",
      "Epoch [1/1], Step [5030/8897], Loss: 5.4505\n",
      "Epoch [1/1], Step [5031/8897], Loss: 5.2481\n",
      "Epoch [1/1], Step [5032/8897], Loss: 5.4136\n",
      "Epoch [1/1], Step [5033/8897], Loss: 5.2719\n",
      "Epoch [1/1], Step [5034/8897], Loss: 5.4191\n",
      "Epoch [1/1], Step [5035/8897], Loss: 5.6338\n",
      "Epoch [1/1], Step [5036/8897], Loss: 5.3779\n",
      "Epoch [1/1], Step [5037/8897], Loss: 5.3477\n",
      "Epoch [1/1], Step [5038/8897], Loss: 5.7981\n",
      "Epoch [1/1], Step [5039/8897], Loss: 5.1776\n",
      "Epoch [1/1], Step [5040/8897], Loss: 5.3067\n",
      "Epoch [1/1], Step [5041/8897], Loss: 5.3190\n",
      "Epoch [1/1], Step [5042/8897], Loss: 5.3331\n",
      "Epoch [1/1], Step [5043/8897], Loss: 5.3346\n",
      "Epoch [1/1], Step [5044/8897], Loss: 5.1965\n",
      "Epoch [1/1], Step [5045/8897], Loss: 5.3270\n",
      "Epoch [1/1], Step [5046/8897], Loss: 5.2402\n",
      "Epoch [1/1], Step [5047/8897], Loss: 5.6748\n",
      "Epoch [1/1], Step [5048/8897], Loss: 5.3315\n",
      "Epoch [1/1], Step [5049/8897], Loss: 5.5185\n",
      "Epoch [1/1], Step [5050/8897], Loss: 5.2264\n",
      "Epoch [1/1], Step [5051/8897], Loss: 5.2823\n",
      "Epoch [1/1], Step [5052/8897], Loss: 5.1969\n",
      "Epoch [1/1], Step [5053/8897], Loss: 5.5764\n",
      "Epoch [1/1], Step [5054/8897], Loss: 5.4215\n",
      "Epoch [1/1], Step [5055/8897], Loss: 5.3792\n",
      "Epoch [1/1], Step [5056/8897], Loss: 5.4436\n",
      "Epoch [1/1], Step [5057/8897], Loss: 5.1601\n",
      "Epoch [1/1], Step [5058/8897], Loss: 5.4471\n",
      "Epoch [1/1], Step [5059/8897], Loss: 5.4500\n",
      "Epoch [1/1], Step [5060/8897], Loss: 5.6303\n",
      "Epoch [1/1], Step [5061/8897], Loss: 5.3203\n",
      "Epoch [1/1], Step [5062/8897], Loss: 5.3747\n",
      "Epoch [1/1], Step [5063/8897], Loss: 5.5956\n",
      "Epoch [1/1], Step [5064/8897], Loss: 5.4103\n",
      "Epoch [1/1], Step [5065/8897], Loss: 4.9109\n",
      "Epoch [1/1], Step [5066/8897], Loss: 5.3237\n",
      "Epoch [1/1], Step [5067/8897], Loss: 5.3584\n",
      "Epoch [1/1], Step [5068/8897], Loss: 5.2044\n",
      "Epoch [1/1], Step [5069/8897], Loss: 5.4263\n",
      "Epoch [1/1], Step [5070/8897], Loss: 5.2924\n",
      "Epoch [1/1], Step [5071/8897], Loss: 5.4206\n",
      "Epoch [1/1], Step [5072/8897], Loss: 5.3927\n",
      "Epoch [1/1], Step [5073/8897], Loss: 4.9324\n",
      "Epoch [1/1], Step [5074/8897], Loss: 5.3711\n",
      "Epoch [1/1], Step [5075/8897], Loss: 5.3711\n",
      "Epoch [1/1], Step [5076/8897], Loss: 5.3766\n",
      "Epoch [1/1], Step [5077/8897], Loss: 5.3455\n",
      "Epoch [1/1], Step [5078/8897], Loss: 5.5002\n",
      "Epoch [1/1], Step [5079/8897], Loss: 5.1637\n",
      "Epoch [1/1], Step [5080/8897], Loss: 5.4382\n",
      "Epoch [1/1], Step [5081/8897], Loss: 5.4620\n",
      "Epoch [1/1], Step [5082/8897], Loss: 5.4804\n",
      "Epoch [1/1], Step [5083/8897], Loss: 5.3019\n",
      "Epoch [1/1], Step [5084/8897], Loss: 5.4520\n",
      "Epoch [1/1], Step [5085/8897], Loss: 5.6126\n",
      "Epoch [1/1], Step [5086/8897], Loss: 5.3678\n",
      "Epoch [1/1], Step [5087/8897], Loss: 5.3786\n",
      "Epoch [1/1], Step [5088/8897], Loss: 5.5665\n",
      "Epoch [1/1], Step [5089/8897], Loss: 5.4951\n",
      "Epoch [1/1], Step [5090/8897], Loss: 5.6810\n",
      "Epoch [1/1], Step [5091/8897], Loss: 5.3513\n",
      "Epoch [1/1], Step [5092/8897], Loss: 5.2604\n",
      "Epoch [1/1], Step [5093/8897], Loss: 5.3096\n",
      "Epoch [1/1], Step [5094/8897], Loss: 5.3432\n",
      "Epoch [1/1], Step [5095/8897], Loss: 5.3094\n",
      "Epoch [1/1], Step [5096/8897], Loss: 5.5290\n",
      "Epoch [1/1], Step [5097/8897], Loss: 5.5042\n",
      "Epoch [1/1], Step [5098/8897], Loss: 5.4854\n",
      "Epoch [1/1], Step [5099/8897], Loss: 5.8207\n",
      "Epoch [1/1], Step [5100/8897], Loss: 5.4962\n",
      "Epoch [1/1], Step [5101/8897], Loss: 5.4639\n",
      "Epoch [1/1], Step [5102/8897], Loss: 5.2012\n",
      "Epoch [1/1], Step [5103/8897], Loss: 4.8643\n",
      "Epoch [1/1], Step [5104/8897], Loss: 5.4687\n",
      "Epoch [1/1], Step [5105/8897], Loss: 5.3477\n",
      "Epoch [1/1], Step [5106/8897], Loss: 5.0903\n",
      "Epoch [1/1], Step [5107/8897], Loss: 5.4321\n",
      "Epoch [1/1], Step [5108/8897], Loss: 5.4503\n",
      "Epoch [1/1], Step [5109/8897], Loss: 5.5504\n",
      "Epoch [1/1], Step [5110/8897], Loss: 5.1617\n",
      "Epoch [1/1], Step [5111/8897], Loss: 5.2460\n",
      "Epoch [1/1], Step [5112/8897], Loss: 5.2778\n",
      "Epoch [1/1], Step [5113/8897], Loss: 5.5609\n",
      "Epoch [1/1], Step [5114/8897], Loss: 5.4687\n",
      "Epoch [1/1], Step [5115/8897], Loss: 5.4729\n",
      "Epoch [1/1], Step [5116/8897], Loss: 5.2142\n",
      "Epoch [1/1], Step [5117/8897], Loss: 5.3161\n",
      "Epoch [1/1], Step [5118/8897], Loss: 5.3676\n",
      "Epoch [1/1], Step [5119/8897], Loss: 5.4745\n",
      "Epoch [1/1], Step [5120/8897], Loss: 5.3708\n",
      "Epoch [1/1], Step [5121/8897], Loss: 5.3605\n",
      "Epoch [1/1], Step [5122/8897], Loss: 5.3832\n",
      "Epoch [1/1], Step [5123/8897], Loss: 5.3585\n",
      "Epoch [1/1], Step [5124/8897], Loss: 5.2220\n",
      "Epoch [1/1], Step [5125/8897], Loss: 5.5453\n",
      "Epoch [1/1], Step [5126/8897], Loss: 5.1288\n",
      "Epoch [1/1], Step [5127/8897], Loss: 5.2883\n",
      "Epoch [1/1], Step [5128/8897], Loss: 5.4609\n",
      "Epoch [1/1], Step [5129/8897], Loss: 5.2940\n",
      "Epoch [1/1], Step [5130/8897], Loss: 5.4582\n",
      "Epoch [1/1], Step [5131/8897], Loss: 5.4974\n",
      "Epoch [1/1], Step [5132/8897], Loss: 5.4524\n",
      "Epoch [1/1], Step [5133/8897], Loss: 5.4631\n",
      "Epoch [1/1], Step [5134/8897], Loss: 5.1943\n",
      "Epoch [1/1], Step [5135/8897], Loss: 5.4873\n",
      "Epoch [1/1], Step [5136/8897], Loss: 5.0767\n",
      "Epoch [1/1], Step [5137/8897], Loss: 5.5310\n",
      "Epoch [1/1], Step [5138/8897], Loss: 5.5868\n",
      "Epoch [1/1], Step [5139/8897], Loss: 5.1566\n",
      "Epoch [1/1], Step [5140/8897], Loss: 5.2438\n",
      "Epoch [1/1], Step [5141/8897], Loss: 5.3319\n",
      "Epoch [1/1], Step [5142/8897], Loss: 5.2256\n",
      "Epoch [1/1], Step [5143/8897], Loss: 5.5669\n",
      "Epoch [1/1], Step [5144/8897], Loss: 5.3086\n",
      "Epoch [1/1], Step [5145/8897], Loss: 5.2660\n",
      "Epoch [1/1], Step [5146/8897], Loss: 5.4292\n",
      "Epoch [1/1], Step [5147/8897], Loss: 5.5207\n",
      "Epoch [1/1], Step [5148/8897], Loss: 5.1665\n",
      "Epoch [1/1], Step [5149/8897], Loss: 5.2095\n",
      "Epoch [1/1], Step [5150/8897], Loss: 5.5062\n",
      "Epoch [1/1], Step [5151/8897], Loss: 5.4780\n",
      "Epoch [1/1], Step [5152/8897], Loss: 5.3192\n",
      "Epoch [1/1], Step [5153/8897], Loss: 5.5423\n",
      "Epoch [1/1], Step [5154/8897], Loss: 5.3152\n",
      "Epoch [1/1], Step [5155/8897], Loss: 5.2077\n",
      "Epoch [1/1], Step [5156/8897], Loss: 5.3292\n",
      "Epoch [1/1], Step [5157/8897], Loss: 5.6990\n",
      "Epoch [1/1], Step [5158/8897], Loss: 5.6257\n",
      "Epoch [1/1], Step [5159/8897], Loss: 5.6904\n",
      "Epoch [1/1], Step [5160/8897], Loss: 5.2665\n",
      "Epoch [1/1], Step [5161/8897], Loss: 5.2708\n",
      "Epoch [1/1], Step [5162/8897], Loss: 5.2554\n",
      "Epoch [1/1], Step [5163/8897], Loss: 5.4464\n",
      "Epoch [1/1], Step [5164/8897], Loss: 5.3643\n",
      "Epoch [1/1], Step [5165/8897], Loss: 5.3049\n",
      "Epoch [1/1], Step [5166/8897], Loss: 5.2570\n",
      "Epoch [1/1], Step [5167/8897], Loss: 5.3005\n",
      "Epoch [1/1], Step [5168/8897], Loss: 5.1458\n",
      "Epoch [1/1], Step [5169/8897], Loss: 5.7812\n",
      "Epoch [1/1], Step [5170/8897], Loss: 5.6063\n",
      "Epoch [1/1], Step [5171/8897], Loss: 5.4650\n",
      "Epoch [1/1], Step [5172/8897], Loss: 5.4900\n",
      "Epoch [1/1], Step [5173/8897], Loss: 5.2678\n",
      "Epoch [1/1], Step [5174/8897], Loss: 5.3154\n",
      "Epoch [1/1], Step [5175/8897], Loss: 5.2702\n",
      "Epoch [1/1], Step [5176/8897], Loss: 5.1365\n",
      "Epoch [1/1], Step [5177/8897], Loss: 5.4139\n",
      "Epoch [1/1], Step [5178/8897], Loss: 5.2368\n",
      "Epoch [1/1], Step [5179/8897], Loss: 5.3322\n",
      "Epoch [1/1], Step [5180/8897], Loss: 5.4606\n",
      "Epoch [1/1], Step [5181/8897], Loss: 5.1873\n",
      "Epoch [1/1], Step [5182/8897], Loss: 5.4001\n",
      "Epoch [1/1], Step [5183/8897], Loss: 5.2592\n",
      "Epoch [1/1], Step [5184/8897], Loss: 5.1367\n",
      "Epoch [1/1], Step [5185/8897], Loss: 5.5174\n",
      "Epoch [1/1], Step [5186/8897], Loss: 5.2287\n",
      "Epoch [1/1], Step [5187/8897], Loss: 5.5044\n",
      "Epoch [1/1], Step [5188/8897], Loss: 5.4039\n",
      "Epoch [1/1], Step [5189/8897], Loss: 5.3783\n",
      "Epoch [1/1], Step [5190/8897], Loss: 5.3359\n",
      "Epoch [1/1], Step [5191/8897], Loss: 5.1745\n",
      "Epoch [1/1], Step [5192/8897], Loss: 5.3629\n",
      "Epoch [1/1], Step [5193/8897], Loss: 5.5019\n",
      "Epoch [1/1], Step [5194/8897], Loss: 5.4312\n",
      "Epoch [1/1], Step [5195/8897], Loss: 5.5578\n",
      "Epoch [1/1], Step [5196/8897], Loss: 5.5473\n",
      "Epoch [1/1], Step [5197/8897], Loss: 5.4506\n",
      "Epoch [1/1], Step [5198/8897], Loss: 5.3157\n",
      "Epoch [1/1], Step [5199/8897], Loss: 5.4121\n",
      "Epoch [1/1], Step [5200/8897], Loss: 5.5843\n",
      "Epoch [1/1], Step [5201/8897], Loss: 5.2233\n",
      "Epoch [1/1], Step [5202/8897], Loss: 5.2689\n",
      "Epoch [1/1], Step [5203/8897], Loss: 5.1665\n",
      "Epoch [1/1], Step [5204/8897], Loss: 5.5921\n",
      "Epoch [1/1], Step [5205/8897], Loss: 5.3610\n",
      "Epoch [1/1], Step [5206/8897], Loss: 5.4631\n",
      "Epoch [1/1], Step [5207/8897], Loss: 5.4108\n",
      "Epoch [1/1], Step [5208/8897], Loss: 5.2301\n",
      "Epoch [1/1], Step [5209/8897], Loss: 5.4592\n",
      "Epoch [1/1], Step [5210/8897], Loss: 5.1697\n",
      "Epoch [1/1], Step [5211/8897], Loss: 5.3977\n",
      "Epoch [1/1], Step [5212/8897], Loss: 5.3103\n",
      "Epoch [1/1], Step [5213/8897], Loss: 5.4380\n",
      "Epoch [1/1], Step [5214/8897], Loss: 5.5143\n",
      "Epoch [1/1], Step [5215/8897], Loss: 5.2636\n",
      "Epoch [1/1], Step [5216/8897], Loss: 5.3098\n",
      "Epoch [1/1], Step [5217/8897], Loss: 5.3946\n",
      "Epoch [1/1], Step [5218/8897], Loss: 5.4790\n",
      "Epoch [1/1], Step [5219/8897], Loss: 5.5341\n",
      "Epoch [1/1], Step [5220/8897], Loss: 5.2462\n",
      "Epoch [1/1], Step [5221/8897], Loss: 5.6009\n",
      "Epoch [1/1], Step [5222/8897], Loss: 5.3165\n",
      "Epoch [1/1], Step [5223/8897], Loss: 4.9315\n",
      "Epoch [1/1], Step [5224/8897], Loss: 5.4284\n",
      "Epoch [1/1], Step [5225/8897], Loss: 5.0722\n",
      "Epoch [1/1], Step [5226/8897], Loss: 5.4049\n",
      "Epoch [1/1], Step [5227/8897], Loss: 5.4246\n",
      "Epoch [1/1], Step [5228/8897], Loss: 5.4876\n",
      "Epoch [1/1], Step [5229/8897], Loss: 5.5028\n",
      "Epoch [1/1], Step [5230/8897], Loss: 5.1925\n",
      "Epoch [1/1], Step [5231/8897], Loss: 5.2606\n",
      "Epoch [1/1], Step [5232/8897], Loss: 5.4713\n",
      "Epoch [1/1], Step [5233/8897], Loss: 5.5024\n",
      "Epoch [1/1], Step [5234/8897], Loss: 5.2543\n",
      "Epoch [1/1], Step [5235/8897], Loss: 5.4205\n",
      "Epoch [1/1], Step [5236/8897], Loss: 5.1305\n",
      "Epoch [1/1], Step [5237/8897], Loss: 5.3921\n",
      "Epoch [1/1], Step [5238/8897], Loss: 5.2711\n",
      "Epoch [1/1], Step [5239/8897], Loss: 5.2465\n",
      "Epoch [1/1], Step [5240/8897], Loss: 5.3696\n",
      "Epoch [1/1], Step [5241/8897], Loss: 5.5750\n",
      "Epoch [1/1], Step [5242/8897], Loss: 5.2965\n",
      "Epoch [1/1], Step [5243/8897], Loss: 5.5139\n",
      "Epoch [1/1], Step [5244/8897], Loss: 5.3819\n",
      "Epoch [1/1], Step [5245/8897], Loss: 5.5324\n",
      "Epoch [1/1], Step [5246/8897], Loss: 5.4840\n",
      "Epoch [1/1], Step [5247/8897], Loss: 5.4111\n",
      "Epoch [1/1], Step [5248/8897], Loss: 5.4393\n",
      "Epoch [1/1], Step [5249/8897], Loss: 5.5826\n",
      "Epoch [1/1], Step [5250/8897], Loss: 5.3845\n",
      "Epoch [1/1], Step [5251/8897], Loss: 5.4284\n",
      "Epoch [1/1], Step [5252/8897], Loss: 5.3579\n",
      "Epoch [1/1], Step [5253/8897], Loss: 5.4469\n",
      "Epoch [1/1], Step [5254/8897], Loss: 5.2580\n",
      "Epoch [1/1], Step [5255/8897], Loss: 5.4601\n",
      "Epoch [1/1], Step [5256/8897], Loss: 5.4867\n",
      "Epoch [1/1], Step [5257/8897], Loss: 5.4339\n",
      "Epoch [1/1], Step [5258/8897], Loss: 5.2926\n",
      "Epoch [1/1], Step [5259/8897], Loss: 5.3818\n",
      "Epoch [1/1], Step [5260/8897], Loss: 5.3373\n",
      "Epoch [1/1], Step [5261/8897], Loss: 5.3714\n",
      "Epoch [1/1], Step [5262/8897], Loss: 5.7373\n",
      "Epoch [1/1], Step [5263/8897], Loss: 5.1505\n",
      "Epoch [1/1], Step [5264/8897], Loss: 5.3247\n",
      "Epoch [1/1], Step [5265/8897], Loss: 5.3678\n",
      "Epoch [1/1], Step [5266/8897], Loss: 5.5363\n",
      "Epoch [1/1], Step [5267/8897], Loss: 5.2213\n",
      "Epoch [1/1], Step [5268/8897], Loss: 5.1542\n",
      "Epoch [1/1], Step [5269/8897], Loss: 5.2420\n",
      "Epoch [1/1], Step [5270/8897], Loss: 5.1327\n",
      "Epoch [1/1], Step [5271/8897], Loss: 5.4748\n",
      "Epoch [1/1], Step [5272/8897], Loss: 5.4822\n",
      "Epoch [1/1], Step [5273/8897], Loss: 5.2445\n",
      "Epoch [1/1], Step [5274/8897], Loss: 5.4818\n",
      "Epoch [1/1], Step [5275/8897], Loss: 5.4750\n",
      "Epoch [1/1], Step [5276/8897], Loss: 5.4592\n",
      "Epoch [1/1], Step [5277/8897], Loss: 5.6341\n",
      "Epoch [1/1], Step [5278/8897], Loss: 5.3770\n",
      "Epoch [1/1], Step [5279/8897], Loss: 5.2547\n",
      "Epoch [1/1], Step [5280/8897], Loss: 5.2394\n",
      "Epoch [1/1], Step [5281/8897], Loss: 5.2788\n",
      "Epoch [1/1], Step [5282/8897], Loss: 5.0680\n",
      "Epoch [1/1], Step [5283/8897], Loss: 5.2918\n",
      "Epoch [1/1], Step [5284/8897], Loss: 5.3883\n",
      "Epoch [1/1], Step [5285/8897], Loss: 5.3833\n",
      "Epoch [1/1], Step [5286/8897], Loss: 5.5689\n",
      "Epoch [1/1], Step [5287/8897], Loss: 5.0001\n",
      "Epoch [1/1], Step [5288/8897], Loss: 5.5358\n",
      "Epoch [1/1], Step [5289/8897], Loss: 5.3617\n",
      "Epoch [1/1], Step [5290/8897], Loss: 5.3700\n",
      "Epoch [1/1], Step [5291/8897], Loss: 5.4411\n",
      "Epoch [1/1], Step [5292/8897], Loss: 5.6392\n",
      "Epoch [1/1], Step [5293/8897], Loss: 5.4381\n",
      "Epoch [1/1], Step [5294/8897], Loss: 5.4146\n",
      "Epoch [1/1], Step [5295/8897], Loss: 5.3593\n",
      "Epoch [1/1], Step [5296/8897], Loss: 5.1267\n",
      "Epoch [1/1], Step [5297/8897], Loss: 5.3966\n",
      "Epoch [1/1], Step [5298/8897], Loss: 5.3616\n",
      "Epoch [1/1], Step [5299/8897], Loss: 5.4162\n",
      "Epoch [1/1], Step [5300/8897], Loss: 5.2905\n",
      "Epoch [1/1], Step [5301/8897], Loss: 5.5505\n",
      "Epoch [1/1], Step [5302/8897], Loss: 5.0639\n",
      "Epoch [1/1], Step [5303/8897], Loss: 5.5350\n",
      "Epoch [1/1], Step [5304/8897], Loss: 5.3022\n",
      "Epoch [1/1], Step [5305/8897], Loss: 5.4876\n",
      "Epoch [1/1], Step [5306/8897], Loss: 5.4254\n",
      "Epoch [1/1], Step [5307/8897], Loss: 5.4799\n",
      "Epoch [1/1], Step [5308/8897], Loss: 5.4561\n",
      "Epoch [1/1], Step [5309/8897], Loss: 5.3884\n",
      "Epoch [1/1], Step [5310/8897], Loss: 5.4735\n",
      "Epoch [1/1], Step [5311/8897], Loss: 5.6363\n",
      "Epoch [1/1], Step [5312/8897], Loss: 5.4937\n",
      "Epoch [1/1], Step [5313/8897], Loss: 5.5452\n",
      "Epoch [1/1], Step [5314/8897], Loss: 5.3970\n",
      "Epoch [1/1], Step [5315/8897], Loss: 5.4082\n",
      "Epoch [1/1], Step [5316/8897], Loss: 5.3553\n",
      "Epoch [1/1], Step [5317/8897], Loss: 5.5274\n",
      "Epoch [1/1], Step [5318/8897], Loss: 5.4154\n",
      "Epoch [1/1], Step [5319/8897], Loss: 5.3255\n",
      "Epoch [1/1], Step [5320/8897], Loss: 5.3902\n",
      "Epoch [1/1], Step [5321/8897], Loss: 5.2741\n",
      "Epoch [1/1], Step [5322/8897], Loss: 5.2588\n",
      "Epoch [1/1], Step [5323/8897], Loss: 5.3291\n",
      "Epoch [1/1], Step [5324/8897], Loss: 5.2428\n",
      "Epoch [1/1], Step [5325/8897], Loss: 5.2806\n",
      "Epoch [1/1], Step [5326/8897], Loss: 5.3913\n",
      "Epoch [1/1], Step [5327/8897], Loss: 5.3063\n",
      "Epoch [1/1], Step [5328/8897], Loss: 5.3096\n",
      "Epoch [1/1], Step [5329/8897], Loss: 5.4808\n",
      "Epoch [1/1], Step [5330/8897], Loss: 5.3187\n",
      "Epoch [1/1], Step [5331/8897], Loss: 5.4272\n",
      "Epoch [1/1], Step [5332/8897], Loss: 5.2429\n",
      "Epoch [1/1], Step [5333/8897], Loss: 5.3751\n",
      "Epoch [1/1], Step [5334/8897], Loss: 5.4209\n",
      "Epoch [1/1], Step [5335/8897], Loss: 5.2945\n",
      "Epoch [1/1], Step [5336/8897], Loss: 5.4350\n",
      "Epoch [1/1], Step [5337/8897], Loss: 5.1234\n",
      "Epoch [1/1], Step [5338/8897], Loss: 5.5085\n",
      "Epoch [1/1], Step [5339/8897], Loss: 5.2055\n",
      "Epoch [1/1], Step [5340/8897], Loss: 5.4611\n",
      "Epoch [1/1], Step [5341/8897], Loss: 5.4563\n",
      "Epoch [1/1], Step [5342/8897], Loss: 5.2893\n",
      "Epoch [1/1], Step [5343/8897], Loss: 5.4653\n",
      "Epoch [1/1], Step [5344/8897], Loss: 5.3891\n",
      "Epoch [1/1], Step [5345/8897], Loss: 5.4615\n",
      "Epoch [1/1], Step [5346/8897], Loss: 5.3722\n",
      "Epoch [1/1], Step [5347/8897], Loss: 5.3265\n",
      "Epoch [1/1], Step [5348/8897], Loss: 5.4258\n",
      "Epoch [1/1], Step [5349/8897], Loss: 5.5105\n",
      "Epoch [1/1], Step [5350/8897], Loss: 5.3995\n",
      "Epoch [1/1], Step [5351/8897], Loss: 5.2741\n",
      "Epoch [1/1], Step [5352/8897], Loss: 5.4909\n",
      "Epoch [1/1], Step [5353/8897], Loss: 5.4164\n",
      "Epoch [1/1], Step [5354/8897], Loss: 5.4054\n",
      "Epoch [1/1], Step [5355/8897], Loss: 5.2667\n",
      "Epoch [1/1], Step [5356/8897], Loss: 5.4002\n",
      "Epoch [1/1], Step [5357/8897], Loss: 5.6241\n",
      "Epoch [1/1], Step [5358/8897], Loss: 5.4986\n",
      "Epoch [1/1], Step [5359/8897], Loss: 5.0787\n",
      "Epoch [1/1], Step [5360/8897], Loss: 5.1974\n",
      "Epoch [1/1], Step [5361/8897], Loss: 5.2623\n",
      "Epoch [1/1], Step [5362/8897], Loss: 5.1284\n",
      "Epoch [1/1], Step [5363/8897], Loss: 5.4337\n",
      "Epoch [1/1], Step [5364/8897], Loss: 5.2474\n",
      "Epoch [1/1], Step [5365/8897], Loss: 5.4396\n",
      "Epoch [1/1], Step [5366/8897], Loss: 5.3082\n",
      "Epoch [1/1], Step [5367/8897], Loss: 5.3348\n",
      "Epoch [1/1], Step [5368/8897], Loss: 5.5624\n",
      "Epoch [1/1], Step [5369/8897], Loss: 5.5834\n",
      "Epoch [1/1], Step [5370/8897], Loss: 5.4444\n",
      "Epoch [1/1], Step [5371/8897], Loss: 5.3451\n",
      "Epoch [1/1], Step [5372/8897], Loss: 5.3447\n",
      "Epoch [1/1], Step [5373/8897], Loss: 5.1765\n",
      "Epoch [1/1], Step [5374/8897], Loss: 5.4742\n",
      "Epoch [1/1], Step [5375/8897], Loss: 5.4171\n",
      "Epoch [1/1], Step [5376/8897], Loss: 5.3178\n",
      "Epoch [1/1], Step [5377/8897], Loss: 5.4650\n",
      "Epoch [1/1], Step [5378/8897], Loss: 5.1903\n",
      "Epoch [1/1], Step [5379/8897], Loss: 5.4788\n",
      "Epoch [1/1], Step [5380/8897], Loss: 5.4125\n",
      "Epoch [1/1], Step [5381/8897], Loss: 5.2712\n",
      "Epoch [1/1], Step [5382/8897], Loss: 5.3833\n",
      "Epoch [1/1], Step [5383/8897], Loss: 5.0264\n",
      "Epoch [1/1], Step [5384/8897], Loss: 5.3555\n",
      "Epoch [1/1], Step [5385/8897], Loss: 5.3792\n",
      "Epoch [1/1], Step [5386/8897], Loss: 5.1646\n",
      "Epoch [1/1], Step [5387/8897], Loss: 5.6898\n",
      "Epoch [1/1], Step [5388/8897], Loss: 5.2794\n",
      "Epoch [1/1], Step [5389/8897], Loss: 5.4901\n",
      "Epoch [1/1], Step [5390/8897], Loss: 5.6061\n",
      "Epoch [1/1], Step [5391/8897], Loss: 5.5674\n",
      "Epoch [1/1], Step [5392/8897], Loss: 5.1580\n",
      "Epoch [1/1], Step [5393/8897], Loss: 5.4918\n",
      "Epoch [1/1], Step [5394/8897], Loss: 5.5815\n",
      "Epoch [1/1], Step [5395/8897], Loss: 5.4983\n",
      "Epoch [1/1], Step [5396/8897], Loss: 5.3451\n",
      "Epoch [1/1], Step [5397/8897], Loss: 5.4004\n",
      "Epoch [1/1], Step [5398/8897], Loss: 5.3489\n",
      "Epoch [1/1], Step [5399/8897], Loss: 5.5116\n",
      "Epoch [1/1], Step [5400/8897], Loss: 5.2454\n",
      "Epoch [1/1], Step [5401/8897], Loss: 5.4879\n",
      "Epoch [1/1], Step [5402/8897], Loss: 5.3398\n",
      "Epoch [1/1], Step [5403/8897], Loss: 5.4129\n",
      "Epoch [1/1], Step [5404/8897], Loss: 5.4396\n",
      "Epoch [1/1], Step [5405/8897], Loss: 5.3451\n",
      "Epoch [1/1], Step [5406/8897], Loss: 5.5308\n",
      "Epoch [1/1], Step [5407/8897], Loss: 5.3983\n",
      "Epoch [1/1], Step [5408/8897], Loss: 5.3146\n",
      "Epoch [1/1], Step [5409/8897], Loss: 5.4861\n",
      "Epoch [1/1], Step [5410/8897], Loss: 5.3788\n",
      "Epoch [1/1], Step [5411/8897], Loss: 5.6704\n",
      "Epoch [1/1], Step [5412/8897], Loss: 5.4131\n",
      "Epoch [1/1], Step [5413/8897], Loss: 5.4743\n",
      "Epoch [1/1], Step [5414/8897], Loss: 5.5506\n",
      "Epoch [1/1], Step [5415/8897], Loss: 5.5477\n",
      "Epoch [1/1], Step [5416/8897], Loss: 5.1909\n",
      "Epoch [1/1], Step [5417/8897], Loss: 5.4972\n",
      "Epoch [1/1], Step [5418/8897], Loss: 5.2856\n",
      "Epoch [1/1], Step [5419/8897], Loss: 5.4469\n",
      "Epoch [1/1], Step [5420/8897], Loss: 5.1928\n",
      "Epoch [1/1], Step [5421/8897], Loss: 5.5080\n",
      "Epoch [1/1], Step [5422/8897], Loss: 5.3079\n",
      "Epoch [1/1], Step [5423/8897], Loss: 5.3593\n",
      "Epoch [1/1], Step [5424/8897], Loss: 5.7005\n",
      "Epoch [1/1], Step [5425/8897], Loss: 5.3436\n",
      "Epoch [1/1], Step [5426/8897], Loss: 5.2745\n",
      "Epoch [1/1], Step [5427/8897], Loss: 5.2627\n",
      "Epoch [1/1], Step [5428/8897], Loss: 5.5199\n",
      "Epoch [1/1], Step [5429/8897], Loss: 5.3871\n",
      "Epoch [1/1], Step [5430/8897], Loss: 5.2039\n",
      "Epoch [1/1], Step [5431/8897], Loss: 5.2425\n",
      "Epoch [1/1], Step [5432/8897], Loss: 5.5335\n",
      "Epoch [1/1], Step [5433/8897], Loss: 5.3011\n",
      "Epoch [1/1], Step [5434/8897], Loss: 5.1164\n",
      "Epoch [1/1], Step [5435/8897], Loss: 5.4833\n",
      "Epoch [1/1], Step [5436/8897], Loss: 5.2869\n",
      "Epoch [1/1], Step [5437/8897], Loss: 5.3497\n",
      "Epoch [1/1], Step [5438/8897], Loss: 5.3928\n",
      "Epoch [1/1], Step [5439/8897], Loss: 5.4275\n",
      "Epoch [1/1], Step [5440/8897], Loss: 5.4304\n",
      "Epoch [1/1], Step [5441/8897], Loss: 5.3463\n",
      "Epoch [1/1], Step [5442/8897], Loss: 5.4308\n",
      "Epoch [1/1], Step [5443/8897], Loss: 5.1781\n",
      "Epoch [1/1], Step [5444/8897], Loss: 5.5539\n",
      "Epoch [1/1], Step [5445/8897], Loss: 5.3082\n",
      "Epoch [1/1], Step [5446/8897], Loss: 5.4257\n",
      "Epoch [1/1], Step [5447/8897], Loss: 5.4480\n",
      "Epoch [1/1], Step [5448/8897], Loss: 5.4727\n",
      "Epoch [1/1], Step [5449/8897], Loss: 5.5209\n",
      "Epoch [1/1], Step [5450/8897], Loss: 5.4050\n",
      "Epoch [1/1], Step [5451/8897], Loss: 5.6272\n",
      "Epoch [1/1], Step [5452/8897], Loss: 5.3237\n",
      "Epoch [1/1], Step [5453/8897], Loss: 5.4470\n",
      "Epoch [1/1], Step [5454/8897], Loss: 5.2815\n",
      "Epoch [1/1], Step [5455/8897], Loss: 5.5425\n",
      "Epoch [1/1], Step [5456/8897], Loss: 5.4742\n",
      "Epoch [1/1], Step [5457/8897], Loss: 5.4273\n",
      "Epoch [1/1], Step [5458/8897], Loss: 5.3075\n",
      "Epoch [1/1], Step [5459/8897], Loss: 5.2487\n",
      "Epoch [1/1], Step [5460/8897], Loss: 5.2066\n",
      "Epoch [1/1], Step [5461/8897], Loss: 5.2752\n",
      "Epoch [1/1], Step [5462/8897], Loss: 5.1604\n",
      "Epoch [1/1], Step [5463/8897], Loss: 5.5106\n",
      "Epoch [1/1], Step [5464/8897], Loss: 5.2511\n",
      "Epoch [1/1], Step [5465/8897], Loss: 5.0539\n",
      "Epoch [1/1], Step [5466/8897], Loss: 5.3162\n",
      "Epoch [1/1], Step [5467/8897], Loss: 5.2655\n",
      "Epoch [1/1], Step [5468/8897], Loss: 5.0949\n",
      "Epoch [1/1], Step [5469/8897], Loss: 5.4620\n",
      "Epoch [1/1], Step [5470/8897], Loss: 5.2667\n",
      "Epoch [1/1], Step [5471/8897], Loss: 5.1730\n",
      "Epoch [1/1], Step [5472/8897], Loss: 5.2854\n",
      "Epoch [1/1], Step [5473/8897], Loss: 5.3592\n",
      "Epoch [1/1], Step [5474/8897], Loss: 5.3507\n",
      "Epoch [1/1], Step [5475/8897], Loss: 5.5772\n",
      "Epoch [1/1], Step [5476/8897], Loss: 5.3903\n",
      "Epoch [1/1], Step [5477/8897], Loss: 5.2637\n",
      "Epoch [1/1], Step [5478/8897], Loss: 5.5518\n",
      "Epoch [1/1], Step [5479/8897], Loss: 5.3338\n",
      "Epoch [1/1], Step [5480/8897], Loss: 5.2175\n",
      "Epoch [1/1], Step [5481/8897], Loss: 5.5776\n",
      "Epoch [1/1], Step [5482/8897], Loss: 5.3968\n",
      "Epoch [1/1], Step [5483/8897], Loss: 5.5364\n",
      "Epoch [1/1], Step [5484/8897], Loss: 5.4971\n",
      "Epoch [1/1], Step [5485/8897], Loss: 5.2927\n",
      "Epoch [1/1], Step [5486/8897], Loss: 5.6195\n",
      "Epoch [1/1], Step [5487/8897], Loss: 5.3273\n",
      "Epoch [1/1], Step [5488/8897], Loss: 5.8765\n",
      "Epoch [1/1], Step [5489/8897], Loss: 5.3939\n",
      "Epoch [1/1], Step [5490/8897], Loss: 5.2643\n",
      "Epoch [1/1], Step [5491/8897], Loss: 5.5646\n",
      "Epoch [1/1], Step [5492/8897], Loss: 5.5049\n",
      "Epoch [1/1], Step [5493/8897], Loss: 5.3450\n",
      "Epoch [1/1], Step [5494/8897], Loss: 5.4449\n",
      "Epoch [1/1], Step [5495/8897], Loss: 5.0707\n",
      "Epoch [1/1], Step [5496/8897], Loss: 5.2587\n",
      "Epoch [1/1], Step [5497/8897], Loss: 5.2924\n",
      "Epoch [1/1], Step [5498/8897], Loss: 5.3177\n",
      "Epoch [1/1], Step [5499/8897], Loss: 5.4714\n",
      "Epoch [1/1], Step [5500/8897], Loss: 5.5152\n",
      "Epoch [1/1], Step [5501/8897], Loss: 5.3356\n",
      "Epoch [1/1], Step [5502/8897], Loss: 5.3683\n",
      "Epoch [1/1], Step [5503/8897], Loss: 4.9987\n",
      "Epoch [1/1], Step [5504/8897], Loss: 5.1398\n",
      "Epoch [1/1], Step [5505/8897], Loss: 5.3997\n",
      "Epoch [1/1], Step [5506/8897], Loss: 5.4246\n",
      "Epoch [1/1], Step [5507/8897], Loss: 5.2054\n",
      "Epoch [1/1], Step [5508/8897], Loss: 5.2784\n",
      "Epoch [1/1], Step [5509/8897], Loss: 5.5325\n",
      "Epoch [1/1], Step [5510/8897], Loss: 5.1879\n",
      "Epoch [1/1], Step [5511/8897], Loss: 5.1762\n",
      "Epoch [1/1], Step [5512/8897], Loss: 5.5256\n",
      "Epoch [1/1], Step [5513/8897], Loss: 5.4195\n",
      "Epoch [1/1], Step [5514/8897], Loss: 5.4765\n",
      "Epoch [1/1], Step [5515/8897], Loss: 5.5977\n",
      "Epoch [1/1], Step [5516/8897], Loss: 5.1831\n",
      "Epoch [1/1], Step [5517/8897], Loss: 5.4798\n",
      "Epoch [1/1], Step [5518/8897], Loss: 5.4637\n",
      "Epoch [1/1], Step [5519/8897], Loss: 5.3025\n",
      "Epoch [1/1], Step [5520/8897], Loss: 5.2614\n",
      "Epoch [1/1], Step [5521/8897], Loss: 5.3341\n",
      "Epoch [1/1], Step [5522/8897], Loss: 5.2127\n",
      "Epoch [1/1], Step [5523/8897], Loss: 5.3340\n",
      "Epoch [1/1], Step [5524/8897], Loss: 5.6568\n",
      "Epoch [1/1], Step [5525/8897], Loss: 5.3635\n",
      "Epoch [1/1], Step [5526/8897], Loss: 5.1744\n",
      "Epoch [1/1], Step [5527/8897], Loss: 5.3160\n",
      "Epoch [1/1], Step [5528/8897], Loss: 5.2529\n",
      "Epoch [1/1], Step [5529/8897], Loss: 5.3013\n",
      "Epoch [1/1], Step [5530/8897], Loss: 5.2195\n",
      "Epoch [1/1], Step [5531/8897], Loss: 5.1438\n",
      "Epoch [1/1], Step [5532/8897], Loss: 5.2107\n",
      "Epoch [1/1], Step [5533/8897], Loss: 5.5522\n",
      "Epoch [1/1], Step [5534/8897], Loss: 5.3978\n",
      "Epoch [1/1], Step [5535/8897], Loss: 5.6076\n",
      "Epoch [1/1], Step [5536/8897], Loss: 5.2903\n",
      "Epoch [1/1], Step [5537/8897], Loss: 5.2031\n",
      "Epoch [1/1], Step [5538/8897], Loss: 5.2780\n",
      "Epoch [1/1], Step [5539/8897], Loss: 5.5070\n",
      "Epoch [1/1], Step [5540/8897], Loss: 5.3590\n",
      "Epoch [1/1], Step [5541/8897], Loss: 5.3400\n",
      "Epoch [1/1], Step [5542/8897], Loss: 5.5322\n",
      "Epoch [1/1], Step [5543/8897], Loss: 5.3737\n",
      "Epoch [1/1], Step [5544/8897], Loss: 5.3539\n",
      "Epoch [1/1], Step [5545/8897], Loss: 5.2244\n",
      "Epoch [1/1], Step [5546/8897], Loss: 5.5317\n",
      "Epoch [1/1], Step [5547/8897], Loss: 5.2411\n",
      "Epoch [1/1], Step [5548/8897], Loss: 5.5553\n",
      "Epoch [1/1], Step [5549/8897], Loss: 5.7164\n",
      "Epoch [1/1], Step [5550/8897], Loss: 5.4851\n",
      "Epoch [1/1], Step [5551/8897], Loss: 5.1649\n",
      "Epoch [1/1], Step [5552/8897], Loss: 5.6436\n",
      "Epoch [1/1], Step [5553/8897], Loss: 5.3238\n",
      "Epoch [1/1], Step [5554/8897], Loss: 5.5055\n",
      "Epoch [1/1], Step [5555/8897], Loss: 5.4464\n",
      "Epoch [1/1], Step [5556/8897], Loss: 5.6060\n",
      "Epoch [1/1], Step [5557/8897], Loss: 5.2477\n",
      "Epoch [1/1], Step [5558/8897], Loss: 5.3588\n",
      "Epoch [1/1], Step [5559/8897], Loss: 5.6690\n",
      "Epoch [1/1], Step [5560/8897], Loss: 5.3501\n",
      "Epoch [1/1], Step [5561/8897], Loss: 5.2249\n",
      "Epoch [1/1], Step [5562/8897], Loss: 5.2447\n",
      "Epoch [1/1], Step [5563/8897], Loss: 5.3516\n",
      "Epoch [1/1], Step [5564/8897], Loss: 5.3685\n",
      "Epoch [1/1], Step [5565/8897], Loss: 5.3379\n",
      "Epoch [1/1], Step [5566/8897], Loss: 5.3559\n",
      "Epoch [1/1], Step [5567/8897], Loss: 5.3379\n",
      "Epoch [1/1], Step [5568/8897], Loss: 5.3038\n",
      "Epoch [1/1], Step [5569/8897], Loss: 5.2900\n",
      "Epoch [1/1], Step [5570/8897], Loss: 5.5414\n",
      "Epoch [1/1], Step [5571/8897], Loss: 5.4452\n",
      "Epoch [1/1], Step [5572/8897], Loss: 5.0578\n",
      "Epoch [1/1], Step [5573/8897], Loss: 5.3637\n",
      "Epoch [1/1], Step [5574/8897], Loss: 5.5526\n",
      "Epoch [1/1], Step [5575/8897], Loss: 5.2507\n",
      "Epoch [1/1], Step [5576/8897], Loss: 5.3220\n",
      "Epoch [1/1], Step [5577/8897], Loss: 5.2509\n",
      "Epoch [1/1], Step [5578/8897], Loss: 5.5118\n",
      "Epoch [1/1], Step [5579/8897], Loss: 5.4384\n",
      "Epoch [1/1], Step [5580/8897], Loss: 5.0968\n",
      "Epoch [1/1], Step [5581/8897], Loss: 5.3264\n",
      "Epoch [1/1], Step [5582/8897], Loss: 5.2494\n",
      "Epoch [1/1], Step [5583/8897], Loss: 5.5748\n",
      "Epoch [1/1], Step [5584/8897], Loss: 5.6055\n",
      "Epoch [1/1], Step [5585/8897], Loss: 5.4029\n",
      "Epoch [1/1], Step [5586/8897], Loss: 5.2285\n",
      "Epoch [1/1], Step [5587/8897], Loss: 5.3139\n",
      "Epoch [1/1], Step [5588/8897], Loss: 5.3581\n",
      "Epoch [1/1], Step [5589/8897], Loss: 5.3513\n",
      "Epoch [1/1], Step [5590/8897], Loss: 5.4330\n",
      "Epoch [1/1], Step [5591/8897], Loss: 5.3016\n",
      "Epoch [1/1], Step [5592/8897], Loss: 5.5564\n",
      "Epoch [1/1], Step [5593/8897], Loss: 5.5222\n",
      "Epoch [1/1], Step [5594/8897], Loss: 5.3550\n",
      "Epoch [1/1], Step [5595/8897], Loss: 5.3206\n",
      "Epoch [1/1], Step [5596/8897], Loss: 5.3772\n",
      "Epoch [1/1], Step [5597/8897], Loss: 5.4358\n",
      "Epoch [1/1], Step [5598/8897], Loss: 5.4039\n",
      "Epoch [1/1], Step [5599/8897], Loss: 5.3382\n",
      "Epoch [1/1], Step [5600/8897], Loss: 5.3887\n",
      "Epoch [1/1], Step [5601/8897], Loss: 5.4259\n",
      "Epoch [1/1], Step [5602/8897], Loss: 5.1970\n",
      "Epoch [1/1], Step [5603/8897], Loss: 5.3779\n",
      "Epoch [1/1], Step [5604/8897], Loss: 5.6732\n",
      "Epoch [1/1], Step [5605/8897], Loss: 5.5536\n",
      "Epoch [1/1], Step [5606/8897], Loss: 5.4728\n",
      "Epoch [1/1], Step [5607/8897], Loss: 5.4146\n",
      "Epoch [1/1], Step [5608/8897], Loss: 5.4215\n",
      "Epoch [1/1], Step [5609/8897], Loss: 5.6008\n",
      "Epoch [1/1], Step [5610/8897], Loss: 5.4552\n",
      "Epoch [1/1], Step [5611/8897], Loss: 5.2565\n",
      "Epoch [1/1], Step [5612/8897], Loss: 5.2683\n",
      "Epoch [1/1], Step [5613/8897], Loss: 5.2957\n",
      "Epoch [1/1], Step [5614/8897], Loss: 5.4692\n",
      "Epoch [1/1], Step [5615/8897], Loss: 5.4724\n",
      "Epoch [1/1], Step [5616/8897], Loss: 5.2305\n",
      "Epoch [1/1], Step [5617/8897], Loss: 5.4865\n",
      "Epoch [1/1], Step [5618/8897], Loss: 5.1298\n",
      "Epoch [1/1], Step [5619/8897], Loss: 5.2188\n",
      "Epoch [1/1], Step [5620/8897], Loss: 5.1880\n",
      "Epoch [1/1], Step [5621/8897], Loss: 5.2861\n",
      "Epoch [1/1], Step [5622/8897], Loss: 5.4964\n",
      "Epoch [1/1], Step [5623/8897], Loss: 5.1876\n",
      "Epoch [1/1], Step [5624/8897], Loss: 5.3370\n",
      "Epoch [1/1], Step [5625/8897], Loss: 5.4867\n",
      "Epoch [1/1], Step [5626/8897], Loss: 5.5107\n",
      "Epoch [1/1], Step [5627/8897], Loss: 5.1874\n",
      "Epoch [1/1], Step [5628/8897], Loss: 5.4676\n",
      "Epoch [1/1], Step [5629/8897], Loss: 5.3168\n",
      "Epoch [1/1], Step [5630/8897], Loss: 5.3595\n",
      "Epoch [1/1], Step [5631/8897], Loss: 5.4847\n",
      "Epoch [1/1], Step [5632/8897], Loss: 5.3129\n",
      "Epoch [1/1], Step [5633/8897], Loss: 5.4403\n",
      "Epoch [1/1], Step [5634/8897], Loss: 5.3787\n",
      "Epoch [1/1], Step [5635/8897], Loss: 5.7916\n",
      "Epoch [1/1], Step [5636/8897], Loss: 5.1821\n",
      "Epoch [1/1], Step [5637/8897], Loss: 5.2000\n",
      "Epoch [1/1], Step [5638/8897], Loss: 5.2036\n",
      "Epoch [1/1], Step [5639/8897], Loss: 5.2328\n",
      "Epoch [1/1], Step [5640/8897], Loss: 5.3552\n",
      "Epoch [1/1], Step [5641/8897], Loss: 5.4439\n",
      "Epoch [1/1], Step [5642/8897], Loss: 5.4583\n",
      "Epoch [1/1], Step [5643/8897], Loss: 5.3680\n",
      "Epoch [1/1], Step [5644/8897], Loss: 5.3765\n",
      "Epoch [1/1], Step [5645/8897], Loss: 5.3237\n",
      "Epoch [1/1], Step [5646/8897], Loss: 5.5840\n",
      "Epoch [1/1], Step [5647/8897], Loss: 5.5240\n",
      "Epoch [1/1], Step [5648/8897], Loss: 5.4716\n",
      "Epoch [1/1], Step [5649/8897], Loss: 5.2053\n",
      "Epoch [1/1], Step [5650/8897], Loss: 5.2392\n",
      "Epoch [1/1], Step [5651/8897], Loss: 5.1305\n",
      "Epoch [1/1], Step [5652/8897], Loss: 5.5380\n",
      "Epoch [1/1], Step [5653/8897], Loss: 5.3744\n",
      "Epoch [1/1], Step [5654/8897], Loss: 5.5598\n",
      "Epoch [1/1], Step [5655/8897], Loss: 5.1723\n",
      "Epoch [1/1], Step [5656/8897], Loss: 5.5314\n",
      "Epoch [1/1], Step [5657/8897], Loss: 5.4077\n",
      "Epoch [1/1], Step [5658/8897], Loss: 5.1045\n",
      "Epoch [1/1], Step [5659/8897], Loss: 5.3556\n",
      "Epoch [1/1], Step [5660/8897], Loss: 5.5795\n",
      "Epoch [1/1], Step [5661/8897], Loss: 5.4381\n",
      "Epoch [1/1], Step [5662/8897], Loss: 5.2083\n",
      "Epoch [1/1], Step [5663/8897], Loss: 5.4096\n",
      "Epoch [1/1], Step [5664/8897], Loss: 5.1221\n",
      "Epoch [1/1], Step [5665/8897], Loss: 5.2656\n",
      "Epoch [1/1], Step [5666/8897], Loss: 5.5663\n",
      "Epoch [1/1], Step [5667/8897], Loss: 5.4464\n",
      "Epoch [1/1], Step [5668/8897], Loss: 5.6288\n",
      "Epoch [1/1], Step [5669/8897], Loss: 5.3669\n",
      "Epoch [1/1], Step [5670/8897], Loss: 5.4648\n",
      "Epoch [1/1], Step [5671/8897], Loss: 5.5132\n",
      "Epoch [1/1], Step [5672/8897], Loss: 5.5506\n",
      "Epoch [1/1], Step [5673/8897], Loss: 5.5323\n",
      "Epoch [1/1], Step [5674/8897], Loss: 5.6458\n",
      "Epoch [1/1], Step [5675/8897], Loss: 5.4003\n",
      "Epoch [1/1], Step [5676/8897], Loss: 5.4304\n",
      "Epoch [1/1], Step [5677/8897], Loss: 5.0340\n",
      "Epoch [1/1], Step [5678/8897], Loss: 5.4831\n",
      "Epoch [1/1], Step [5679/8897], Loss: 5.1881\n",
      "Epoch [1/1], Step [5680/8897], Loss: 5.5020\n",
      "Epoch [1/1], Step [5681/8897], Loss: 5.3849\n",
      "Epoch [1/1], Step [5682/8897], Loss: 5.3121\n",
      "Epoch [1/1], Step [5683/8897], Loss: 5.1683\n",
      "Epoch [1/1], Step [5684/8897], Loss: 5.2318\n",
      "Epoch [1/1], Step [5685/8897], Loss: 5.3383\n",
      "Epoch [1/1], Step [5686/8897], Loss: 5.4061\n",
      "Epoch [1/1], Step [5687/8897], Loss: 5.2577\n",
      "Epoch [1/1], Step [5688/8897], Loss: 5.3489\n",
      "Epoch [1/1], Step [5689/8897], Loss: 5.5713\n",
      "Epoch [1/1], Step [5690/8897], Loss: 5.3820\n",
      "Epoch [1/1], Step [5691/8897], Loss: 5.3046\n",
      "Epoch [1/1], Step [5692/8897], Loss: 5.2464\n",
      "Epoch [1/1], Step [5693/8897], Loss: 5.3661\n",
      "Epoch [1/1], Step [5694/8897], Loss: 5.5672\n",
      "Epoch [1/1], Step [5695/8897], Loss: 5.3896\n",
      "Epoch [1/1], Step [5696/8897], Loss: 5.2898\n",
      "Epoch [1/1], Step [5697/8897], Loss: 5.4538\n",
      "Epoch [1/1], Step [5698/8897], Loss: 5.3727\n",
      "Epoch [1/1], Step [5699/8897], Loss: 5.5958\n",
      "Epoch [1/1], Step [5700/8897], Loss: 5.4668\n",
      "Epoch [1/1], Step [5701/8897], Loss: 5.5624\n",
      "Epoch [1/1], Step [5702/8897], Loss: 5.5117\n",
      "Epoch [1/1], Step [5703/8897], Loss: 5.0892\n",
      "Epoch [1/1], Step [5704/8897], Loss: 5.1762\n",
      "Epoch [1/1], Step [5705/8897], Loss: 5.6312\n",
      "Epoch [1/1], Step [5706/8897], Loss: 5.2114\n",
      "Epoch [1/1], Step [5707/8897], Loss: 5.5044\n",
      "Epoch [1/1], Step [5708/8897], Loss: 5.4288\n",
      "Epoch [1/1], Step [5709/8897], Loss: 5.3828\n",
      "Epoch [1/1], Step [5710/8897], Loss: 5.4530\n",
      "Epoch [1/1], Step [5711/8897], Loss: 5.4486\n",
      "Epoch [1/1], Step [5712/8897], Loss: 5.4434\n",
      "Epoch [1/1], Step [5713/8897], Loss: 5.3672\n",
      "Epoch [1/1], Step [5714/8897], Loss: 5.0453\n",
      "Epoch [1/1], Step [5715/8897], Loss: 5.3188\n",
      "Epoch [1/1], Step [5716/8897], Loss: 5.7493\n",
      "Epoch [1/1], Step [5717/8897], Loss: 5.6611\n",
      "Epoch [1/1], Step [5718/8897], Loss: 5.8277\n",
      "Epoch [1/1], Step [5719/8897], Loss: 5.3955\n",
      "Epoch [1/1], Step [5720/8897], Loss: 5.5577\n",
      "Epoch [1/1], Step [5721/8897], Loss: 5.3842\n",
      "Epoch [1/1], Step [5722/8897], Loss: 5.2477\n",
      "Epoch [1/1], Step [5723/8897], Loss: 5.2635\n",
      "Epoch [1/1], Step [5724/8897], Loss: 5.3252\n",
      "Epoch [1/1], Step [5725/8897], Loss: 5.3916\n",
      "Epoch [1/1], Step [5726/8897], Loss: 5.4379\n",
      "Epoch [1/1], Step [5727/8897], Loss: 5.5305\n",
      "Epoch [1/1], Step [5728/8897], Loss: 5.5203\n",
      "Epoch [1/1], Step [5729/8897], Loss: 5.1556\n",
      "Epoch [1/1], Step [5730/8897], Loss: 5.4388\n",
      "Epoch [1/1], Step [5731/8897], Loss: 5.5588\n",
      "Epoch [1/1], Step [5732/8897], Loss: 5.2356\n",
      "Epoch [1/1], Step [5733/8897], Loss: 5.4510\n",
      "Epoch [1/1], Step [5734/8897], Loss: 5.4389\n",
      "Epoch [1/1], Step [5735/8897], Loss: 5.3019\n",
      "Epoch [1/1], Step [5736/8897], Loss: 5.3397\n",
      "Epoch [1/1], Step [5737/8897], Loss: 5.3673\n",
      "Epoch [1/1], Step [5738/8897], Loss: 5.3735\n",
      "Epoch [1/1], Step [5739/8897], Loss: 5.3936\n",
      "Epoch [1/1], Step [5740/8897], Loss: 5.5405\n",
      "Epoch [1/1], Step [5741/8897], Loss: 5.6486\n",
      "Epoch [1/1], Step [5742/8897], Loss: 5.5962\n",
      "Epoch [1/1], Step [5743/8897], Loss: 5.4458\n",
      "Epoch [1/1], Step [5744/8897], Loss: 5.1725\n",
      "Epoch [1/1], Step [5745/8897], Loss: 5.3520\n",
      "Epoch [1/1], Step [5746/8897], Loss: 5.3945\n",
      "Epoch [1/1], Step [5747/8897], Loss: 5.2007\n",
      "Epoch [1/1], Step [5748/8897], Loss: 5.1723\n",
      "Epoch [1/1], Step [5749/8897], Loss: 5.2440\n",
      "Epoch [1/1], Step [5750/8897], Loss: 5.2755\n",
      "Epoch [1/1], Step [5751/8897], Loss: 5.2364\n",
      "Epoch [1/1], Step [5752/8897], Loss: 5.4733\n",
      "Epoch [1/1], Step [5753/8897], Loss: 5.5827\n",
      "Epoch [1/1], Step [5754/8897], Loss: 5.4033\n",
      "Epoch [1/1], Step [5755/8897], Loss: 5.3159\n",
      "Epoch [1/1], Step [5756/8897], Loss: 5.3326\n",
      "Epoch [1/1], Step [5757/8897], Loss: 5.4479\n",
      "Epoch [1/1], Step [5758/8897], Loss: 5.7600\n",
      "Epoch [1/1], Step [5759/8897], Loss: 5.7300\n",
      "Epoch [1/1], Step [5760/8897], Loss: 5.4478\n",
      "Epoch [1/1], Step [5761/8897], Loss: 5.3999\n",
      "Epoch [1/1], Step [5762/8897], Loss: 5.5313\n",
      "Epoch [1/1], Step [5763/8897], Loss: 5.5079\n",
      "Epoch [1/1], Step [5764/8897], Loss: 5.2039\n",
      "Epoch [1/1], Step [5765/8897], Loss: 5.3031\n",
      "Epoch [1/1], Step [5766/8897], Loss: 5.4752\n",
      "Epoch [1/1], Step [5767/8897], Loss: 5.3699\n",
      "Epoch [1/1], Step [5768/8897], Loss: 5.3993\n",
      "Epoch [1/1], Step [5769/8897], Loss: 5.5010\n",
      "Epoch [1/1], Step [5770/8897], Loss: 5.3756\n",
      "Epoch [1/1], Step [5771/8897], Loss: 5.4947\n",
      "Epoch [1/1], Step [5772/8897], Loss: 5.4697\n",
      "Epoch [1/1], Step [5773/8897], Loss: 5.5512\n",
      "Epoch [1/1], Step [5774/8897], Loss: 5.1460\n",
      "Epoch [1/1], Step [5775/8897], Loss: 5.0918\n",
      "Epoch [1/1], Step [5776/8897], Loss: 5.3723\n",
      "Epoch [1/1], Step [5777/8897], Loss: 5.2978\n",
      "Epoch [1/1], Step [5778/8897], Loss: 5.3167\n",
      "Epoch [1/1], Step [5779/8897], Loss: 5.3743\n",
      "Epoch [1/1], Step [5780/8897], Loss: 5.3986\n",
      "Epoch [1/1], Step [5781/8897], Loss: 5.2760\n",
      "Epoch [1/1], Step [5782/8897], Loss: 5.6105\n",
      "Epoch [1/1], Step [5783/8897], Loss: 5.2248\n",
      "Epoch [1/1], Step [5784/8897], Loss: 5.4452\n",
      "Epoch [1/1], Step [5785/8897], Loss: 5.6630\n",
      "Epoch [1/1], Step [5786/8897], Loss: 5.5711\n",
      "Epoch [1/1], Step [5787/8897], Loss: 5.4296\n",
      "Epoch [1/1], Step [5788/8897], Loss: 5.3691\n",
      "Epoch [1/1], Step [5789/8897], Loss: 5.2769\n",
      "Epoch [1/1], Step [5790/8897], Loss: 5.4497\n",
      "Epoch [1/1], Step [5791/8897], Loss: 5.4299\n",
      "Epoch [1/1], Step [5792/8897], Loss: 5.1455\n",
      "Epoch [1/1], Step [5793/8897], Loss: 5.3099\n",
      "Epoch [1/1], Step [5794/8897], Loss: 5.4158\n",
      "Epoch [1/1], Step [5795/8897], Loss: 5.0839\n",
      "Epoch [1/1], Step [5796/8897], Loss: 5.2241\n",
      "Epoch [1/1], Step [5797/8897], Loss: 5.3244\n",
      "Epoch [1/1], Step [5798/8897], Loss: 5.5131\n",
      "Epoch [1/1], Step [5799/8897], Loss: 5.6484\n",
      "Epoch [1/1], Step [5800/8897], Loss: 5.3193\n",
      "Epoch [1/1], Step [5801/8897], Loss: 5.4258\n",
      "Epoch [1/1], Step [5802/8897], Loss: 5.0751\n",
      "Epoch [1/1], Step [5803/8897], Loss: 5.3748\n",
      "Epoch [1/1], Step [5804/8897], Loss: 5.5407\n",
      "Epoch [1/1], Step [5805/8897], Loss: 5.4818\n",
      "Epoch [1/1], Step [5806/8897], Loss: 5.1579\n",
      "Epoch [1/1], Step [5807/8897], Loss: 5.4729\n",
      "Epoch [1/1], Step [5808/8897], Loss: 5.3592\n",
      "Epoch [1/1], Step [5809/8897], Loss: 5.6308\n",
      "Epoch [1/1], Step [5810/8897], Loss: 5.3815\n",
      "Epoch [1/1], Step [5811/8897], Loss: 5.4773\n",
      "Epoch [1/1], Step [5812/8897], Loss: 5.5095\n",
      "Epoch [1/1], Step [5813/8897], Loss: 5.3784\n",
      "Epoch [1/1], Step [5814/8897], Loss: 5.4324\n",
      "Epoch [1/1], Step [5815/8897], Loss: 5.3651\n",
      "Epoch [1/1], Step [5816/8897], Loss: 5.5624\n",
      "Epoch [1/1], Step [5817/8897], Loss: 5.4320\n",
      "Epoch [1/1], Step [5818/8897], Loss: 5.5297\n",
      "Epoch [1/1], Step [5819/8897], Loss: 5.4952\n",
      "Epoch [1/1], Step [5820/8897], Loss: 5.4392\n",
      "Epoch [1/1], Step [5821/8897], Loss: 5.3127\n",
      "Epoch [1/1], Step [5822/8897], Loss: 5.5336\n",
      "Epoch [1/1], Step [5823/8897], Loss: 5.0709\n",
      "Epoch [1/1], Step [5824/8897], Loss: 5.3550\n",
      "Epoch [1/1], Step [5825/8897], Loss: 5.4981\n",
      "Epoch [1/1], Step [5826/8897], Loss: 5.4853\n",
      "Epoch [1/1], Step [5827/8897], Loss: 5.6199\n",
      "Epoch [1/1], Step [5828/8897], Loss: 5.2675\n",
      "Epoch [1/1], Step [5829/8897], Loss: 5.4830\n",
      "Epoch [1/1], Step [5830/8897], Loss: 5.3357\n",
      "Epoch [1/1], Step [5831/8897], Loss: 5.4818\n",
      "Epoch [1/1], Step [5832/8897], Loss: 5.3345\n",
      "Epoch [1/1], Step [5833/8897], Loss: 5.2853\n",
      "Epoch [1/1], Step [5834/8897], Loss: 5.0028\n",
      "Epoch [1/1], Step [5835/8897], Loss: 5.5939\n",
      "Epoch [1/1], Step [5836/8897], Loss: 5.2147\n",
      "Epoch [1/1], Step [5837/8897], Loss: 5.4521\n",
      "Epoch [1/1], Step [5838/8897], Loss: 5.1675\n",
      "Epoch [1/1], Step [5839/8897], Loss: 5.2494\n",
      "Epoch [1/1], Step [5840/8897], Loss: 5.5160\n",
      "Epoch [1/1], Step [5841/8897], Loss: 5.7950\n",
      "Epoch [1/1], Step [5842/8897], Loss: 5.3684\n",
      "Epoch [1/1], Step [5843/8897], Loss: 5.4421\n",
      "Epoch [1/1], Step [5844/8897], Loss: 5.3347\n",
      "Epoch [1/1], Step [5845/8897], Loss: 5.7217\n",
      "Epoch [1/1], Step [5846/8897], Loss: 5.2727\n",
      "Epoch [1/1], Step [5847/8897], Loss: 5.1529\n",
      "Epoch [1/1], Step [5848/8897], Loss: 5.4101\n",
      "Epoch [1/1], Step [5849/8897], Loss: 5.4481\n",
      "Epoch [1/1], Step [5850/8897], Loss: 5.7238\n",
      "Epoch [1/1], Step [5851/8897], Loss: 5.3074\n",
      "Epoch [1/1], Step [5852/8897], Loss: 5.4579\n",
      "Epoch [1/1], Step [5853/8897], Loss: 5.3545\n",
      "Epoch [1/1], Step [5854/8897], Loss: 5.2971\n",
      "Epoch [1/1], Step [5855/8897], Loss: 5.4226\n",
      "Epoch [1/1], Step [5856/8897], Loss: 5.5370\n",
      "Epoch [1/1], Step [5857/8897], Loss: 5.2520\n",
      "Epoch [1/1], Step [5858/8897], Loss: 5.2407\n",
      "Epoch [1/1], Step [5859/8897], Loss: 5.1500\n",
      "Epoch [1/1], Step [5860/8897], Loss: 5.3600\n",
      "Epoch [1/1], Step [5861/8897], Loss: 5.3076\n",
      "Epoch [1/1], Step [5862/8897], Loss: 5.1733\n",
      "Epoch [1/1], Step [5863/8897], Loss: 5.2850\n",
      "Epoch [1/1], Step [5864/8897], Loss: 5.2457\n",
      "Epoch [1/1], Step [5865/8897], Loss: 5.3558\n",
      "Epoch [1/1], Step [5866/8897], Loss: 5.1843\n",
      "Epoch [1/1], Step [5867/8897], Loss: 5.2768\n",
      "Epoch [1/1], Step [5868/8897], Loss: 5.2339\n",
      "Epoch [1/1], Step [5869/8897], Loss: 5.3834\n",
      "Epoch [1/1], Step [5870/8897], Loss: 5.4876\n",
      "Epoch [1/1], Step [5871/8897], Loss: 5.4645\n",
      "Epoch [1/1], Step [5872/8897], Loss: 5.0756\n",
      "Epoch [1/1], Step [5873/8897], Loss: 5.3060\n",
      "Epoch [1/1], Step [5874/8897], Loss: 5.2079\n",
      "Epoch [1/1], Step [5875/8897], Loss: 5.3555\n",
      "Epoch [1/1], Step [5876/8897], Loss: 5.3504\n",
      "Epoch [1/1], Step [5877/8897], Loss: 5.5944\n",
      "Epoch [1/1], Step [5878/8897], Loss: 5.4097\n",
      "Epoch [1/1], Step [5879/8897], Loss: 5.3318\n",
      "Epoch [1/1], Step [5880/8897], Loss: 5.3992\n",
      "Epoch [1/1], Step [5881/8897], Loss: 5.3010\n",
      "Epoch [1/1], Step [5882/8897], Loss: 5.5310\n",
      "Epoch [1/1], Step [5883/8897], Loss: 5.0850\n",
      "Epoch [1/1], Step [5884/8897], Loss: 5.4517\n",
      "Epoch [1/1], Step [5885/8897], Loss: 5.5118\n",
      "Epoch [1/1], Step [5886/8897], Loss: 5.5752\n",
      "Epoch [1/1], Step [5887/8897], Loss: 5.2760\n",
      "Epoch [1/1], Step [5888/8897], Loss: 5.5795\n",
      "Epoch [1/1], Step [5889/8897], Loss: 5.2287\n",
      "Epoch [1/1], Step [5890/8897], Loss: 5.4214\n",
      "Epoch [1/1], Step [5891/8897], Loss: 5.5362\n",
      "Epoch [1/1], Step [5892/8897], Loss: 5.3938\n",
      "Epoch [1/1], Step [5893/8897], Loss: 5.6782\n",
      "Epoch [1/1], Step [5894/8897], Loss: 5.3697\n",
      "Epoch [1/1], Step [5895/8897], Loss: 5.2722\n",
      "Epoch [1/1], Step [5896/8897], Loss: 5.3672\n",
      "Epoch [1/1], Step [5897/8897], Loss: 5.6695\n",
      "Epoch [1/1], Step [5898/8897], Loss: 5.3838\n",
      "Epoch [1/1], Step [5899/8897], Loss: 5.2191\n",
      "Epoch [1/1], Step [5900/8897], Loss: 5.4248\n",
      "Epoch [1/1], Step [5901/8897], Loss: 5.4405\n",
      "Epoch [1/1], Step [5902/8897], Loss: 5.2227\n",
      "Epoch [1/1], Step [5903/8897], Loss: 5.4159\n",
      "Epoch [1/1], Step [5904/8897], Loss: 5.2332\n",
      "Epoch [1/1], Step [5905/8897], Loss: 5.2005\n",
      "Epoch [1/1], Step [5906/8897], Loss: 5.3572\n",
      "Epoch [1/1], Step [5907/8897], Loss: 5.5130\n",
      "Epoch [1/1], Step [5908/8897], Loss: 5.3035\n",
      "Epoch [1/1], Step [5909/8897], Loss: 5.4581\n",
      "Epoch [1/1], Step [5910/8897], Loss: 5.5596\n",
      "Epoch [1/1], Step [5911/8897], Loss: 5.3058\n",
      "Epoch [1/1], Step [5912/8897], Loss: 5.2524\n",
      "Epoch [1/1], Step [5913/8897], Loss: 5.3972\n",
      "Epoch [1/1], Step [5914/8897], Loss: 5.4330\n",
      "Epoch [1/1], Step [5915/8897], Loss: 5.4005\n",
      "Epoch [1/1], Step [5916/8897], Loss: 5.3418\n",
      "Epoch [1/1], Step [5917/8897], Loss: 5.4502\n",
      "Epoch [1/1], Step [5918/8897], Loss: 5.6213\n",
      "Epoch [1/1], Step [5919/8897], Loss: 5.3165\n",
      "Epoch [1/1], Step [5920/8897], Loss: 5.2799\n",
      "Epoch [1/1], Step [5921/8897], Loss: 5.4578\n",
      "Epoch [1/1], Step [5922/8897], Loss: 5.5721\n",
      "Epoch [1/1], Step [5923/8897], Loss: 5.6078\n",
      "Epoch [1/1], Step [5924/8897], Loss: 5.4928\n",
      "Epoch [1/1], Step [5925/8897], Loss: 5.3606\n",
      "Epoch [1/1], Step [5926/8897], Loss: 5.1285\n",
      "Epoch [1/1], Step [5927/8897], Loss: 5.3532\n",
      "Epoch [1/1], Step [5928/8897], Loss: 5.1481\n",
      "Epoch [1/1], Step [5929/8897], Loss: 5.3659\n",
      "Epoch [1/1], Step [5930/8897], Loss: 5.4413\n",
      "Epoch [1/1], Step [5931/8897], Loss: 5.4981\n",
      "Epoch [1/1], Step [5932/8897], Loss: 5.3933\n",
      "Epoch [1/1], Step [5933/8897], Loss: 5.2903\n",
      "Epoch [1/1], Step [5934/8897], Loss: 5.2260\n",
      "Epoch [1/1], Step [5935/8897], Loss: 5.5570\n",
      "Epoch [1/1], Step [5936/8897], Loss: 5.3457\n",
      "Epoch [1/1], Step [5937/8897], Loss: 5.5006\n",
      "Epoch [1/1], Step [5938/8897], Loss: 5.1317\n",
      "Epoch [1/1], Step [5939/8897], Loss: 5.3406\n",
      "Epoch [1/1], Step [5940/8897], Loss: 5.4255\n",
      "Epoch [1/1], Step [5941/8897], Loss: 5.3918\n",
      "Epoch [1/1], Step [5942/8897], Loss: 5.5032\n",
      "Epoch [1/1], Step [5943/8897], Loss: 5.2403\n",
      "Epoch [1/1], Step [5944/8897], Loss: 5.2413\n",
      "Epoch [1/1], Step [5945/8897], Loss: 5.3680\n",
      "Epoch [1/1], Step [5946/8897], Loss: 5.1677\n",
      "Epoch [1/1], Step [5947/8897], Loss: 5.0055\n",
      "Epoch [1/1], Step [5948/8897], Loss: 5.2884\n",
      "Epoch [1/1], Step [5949/8897], Loss: 5.3171\n",
      "Epoch [1/1], Step [5950/8897], Loss: 5.3131\n",
      "Epoch [1/1], Step [5951/8897], Loss: 5.3869\n",
      "Epoch [1/1], Step [5952/8897], Loss: 5.4748\n",
      "Epoch [1/1], Step [5953/8897], Loss: 5.5893\n",
      "Epoch [1/1], Step [5954/8897], Loss: 5.0736\n",
      "Epoch [1/1], Step [5955/8897], Loss: 5.3660\n",
      "Epoch [1/1], Step [5956/8897], Loss: 5.2937\n",
      "Epoch [1/1], Step [5957/8897], Loss: 5.6143\n",
      "Epoch [1/1], Step [5958/8897], Loss: 5.2466\n",
      "Epoch [1/1], Step [5959/8897], Loss: 5.3555\n",
      "Epoch [1/1], Step [5960/8897], Loss: 5.3376\n",
      "Epoch [1/1], Step [5961/8897], Loss: 5.3098\n",
      "Epoch [1/1], Step [5962/8897], Loss: 5.3026\n",
      "Epoch [1/1], Step [5963/8897], Loss: 5.5535\n",
      "Epoch [1/1], Step [5964/8897], Loss: 5.4050\n",
      "Epoch [1/1], Step [5965/8897], Loss: 5.2283\n",
      "Epoch [1/1], Step [5966/8897], Loss: 5.1122\n",
      "Epoch [1/1], Step [5967/8897], Loss: 5.3868\n",
      "Epoch [1/1], Step [5968/8897], Loss: 4.9512\n",
      "Epoch [1/1], Step [5969/8897], Loss: 5.3399\n",
      "Epoch [1/1], Step [5970/8897], Loss: 5.6747\n",
      "Epoch [1/1], Step [5971/8897], Loss: 5.6817\n",
      "Epoch [1/1], Step [5972/8897], Loss: 5.2323\n",
      "Epoch [1/1], Step [5973/8897], Loss: 5.3147\n",
      "Epoch [1/1], Step [5974/8897], Loss: 5.4578\n",
      "Epoch [1/1], Step [5975/8897], Loss: 5.3233\n",
      "Epoch [1/1], Step [5976/8897], Loss: 5.2333\n",
      "Epoch [1/1], Step [5977/8897], Loss: 5.2601\n",
      "Epoch [1/1], Step [5978/8897], Loss: 5.3772\n",
      "Epoch [1/1], Step [5979/8897], Loss: 5.5588\n",
      "Epoch [1/1], Step [5980/8897], Loss: 5.4221\n",
      "Epoch [1/1], Step [5981/8897], Loss: 5.1887\n",
      "Epoch [1/1], Step [5982/8897], Loss: 5.3842\n",
      "Epoch [1/1], Step [5983/8897], Loss: 5.2686\n",
      "Epoch [1/1], Step [5984/8897], Loss: 5.3829\n",
      "Epoch [1/1], Step [5985/8897], Loss: 5.1476\n",
      "Epoch [1/1], Step [5986/8897], Loss: 5.2794\n",
      "Epoch [1/1], Step [5987/8897], Loss: 5.5672\n",
      "Epoch [1/1], Step [5988/8897], Loss: 5.4678\n",
      "Epoch [1/1], Step [5989/8897], Loss: 5.1917\n",
      "Epoch [1/1], Step [5990/8897], Loss: 5.1245\n",
      "Epoch [1/1], Step [5991/8897], Loss: 5.3077\n",
      "Epoch [1/1], Step [5992/8897], Loss: 5.6047\n",
      "Epoch [1/1], Step [5993/8897], Loss: 5.4077\n",
      "Epoch [1/1], Step [5994/8897], Loss: 5.4816\n",
      "Epoch [1/1], Step [5995/8897], Loss: 5.5170\n",
      "Epoch [1/1], Step [5996/8897], Loss: 5.2489\n",
      "Epoch [1/1], Step [5997/8897], Loss: 5.5785\n",
      "Epoch [1/1], Step [5998/8897], Loss: 5.2205\n",
      "Epoch [1/1], Step [5999/8897], Loss: 5.1284\n",
      "Epoch [1/1], Step [6000/8897], Loss: 5.1709\n",
      "Epoch [1/1], Step [6001/8897], Loss: 5.7822\n",
      "Epoch [1/1], Step [6002/8897], Loss: 5.3229\n",
      "Epoch [1/1], Step [6003/8897], Loss: 5.5817\n",
      "Epoch [1/1], Step [6004/8897], Loss: 5.2989\n",
      "Epoch [1/1], Step [6005/8897], Loss: 5.1941\n",
      "Epoch [1/1], Step [6006/8897], Loss: 5.2516\n",
      "Epoch [1/1], Step [6007/8897], Loss: 5.1701\n",
      "Epoch [1/1], Step [6008/8897], Loss: 5.3097\n",
      "Epoch [1/1], Step [6009/8897], Loss: 5.2551\n",
      "Epoch [1/1], Step [6010/8897], Loss: 5.4028\n",
      "Epoch [1/1], Step [6011/8897], Loss: 5.3470\n",
      "Epoch [1/1], Step [6012/8897], Loss: 5.2100\n",
      "Epoch [1/1], Step [6013/8897], Loss: 5.2549\n",
      "Epoch [1/1], Step [6014/8897], Loss: 5.2536\n",
      "Epoch [1/1], Step [6015/8897], Loss: 5.4830\n",
      "Epoch [1/1], Step [6016/8897], Loss: 4.9626\n",
      "Epoch [1/1], Step [6017/8897], Loss: 5.2868\n",
      "Epoch [1/1], Step [6018/8897], Loss: 5.2475\n",
      "Epoch [1/1], Step [6019/8897], Loss: 5.3895\n",
      "Epoch [1/1], Step [6020/8897], Loss: 5.3924\n",
      "Epoch [1/1], Step [6021/8897], Loss: 5.3964\n",
      "Epoch [1/1], Step [6022/8897], Loss: 5.3860\n",
      "Epoch [1/1], Step [6023/8897], Loss: 5.4295\n",
      "Epoch [1/1], Step [6024/8897], Loss: 5.4080\n",
      "Epoch [1/1], Step [6025/8897], Loss: 5.3217\n",
      "Epoch [1/1], Step [6026/8897], Loss: 5.3090\n",
      "Epoch [1/1], Step [6027/8897], Loss: 5.6680\n",
      "Epoch [1/1], Step [6028/8897], Loss: 5.4129\n",
      "Epoch [1/1], Step [6029/8897], Loss: 5.2465\n",
      "Epoch [1/1], Step [6030/8897], Loss: 5.3013\n",
      "Epoch [1/1], Step [6031/8897], Loss: 5.2374\n",
      "Epoch [1/1], Step [6032/8897], Loss: 5.3631\n",
      "Epoch [1/1], Step [6033/8897], Loss: 5.4732\n",
      "Epoch [1/1], Step [6034/8897], Loss: 5.4908\n",
      "Epoch [1/1], Step [6035/8897], Loss: 5.3688\n",
      "Epoch [1/1], Step [6036/8897], Loss: 5.3428\n",
      "Epoch [1/1], Step [6037/8897], Loss: 5.2940\n",
      "Epoch [1/1], Step [6038/8897], Loss: 5.6356\n",
      "Epoch [1/1], Step [6039/8897], Loss: 5.5805\n",
      "Epoch [1/1], Step [6040/8897], Loss: 5.3547\n",
      "Epoch [1/1], Step [6041/8897], Loss: 5.3027\n",
      "Epoch [1/1], Step [6042/8897], Loss: 5.2986\n",
      "Epoch [1/1], Step [6043/8897], Loss: 5.1696\n",
      "Epoch [1/1], Step [6044/8897], Loss: 5.2800\n",
      "Epoch [1/1], Step [6045/8897], Loss: 5.5545\n",
      "Epoch [1/1], Step [6046/8897], Loss: 5.3982\n",
      "Epoch [1/1], Step [6047/8897], Loss: 5.3533\n",
      "Epoch [1/1], Step [6048/8897], Loss: 5.4239\n",
      "Epoch [1/1], Step [6049/8897], Loss: 5.4051\n",
      "Epoch [1/1], Step [6050/8897], Loss: 5.2902\n",
      "Epoch [1/1], Step [6051/8897], Loss: 5.5988\n",
      "Epoch [1/1], Step [6052/8897], Loss: 5.0877\n",
      "Epoch [1/1], Step [6053/8897], Loss: 5.7140\n",
      "Epoch [1/1], Step [6054/8897], Loss: 5.4446\n",
      "Epoch [1/1], Step [6055/8897], Loss: 5.3144\n",
      "Epoch [1/1], Step [6056/8897], Loss: 5.2468\n",
      "Epoch [1/1], Step [6057/8897], Loss: 5.1396\n",
      "Epoch [1/1], Step [6058/8897], Loss: 5.1879\n",
      "Epoch [1/1], Step [6059/8897], Loss: 5.3747\n",
      "Epoch [1/1], Step [6060/8897], Loss: 5.4272\n",
      "Epoch [1/1], Step [6061/8897], Loss: 5.5980\n",
      "Epoch [1/1], Step [6062/8897], Loss: 5.4263\n",
      "Epoch [1/1], Step [6063/8897], Loss: 5.3111\n",
      "Epoch [1/1], Step [6064/8897], Loss: 5.4824\n",
      "Epoch [1/1], Step [6065/8897], Loss: 5.3396\n",
      "Epoch [1/1], Step [6066/8897], Loss: 5.2005\n",
      "Epoch [1/1], Step [6067/8897], Loss: 5.2220\n",
      "Epoch [1/1], Step [6068/8897], Loss: 5.4782\n",
      "Epoch [1/1], Step [6069/8897], Loss: 5.2207\n",
      "Epoch [1/1], Step [6070/8897], Loss: 5.2046\n",
      "Epoch [1/1], Step [6071/8897], Loss: 5.3695\n",
      "Epoch [1/1], Step [6072/8897], Loss: 5.3111\n",
      "Epoch [1/1], Step [6073/8897], Loss: 5.2562\n",
      "Epoch [1/1], Step [6074/8897], Loss: 5.4060\n",
      "Epoch [1/1], Step [6075/8897], Loss: 5.4509\n",
      "Epoch [1/1], Step [6076/8897], Loss: 5.6995\n",
      "Epoch [1/1], Step [6077/8897], Loss: 5.3433\n",
      "Epoch [1/1], Step [6078/8897], Loss: 5.1015\n",
      "Epoch [1/1], Step [6079/8897], Loss: 5.3739\n",
      "Epoch [1/1], Step [6080/8897], Loss: 5.6866\n",
      "Epoch [1/1], Step [6081/8897], Loss: 5.5015\n",
      "Epoch [1/1], Step [6082/8897], Loss: 5.3189\n",
      "Epoch [1/1], Step [6083/8897], Loss: 5.2778\n",
      "Epoch [1/1], Step [6084/8897], Loss: 5.2403\n",
      "Epoch [1/1], Step [6085/8897], Loss: 5.3248\n",
      "Epoch [1/1], Step [6086/8897], Loss: 5.3455\n",
      "Epoch [1/1], Step [6087/8897], Loss: 5.4586\n",
      "Epoch [1/1], Step [6088/8897], Loss: 5.6152\n",
      "Epoch [1/1], Step [6089/8897], Loss: 5.3794\n",
      "Epoch [1/1], Step [6090/8897], Loss: 5.2698\n",
      "Epoch [1/1], Step [6091/8897], Loss: 5.3939\n",
      "Epoch [1/1], Step [6092/8897], Loss: 5.3616\n",
      "Epoch [1/1], Step [6093/8897], Loss: 5.4092\n",
      "Epoch [1/1], Step [6094/8897], Loss: 5.4000\n",
      "Epoch [1/1], Step [6095/8897], Loss: 5.4358\n",
      "Epoch [1/1], Step [6096/8897], Loss: 5.5618\n",
      "Epoch [1/1], Step [6097/8897], Loss: 5.5159\n",
      "Epoch [1/1], Step [6098/8897], Loss: 5.4095\n",
      "Epoch [1/1], Step [6099/8897], Loss: 5.2303\n",
      "Epoch [1/1], Step [6100/8897], Loss: 5.3873\n",
      "Epoch [1/1], Step [6101/8897], Loss: 5.2783\n",
      "Epoch [1/1], Step [6102/8897], Loss: 5.3871\n",
      "Epoch [1/1], Step [6103/8897], Loss: 5.3596\n",
      "Epoch [1/1], Step [6104/8897], Loss: 5.4742\n",
      "Epoch [1/1], Step [6105/8897], Loss: 5.4221\n",
      "Epoch [1/1], Step [6106/8897], Loss: 5.3509\n",
      "Epoch [1/1], Step [6107/8897], Loss: 5.3104\n",
      "Epoch [1/1], Step [6108/8897], Loss: 5.3440\n",
      "Epoch [1/1], Step [6109/8897], Loss: 5.1928\n",
      "Epoch [1/1], Step [6110/8897], Loss: 5.2515\n",
      "Epoch [1/1], Step [6111/8897], Loss: 5.2282\n",
      "Epoch [1/1], Step [6112/8897], Loss: 5.4493\n",
      "Epoch [1/1], Step [6113/8897], Loss: 5.3458\n",
      "Epoch [1/1], Step [6114/8897], Loss: 5.6351\n",
      "Epoch [1/1], Step [6115/8897], Loss: 5.2036\n",
      "Epoch [1/1], Step [6116/8897], Loss: 5.1983\n",
      "Epoch [1/1], Step [6117/8897], Loss: 5.1801\n",
      "Epoch [1/1], Step [6118/8897], Loss: 5.5103\n",
      "Epoch [1/1], Step [6119/8897], Loss: 5.4596\n",
      "Epoch [1/1], Step [6120/8897], Loss: 5.4521\n",
      "Epoch [1/1], Step [6121/8897], Loss: 5.4641\n",
      "Epoch [1/1], Step [6122/8897], Loss: 5.3606\n",
      "Epoch [1/1], Step [6123/8897], Loss: 5.4794\n",
      "Epoch [1/1], Step [6124/8897], Loss: 5.2370\n",
      "Epoch [1/1], Step [6125/8897], Loss: 5.3914\n",
      "Epoch [1/1], Step [6126/8897], Loss: 5.2171\n",
      "Epoch [1/1], Step [6127/8897], Loss: 5.2244\n",
      "Epoch [1/1], Step [6128/8897], Loss: 5.2442\n",
      "Epoch [1/1], Step [6129/8897], Loss: 5.4349\n",
      "Epoch [1/1], Step [6130/8897], Loss: 5.5059\n",
      "Epoch [1/1], Step [6131/8897], Loss: 5.2619\n",
      "Epoch [1/1], Step [6132/8897], Loss: 5.2486\n",
      "Epoch [1/1], Step [6133/8897], Loss: 5.5251\n",
      "Epoch [1/1], Step [6134/8897], Loss: 5.5796\n",
      "Epoch [1/1], Step [6135/8897], Loss: 5.4159\n",
      "Epoch [1/1], Step [6136/8897], Loss: 5.4374\n",
      "Epoch [1/1], Step [6137/8897], Loss: 5.3563\n",
      "Epoch [1/1], Step [6138/8897], Loss: 5.1755\n",
      "Epoch [1/1], Step [6139/8897], Loss: 5.2963\n",
      "Epoch [1/1], Step [6140/8897], Loss: 5.2422\n",
      "Epoch [1/1], Step [6141/8897], Loss: 5.4548\n",
      "Epoch [1/1], Step [6142/8897], Loss: 5.2468\n",
      "Epoch [1/1], Step [6143/8897], Loss: 5.3924\n",
      "Epoch [1/1], Step [6144/8897], Loss: 5.8199\n",
      "Epoch [1/1], Step [6145/8897], Loss: 5.3609\n",
      "Epoch [1/1], Step [6146/8897], Loss: 5.2530\n",
      "Epoch [1/1], Step [6147/8897], Loss: 5.3022\n",
      "Epoch [1/1], Step [6148/8897], Loss: 5.3123\n",
      "Epoch [1/1], Step [6149/8897], Loss: 5.5100\n",
      "Epoch [1/1], Step [6150/8897], Loss: 5.7941\n",
      "Epoch [1/1], Step [6151/8897], Loss: 5.5669\n",
      "Epoch [1/1], Step [6152/8897], Loss: 5.1928\n",
      "Epoch [1/1], Step [6153/8897], Loss: 5.4583\n",
      "Epoch [1/1], Step [6154/8897], Loss: 5.4566\n",
      "Epoch [1/1], Step [6155/8897], Loss: 5.4890\n",
      "Epoch [1/1], Step [6156/8897], Loss: 5.2207\n",
      "Epoch [1/1], Step [6157/8897], Loss: 5.2596\n",
      "Epoch [1/1], Step [6158/8897], Loss: 5.4483\n",
      "Epoch [1/1], Step [6159/8897], Loss: 5.3967\n",
      "Epoch [1/1], Step [6160/8897], Loss: 5.4476\n",
      "Epoch [1/1], Step [6161/8897], Loss: 5.6342\n",
      "Epoch [1/1], Step [6162/8897], Loss: 5.6121\n",
      "Epoch [1/1], Step [6163/8897], Loss: 5.2775\n",
      "Epoch [1/1], Step [6164/8897], Loss: 5.2893\n",
      "Epoch [1/1], Step [6165/8897], Loss: 5.4562\n",
      "Epoch [1/1], Step [6166/8897], Loss: 5.3689\n",
      "Epoch [1/1], Step [6167/8897], Loss: 5.1689\n",
      "Epoch [1/1], Step [6168/8897], Loss: 5.4866\n",
      "Epoch [1/1], Step [6169/8897], Loss: 5.1826\n",
      "Epoch [1/1], Step [6170/8897], Loss: 5.3938\n",
      "Epoch [1/1], Step [6171/8897], Loss: 5.3536\n",
      "Epoch [1/1], Step [6172/8897], Loss: 5.2375\n",
      "Epoch [1/1], Step [6173/8897], Loss: 5.4114\n",
      "Epoch [1/1], Step [6174/8897], Loss: 5.1781\n",
      "Epoch [1/1], Step [6175/8897], Loss: 5.5854\n",
      "Epoch [1/1], Step [6176/8897], Loss: 5.4348\n",
      "Epoch [1/1], Step [6177/8897], Loss: 5.4484\n",
      "Epoch [1/1], Step [6178/8897], Loss: 5.3867\n",
      "Epoch [1/1], Step [6179/8897], Loss: 5.1210\n",
      "Epoch [1/1], Step [6180/8897], Loss: 5.4869\n",
      "Epoch [1/1], Step [6181/8897], Loss: 5.5930\n",
      "Epoch [1/1], Step [6182/8897], Loss: 5.5614\n",
      "Epoch [1/1], Step [6183/8897], Loss: 5.4033\n",
      "Epoch [1/1], Step [6184/8897], Loss: 5.4704\n",
      "Epoch [1/1], Step [6185/8897], Loss: 5.4890\n",
      "Epoch [1/1], Step [6186/8897], Loss: 5.6959\n",
      "Epoch [1/1], Step [6187/8897], Loss: 5.4673\n",
      "Epoch [1/1], Step [6188/8897], Loss: 5.4773\n",
      "Epoch [1/1], Step [6189/8897], Loss: 5.2414\n",
      "Epoch [1/1], Step [6190/8897], Loss: 5.2431\n",
      "Epoch [1/1], Step [6191/8897], Loss: 5.3590\n",
      "Epoch [1/1], Step [6192/8897], Loss: 5.3734\n",
      "Epoch [1/1], Step [6193/8897], Loss: 5.1288\n",
      "Epoch [1/1], Step [6194/8897], Loss: 5.4172\n",
      "Epoch [1/1], Step [6195/8897], Loss: 5.0141\n",
      "Epoch [1/1], Step [6196/8897], Loss: 4.8739\n",
      "Epoch [1/1], Step [6197/8897], Loss: 5.3993\n",
      "Epoch [1/1], Step [6198/8897], Loss: 5.3202\n",
      "Epoch [1/1], Step [6199/8897], Loss: 5.0845\n",
      "Epoch [1/1], Step [6200/8897], Loss: 5.3466\n",
      "Epoch [1/1], Step [6201/8897], Loss: 5.3453\n",
      "Epoch [1/1], Step [6202/8897], Loss: 5.4394\n",
      "Epoch [1/1], Step [6203/8897], Loss: 5.2912\n",
      "Epoch [1/1], Step [6204/8897], Loss: 5.3573\n",
      "Epoch [1/1], Step [6205/8897], Loss: 5.4259\n",
      "Epoch [1/1], Step [6206/8897], Loss: 5.4009\n",
      "Epoch [1/1], Step [6207/8897], Loss: 5.3293\n",
      "Epoch [1/1], Step [6208/8897], Loss: 5.2196\n",
      "Epoch [1/1], Step [6209/8897], Loss: 5.2006\n",
      "Epoch [1/1], Step [6210/8897], Loss: 5.4404\n",
      "Epoch [1/1], Step [6211/8897], Loss: 5.3979\n",
      "Epoch [1/1], Step [6212/8897], Loss: 5.4051\n",
      "Epoch [1/1], Step [6213/8897], Loss: 5.4782\n",
      "Epoch [1/1], Step [6214/8897], Loss: 5.3018\n",
      "Epoch [1/1], Step [6215/8897], Loss: 5.3348\n",
      "Epoch [1/1], Step [6216/8897], Loss: 5.3515\n",
      "Epoch [1/1], Step [6217/8897], Loss: 5.3976\n",
      "Epoch [1/1], Step [6218/8897], Loss: 5.3979\n",
      "Epoch [1/1], Step [6219/8897], Loss: 5.3479\n",
      "Epoch [1/1], Step [6220/8897], Loss: 5.2964\n",
      "Epoch [1/1], Step [6221/8897], Loss: 5.4803\n",
      "Epoch [1/1], Step [6222/8897], Loss: 5.3257\n",
      "Epoch [1/1], Step [6223/8897], Loss: 5.3228\n",
      "Epoch [1/1], Step [6224/8897], Loss: 5.4186\n",
      "Epoch [1/1], Step [6225/8897], Loss: 5.4328\n",
      "Epoch [1/1], Step [6226/8897], Loss: 5.2017\n",
      "Epoch [1/1], Step [6227/8897], Loss: 5.2114\n",
      "Epoch [1/1], Step [6228/8897], Loss: 5.5844\n",
      "Epoch [1/1], Step [6229/8897], Loss: 5.2703\n",
      "Epoch [1/1], Step [6230/8897], Loss: 5.6035\n",
      "Epoch [1/1], Step [6231/8897], Loss: 5.3756\n",
      "Epoch [1/1], Step [6232/8897], Loss: 5.6209\n",
      "Epoch [1/1], Step [6233/8897], Loss: 5.4643\n",
      "Epoch [1/1], Step [6234/8897], Loss: 5.3896\n",
      "Epoch [1/1], Step [6235/8897], Loss: 5.5554\n",
      "Epoch [1/1], Step [6236/8897], Loss: 5.5231\n",
      "Epoch [1/1], Step [6237/8897], Loss: 5.4208\n",
      "Epoch [1/1], Step [6238/8897], Loss: 5.4237\n",
      "Epoch [1/1], Step [6239/8897], Loss: 5.1173\n",
      "Epoch [1/1], Step [6240/8897], Loss: 5.4346\n",
      "Epoch [1/1], Step [6241/8897], Loss: 5.2181\n",
      "Epoch [1/1], Step [6242/8897], Loss: 5.4611\n",
      "Epoch [1/1], Step [6243/8897], Loss: 5.3497\n",
      "Epoch [1/1], Step [6244/8897], Loss: 5.3121\n",
      "Epoch [1/1], Step [6245/8897], Loss: 5.3328\n",
      "Epoch [1/1], Step [6246/8897], Loss: 5.2266\n",
      "Epoch [1/1], Step [6247/8897], Loss: 5.3326\n",
      "Epoch [1/1], Step [6248/8897], Loss: 5.2030\n",
      "Epoch [1/1], Step [6249/8897], Loss: 5.4046\n",
      "Epoch [1/1], Step [6250/8897], Loss: 5.6192\n",
      "Epoch [1/1], Step [6251/8897], Loss: 5.4155\n",
      "Epoch [1/1], Step [6252/8897], Loss: 5.5037\n",
      "Epoch [1/1], Step [6253/8897], Loss: 5.4629\n",
      "Epoch [1/1], Step [6254/8897], Loss: 5.5670\n",
      "Epoch [1/1], Step [6255/8897], Loss: 5.2178\n",
      "Epoch [1/1], Step [6256/8897], Loss: 5.3865\n",
      "Epoch [1/1], Step [6257/8897], Loss: 5.3612\n",
      "Epoch [1/1], Step [6258/8897], Loss: 5.5362\n",
      "Epoch [1/1], Step [6259/8897], Loss: 5.2922\n",
      "Epoch [1/1], Step [6260/8897], Loss: 5.3695\n",
      "Epoch [1/1], Step [6261/8897], Loss: 5.3318\n",
      "Epoch [1/1], Step [6262/8897], Loss: 5.3514\n",
      "Epoch [1/1], Step [6263/8897], Loss: 5.6415\n",
      "Epoch [1/1], Step [6264/8897], Loss: 5.2324\n",
      "Epoch [1/1], Step [6265/8897], Loss: 5.2095\n",
      "Epoch [1/1], Step [6266/8897], Loss: 5.2438\n",
      "Epoch [1/1], Step [6267/8897], Loss: 5.7413\n",
      "Epoch [1/1], Step [6268/8897], Loss: 5.2437\n",
      "Epoch [1/1], Step [6269/8897], Loss: 5.0634\n",
      "Epoch [1/1], Step [6270/8897], Loss: 5.4080\n",
      "Epoch [1/1], Step [6271/8897], Loss: 5.4764\n",
      "Epoch [1/1], Step [6272/8897], Loss: 5.6537\n",
      "Epoch [1/1], Step [6273/8897], Loss: 5.1052\n",
      "Epoch [1/1], Step [6274/8897], Loss: 5.3850\n",
      "Epoch [1/1], Step [6275/8897], Loss: 5.1913\n",
      "Epoch [1/1], Step [6276/8897], Loss: 5.0804\n",
      "Epoch [1/1], Step [6277/8897], Loss: 5.5357\n",
      "Epoch [1/1], Step [6278/8897], Loss: 5.4821\n",
      "Epoch [1/1], Step [6279/8897], Loss: 5.4189\n",
      "Epoch [1/1], Step [6280/8897], Loss: 5.3431\n",
      "Epoch [1/1], Step [6281/8897], Loss: 5.6237\n",
      "Epoch [1/1], Step [6282/8897], Loss: 5.2889\n",
      "Epoch [1/1], Step [6283/8897], Loss: 5.4040\n",
      "Epoch [1/1], Step [6284/8897], Loss: 5.4434\n",
      "Epoch [1/1], Step [6285/8897], Loss: 5.3374\n",
      "Epoch [1/1], Step [6286/8897], Loss: 5.4826\n",
      "Epoch [1/1], Step [6287/8897], Loss: 5.1439\n",
      "Epoch [1/1], Step [6288/8897], Loss: 5.5450\n",
      "Epoch [1/1], Step [6289/8897], Loss: 5.4398\n",
      "Epoch [1/1], Step [6290/8897], Loss: 5.5884\n",
      "Epoch [1/1], Step [6291/8897], Loss: 5.2576\n",
      "Epoch [1/1], Step [6292/8897], Loss: 5.3355\n",
      "Epoch [1/1], Step [6293/8897], Loss: 5.2427\n",
      "Epoch [1/1], Step [6294/8897], Loss: 5.2077\n",
      "Epoch [1/1], Step [6295/8897], Loss: 5.4035\n",
      "Epoch [1/1], Step [6296/8897], Loss: 5.5423\n",
      "Epoch [1/1], Step [6297/8897], Loss: 5.3615\n",
      "Epoch [1/1], Step [6298/8897], Loss: 5.3751\n",
      "Epoch [1/1], Step [6299/8897], Loss: 5.4397\n",
      "Epoch [1/1], Step [6300/8897], Loss: 5.2908\n",
      "Epoch [1/1], Step [6301/8897], Loss: 5.7032\n",
      "Epoch [1/1], Step [6302/8897], Loss: 5.4078\n",
      "Epoch [1/1], Step [6303/8897], Loss: 5.4395\n",
      "Epoch [1/1], Step [6304/8897], Loss: 5.6011\n",
      "Epoch [1/1], Step [6305/8897], Loss: 5.3896\n",
      "Epoch [1/1], Step [6306/8897], Loss: 5.4414\n",
      "Epoch [1/1], Step [6307/8897], Loss: 5.4762\n",
      "Epoch [1/1], Step [6308/8897], Loss: 5.4739\n",
      "Epoch [1/1], Step [6309/8897], Loss: 5.4667\n",
      "Epoch [1/1], Step [6310/8897], Loss: 5.4804\n",
      "Epoch [1/1], Step [6311/8897], Loss: 5.4533\n",
      "Epoch [1/1], Step [6312/8897], Loss: 5.2852\n",
      "Epoch [1/1], Step [6313/8897], Loss: 5.1708\n",
      "Epoch [1/1], Step [6314/8897], Loss: 5.2303\n",
      "Epoch [1/1], Step [6315/8897], Loss: 5.3018\n",
      "Epoch [1/1], Step [6316/8897], Loss: 5.6038\n",
      "Epoch [1/1], Step [6317/8897], Loss: 5.4338\n",
      "Epoch [1/1], Step [6318/8897], Loss: 5.2842\n",
      "Epoch [1/1], Step [6319/8897], Loss: 5.4653\n",
      "Epoch [1/1], Step [6320/8897], Loss: 5.4884\n",
      "Epoch [1/1], Step [6321/8897], Loss: 5.3022\n",
      "Epoch [1/1], Step [6322/8897], Loss: 5.1780\n",
      "Epoch [1/1], Step [6323/8897], Loss: 5.3051\n",
      "Epoch [1/1], Step [6324/8897], Loss: 5.1769\n",
      "Epoch [1/1], Step [6325/8897], Loss: 5.2255\n",
      "Epoch [1/1], Step [6326/8897], Loss: 5.6904\n",
      "Epoch [1/1], Step [6327/8897], Loss: 5.3900\n",
      "Epoch [1/1], Step [6328/8897], Loss: 5.5089\n",
      "Epoch [1/1], Step [6329/8897], Loss: 5.4635\n",
      "Epoch [1/1], Step [6330/8897], Loss: 5.2658\n",
      "Epoch [1/1], Step [6331/8897], Loss: 5.4242\n",
      "Epoch [1/1], Step [6332/8897], Loss: 5.4222\n",
      "Epoch [1/1], Step [6333/8897], Loss: 5.1960\n",
      "Epoch [1/1], Step [6334/8897], Loss: 5.4163\n",
      "Epoch [1/1], Step [6335/8897], Loss: 5.4192\n",
      "Epoch [1/1], Step [6336/8897], Loss: 5.3789\n",
      "Epoch [1/1], Step [6337/8897], Loss: 5.4375\n",
      "Epoch [1/1], Step [6338/8897], Loss: 5.2873\n",
      "Epoch [1/1], Step [6339/8897], Loss: 5.6002\n",
      "Epoch [1/1], Step [6340/8897], Loss: 5.3911\n",
      "Epoch [1/1], Step [6341/8897], Loss: 5.5847\n",
      "Epoch [1/1], Step [6342/8897], Loss: 5.2881\n",
      "Epoch [1/1], Step [6343/8897], Loss: 5.3767\n",
      "Epoch [1/1], Step [6344/8897], Loss: 5.4711\n",
      "Epoch [1/1], Step [6345/8897], Loss: 5.3119\n",
      "Epoch [1/1], Step [6346/8897], Loss: 5.4201\n",
      "Epoch [1/1], Step [6347/8897], Loss: 5.3263\n",
      "Epoch [1/1], Step [6348/8897], Loss: 5.2966\n",
      "Epoch [1/1], Step [6349/8897], Loss: 5.4291\n",
      "Epoch [1/1], Step [6350/8897], Loss: 5.3936\n",
      "Epoch [1/1], Step [6351/8897], Loss: 5.3127\n",
      "Epoch [1/1], Step [6352/8897], Loss: 5.4992\n",
      "Epoch [1/1], Step [6353/8897], Loss: 5.4541\n",
      "Epoch [1/1], Step [6354/8897], Loss: 5.2338\n",
      "Epoch [1/1], Step [6355/8897], Loss: 5.3730\n",
      "Epoch [1/1], Step [6356/8897], Loss: 5.2962\n",
      "Epoch [1/1], Step [6357/8897], Loss: 5.4341\n",
      "Epoch [1/1], Step [6358/8897], Loss: 5.4685\n",
      "Epoch [1/1], Step [6359/8897], Loss: 5.3372\n",
      "Epoch [1/1], Step [6360/8897], Loss: 5.3355\n",
      "Epoch [1/1], Step [6361/8897], Loss: 5.0472\n",
      "Epoch [1/1], Step [6362/8897], Loss: 5.2647\n",
      "Epoch [1/1], Step [6363/8897], Loss: 5.2264\n",
      "Epoch [1/1], Step [6364/8897], Loss: 5.2159\n",
      "Epoch [1/1], Step [6365/8897], Loss: 5.4319\n",
      "Epoch [1/1], Step [6366/8897], Loss: 5.5685\n",
      "Epoch [1/1], Step [6367/8897], Loss: 5.4849\n",
      "Epoch [1/1], Step [6368/8897], Loss: 5.2347\n",
      "Epoch [1/1], Step [6369/8897], Loss: 5.2728\n",
      "Epoch [1/1], Step [6370/8897], Loss: 5.4673\n",
      "Epoch [1/1], Step [6371/8897], Loss: 5.3903\n",
      "Epoch [1/1], Step [6372/8897], Loss: 5.2529\n",
      "Epoch [1/1], Step [6373/8897], Loss: 5.5401\n",
      "Epoch [1/1], Step [6374/8897], Loss: 5.4031\n",
      "Epoch [1/1], Step [6375/8897], Loss: 5.6338\n",
      "Epoch [1/1], Step [6376/8897], Loss: 5.2875\n",
      "Epoch [1/1], Step [6377/8897], Loss: 5.4381\n",
      "Epoch [1/1], Step [6378/8897], Loss: 5.2745\n",
      "Epoch [1/1], Step [6379/8897], Loss: 5.0652\n",
      "Epoch [1/1], Step [6380/8897], Loss: 5.2701\n",
      "Epoch [1/1], Step [6381/8897], Loss: 5.7517\n",
      "Epoch [1/1], Step [6382/8897], Loss: 5.3631\n",
      "Epoch [1/1], Step [6383/8897], Loss: 5.1034\n",
      "Epoch [1/1], Step [6384/8897], Loss: 5.3817\n",
      "Epoch [1/1], Step [6385/8897], Loss: 5.4174\n",
      "Epoch [1/1], Step [6386/8897], Loss: 5.3340\n",
      "Epoch [1/1], Step [6387/8897], Loss: 5.5342\n",
      "Epoch [1/1], Step [6388/8897], Loss: 5.7018\n",
      "Epoch [1/1], Step [6389/8897], Loss: 5.5090\n",
      "Epoch [1/1], Step [6390/8897], Loss: 5.2948\n",
      "Epoch [1/1], Step [6391/8897], Loss: 5.3169\n",
      "Epoch [1/1], Step [6392/8897], Loss: 5.4961\n",
      "Epoch [1/1], Step [6393/8897], Loss: 5.5220\n",
      "Epoch [1/1], Step [6394/8897], Loss: 5.5794\n",
      "Epoch [1/1], Step [6395/8897], Loss: 5.2388\n",
      "Epoch [1/1], Step [6396/8897], Loss: 5.3977\n",
      "Epoch [1/1], Step [6397/8897], Loss: 5.7053\n",
      "Epoch [1/1], Step [6398/8897], Loss: 5.4399\n",
      "Epoch [1/1], Step [6399/8897], Loss: 5.3941\n",
      "Epoch [1/1], Step [6400/8897], Loss: 5.3474\n",
      "Epoch [1/1], Step [6401/8897], Loss: 5.4721\n",
      "Epoch [1/1], Step [6402/8897], Loss: 5.0993\n",
      "Epoch [1/1], Step [6403/8897], Loss: 5.3020\n",
      "Epoch [1/1], Step [6404/8897], Loss: 5.3849\n",
      "Epoch [1/1], Step [6405/8897], Loss: 5.2107\n",
      "Epoch [1/1], Step [6406/8897], Loss: 5.6235\n",
      "Epoch [1/1], Step [6407/8897], Loss: 5.3848\n",
      "Epoch [1/1], Step [6408/8897], Loss: 5.1640\n",
      "Epoch [1/1], Step [6409/8897], Loss: 5.3217\n",
      "Epoch [1/1], Step [6410/8897], Loss: 5.2040\n",
      "Epoch [1/1], Step [6411/8897], Loss: 5.1233\n",
      "Epoch [1/1], Step [6412/8897], Loss: 5.3187\n",
      "Epoch [1/1], Step [6413/8897], Loss: 5.4383\n",
      "Epoch [1/1], Step [6414/8897], Loss: 5.4629\n",
      "Epoch [1/1], Step [6415/8897], Loss: 5.5191\n",
      "Epoch [1/1], Step [6416/8897], Loss: 5.2494\n",
      "Epoch [1/1], Step [6417/8897], Loss: 5.1916\n",
      "Epoch [1/1], Step [6418/8897], Loss: 5.2486\n",
      "Epoch [1/1], Step [6419/8897], Loss: 5.1666\n",
      "Epoch [1/1], Step [6420/8897], Loss: 5.1901\n",
      "Epoch [1/1], Step [6421/8897], Loss: 5.1981\n",
      "Epoch [1/1], Step [6422/8897], Loss: 5.1051\n",
      "Epoch [1/1], Step [6423/8897], Loss: 5.3837\n",
      "Epoch [1/1], Step [6424/8897], Loss: 5.4581\n",
      "Epoch [1/1], Step [6425/8897], Loss: 5.2523\n",
      "Epoch [1/1], Step [6426/8897], Loss: 5.5030\n",
      "Epoch [1/1], Step [6427/8897], Loss: 5.7307\n",
      "Epoch [1/1], Step [6428/8897], Loss: 5.6509\n",
      "Epoch [1/1], Step [6429/8897], Loss: 5.3916\n",
      "Epoch [1/1], Step [6430/8897], Loss: 5.4043\n",
      "Epoch [1/1], Step [6431/8897], Loss: 5.3848\n",
      "Epoch [1/1], Step [6432/8897], Loss: 5.3968\n",
      "Epoch [1/1], Step [6433/8897], Loss: 5.5784\n",
      "Epoch [1/1], Step [6434/8897], Loss: 5.4124\n",
      "Epoch [1/1], Step [6435/8897], Loss: 5.4057\n",
      "Epoch [1/1], Step [6436/8897], Loss: 5.3679\n",
      "Epoch [1/1], Step [6437/8897], Loss: 5.1366\n",
      "Epoch [1/1], Step [6438/8897], Loss: 5.3355\n",
      "Epoch [1/1], Step [6439/8897], Loss: 5.4283\n",
      "Epoch [1/1], Step [6440/8897], Loss: 5.3142\n",
      "Epoch [1/1], Step [6441/8897], Loss: 5.2643\n",
      "Epoch [1/1], Step [6442/8897], Loss: 5.3889\n",
      "Epoch [1/1], Step [6443/8897], Loss: 5.3095\n",
      "Epoch [1/1], Step [6444/8897], Loss: 5.3849\n",
      "Epoch [1/1], Step [6445/8897], Loss: 5.0516\n",
      "Epoch [1/1], Step [6446/8897], Loss: 5.4002\n",
      "Epoch [1/1], Step [6447/8897], Loss: 5.4263\n",
      "Epoch [1/1], Step [6448/8897], Loss: 5.3494\n",
      "Epoch [1/1], Step [6449/8897], Loss: 5.3507\n",
      "Epoch [1/1], Step [6450/8897], Loss: 5.4582\n",
      "Epoch [1/1], Step [6451/8897], Loss: 5.4543\n",
      "Epoch [1/1], Step [6452/8897], Loss: 5.4706\n",
      "Epoch [1/1], Step [6453/8897], Loss: 5.3982\n",
      "Epoch [1/1], Step [6454/8897], Loss: 5.3681\n",
      "Epoch [1/1], Step [6455/8897], Loss: 5.1906\n",
      "Epoch [1/1], Step [6456/8897], Loss: 5.5814\n",
      "Epoch [1/1], Step [6457/8897], Loss: 5.5027\n",
      "Epoch [1/1], Step [6458/8897], Loss: 5.4888\n",
      "Epoch [1/1], Step [6459/8897], Loss: 5.3558\n",
      "Epoch [1/1], Step [6460/8897], Loss: 5.0552\n",
      "Epoch [1/1], Step [6461/8897], Loss: 5.5780\n",
      "Epoch [1/1], Step [6462/8897], Loss: 5.6639\n",
      "Epoch [1/1], Step [6463/8897], Loss: 5.2659\n",
      "Epoch [1/1], Step [6464/8897], Loss: 5.4890\n",
      "Epoch [1/1], Step [6465/8897], Loss: 5.3639\n",
      "Epoch [1/1], Step [6466/8897], Loss: 5.2252\n",
      "Epoch [1/1], Step [6467/8897], Loss: 5.0847\n",
      "Epoch [1/1], Step [6468/8897], Loss: 5.4124\n",
      "Epoch [1/1], Step [6469/8897], Loss: 5.2306\n",
      "Epoch [1/1], Step [6470/8897], Loss: 5.6814\n",
      "Epoch [1/1], Step [6471/8897], Loss: 5.5226\n",
      "Epoch [1/1], Step [6472/8897], Loss: 5.3402\n",
      "Epoch [1/1], Step [6473/8897], Loss: 5.0262\n",
      "Epoch [1/1], Step [6474/8897], Loss: 5.2376\n",
      "Epoch [1/1], Step [6475/8897], Loss: 5.5484\n",
      "Epoch [1/1], Step [6476/8897], Loss: 5.5811\n",
      "Epoch [1/1], Step [6477/8897], Loss: 5.5247\n",
      "Epoch [1/1], Step [6478/8897], Loss: 5.4941\n",
      "Epoch [1/1], Step [6479/8897], Loss: 5.2474\n",
      "Epoch [1/1], Step [6480/8897], Loss: 5.4786\n",
      "Epoch [1/1], Step [6481/8897], Loss: 5.3953\n",
      "Epoch [1/1], Step [6482/8897], Loss: 5.3572\n",
      "Epoch [1/1], Step [6483/8897], Loss: 5.3615\n",
      "Epoch [1/1], Step [6484/8897], Loss: 5.3120\n",
      "Epoch [1/1], Step [6485/8897], Loss: 5.3463\n",
      "Epoch [1/1], Step [6486/8897], Loss: 5.2702\n",
      "Epoch [1/1], Step [6487/8897], Loss: 5.4103\n",
      "Epoch [1/1], Step [6488/8897], Loss: 5.4313\n",
      "Epoch [1/1], Step [6489/8897], Loss: 5.3850\n",
      "Epoch [1/1], Step [6490/8897], Loss: 5.4630\n",
      "Epoch [1/1], Step [6491/8897], Loss: 5.4246\n",
      "Epoch [1/1], Step [6492/8897], Loss: 5.5114\n",
      "Epoch [1/1], Step [6493/8897], Loss: 5.0458\n",
      "Epoch [1/1], Step [6494/8897], Loss: 5.3595\n",
      "Epoch [1/1], Step [6495/8897], Loss: 5.2740\n",
      "Epoch [1/1], Step [6496/8897], Loss: 5.1935\n",
      "Epoch [1/1], Step [6497/8897], Loss: 5.3743\n",
      "Epoch [1/1], Step [6498/8897], Loss: 5.6367\n",
      "Epoch [1/1], Step [6499/8897], Loss: 5.5198\n",
      "Epoch [1/1], Step [6500/8897], Loss: 5.0902\n",
      "Epoch [1/1], Step [6501/8897], Loss: 5.3195\n",
      "Epoch [1/1], Step [6502/8897], Loss: 5.7331\n",
      "Epoch [1/1], Step [6503/8897], Loss: 5.1567\n",
      "Epoch [1/1], Step [6504/8897], Loss: 5.3378\n",
      "Epoch [1/1], Step [6505/8897], Loss: 5.6128\n",
      "Epoch [1/1], Step [6506/8897], Loss: 5.4005\n",
      "Epoch [1/1], Step [6507/8897], Loss: 5.1811\n",
      "Epoch [1/1], Step [6508/8897], Loss: 5.2353\n",
      "Epoch [1/1], Step [6509/8897], Loss: 5.5961\n",
      "Epoch [1/1], Step [6510/8897], Loss: 5.4526\n",
      "Epoch [1/1], Step [6511/8897], Loss: 5.3713\n",
      "Epoch [1/1], Step [6512/8897], Loss: 5.5329\n",
      "Epoch [1/1], Step [6513/8897], Loss: 5.5022\n",
      "Epoch [1/1], Step [6514/8897], Loss: 5.2625\n",
      "Epoch [1/1], Step [6515/8897], Loss: 5.3685\n",
      "Epoch [1/1], Step [6516/8897], Loss: 5.3603\n",
      "Epoch [1/1], Step [6517/8897], Loss: 5.2793\n",
      "Epoch [1/1], Step [6518/8897], Loss: 5.3094\n",
      "Epoch [1/1], Step [6519/8897], Loss: 5.1899\n",
      "Epoch [1/1], Step [6520/8897], Loss: 5.2861\n",
      "Epoch [1/1], Step [6521/8897], Loss: 5.3426\n",
      "Epoch [1/1], Step [6522/8897], Loss: 5.1588\n",
      "Epoch [1/1], Step [6523/8897], Loss: 5.4213\n",
      "Epoch [1/1], Step [6524/8897], Loss: 5.2268\n",
      "Epoch [1/1], Step [6525/8897], Loss: 5.2587\n",
      "Epoch [1/1], Step [6526/8897], Loss: 5.1036\n",
      "Epoch [1/1], Step [6527/8897], Loss: 5.5361\n",
      "Epoch [1/1], Step [6528/8897], Loss: 5.3349\n",
      "Epoch [1/1], Step [6529/8897], Loss: 5.3760\n",
      "Epoch [1/1], Step [6530/8897], Loss: 5.4993\n",
      "Epoch [1/1], Step [6531/8897], Loss: 5.3429\n",
      "Epoch [1/1], Step [6532/8897], Loss: 5.5383\n",
      "Epoch [1/1], Step [6533/8897], Loss: 5.2733\n",
      "Epoch [1/1], Step [6534/8897], Loss: 5.2176\n",
      "Epoch [1/1], Step [6535/8897], Loss: 5.3501\n",
      "Epoch [1/1], Step [6536/8897], Loss: 5.3561\n",
      "Epoch [1/1], Step [6537/8897], Loss: 5.0754\n",
      "Epoch [1/1], Step [6538/8897], Loss: 5.6140\n",
      "Epoch [1/1], Step [6539/8897], Loss: 5.1503\n",
      "Epoch [1/1], Step [6540/8897], Loss: 5.2244\n",
      "Epoch [1/1], Step [6541/8897], Loss: 5.4541\n",
      "Epoch [1/1], Step [6542/8897], Loss: 5.2354\n",
      "Epoch [1/1], Step [6543/8897], Loss: 5.4086\n",
      "Epoch [1/1], Step [6544/8897], Loss: 5.4032\n",
      "Epoch [1/1], Step [6545/8897], Loss: 5.1784\n",
      "Epoch [1/1], Step [6546/8897], Loss: 5.5155\n",
      "Epoch [1/1], Step [6547/8897], Loss: 5.3527\n",
      "Epoch [1/1], Step [6548/8897], Loss: 5.5589\n",
      "Epoch [1/1], Step [6549/8897], Loss: 5.1891\n",
      "Epoch [1/1], Step [6550/8897], Loss: 5.4232\n",
      "Epoch [1/1], Step [6551/8897], Loss: 5.4768\n",
      "Epoch [1/1], Step [6552/8897], Loss: 5.4929\n",
      "Epoch [1/1], Step [6553/8897], Loss: 5.5501\n",
      "Epoch [1/1], Step [6554/8897], Loss: 5.2177\n",
      "Epoch [1/1], Step [6555/8897], Loss: 5.5617\n",
      "Epoch [1/1], Step [6556/8897], Loss: 5.2852\n",
      "Epoch [1/1], Step [6557/8897], Loss: 5.0731\n",
      "Epoch [1/1], Step [6558/8897], Loss: 5.4558\n",
      "Epoch [1/1], Step [6559/8897], Loss: 5.5608\n",
      "Epoch [1/1], Step [6560/8897], Loss: 5.2622\n",
      "Epoch [1/1], Step [6561/8897], Loss: 5.3208\n",
      "Epoch [1/1], Step [6562/8897], Loss: 5.4671\n",
      "Epoch [1/1], Step [6563/8897], Loss: 5.4233\n",
      "Epoch [1/1], Step [6564/8897], Loss: 5.1421\n",
      "Epoch [1/1], Step [6565/8897], Loss: 5.4230\n",
      "Epoch [1/1], Step [6566/8897], Loss: 5.1528\n",
      "Epoch [1/1], Step [6567/8897], Loss: 5.5960\n",
      "Epoch [1/1], Step [6568/8897], Loss: 5.4092\n",
      "Epoch [1/1], Step [6569/8897], Loss: 5.4821\n",
      "Epoch [1/1], Step [6570/8897], Loss: 5.2217\n",
      "Epoch [1/1], Step [6571/8897], Loss: 5.4697\n",
      "Epoch [1/1], Step [6572/8897], Loss: 5.2823\n",
      "Epoch [1/1], Step [6573/8897], Loss: 5.3567\n",
      "Epoch [1/1], Step [6574/8897], Loss: 5.1167\n",
      "Epoch [1/1], Step [6575/8897], Loss: 5.4208\n",
      "Epoch [1/1], Step [6576/8897], Loss: 5.4496\n",
      "Epoch [1/1], Step [6577/8897], Loss: 5.6938\n",
      "Epoch [1/1], Step [6578/8897], Loss: 5.3099\n",
      "Epoch [1/1], Step [6579/8897], Loss: 5.4111\n",
      "Epoch [1/1], Step [6580/8897], Loss: 5.2699\n",
      "Epoch [1/1], Step [6581/8897], Loss: 5.5227\n",
      "Epoch [1/1], Step [6582/8897], Loss: 5.3134\n",
      "Epoch [1/1], Step [6583/8897], Loss: 5.3309\n",
      "Epoch [1/1], Step [6584/8897], Loss: 5.3366\n",
      "Epoch [1/1], Step [6585/8897], Loss: 5.1632\n",
      "Epoch [1/1], Step [6586/8897], Loss: 5.3699\n",
      "Epoch [1/1], Step [6587/8897], Loss: 5.3397\n",
      "Epoch [1/1], Step [6588/8897], Loss: 5.4094\n",
      "Epoch [1/1], Step [6589/8897], Loss: 5.6659\n",
      "Epoch [1/1], Step [6590/8897], Loss: 5.0044\n",
      "Epoch [1/1], Step [6591/8897], Loss: 5.2654\n",
      "Epoch [1/1], Step [6592/8897], Loss: 5.2511\n",
      "Epoch [1/1], Step [6593/8897], Loss: 6.0444\n",
      "Epoch [1/1], Step [6594/8897], Loss: 5.2798\n",
      "Epoch [1/1], Step [6595/8897], Loss: 5.3961\n",
      "Epoch [1/1], Step [6596/8897], Loss: 5.3652\n",
      "Epoch [1/1], Step [6597/8897], Loss: 5.3658\n",
      "Epoch [1/1], Step [6598/8897], Loss: 5.4058\n",
      "Epoch [1/1], Step [6599/8897], Loss: 5.3546\n",
      "Epoch [1/1], Step [6600/8897], Loss: 5.4470\n",
      "Epoch [1/1], Step [6601/8897], Loss: 5.2081\n",
      "Epoch [1/1], Step [6602/8897], Loss: 5.3626\n",
      "Epoch [1/1], Step [6603/8897], Loss: 5.2329\n",
      "Epoch [1/1], Step [6604/8897], Loss: 5.5099\n",
      "Epoch [1/1], Step [6605/8897], Loss: 5.2403\n",
      "Epoch [1/1], Step [6606/8897], Loss: 5.5622\n",
      "Epoch [1/1], Step [6607/8897], Loss: 5.2550\n",
      "Epoch [1/1], Step [6608/8897], Loss: 5.4841\n",
      "Epoch [1/1], Step [6609/8897], Loss: 5.2088\n",
      "Epoch [1/1], Step [6610/8897], Loss: 5.5758\n",
      "Epoch [1/1], Step [6611/8897], Loss: 5.3343\n",
      "Epoch [1/1], Step [6612/8897], Loss: 5.6380\n",
      "Epoch [1/1], Step [6613/8897], Loss: 5.3784\n",
      "Epoch [1/1], Step [6614/8897], Loss: 5.5704\n",
      "Epoch [1/1], Step [6615/8897], Loss: 5.4932\n",
      "Epoch [1/1], Step [6616/8897], Loss: 5.4213\n",
      "Epoch [1/1], Step [6617/8897], Loss: 5.3531\n",
      "Epoch [1/1], Step [6618/8897], Loss: 5.4996\n",
      "Epoch [1/1], Step [6619/8897], Loss: 5.1790\n",
      "Epoch [1/1], Step [6620/8897], Loss: 5.0940\n",
      "Epoch [1/1], Step [6621/8897], Loss: 5.2310\n",
      "Epoch [1/1], Step [6622/8897], Loss: 5.3343\n",
      "Epoch [1/1], Step [6623/8897], Loss: 5.3439\n",
      "Epoch [1/1], Step [6624/8897], Loss: 5.4497\n",
      "Epoch [1/1], Step [6625/8897], Loss: 5.4403\n",
      "Epoch [1/1], Step [6626/8897], Loss: 5.2163\n",
      "Epoch [1/1], Step [6627/8897], Loss: 5.3375\n",
      "Epoch [1/1], Step [6628/8897], Loss: 5.6434\n",
      "Epoch [1/1], Step [6629/8897], Loss: 5.3222\n",
      "Epoch [1/1], Step [6630/8897], Loss: 5.5460\n",
      "Epoch [1/1], Step [6631/8897], Loss: 5.3120\n",
      "Epoch [1/1], Step [6632/8897], Loss: 5.3685\n",
      "Epoch [1/1], Step [6633/8897], Loss: 5.1778\n",
      "Epoch [1/1], Step [6634/8897], Loss: 5.3464\n",
      "Epoch [1/1], Step [6635/8897], Loss: 5.5502\n",
      "Epoch [1/1], Step [6636/8897], Loss: 5.2731\n",
      "Epoch [1/1], Step [6637/8897], Loss: 5.4423\n",
      "Epoch [1/1], Step [6638/8897], Loss: 5.2413\n",
      "Epoch [1/1], Step [6639/8897], Loss: 5.3409\n",
      "Epoch [1/1], Step [6640/8897], Loss: 5.4039\n",
      "Epoch [1/1], Step [6641/8897], Loss: 5.4728\n",
      "Epoch [1/1], Step [6642/8897], Loss: 5.6432\n",
      "Epoch [1/1], Step [6643/8897], Loss: 5.3582\n",
      "Epoch [1/1], Step [6644/8897], Loss: 5.4756\n",
      "Epoch [1/1], Step [6645/8897], Loss: 5.3553\n",
      "Epoch [1/1], Step [6646/8897], Loss: 5.2918\n",
      "Epoch [1/1], Step [6647/8897], Loss: 5.4313\n",
      "Epoch [1/1], Step [6648/8897], Loss: 5.3583\n",
      "Epoch [1/1], Step [6649/8897], Loss: 5.3617\n",
      "Epoch [1/1], Step [6650/8897], Loss: 5.1220\n",
      "Epoch [1/1], Step [6651/8897], Loss: 5.1514\n",
      "Epoch [1/1], Step [6652/8897], Loss: 5.3008\n",
      "Epoch [1/1], Step [6653/8897], Loss: 5.4521\n",
      "Epoch [1/1], Step [6654/8897], Loss: 5.6479\n",
      "Epoch [1/1], Step [6655/8897], Loss: 5.2133\n",
      "Epoch [1/1], Step [6656/8897], Loss: 5.4940\n",
      "Epoch [1/1], Step [6657/8897], Loss: 5.2719\n",
      "Epoch [1/1], Step [6658/8897], Loss: 5.4846\n",
      "Epoch [1/1], Step [6659/8897], Loss: 5.4667\n",
      "Epoch [1/1], Step [6660/8897], Loss: 5.3784\n",
      "Epoch [1/1], Step [6661/8897], Loss: 5.1522\n",
      "Epoch [1/1], Step [6662/8897], Loss: 5.2356\n",
      "Epoch [1/1], Step [6663/8897], Loss: 5.0957\n",
      "Epoch [1/1], Step [6664/8897], Loss: 5.1799\n",
      "Epoch [1/1], Step [6665/8897], Loss: 5.3612\n",
      "Epoch [1/1], Step [6666/8897], Loss: 5.3419\n",
      "Epoch [1/1], Step [6667/8897], Loss: 5.4009\n",
      "Epoch [1/1], Step [6668/8897], Loss: 5.3676\n",
      "Epoch [1/1], Step [6669/8897], Loss: 5.4163\n",
      "Epoch [1/1], Step [6670/8897], Loss: 5.2446\n",
      "Epoch [1/1], Step [6671/8897], Loss: 5.4526\n",
      "Epoch [1/1], Step [6672/8897], Loss: 5.0469\n",
      "Epoch [1/1], Step [6673/8897], Loss: 5.2308\n",
      "Epoch [1/1], Step [6674/8897], Loss: 5.4786\n",
      "Epoch [1/1], Step [6675/8897], Loss: 5.3540\n",
      "Epoch [1/1], Step [6676/8897], Loss: 5.4659\n",
      "Epoch [1/1], Step [6677/8897], Loss: 5.4302\n",
      "Epoch [1/1], Step [6678/8897], Loss: 5.3737\n",
      "Epoch [1/1], Step [6679/8897], Loss: 5.3596\n",
      "Epoch [1/1], Step [6680/8897], Loss: 5.5561\n",
      "Epoch [1/1], Step [6681/8897], Loss: 5.0848\n",
      "Epoch [1/1], Step [6682/8897], Loss: 5.3396\n",
      "Epoch [1/1], Step [6683/8897], Loss: 5.4715\n",
      "Epoch [1/1], Step [6684/8897], Loss: 5.3063\n",
      "Epoch [1/1], Step [6685/8897], Loss: 5.5373\n",
      "Epoch [1/1], Step [6686/8897], Loss: 5.3555\n",
      "Epoch [1/1], Step [6687/8897], Loss: 5.3492\n",
      "Epoch [1/1], Step [6688/8897], Loss: 5.4008\n",
      "Epoch [1/1], Step [6689/8897], Loss: 5.3478\n",
      "Epoch [1/1], Step [6690/8897], Loss: 5.2564\n",
      "Epoch [1/1], Step [6691/8897], Loss: 5.4459\n",
      "Epoch [1/1], Step [6692/8897], Loss: 5.0915\n",
      "Epoch [1/1], Step [6693/8897], Loss: 5.1656\n",
      "Epoch [1/1], Step [6694/8897], Loss: 5.1640\n",
      "Epoch [1/1], Step [6695/8897], Loss: 5.3111\n",
      "Epoch [1/1], Step [6696/8897], Loss: 5.4896\n",
      "Epoch [1/1], Step [6697/8897], Loss: 5.6359\n",
      "Epoch [1/1], Step [6698/8897], Loss: 5.5209\n",
      "Epoch [1/1], Step [6699/8897], Loss: 5.5921\n",
      "Epoch [1/1], Step [6700/8897], Loss: 5.2980\n",
      "Epoch [1/1], Step [6701/8897], Loss: 5.4567\n",
      "Epoch [1/1], Step [6702/8897], Loss: 5.3142\n",
      "Epoch [1/1], Step [6703/8897], Loss: 5.7394\n",
      "Epoch [1/1], Step [6704/8897], Loss: 5.4756\n",
      "Epoch [1/1], Step [6705/8897], Loss: 5.3262\n",
      "Epoch [1/1], Step [6706/8897], Loss: 5.2724\n",
      "Epoch [1/1], Step [6707/8897], Loss: 5.2486\n",
      "Epoch [1/1], Step [6708/8897], Loss: 5.1649\n",
      "Epoch [1/1], Step [6709/8897], Loss: 5.4193\n",
      "Epoch [1/1], Step [6710/8897], Loss: 5.2050\n",
      "Epoch [1/1], Step [6711/8897], Loss: 5.4828\n",
      "Epoch [1/1], Step [6712/8897], Loss: 5.6937\n",
      "Epoch [1/1], Step [6713/8897], Loss: 5.1656\n",
      "Epoch [1/1], Step [6714/8897], Loss: 5.2920\n",
      "Epoch [1/1], Step [6715/8897], Loss: 5.3525\n",
      "Epoch [1/1], Step [6716/8897], Loss: 5.4594\n",
      "Epoch [1/1], Step [6717/8897], Loss: 5.3527\n",
      "Epoch [1/1], Step [6718/8897], Loss: 5.5718\n",
      "Epoch [1/1], Step [6719/8897], Loss: 5.5157\n",
      "Epoch [1/1], Step [6720/8897], Loss: 5.4093\n",
      "Epoch [1/1], Step [6721/8897], Loss: 5.4532\n",
      "Epoch [1/1], Step [6722/8897], Loss: 5.3759\n",
      "Epoch [1/1], Step [6723/8897], Loss: 5.3907\n",
      "Epoch [1/1], Step [6724/8897], Loss: 5.5846\n",
      "Epoch [1/1], Step [6725/8897], Loss: 5.1531\n",
      "Epoch [1/1], Step [6726/8897], Loss: 5.4305\n",
      "Epoch [1/1], Step [6727/8897], Loss: 5.4314\n",
      "Epoch [1/1], Step [6728/8897], Loss: 5.2005\n",
      "Epoch [1/1], Step [6729/8897], Loss: 5.5849\n",
      "Epoch [1/1], Step [6730/8897], Loss: 5.5524\n",
      "Epoch [1/1], Step [6731/8897], Loss: 5.3793\n",
      "Epoch [1/1], Step [6732/8897], Loss: 5.3841\n",
      "Epoch [1/1], Step [6733/8897], Loss: 5.3228\n",
      "Epoch [1/1], Step [6734/8897], Loss: 5.5002\n",
      "Epoch [1/1], Step [6735/8897], Loss: 5.4885\n",
      "Epoch [1/1], Step [6736/8897], Loss: 5.3344\n",
      "Epoch [1/1], Step [6737/8897], Loss: 5.2987\n",
      "Epoch [1/1], Step [6738/8897], Loss: 5.6306\n",
      "Epoch [1/1], Step [6739/8897], Loss: 5.3209\n",
      "Epoch [1/1], Step [6740/8897], Loss: 5.1468\n",
      "Epoch [1/1], Step [6741/8897], Loss: 5.3385\n",
      "Epoch [1/1], Step [6742/8897], Loss: 5.4823\n",
      "Epoch [1/1], Step [6743/8897], Loss: 5.4636\n",
      "Epoch [1/1], Step [6744/8897], Loss: 5.3611\n",
      "Epoch [1/1], Step [6745/8897], Loss: 5.2448\n",
      "Epoch [1/1], Step [6746/8897], Loss: 5.2691\n",
      "Epoch [1/1], Step [6747/8897], Loss: 5.4267\n",
      "Epoch [1/1], Step [6748/8897], Loss: 5.7132\n",
      "Epoch [1/1], Step [6749/8897], Loss: 5.3467\n",
      "Epoch [1/1], Step [6750/8897], Loss: 5.6139\n",
      "Epoch [1/1], Step [6751/8897], Loss: 5.4487\n",
      "Epoch [1/1], Step [6752/8897], Loss: 5.2764\n",
      "Epoch [1/1], Step [6753/8897], Loss: 5.2276\n",
      "Epoch [1/1], Step [6754/8897], Loss: 5.6078\n",
      "Epoch [1/1], Step [6755/8897], Loss: 5.2227\n",
      "Epoch [1/1], Step [6756/8897], Loss: 5.5568\n",
      "Epoch [1/1], Step [6757/8897], Loss: 5.3107\n",
      "Epoch [1/1], Step [6758/8897], Loss: 5.3230\n",
      "Epoch [1/1], Step [6759/8897], Loss: 5.5917\n",
      "Epoch [1/1], Step [6760/8897], Loss: 5.3172\n",
      "Epoch [1/1], Step [6761/8897], Loss: 5.4447\n",
      "Epoch [1/1], Step [6762/8897], Loss: 5.2999\n",
      "Epoch [1/1], Step [6763/8897], Loss: 5.2850\n",
      "Epoch [1/1], Step [6764/8897], Loss: 5.1430\n",
      "Epoch [1/1], Step [6765/8897], Loss: 5.4661\n",
      "Epoch [1/1], Step [6766/8897], Loss: 5.7060\n",
      "Epoch [1/1], Step [6767/8897], Loss: 5.3831\n",
      "Epoch [1/1], Step [6768/8897], Loss: 5.5540\n",
      "Epoch [1/1], Step [6769/8897], Loss: 5.2525\n",
      "Epoch [1/1], Step [6770/8897], Loss: 5.2924\n",
      "Epoch [1/1], Step [6771/8897], Loss: 5.2703\n",
      "Epoch [1/1], Step [6772/8897], Loss: 5.2769\n",
      "Epoch [1/1], Step [6773/8897], Loss: 5.4330\n",
      "Epoch [1/1], Step [6774/8897], Loss: 5.3243\n",
      "Epoch [1/1], Step [6775/8897], Loss: 5.3245\n",
      "Epoch [1/1], Step [6776/8897], Loss: 5.1848\n",
      "Epoch [1/1], Step [6777/8897], Loss: 5.3056\n",
      "Epoch [1/1], Step [6778/8897], Loss: 5.1231\n",
      "Epoch [1/1], Step [6779/8897], Loss: 5.2904\n",
      "Epoch [1/1], Step [6780/8897], Loss: 5.5517\n",
      "Epoch [1/1], Step [6781/8897], Loss: 5.1431\n",
      "Epoch [1/1], Step [6782/8897], Loss: 5.2814\n",
      "Epoch [1/1], Step [6783/8897], Loss: 5.4495\n",
      "Epoch [1/1], Step [6784/8897], Loss: 5.3516\n",
      "Epoch [1/1], Step [6785/8897], Loss: 5.5879\n",
      "Epoch [1/1], Step [6786/8897], Loss: 5.3803\n",
      "Epoch [1/1], Step [6787/8897], Loss: 5.4292\n",
      "Epoch [1/1], Step [6788/8897], Loss: 5.4892\n",
      "Epoch [1/1], Step [6789/8897], Loss: 5.4319\n",
      "Epoch [1/1], Step [6790/8897], Loss: 5.5687\n",
      "Epoch [1/1], Step [6791/8897], Loss: 5.2257\n",
      "Epoch [1/1], Step [6792/8897], Loss: 5.3374\n",
      "Epoch [1/1], Step [6793/8897], Loss: 5.3978\n",
      "Epoch [1/1], Step [6794/8897], Loss: 5.1485\n",
      "Epoch [1/1], Step [6795/8897], Loss: 5.3013\n",
      "Epoch [1/1], Step [6796/8897], Loss: 5.3876\n",
      "Epoch [1/1], Step [6797/8897], Loss: 5.2657\n",
      "Epoch [1/1], Step [6798/8897], Loss: 5.5849\n",
      "Epoch [1/1], Step [6799/8897], Loss: 5.1957\n",
      "Epoch [1/1], Step [6800/8897], Loss: 5.3976\n",
      "Epoch [1/1], Step [6801/8897], Loss: 5.1500\n",
      "Epoch [1/1], Step [6802/8897], Loss: 5.3273\n",
      "Epoch [1/1], Step [6803/8897], Loss: 5.5125\n",
      "Epoch [1/1], Step [6804/8897], Loss: 5.3558\n",
      "Epoch [1/1], Step [6805/8897], Loss: 5.1094\n",
      "Epoch [1/1], Step [6806/8897], Loss: 5.3586\n",
      "Epoch [1/1], Step [6807/8897], Loss: 5.5088\n",
      "Epoch [1/1], Step [6808/8897], Loss: 5.4564\n",
      "Epoch [1/1], Step [6809/8897], Loss: 5.3835\n",
      "Epoch [1/1], Step [6810/8897], Loss: 5.1488\n",
      "Epoch [1/1], Step [6811/8897], Loss: 5.0889\n",
      "Epoch [1/1], Step [6812/8897], Loss: 5.2002\n",
      "Epoch [1/1], Step [6813/8897], Loss: 5.3974\n",
      "Epoch [1/1], Step [6814/8897], Loss: 5.2098\n",
      "Epoch [1/1], Step [6815/8897], Loss: 5.7701\n",
      "Epoch [1/1], Step [6816/8897], Loss: 5.5073\n",
      "Epoch [1/1], Step [6817/8897], Loss: 5.3880\n",
      "Epoch [1/1], Step [6818/8897], Loss: 5.1921\n",
      "Epoch [1/1], Step [6819/8897], Loss: 5.3375\n",
      "Epoch [1/1], Step [6820/8897], Loss: 5.3828\n",
      "Epoch [1/1], Step [6821/8897], Loss: 5.3094\n",
      "Epoch [1/1], Step [6822/8897], Loss: 5.3086\n",
      "Epoch [1/1], Step [6823/8897], Loss: 5.3699\n",
      "Epoch [1/1], Step [6824/8897], Loss: 5.4741\n",
      "Epoch [1/1], Step [6825/8897], Loss: 5.3040\n",
      "Epoch [1/1], Step [6826/8897], Loss: 5.4473\n",
      "Epoch [1/1], Step [6827/8897], Loss: 5.2211\n",
      "Epoch [1/1], Step [6828/8897], Loss: 5.2749\n",
      "Epoch [1/1], Step [6829/8897], Loss: 5.3608\n",
      "Epoch [1/1], Step [6830/8897], Loss: 5.6422\n",
      "Epoch [1/1], Step [6831/8897], Loss: 5.2130\n",
      "Epoch [1/1], Step [6832/8897], Loss: 5.3969\n",
      "Epoch [1/1], Step [6833/8897], Loss: 5.2539\n",
      "Epoch [1/1], Step [6834/8897], Loss: 5.2200\n",
      "Epoch [1/1], Step [6835/8897], Loss: 5.4084\n",
      "Epoch [1/1], Step [6836/8897], Loss: 5.3959\n",
      "Epoch [1/1], Step [6837/8897], Loss: 5.5088\n",
      "Epoch [1/1], Step [6838/8897], Loss: 5.2165\n",
      "Epoch [1/1], Step [6839/8897], Loss: 5.5231\n",
      "Epoch [1/1], Step [6840/8897], Loss: 5.3534\n",
      "Epoch [1/1], Step [6841/8897], Loss: 5.3592\n",
      "Epoch [1/1], Step [6842/8897], Loss: 5.1950\n",
      "Epoch [1/1], Step [6843/8897], Loss: 5.4510\n",
      "Epoch [1/1], Step [6844/8897], Loss: 5.0984\n",
      "Epoch [1/1], Step [6845/8897], Loss: 5.4034\n",
      "Epoch [1/1], Step [6846/8897], Loss: 5.4582\n",
      "Epoch [1/1], Step [6847/8897], Loss: 5.2993\n",
      "Epoch [1/1], Step [6848/8897], Loss: 5.0050\n",
      "Epoch [1/1], Step [6849/8897], Loss: 5.0416\n",
      "Epoch [1/1], Step [6850/8897], Loss: 5.4659\n",
      "Epoch [1/1], Step [6851/8897], Loss: 5.2980\n",
      "Epoch [1/1], Step [6852/8897], Loss: 5.5295\n",
      "Epoch [1/1], Step [6853/8897], Loss: 5.1605\n",
      "Epoch [1/1], Step [6854/8897], Loss: 5.2900\n",
      "Epoch [1/1], Step [6855/8897], Loss: 5.4694\n",
      "Epoch [1/1], Step [6856/8897], Loss: 5.5024\n",
      "Epoch [1/1], Step [6857/8897], Loss: 5.3933\n",
      "Epoch [1/1], Step [6858/8897], Loss: 5.4742\n",
      "Epoch [1/1], Step [6859/8897], Loss: 5.3491\n",
      "Epoch [1/1], Step [6860/8897], Loss: 5.2586\n",
      "Epoch [1/1], Step [6861/8897], Loss: 5.5391\n",
      "Epoch [1/1], Step [6862/8897], Loss: 5.5201\n",
      "Epoch [1/1], Step [6863/8897], Loss: 5.1734\n",
      "Epoch [1/1], Step [6864/8897], Loss: 5.3064\n",
      "Epoch [1/1], Step [6865/8897], Loss: 4.9304\n",
      "Epoch [1/1], Step [6866/8897], Loss: 5.1509\n",
      "Epoch [1/1], Step [6867/8897], Loss: 5.5538\n",
      "Epoch [1/1], Step [6868/8897], Loss: 5.4732\n",
      "Epoch [1/1], Step [6869/8897], Loss: 5.5483\n",
      "Epoch [1/1], Step [6870/8897], Loss: 5.4155\n",
      "Epoch [1/1], Step [6871/8897], Loss: 5.3650\n",
      "Epoch [1/1], Step [6872/8897], Loss: 5.4072\n",
      "Epoch [1/1], Step [6873/8897], Loss: 5.3342\n",
      "Epoch [1/1], Step [6874/8897], Loss: 5.3370\n",
      "Epoch [1/1], Step [6875/8897], Loss: 5.5527\n",
      "Epoch [1/1], Step [6876/8897], Loss: 5.3874\n",
      "Epoch [1/1], Step [6877/8897], Loss: 5.1260\n",
      "Epoch [1/1], Step [6878/8897], Loss: 4.9050\n",
      "Epoch [1/1], Step [6879/8897], Loss: 5.3140\n",
      "Epoch [1/1], Step [6880/8897], Loss: 5.3908\n",
      "Epoch [1/1], Step [6881/8897], Loss: 5.3233\n",
      "Epoch [1/1], Step [6882/8897], Loss: 5.5861\n",
      "Epoch [1/1], Step [6883/8897], Loss: 5.4529\n",
      "Epoch [1/1], Step [6884/8897], Loss: 5.3130\n",
      "Epoch [1/1], Step [6885/8897], Loss: 5.1826\n",
      "Epoch [1/1], Step [6886/8897], Loss: 5.1719\n",
      "Epoch [1/1], Step [6887/8897], Loss: 5.3902\n",
      "Epoch [1/1], Step [6888/8897], Loss: 5.4794\n",
      "Epoch [1/1], Step [6889/8897], Loss: 5.4382\n",
      "Epoch [1/1], Step [6890/8897], Loss: 5.3860\n",
      "Epoch [1/1], Step [6891/8897], Loss: 5.4582\n",
      "Epoch [1/1], Step [6892/8897], Loss: 5.4830\n",
      "Epoch [1/1], Step [6893/8897], Loss: 5.2922\n",
      "Epoch [1/1], Step [6894/8897], Loss: 5.2278\n",
      "Epoch [1/1], Step [6895/8897], Loss: 5.4958\n",
      "Epoch [1/1], Step [6896/8897], Loss: 5.2153\n",
      "Epoch [1/1], Step [6897/8897], Loss: 5.4888\n",
      "Epoch [1/1], Step [6898/8897], Loss: 5.3547\n",
      "Epoch [1/1], Step [6899/8897], Loss: 5.5568\n",
      "Epoch [1/1], Step [6900/8897], Loss: 5.3330\n",
      "Epoch [1/1], Step [6901/8897], Loss: 5.3178\n",
      "Epoch [1/1], Step [6902/8897], Loss: 5.6065\n",
      "Epoch [1/1], Step [6903/8897], Loss: 5.4867\n",
      "Epoch [1/1], Step [6904/8897], Loss: 5.2479\n",
      "Epoch [1/1], Step [6905/8897], Loss: 5.0642\n",
      "Epoch [1/1], Step [6906/8897], Loss: 5.3247\n",
      "Epoch [1/1], Step [6907/8897], Loss: 5.1692\n",
      "Epoch [1/1], Step [6908/8897], Loss: 5.3553\n",
      "Epoch [1/1], Step [6909/8897], Loss: 5.8498\n",
      "Epoch [1/1], Step [6910/8897], Loss: 5.3756\n",
      "Epoch [1/1], Step [6911/8897], Loss: 5.2739\n",
      "Epoch [1/1], Step [6912/8897], Loss: 5.2979\n",
      "Epoch [1/1], Step [6913/8897], Loss: 5.6445\n",
      "Epoch [1/1], Step [6914/8897], Loss: 5.3764\n",
      "Epoch [1/1], Step [6915/8897], Loss: 5.4910\n",
      "Epoch [1/1], Step [6916/8897], Loss: 5.2548\n",
      "Epoch [1/1], Step [6917/8897], Loss: 5.2607\n",
      "Epoch [1/1], Step [6918/8897], Loss: 5.5620\n",
      "Epoch [1/1], Step [6919/8897], Loss: 5.1999\n",
      "Epoch [1/1], Step [6920/8897], Loss: 5.5817\n",
      "Epoch [1/1], Step [6921/8897], Loss: 5.5122\n",
      "Epoch [1/1], Step [6922/8897], Loss: 5.4561\n",
      "Epoch [1/1], Step [6923/8897], Loss: 5.4589\n",
      "Epoch [1/1], Step [6924/8897], Loss: 5.3373\n",
      "Epoch [1/1], Step [6925/8897], Loss: 5.3455\n",
      "Epoch [1/1], Step [6926/8897], Loss: 5.3972\n",
      "Epoch [1/1], Step [6927/8897], Loss: 5.3417\n",
      "Epoch [1/1], Step [6928/8897], Loss: 5.2135\n",
      "Epoch [1/1], Step [6929/8897], Loss: 5.3201\n",
      "Epoch [1/1], Step [6930/8897], Loss: 5.2431\n",
      "Epoch [1/1], Step [6931/8897], Loss: 5.3009\n",
      "Epoch [1/1], Step [6932/8897], Loss: 5.4281\n",
      "Epoch [1/1], Step [6933/8897], Loss: 5.3839\n",
      "Epoch [1/1], Step [6934/8897], Loss: 5.0477\n",
      "Epoch [1/1], Step [6935/8897], Loss: 5.4233\n",
      "Epoch [1/1], Step [6936/8897], Loss: 5.3793\n",
      "Epoch [1/1], Step [6937/8897], Loss: 5.4643\n",
      "Epoch [1/1], Step [6938/8897], Loss: 5.6600\n",
      "Epoch [1/1], Step [6939/8897], Loss: 5.3447\n",
      "Epoch [1/1], Step [6940/8897], Loss: 5.2486\n",
      "Epoch [1/1], Step [6941/8897], Loss: 5.1281\n",
      "Epoch [1/1], Step [6942/8897], Loss: 5.5738\n",
      "Epoch [1/1], Step [6943/8897], Loss: 5.5850\n",
      "Epoch [1/1], Step [6944/8897], Loss: 5.3355\n",
      "Epoch [1/1], Step [6945/8897], Loss: 5.4341\n",
      "Epoch [1/1], Step [6946/8897], Loss: 5.4217\n",
      "Epoch [1/1], Step [6947/8897], Loss: 5.3367\n",
      "Epoch [1/1], Step [6948/8897], Loss: 5.3634\n",
      "Epoch [1/1], Step [6949/8897], Loss: 5.2819\n",
      "Epoch [1/1], Step [6950/8897], Loss: 5.3381\n",
      "Epoch [1/1], Step [6951/8897], Loss: 5.3637\n",
      "Epoch [1/1], Step [6952/8897], Loss: 5.2592\n",
      "Epoch [1/1], Step [6953/8897], Loss: 5.3700\n",
      "Epoch [1/1], Step [6954/8897], Loss: 5.4128\n",
      "Epoch [1/1], Step [6955/8897], Loss: 5.6489\n",
      "Epoch [1/1], Step [6956/8897], Loss: 5.3008\n",
      "Epoch [1/1], Step [6957/8897], Loss: 5.3443\n",
      "Epoch [1/1], Step [6958/8897], Loss: 5.3018\n",
      "Epoch [1/1], Step [6959/8897], Loss: 5.4103\n",
      "Epoch [1/1], Step [6960/8897], Loss: 5.4734\n",
      "Epoch [1/1], Step [6961/8897], Loss: 5.1106\n",
      "Epoch [1/1], Step [6962/8897], Loss: 5.2377\n",
      "Epoch [1/1], Step [6963/8897], Loss: 5.5056\n",
      "Epoch [1/1], Step [6964/8897], Loss: 5.3328\n",
      "Epoch [1/1], Step [6965/8897], Loss: 5.3184\n",
      "Epoch [1/1], Step [6966/8897], Loss: 5.4508\n",
      "Epoch [1/1], Step [6967/8897], Loss: 5.5045\n",
      "Epoch [1/1], Step [6968/8897], Loss: 5.4630\n",
      "Epoch [1/1], Step [6969/8897], Loss: 5.4540\n",
      "Epoch [1/1], Step [6970/8897], Loss: 5.4916\n",
      "Epoch [1/1], Step [6971/8897], Loss: 5.3909\n",
      "Epoch [1/1], Step [6972/8897], Loss: 5.4084\n",
      "Epoch [1/1], Step [6973/8897], Loss: 5.3536\n",
      "Epoch [1/1], Step [6974/8897], Loss: 5.2766\n",
      "Epoch [1/1], Step [6975/8897], Loss: 5.4628\n",
      "Epoch [1/1], Step [6976/8897], Loss: 5.3296\n",
      "Epoch [1/1], Step [6977/8897], Loss: 5.1569\n",
      "Epoch [1/1], Step [6978/8897], Loss: 5.2121\n",
      "Epoch [1/1], Step [6979/8897], Loss: 5.4548\n",
      "Epoch [1/1], Step [6980/8897], Loss: 5.3316\n",
      "Epoch [1/1], Step [6981/8897], Loss: 5.4899\n",
      "Epoch [1/1], Step [6982/8897], Loss: 5.3807\n",
      "Epoch [1/1], Step [6983/8897], Loss: 5.2596\n",
      "Epoch [1/1], Step [6984/8897], Loss: 5.3915\n",
      "Epoch [1/1], Step [6985/8897], Loss: 5.0802\n",
      "Epoch [1/1], Step [6986/8897], Loss: 5.6007\n",
      "Epoch [1/1], Step [6987/8897], Loss: 5.4468\n",
      "Epoch [1/1], Step [6988/8897], Loss: 5.3728\n",
      "Epoch [1/1], Step [6989/8897], Loss: 5.3161\n",
      "Epoch [1/1], Step [6990/8897], Loss: 5.4910\n",
      "Epoch [1/1], Step [6991/8897], Loss: 5.6502\n",
      "Epoch [1/1], Step [6992/8897], Loss: 5.2337\n",
      "Epoch [1/1], Step [6993/8897], Loss: 5.2169\n",
      "Epoch [1/1], Step [6994/8897], Loss: 5.1355\n",
      "Epoch [1/1], Step [6995/8897], Loss: 5.5940\n",
      "Epoch [1/1], Step [6996/8897], Loss: 5.3353\n",
      "Epoch [1/1], Step [6997/8897], Loss: 5.2924\n",
      "Epoch [1/1], Step [6998/8897], Loss: 5.4597\n",
      "Epoch [1/1], Step [6999/8897], Loss: 5.3587\n",
      "Epoch [1/1], Step [7000/8897], Loss: 5.3477\n",
      "Epoch [1/1], Step [7001/8897], Loss: 5.4361\n",
      "Epoch [1/1], Step [7002/8897], Loss: 5.2499\n",
      "Epoch [1/1], Step [7003/8897], Loss: 5.3224\n",
      "Epoch [1/1], Step [7004/8897], Loss: 5.3564\n",
      "Epoch [1/1], Step [7005/8897], Loss: 5.1520\n",
      "Epoch [1/1], Step [7006/8897], Loss: 5.1626\n",
      "Epoch [1/1], Step [7007/8897], Loss: 5.3078\n",
      "Epoch [1/1], Step [7008/8897], Loss: 5.2229\n",
      "Epoch [1/1], Step [7009/8897], Loss: 5.5057\n",
      "Epoch [1/1], Step [7010/8897], Loss: 5.4821\n",
      "Epoch [1/1], Step [7011/8897], Loss: 5.4034\n",
      "Epoch [1/1], Step [7012/8897], Loss: 5.5097\n",
      "Epoch [1/1], Step [7013/8897], Loss: 5.4454\n",
      "Epoch [1/1], Step [7014/8897], Loss: 5.6448\n",
      "Epoch [1/1], Step [7015/8897], Loss: 5.3936\n",
      "Epoch [1/1], Step [7016/8897], Loss: 5.1747\n",
      "Epoch [1/1], Step [7017/8897], Loss: 5.4055\n",
      "Epoch [1/1], Step [7018/8897], Loss: 5.2626\n",
      "Epoch [1/1], Step [7019/8897], Loss: 5.3046\n",
      "Epoch [1/1], Step [7020/8897], Loss: 5.3392\n",
      "Epoch [1/1], Step [7021/8897], Loss: 5.2481\n",
      "Epoch [1/1], Step [7022/8897], Loss: 5.5398\n",
      "Epoch [1/1], Step [7023/8897], Loss: 5.5545\n",
      "Epoch [1/1], Step [7024/8897], Loss: 5.3626\n",
      "Epoch [1/1], Step [7025/8897], Loss: 5.4026\n",
      "Epoch [1/1], Step [7026/8897], Loss: 5.3799\n",
      "Epoch [1/1], Step [7027/8897], Loss: 5.3177\n",
      "Epoch [1/1], Step [7028/8897], Loss: 5.4918\n",
      "Epoch [1/1], Step [7029/8897], Loss: 5.4497\n",
      "Epoch [1/1], Step [7030/8897], Loss: 5.6394\n",
      "Epoch [1/1], Step [7031/8897], Loss: 5.1364\n",
      "Epoch [1/1], Step [7032/8897], Loss: 5.5441\n",
      "Epoch [1/1], Step [7033/8897], Loss: 5.2545\n",
      "Epoch [1/1], Step [7034/8897], Loss: 5.2372\n",
      "Epoch [1/1], Step [7035/8897], Loss: 5.3140\n",
      "Epoch [1/1], Step [7036/8897], Loss: 5.3927\n",
      "Epoch [1/1], Step [7037/8897], Loss: 5.3354\n",
      "Epoch [1/1], Step [7038/8897], Loss: 5.1489\n",
      "Epoch [1/1], Step [7039/8897], Loss: 5.1126\n",
      "Epoch [1/1], Step [7040/8897], Loss: 5.4614\n",
      "Epoch [1/1], Step [7041/8897], Loss: 5.4156\n",
      "Epoch [1/1], Step [7042/8897], Loss: 5.3447\n",
      "Epoch [1/1], Step [7043/8897], Loss: 5.3744\n",
      "Epoch [1/1], Step [7044/8897], Loss: 5.0468\n",
      "Epoch [1/1], Step [7045/8897], Loss: 5.2259\n",
      "Epoch [1/1], Step [7046/8897], Loss: 5.2867\n",
      "Epoch [1/1], Step [7047/8897], Loss: 5.1345\n",
      "Epoch [1/1], Step [7048/8897], Loss: 5.3748\n",
      "Epoch [1/1], Step [7049/8897], Loss: 5.3127\n",
      "Epoch [1/1], Step [7050/8897], Loss: 5.4418\n",
      "Epoch [1/1], Step [7051/8897], Loss: 5.4362\n",
      "Epoch [1/1], Step [7052/8897], Loss: 5.1859\n",
      "Epoch [1/1], Step [7053/8897], Loss: 5.5816\n",
      "Epoch [1/1], Step [7054/8897], Loss: 5.3679\n",
      "Epoch [1/1], Step [7055/8897], Loss: 5.4969\n",
      "Epoch [1/1], Step [7056/8897], Loss: 5.3818\n",
      "Epoch [1/1], Step [7057/8897], Loss: 5.2576\n",
      "Epoch [1/1], Step [7058/8897], Loss: 5.4446\n",
      "Epoch [1/1], Step [7059/8897], Loss: 5.4767\n",
      "Epoch [1/1], Step [7060/8897], Loss: 5.2861\n",
      "Epoch [1/1], Step [7061/8897], Loss: 5.3187\n",
      "Epoch [1/1], Step [7062/8897], Loss: 5.5388\n",
      "Epoch [1/1], Step [7063/8897], Loss: 5.4068\n",
      "Epoch [1/1], Step [7064/8897], Loss: 5.3381\n",
      "Epoch [1/1], Step [7065/8897], Loss: 5.2274\n",
      "Epoch [1/1], Step [7066/8897], Loss: 5.4156\n",
      "Epoch [1/1], Step [7067/8897], Loss: 5.4382\n",
      "Epoch [1/1], Step [7068/8897], Loss: 5.4187\n",
      "Epoch [1/1], Step [7069/8897], Loss: 5.5251\n",
      "Epoch [1/1], Step [7070/8897], Loss: 5.1214\n",
      "Epoch [1/1], Step [7071/8897], Loss: 5.3240\n",
      "Epoch [1/1], Step [7072/8897], Loss: 5.2922\n",
      "Epoch [1/1], Step [7073/8897], Loss: 5.5120\n",
      "Epoch [1/1], Step [7074/8897], Loss: 5.1935\n",
      "Epoch [1/1], Step [7075/8897], Loss: 5.4394\n",
      "Epoch [1/1], Step [7076/8897], Loss: 5.4469\n",
      "Epoch [1/1], Step [7077/8897], Loss: 5.4155\n",
      "Epoch [1/1], Step [7078/8897], Loss: 5.1614\n",
      "Epoch [1/1], Step [7079/8897], Loss: 5.3710\n",
      "Epoch [1/1], Step [7080/8897], Loss: 5.3261\n",
      "Epoch [1/1], Step [7081/8897], Loss: 5.1495\n",
      "Epoch [1/1], Step [7082/8897], Loss: 5.2470\n",
      "Epoch [1/1], Step [7083/8897], Loss: 5.1406\n",
      "Epoch [1/1], Step [7084/8897], Loss: 5.4253\n",
      "Epoch [1/1], Step [7085/8897], Loss: 5.1386\n",
      "Epoch [1/1], Step [7086/8897], Loss: 5.6852\n",
      "Epoch [1/1], Step [7087/8897], Loss: 5.6555\n",
      "Epoch [1/1], Step [7088/8897], Loss: 5.5419\n",
      "Epoch [1/1], Step [7089/8897], Loss: 5.5098\n",
      "Epoch [1/1], Step [7090/8897], Loss: 5.5891\n",
      "Epoch [1/1], Step [7091/8897], Loss: 5.4249\n",
      "Epoch [1/1], Step [7092/8897], Loss: 5.6224\n",
      "Epoch [1/1], Step [7093/8897], Loss: 5.3471\n",
      "Epoch [1/1], Step [7094/8897], Loss: 5.5684\n",
      "Epoch [1/1], Step [7095/8897], Loss: 5.4588\n",
      "Epoch [1/1], Step [7096/8897], Loss: 5.3571\n",
      "Epoch [1/1], Step [7097/8897], Loss: 5.5275\n",
      "Epoch [1/1], Step [7098/8897], Loss: 5.2065\n",
      "Epoch [1/1], Step [7099/8897], Loss: 5.2420\n",
      "Epoch [1/1], Step [7100/8897], Loss: 5.0659\n",
      "Epoch [1/1], Step [7101/8897], Loss: 5.4108\n",
      "Epoch [1/1], Step [7102/8897], Loss: 5.7220\n",
      "Epoch [1/1], Step [7103/8897], Loss: 5.3194\n",
      "Epoch [1/1], Step [7104/8897], Loss: 5.4651\n",
      "Epoch [1/1], Step [7105/8897], Loss: 5.4591\n",
      "Epoch [1/1], Step [7106/8897], Loss: 5.4613\n",
      "Epoch [1/1], Step [7107/8897], Loss: 5.3537\n",
      "Epoch [1/1], Step [7108/8897], Loss: 5.3685\n",
      "Epoch [1/1], Step [7109/8897], Loss: 5.2901\n",
      "Epoch [1/1], Step [7110/8897], Loss: 5.1993\n",
      "Epoch [1/1], Step [7111/8897], Loss: 5.2610\n",
      "Epoch [1/1], Step [7112/8897], Loss: 5.3803\n",
      "Epoch [1/1], Step [7113/8897], Loss: 5.3643\n",
      "Epoch [1/1], Step [7114/8897], Loss: 5.4511\n",
      "Epoch [1/1], Step [7115/8897], Loss: 5.4815\n",
      "Epoch [1/1], Step [7116/8897], Loss: 5.3454\n",
      "Epoch [1/1], Step [7117/8897], Loss: 5.3965\n",
      "Epoch [1/1], Step [7118/8897], Loss: 5.5395\n",
      "Epoch [1/1], Step [7119/8897], Loss: 5.4671\n",
      "Epoch [1/1], Step [7120/8897], Loss: 5.3459\n",
      "Epoch [1/1], Step [7121/8897], Loss: 5.1127\n",
      "Epoch [1/1], Step [7122/8897], Loss: 5.3266\n",
      "Epoch [1/1], Step [7123/8897], Loss: 5.4212\n",
      "Epoch [1/1], Step [7124/8897], Loss: 5.2465\n",
      "Epoch [1/1], Step [7125/8897], Loss: 5.1818\n",
      "Epoch [1/1], Step [7126/8897], Loss: 5.3761\n",
      "Epoch [1/1], Step [7127/8897], Loss: 5.4645\n",
      "Epoch [1/1], Step [7128/8897], Loss: 5.4434\n",
      "Epoch [1/1], Step [7129/8897], Loss: 5.2290\n",
      "Epoch [1/1], Step [7130/8897], Loss: 5.2841\n",
      "Epoch [1/1], Step [7131/8897], Loss: 5.4669\n",
      "Epoch [1/1], Step [7132/8897], Loss: 5.0435\n",
      "Epoch [1/1], Step [7133/8897], Loss: 5.2797\n",
      "Epoch [1/1], Step [7134/8897], Loss: 5.4448\n",
      "Epoch [1/1], Step [7135/8897], Loss: 5.3557\n",
      "Epoch [1/1], Step [7136/8897], Loss: 5.4615\n",
      "Epoch [1/1], Step [7137/8897], Loss: 5.2090\n",
      "Epoch [1/1], Step [7138/8897], Loss: 5.3870\n",
      "Epoch [1/1], Step [7139/8897], Loss: 5.3696\n",
      "Epoch [1/1], Step [7140/8897], Loss: 5.4285\n",
      "Epoch [1/1], Step [7141/8897], Loss: 5.6074\n",
      "Epoch [1/1], Step [7142/8897], Loss: 5.3565\n",
      "Epoch [1/1], Step [7143/8897], Loss: 5.3873\n",
      "Epoch [1/1], Step [7144/8897], Loss: 5.2637\n",
      "Epoch [1/1], Step [7145/8897], Loss: 5.2747\n",
      "Epoch [1/1], Step [7146/8897], Loss: 5.4364\n",
      "Epoch [1/1], Step [7147/8897], Loss: 5.2078\n",
      "Epoch [1/1], Step [7148/8897], Loss: 5.3307\n",
      "Epoch [1/1], Step [7149/8897], Loss: 5.3548\n",
      "Epoch [1/1], Step [7150/8897], Loss: 5.3869\n",
      "Epoch [1/1], Step [7151/8897], Loss: 5.3920\n",
      "Epoch [1/1], Step [7152/8897], Loss: 5.2335\n",
      "Epoch [1/1], Step [7153/8897], Loss: 5.4691\n",
      "Epoch [1/1], Step [7154/8897], Loss: 5.4936\n",
      "Epoch [1/1], Step [7155/8897], Loss: 5.2698\n",
      "Epoch [1/1], Step [7156/8897], Loss: 5.2993\n",
      "Epoch [1/1], Step [7157/8897], Loss: 5.5288\n",
      "Epoch [1/1], Step [7158/8897], Loss: 5.4846\n",
      "Epoch [1/1], Step [7159/8897], Loss: 5.3756\n",
      "Epoch [1/1], Step [7160/8897], Loss: 5.3748\n",
      "Epoch [1/1], Step [7161/8897], Loss: 5.4465\n",
      "Epoch [1/1], Step [7162/8897], Loss: 5.5904\n",
      "Epoch [1/1], Step [7163/8897], Loss: 5.3150\n",
      "Epoch [1/1], Step [7164/8897], Loss: 5.4481\n",
      "Epoch [1/1], Step [7165/8897], Loss: 5.5193\n",
      "Epoch [1/1], Step [7166/8897], Loss: 5.3460\n",
      "Epoch [1/1], Step [7167/8897], Loss: 5.4746\n",
      "Epoch [1/1], Step [7168/8897], Loss: 5.5493\n",
      "Epoch [1/1], Step [7169/8897], Loss: 5.3924\n",
      "Epoch [1/1], Step [7170/8897], Loss: 5.3348\n",
      "Epoch [1/1], Step [7171/8897], Loss: 5.7180\n",
      "Epoch [1/1], Step [7172/8897], Loss: 5.5350\n",
      "Epoch [1/1], Step [7173/8897], Loss: 5.4083\n",
      "Epoch [1/1], Step [7174/8897], Loss: 5.1745\n",
      "Epoch [1/1], Step [7175/8897], Loss: 5.3498\n",
      "Epoch [1/1], Step [7176/8897], Loss: 5.3660\n",
      "Epoch [1/1], Step [7177/8897], Loss: 5.2987\n",
      "Epoch [1/1], Step [7178/8897], Loss: 5.2756\n",
      "Epoch [1/1], Step [7179/8897], Loss: 5.2398\n",
      "Epoch [1/1], Step [7180/8897], Loss: 5.5268\n",
      "Epoch [1/1], Step [7181/8897], Loss: 5.5633\n",
      "Epoch [1/1], Step [7182/8897], Loss: 5.4283\n",
      "Epoch [1/1], Step [7183/8897], Loss: 5.4028\n",
      "Epoch [1/1], Step [7184/8897], Loss: 5.2952\n",
      "Epoch [1/1], Step [7185/8897], Loss: 5.3789\n",
      "Epoch [1/1], Step [7186/8897], Loss: 5.3892\n",
      "Epoch [1/1], Step [7187/8897], Loss: 5.3819\n",
      "Epoch [1/1], Step [7188/8897], Loss: 5.3289\n",
      "Epoch [1/1], Step [7189/8897], Loss: 5.3370\n",
      "Epoch [1/1], Step [7190/8897], Loss: 5.2777\n",
      "Epoch [1/1], Step [7191/8897], Loss: 5.3295\n",
      "Epoch [1/1], Step [7192/8897], Loss: 5.2892\n",
      "Epoch [1/1], Step [7193/8897], Loss: 5.6009\n",
      "Epoch [1/1], Step [7194/8897], Loss: 5.1110\n",
      "Epoch [1/1], Step [7195/8897], Loss: 5.5211\n",
      "Epoch [1/1], Step [7196/8897], Loss: 5.4337\n",
      "Epoch [1/1], Step [7197/8897], Loss: 5.1998\n",
      "Epoch [1/1], Step [7198/8897], Loss: 5.2856\n",
      "Epoch [1/1], Step [7199/8897], Loss: 5.3078\n",
      "Epoch [1/1], Step [7200/8897], Loss: 5.3188\n",
      "Epoch [1/1], Step [7201/8897], Loss: 5.4555\n",
      "Epoch [1/1], Step [7202/8897], Loss: 5.5538\n",
      "Epoch [1/1], Step [7203/8897], Loss: 5.4501\n",
      "Epoch [1/1], Step [7204/8897], Loss: 5.5741\n",
      "Epoch [1/1], Step [7205/8897], Loss: 5.5493\n",
      "Epoch [1/1], Step [7206/8897], Loss: 5.3700\n",
      "Epoch [1/1], Step [7207/8897], Loss: 5.7615\n",
      "Epoch [1/1], Step [7208/8897], Loss: 5.4172\n",
      "Epoch [1/1], Step [7209/8897], Loss: 5.4176\n",
      "Epoch [1/1], Step [7210/8897], Loss: 5.2061\n",
      "Epoch [1/1], Step [7211/8897], Loss: 5.1991\n",
      "Epoch [1/1], Step [7212/8897], Loss: 5.3000\n",
      "Epoch [1/1], Step [7213/8897], Loss: 5.4203\n",
      "Epoch [1/1], Step [7214/8897], Loss: 5.1375\n",
      "Epoch [1/1], Step [7215/8897], Loss: 5.1670\n",
      "Epoch [1/1], Step [7216/8897], Loss: 5.5095\n",
      "Epoch [1/1], Step [7217/8897], Loss: 5.5109\n",
      "Epoch [1/1], Step [7218/8897], Loss: 5.2067\n",
      "Epoch [1/1], Step [7219/8897], Loss: 5.4655\n",
      "Epoch [1/1], Step [7220/8897], Loss: 5.3221\n",
      "Epoch [1/1], Step [7221/8897], Loss: 5.3789\n",
      "Epoch [1/1], Step [7222/8897], Loss: 5.5064\n",
      "Epoch [1/1], Step [7223/8897], Loss: 5.4399\n",
      "Epoch [1/1], Step [7224/8897], Loss: 5.3646\n",
      "Epoch [1/1], Step [7225/8897], Loss: 5.1166\n",
      "Epoch [1/1], Step [7226/8897], Loss: 5.1760\n",
      "Epoch [1/1], Step [7227/8897], Loss: 5.4736\n",
      "Epoch [1/1], Step [7228/8897], Loss: 5.4761\n",
      "Epoch [1/1], Step [7229/8897], Loss: 5.2058\n",
      "Epoch [1/1], Step [7230/8897], Loss: 5.2791\n",
      "Epoch [1/1], Step [7231/8897], Loss: 5.3054\n",
      "Epoch [1/1], Step [7232/8897], Loss: 5.3876\n",
      "Epoch [1/1], Step [7233/8897], Loss: 5.2351\n",
      "Epoch [1/1], Step [7234/8897], Loss: 5.4228\n",
      "Epoch [1/1], Step [7235/8897], Loss: 5.3532\n",
      "Epoch [1/1], Step [7236/8897], Loss: 5.3282\n",
      "Epoch [1/1], Step [7237/8897], Loss: 5.2486\n",
      "Epoch [1/1], Step [7238/8897], Loss: 5.3769\n",
      "Epoch [1/1], Step [7239/8897], Loss: 5.4091\n",
      "Epoch [1/1], Step [7240/8897], Loss: 5.5551\n",
      "Epoch [1/1], Step [7241/8897], Loss: 5.5691\n",
      "Epoch [1/1], Step [7242/8897], Loss: 5.4326\n",
      "Epoch [1/1], Step [7243/8897], Loss: 5.6468\n",
      "Epoch [1/1], Step [7244/8897], Loss: 5.1920\n",
      "Epoch [1/1], Step [7245/8897], Loss: 5.4772\n",
      "Epoch [1/1], Step [7246/8897], Loss: 5.6942\n",
      "Epoch [1/1], Step [7247/8897], Loss: 5.7131\n",
      "Epoch [1/1], Step [7248/8897], Loss: 5.3836\n",
      "Epoch [1/1], Step [7249/8897], Loss: 5.2086\n",
      "Epoch [1/1], Step [7250/8897], Loss: 5.3890\n",
      "Epoch [1/1], Step [7251/8897], Loss: 5.4416\n",
      "Epoch [1/1], Step [7252/8897], Loss: 5.0932\n",
      "Epoch [1/1], Step [7253/8897], Loss: 5.3578\n",
      "Epoch [1/1], Step [7254/8897], Loss: 5.0746\n",
      "Epoch [1/1], Step [7255/8897], Loss: 5.2345\n",
      "Epoch [1/1], Step [7256/8897], Loss: 5.2752\n",
      "Epoch [1/1], Step [7257/8897], Loss: 5.1755\n",
      "Epoch [1/1], Step [7258/8897], Loss: 5.3114\n",
      "Epoch [1/1], Step [7259/8897], Loss: 5.4187\n",
      "Epoch [1/1], Step [7260/8897], Loss: 5.2888\n",
      "Epoch [1/1], Step [7261/8897], Loss: 5.3320\n",
      "Epoch [1/1], Step [7262/8897], Loss: 5.4521\n",
      "Epoch [1/1], Step [7263/8897], Loss: 5.3689\n",
      "Epoch [1/1], Step [7264/8897], Loss: 5.3307\n",
      "Epoch [1/1], Step [7265/8897], Loss: 5.5669\n",
      "Epoch [1/1], Step [7266/8897], Loss: 5.3498\n",
      "Epoch [1/1], Step [7267/8897], Loss: 5.1621\n",
      "Epoch [1/1], Step [7268/8897], Loss: 5.2320\n",
      "Epoch [1/1], Step [7269/8897], Loss: 5.6291\n",
      "Epoch [1/1], Step [7270/8897], Loss: 5.3119\n",
      "Epoch [1/1], Step [7271/8897], Loss: 5.2272\n",
      "Epoch [1/1], Step [7272/8897], Loss: 5.1364\n",
      "Epoch [1/1], Step [7273/8897], Loss: 5.2869\n",
      "Epoch [1/1], Step [7274/8897], Loss: 5.5556\n",
      "Epoch [1/1], Step [7275/8897], Loss: 5.1968\n",
      "Epoch [1/1], Step [7276/8897], Loss: 5.4020\n",
      "Epoch [1/1], Step [7277/8897], Loss: 5.4979\n",
      "Epoch [1/1], Step [7278/8897], Loss: 5.4235\n",
      "Epoch [1/1], Step [7279/8897], Loss: 5.4206\n",
      "Epoch [1/1], Step [7280/8897], Loss: 5.4505\n",
      "Epoch [1/1], Step [7281/8897], Loss: 5.4484\n",
      "Epoch [1/1], Step [7282/8897], Loss: 5.5079\n",
      "Epoch [1/1], Step [7283/8897], Loss: 5.4042\n",
      "Epoch [1/1], Step [7284/8897], Loss: 5.4421\n",
      "Epoch [1/1], Step [7285/8897], Loss: 5.3912\n",
      "Epoch [1/1], Step [7286/8897], Loss: 5.4760\n",
      "Epoch [1/1], Step [7287/8897], Loss: 5.3906\n",
      "Epoch [1/1], Step [7288/8897], Loss: 5.2690\n",
      "Epoch [1/1], Step [7289/8897], Loss: 5.3289\n",
      "Epoch [1/1], Step [7290/8897], Loss: 5.4888\n",
      "Epoch [1/1], Step [7291/8897], Loss: 5.3631\n",
      "Epoch [1/1], Step [7292/8897], Loss: 4.9632\n",
      "Epoch [1/1], Step [7293/8897], Loss: 5.1545\n",
      "Epoch [1/1], Step [7294/8897], Loss: 5.1857\n",
      "Epoch [1/1], Step [7295/8897], Loss: 5.1651\n",
      "Epoch [1/1], Step [7296/8897], Loss: 5.2424\n",
      "Epoch [1/1], Step [7297/8897], Loss: 5.4387\n",
      "Epoch [1/1], Step [7298/8897], Loss: 5.4151\n",
      "Epoch [1/1], Step [7299/8897], Loss: 5.4140\n",
      "Epoch [1/1], Step [7300/8897], Loss: 5.3554\n",
      "Epoch [1/1], Step [7301/8897], Loss: 5.2927\n",
      "Epoch [1/1], Step [7302/8897], Loss: 5.6508\n",
      "Epoch [1/1], Step [7303/8897], Loss: 5.6804\n",
      "Epoch [1/1], Step [7304/8897], Loss: 5.0943\n",
      "Epoch [1/1], Step [7305/8897], Loss: 5.5358\n",
      "Epoch [1/1], Step [7306/8897], Loss: 5.7956\n",
      "Epoch [1/1], Step [7307/8897], Loss: 5.3574\n",
      "Epoch [1/1], Step [7308/8897], Loss: 5.5017\n",
      "Epoch [1/1], Step [7309/8897], Loss: 5.2876\n",
      "Epoch [1/1], Step [7310/8897], Loss: 5.4587\n",
      "Epoch [1/1], Step [7311/8897], Loss: 5.0340\n",
      "Epoch [1/1], Step [7312/8897], Loss: 5.1875\n",
      "Epoch [1/1], Step [7313/8897], Loss: 5.4984\n",
      "Epoch [1/1], Step [7314/8897], Loss: 5.5062\n",
      "Epoch [1/1], Step [7315/8897], Loss: 5.2221\n",
      "Epoch [1/1], Step [7316/8897], Loss: 5.3458\n",
      "Epoch [1/1], Step [7317/8897], Loss: 5.2138\n",
      "Epoch [1/1], Step [7318/8897], Loss: 5.2632\n",
      "Epoch [1/1], Step [7319/8897], Loss: 5.5030\n",
      "Epoch [1/1], Step [7320/8897], Loss: 5.4959\n",
      "Epoch [1/1], Step [7321/8897], Loss: 5.3624\n",
      "Epoch [1/1], Step [7322/8897], Loss: 5.3995\n",
      "Epoch [1/1], Step [7323/8897], Loss: 5.5634\n",
      "Epoch [1/1], Step [7324/8897], Loss: 5.2177\n",
      "Epoch [1/1], Step [7325/8897], Loss: 5.6094\n",
      "Epoch [1/1], Step [7326/8897], Loss: 5.4612\n",
      "Epoch [1/1], Step [7327/8897], Loss: 5.3080\n",
      "Epoch [1/1], Step [7328/8897], Loss: 5.3348\n",
      "Epoch [1/1], Step [7329/8897], Loss: 5.2400\n",
      "Epoch [1/1], Step [7330/8897], Loss: 5.6208\n",
      "Epoch [1/1], Step [7331/8897], Loss: 5.1381\n",
      "Epoch [1/1], Step [7332/8897], Loss: 5.1390\n",
      "Epoch [1/1], Step [7333/8897], Loss: 5.4532\n",
      "Epoch [1/1], Step [7334/8897], Loss: 5.2767\n",
      "Epoch [1/1], Step [7335/8897], Loss: 5.6163\n",
      "Epoch [1/1], Step [7336/8897], Loss: 5.2804\n",
      "Epoch [1/1], Step [7337/8897], Loss: 5.2307\n",
      "Epoch [1/1], Step [7338/8897], Loss: 5.4225\n",
      "Epoch [1/1], Step [7339/8897], Loss: 5.3811\n",
      "Epoch [1/1], Step [7340/8897], Loss: 5.3243\n",
      "Epoch [1/1], Step [7341/8897], Loss: 5.4311\n",
      "Epoch [1/1], Step [7342/8897], Loss: 5.5887\n",
      "Epoch [1/1], Step [7343/8897], Loss: 5.3033\n",
      "Epoch [1/1], Step [7344/8897], Loss: 5.1188\n",
      "Epoch [1/1], Step [7345/8897], Loss: 5.3044\n",
      "Epoch [1/1], Step [7346/8897], Loss: 5.4445\n",
      "Epoch [1/1], Step [7347/8897], Loss: 5.2970\n",
      "Epoch [1/1], Step [7348/8897], Loss: 5.5874\n",
      "Epoch [1/1], Step [7349/8897], Loss: 5.4939\n",
      "Epoch [1/1], Step [7350/8897], Loss: 5.6508\n",
      "Epoch [1/1], Step [7351/8897], Loss: 5.4074\n",
      "Epoch [1/1], Step [7352/8897], Loss: 5.0926\n",
      "Epoch [1/1], Step [7353/8897], Loss: 5.1873\n",
      "Epoch [1/1], Step [7354/8897], Loss: 5.1277\n",
      "Epoch [1/1], Step [7355/8897], Loss: 5.3016\n",
      "Epoch [1/1], Step [7356/8897], Loss: 5.1320\n",
      "Epoch [1/1], Step [7357/8897], Loss: 5.5848\n",
      "Epoch [1/1], Step [7358/8897], Loss: 5.2874\n",
      "Epoch [1/1], Step [7359/8897], Loss: 5.4570\n",
      "Epoch [1/1], Step [7360/8897], Loss: 5.5928\n",
      "Epoch [1/1], Step [7361/8897], Loss: 5.2153\n",
      "Epoch [1/1], Step [7362/8897], Loss: 5.3763\n",
      "Epoch [1/1], Step [7363/8897], Loss: 5.3130\n",
      "Epoch [1/1], Step [7364/8897], Loss: 5.4313\n",
      "Epoch [1/1], Step [7365/8897], Loss: 5.2819\n",
      "Epoch [1/1], Step [7366/8897], Loss: 5.3710\n",
      "Epoch [1/1], Step [7367/8897], Loss: 5.3267\n",
      "Epoch [1/1], Step [7368/8897], Loss: 5.2186\n",
      "Epoch [1/1], Step [7369/8897], Loss: 5.4433\n",
      "Epoch [1/1], Step [7370/8897], Loss: 5.2442\n",
      "Epoch [1/1], Step [7371/8897], Loss: 5.4266\n",
      "Epoch [1/1], Step [7372/8897], Loss: 5.5074\n",
      "Epoch [1/1], Step [7373/8897], Loss: 5.2535\n",
      "Epoch [1/1], Step [7374/8897], Loss: 5.2117\n",
      "Epoch [1/1], Step [7375/8897], Loss: 5.4069\n",
      "Epoch [1/1], Step [7376/8897], Loss: 5.3902\n",
      "Epoch [1/1], Step [7377/8897], Loss: 5.1552\n",
      "Epoch [1/1], Step [7378/8897], Loss: 5.5177\n",
      "Epoch [1/1], Step [7379/8897], Loss: 5.3088\n",
      "Epoch [1/1], Step [7380/8897], Loss: 5.2097\n",
      "Epoch [1/1], Step [7381/8897], Loss: 5.3570\n",
      "Epoch [1/1], Step [7382/8897], Loss: 5.0806\n",
      "Epoch [1/1], Step [7383/8897], Loss: 5.4833\n",
      "Epoch [1/1], Step [7384/8897], Loss: 5.1993\n",
      "Epoch [1/1], Step [7385/8897], Loss: 5.1159\n",
      "Epoch [1/1], Step [7386/8897], Loss: 5.3570\n",
      "Epoch [1/1], Step [7387/8897], Loss: 4.9923\n",
      "Epoch [1/1], Step [7388/8897], Loss: 5.4038\n",
      "Epoch [1/1], Step [7389/8897], Loss: 5.1970\n",
      "Epoch [1/1], Step [7390/8897], Loss: 5.2424\n",
      "Epoch [1/1], Step [7391/8897], Loss: 5.1708\n",
      "Epoch [1/1], Step [7392/8897], Loss: 5.5083\n",
      "Epoch [1/1], Step [7393/8897], Loss: 5.0202\n",
      "Epoch [1/1], Step [7394/8897], Loss: 5.1618\n",
      "Epoch [1/1], Step [7395/8897], Loss: 5.1068\n",
      "Epoch [1/1], Step [7396/8897], Loss: 5.3681\n",
      "Epoch [1/1], Step [7397/8897], Loss: 5.4074\n",
      "Epoch [1/1], Step [7398/8897], Loss: 5.3756\n",
      "Epoch [1/1], Step [7399/8897], Loss: 5.5677\n",
      "Epoch [1/1], Step [7400/8897], Loss: 5.3615\n",
      "Epoch [1/1], Step [7401/8897], Loss: 5.1599\n",
      "Epoch [1/1], Step [7402/8897], Loss: 5.3557\n",
      "Epoch [1/1], Step [7403/8897], Loss: 5.3164\n",
      "Epoch [1/1], Step [7404/8897], Loss: 5.2775\n",
      "Epoch [1/1], Step [7405/8897], Loss: 5.4216\n",
      "Epoch [1/1], Step [7406/8897], Loss: 5.5062\n",
      "Epoch [1/1], Step [7407/8897], Loss: 5.4939\n",
      "Epoch [1/1], Step [7408/8897], Loss: 5.4296\n",
      "Epoch [1/1], Step [7409/8897], Loss: 5.2108\n",
      "Epoch [1/1], Step [7410/8897], Loss: 4.9623\n",
      "Epoch [1/1], Step [7411/8897], Loss: 5.2449\n",
      "Epoch [1/1], Step [7412/8897], Loss: 5.4616\n",
      "Epoch [1/1], Step [7413/8897], Loss: 5.3397\n",
      "Epoch [1/1], Step [7414/8897], Loss: 5.3948\n",
      "Epoch [1/1], Step [7415/8897], Loss: 5.3050\n",
      "Epoch [1/1], Step [7416/8897], Loss: 5.3189\n",
      "Epoch [1/1], Step [7417/8897], Loss: 5.4651\n",
      "Epoch [1/1], Step [7418/8897], Loss: 5.2529\n",
      "Epoch [1/1], Step [7419/8897], Loss: 5.6716\n",
      "Epoch [1/1], Step [7420/8897], Loss: 5.4700\n",
      "Epoch [1/1], Step [7421/8897], Loss: 5.3238\n",
      "Epoch [1/1], Step [7422/8897], Loss: 5.5198\n",
      "Epoch [1/1], Step [7423/8897], Loss: 5.2448\n",
      "Epoch [1/1], Step [7424/8897], Loss: 5.2581\n",
      "Epoch [1/1], Step [7425/8897], Loss: 5.4139\n",
      "Epoch [1/1], Step [7426/8897], Loss: 5.2563\n",
      "Epoch [1/1], Step [7427/8897], Loss: 5.5435\n",
      "Epoch [1/1], Step [7428/8897], Loss: 5.4656\n",
      "Epoch [1/1], Step [7429/8897], Loss: 5.1106\n",
      "Epoch [1/1], Step [7430/8897], Loss: 4.9215\n",
      "Epoch [1/1], Step [7431/8897], Loss: 4.9020\n",
      "Epoch [1/1], Step [7432/8897], Loss: 5.5406\n",
      "Epoch [1/1], Step [7433/8897], Loss: 5.4010\n",
      "Epoch [1/1], Step [7434/8897], Loss: 5.2703\n",
      "Epoch [1/1], Step [7435/8897], Loss: 5.3802\n",
      "Epoch [1/1], Step [7436/8897], Loss: 5.4601\n",
      "Epoch [1/1], Step [7437/8897], Loss: 5.1482\n",
      "Epoch [1/1], Step [7438/8897], Loss: 5.3968\n",
      "Epoch [1/1], Step [7439/8897], Loss: 5.4426\n",
      "Epoch [1/1], Step [7440/8897], Loss: 5.4370\n",
      "Epoch [1/1], Step [7441/8897], Loss: 5.2702\n",
      "Epoch [1/1], Step [7442/8897], Loss: 5.4553\n",
      "Epoch [1/1], Step [7443/8897], Loss: 5.3946\n",
      "Epoch [1/1], Step [7444/8897], Loss: 5.2893\n",
      "Epoch [1/1], Step [7445/8897], Loss: 5.2804\n",
      "Epoch [1/1], Step [7446/8897], Loss: 5.4126\n",
      "Epoch [1/1], Step [7447/8897], Loss: 5.3037\n",
      "Epoch [1/1], Step [7448/8897], Loss: 5.5726\n",
      "Epoch [1/1], Step [7449/8897], Loss: 5.1064\n",
      "Epoch [1/1], Step [7450/8897], Loss: 5.2482\n",
      "Epoch [1/1], Step [7451/8897], Loss: 5.4927\n",
      "Epoch [1/1], Step [7452/8897], Loss: 5.5157\n",
      "Epoch [1/1], Step [7453/8897], Loss: 5.5373\n",
      "Epoch [1/1], Step [7454/8897], Loss: 5.1861\n",
      "Epoch [1/1], Step [7455/8897], Loss: 5.4177\n",
      "Epoch [1/1], Step [7456/8897], Loss: 5.3585\n",
      "Epoch [1/1], Step [7457/8897], Loss: 5.5841\n",
      "Epoch [1/1], Step [7458/8897], Loss: 5.4197\n",
      "Epoch [1/1], Step [7459/8897], Loss: 5.3671\n",
      "Epoch [1/1], Step [7460/8897], Loss: 5.3042\n",
      "Epoch [1/1], Step [7461/8897], Loss: 5.4623\n",
      "Epoch [1/1], Step [7462/8897], Loss: 5.3075\n",
      "Epoch [1/1], Step [7463/8897], Loss: 5.2831\n",
      "Epoch [1/1], Step [7464/8897], Loss: 5.3611\n",
      "Epoch [1/1], Step [7465/8897], Loss: 5.3465\n",
      "Epoch [1/1], Step [7466/8897], Loss: 5.4282\n",
      "Epoch [1/1], Step [7467/8897], Loss: 5.1432\n",
      "Epoch [1/1], Step [7468/8897], Loss: 5.2900\n",
      "Epoch [1/1], Step [7469/8897], Loss: 5.1544\n",
      "Epoch [1/1], Step [7470/8897], Loss: 5.2296\n",
      "Epoch [1/1], Step [7471/8897], Loss: 5.3539\n",
      "Epoch [1/1], Step [7472/8897], Loss: 5.1270\n",
      "Epoch [1/1], Step [7473/8897], Loss: 5.3553\n",
      "Epoch [1/1], Step [7474/8897], Loss: 5.3311\n",
      "Epoch [1/1], Step [7475/8897], Loss: 5.2824\n",
      "Epoch [1/1], Step [7476/8897], Loss: 5.3063\n",
      "Epoch [1/1], Step [7477/8897], Loss: 5.4825\n",
      "Epoch [1/1], Step [7478/8897], Loss: 5.3438\n",
      "Epoch [1/1], Step [7479/8897], Loss: 5.3150\n",
      "Epoch [1/1], Step [7480/8897], Loss: 5.5499\n",
      "Epoch [1/1], Step [7481/8897], Loss: 5.3749\n",
      "Epoch [1/1], Step [7482/8897], Loss: 5.2338\n",
      "Epoch [1/1], Step [7483/8897], Loss: 5.5237\n",
      "Epoch [1/1], Step [7484/8897], Loss: 5.4561\n",
      "Epoch [1/1], Step [7485/8897], Loss: 5.3524\n",
      "Epoch [1/1], Step [7486/8897], Loss: 5.2315\n",
      "Epoch [1/1], Step [7487/8897], Loss: 5.4516\n",
      "Epoch [1/1], Step [7488/8897], Loss: 5.2485\n",
      "Epoch [1/1], Step [7489/8897], Loss: 5.1628\n",
      "Epoch [1/1], Step [7490/8897], Loss: 5.0384\n",
      "Epoch [1/1], Step [7491/8897], Loss: 5.2278\n",
      "Epoch [1/1], Step [7492/8897], Loss: 5.1136\n",
      "Epoch [1/1], Step [7493/8897], Loss: 5.1385\n",
      "Epoch [1/1], Step [7494/8897], Loss: 5.4040\n",
      "Epoch [1/1], Step [7495/8897], Loss: 5.2881\n",
      "Epoch [1/1], Step [7496/8897], Loss: 5.3822\n",
      "Epoch [1/1], Step [7497/8897], Loss: 5.3314\n",
      "Epoch [1/1], Step [7498/8897], Loss: 5.4665\n",
      "Epoch [1/1], Step [7499/8897], Loss: 5.2146\n",
      "Epoch [1/1], Step [7500/8897], Loss: 5.4385\n",
      "Epoch [1/1], Step [7501/8897], Loss: 5.2787\n",
      "Epoch [1/1], Step [7502/8897], Loss: 5.2487\n",
      "Epoch [1/1], Step [7503/8897], Loss: 5.1931\n",
      "Epoch [1/1], Step [7504/8897], Loss: 5.3270\n",
      "Epoch [1/1], Step [7505/8897], Loss: 5.2880\n",
      "Epoch [1/1], Step [7506/8897], Loss: 5.2710\n",
      "Epoch [1/1], Step [7507/8897], Loss: 5.1901\n",
      "Epoch [1/1], Step [7508/8897], Loss: 5.4321\n",
      "Epoch [1/1], Step [7509/8897], Loss: 5.2246\n",
      "Epoch [1/1], Step [7510/8897], Loss: 5.3516\n",
      "Epoch [1/1], Step [7511/8897], Loss: 5.3320\n",
      "Epoch [1/1], Step [7512/8897], Loss: 5.2865\n",
      "Epoch [1/1], Step [7513/8897], Loss: 5.5713\n",
      "Epoch [1/1], Step [7514/8897], Loss: 5.4435\n",
      "Epoch [1/1], Step [7515/8897], Loss: 5.4415\n",
      "Epoch [1/1], Step [7516/8897], Loss: 5.4258\n",
      "Epoch [1/1], Step [7517/8897], Loss: 5.4684\n",
      "Epoch [1/1], Step [7518/8897], Loss: 5.1846\n",
      "Epoch [1/1], Step [7519/8897], Loss: 5.3502\n",
      "Epoch [1/1], Step [7520/8897], Loss: 5.3606\n",
      "Epoch [1/1], Step [7521/8897], Loss: 5.5954\n",
      "Epoch [1/1], Step [7522/8897], Loss: 5.6546\n",
      "Epoch [1/1], Step [7523/8897], Loss: 5.2958\n",
      "Epoch [1/1], Step [7524/8897], Loss: 5.4040\n",
      "Epoch [1/1], Step [7525/8897], Loss: 5.3640\n",
      "Epoch [1/1], Step [7526/8897], Loss: 5.6141\n",
      "Epoch [1/1], Step [7527/8897], Loss: 5.5025\n",
      "Epoch [1/1], Step [7528/8897], Loss: 5.4853\n",
      "Epoch [1/1], Step [7529/8897], Loss: 5.2016\n",
      "Epoch [1/1], Step [7530/8897], Loss: 5.3106\n",
      "Epoch [1/1], Step [7531/8897], Loss: 5.2025\n",
      "Epoch [1/1], Step [7532/8897], Loss: 5.5514\n",
      "Epoch [1/1], Step [7533/8897], Loss: 5.0794\n",
      "Epoch [1/1], Step [7534/8897], Loss: 5.4343\n",
      "Epoch [1/1], Step [7535/8897], Loss: 5.4111\n",
      "Epoch [1/1], Step [7536/8897], Loss: 5.5080\n",
      "Epoch [1/1], Step [7537/8897], Loss: 5.3537\n",
      "Epoch [1/1], Step [7538/8897], Loss: 5.5721\n",
      "Epoch [1/1], Step [7539/8897], Loss: 5.3751\n",
      "Epoch [1/1], Step [7540/8897], Loss: 5.5707\n",
      "Epoch [1/1], Step [7541/8897], Loss: 5.2353\n",
      "Epoch [1/1], Step [7542/8897], Loss: 5.4092\n",
      "Epoch [1/1], Step [7543/8897], Loss: 5.3545\n",
      "Epoch [1/1], Step [7544/8897], Loss: 5.2033\n",
      "Epoch [1/1], Step [7545/8897], Loss: 5.1288\n",
      "Epoch [1/1], Step [7546/8897], Loss: 5.3960\n",
      "Epoch [1/1], Step [7547/8897], Loss: 5.5825\n",
      "Epoch [1/1], Step [7548/8897], Loss: 5.5461\n",
      "Epoch [1/1], Step [7549/8897], Loss: 5.3638\n",
      "Epoch [1/1], Step [7550/8897], Loss: 5.1044\n",
      "Epoch [1/1], Step [7551/8897], Loss: 5.2718\n",
      "Epoch [1/1], Step [7552/8897], Loss: 5.3725\n",
      "Epoch [1/1], Step [7553/8897], Loss: 5.4162\n",
      "Epoch [1/1], Step [7554/8897], Loss: 5.3563\n",
      "Epoch [1/1], Step [7555/8897], Loss: 5.4008\n",
      "Epoch [1/1], Step [7556/8897], Loss: 5.1531\n",
      "Epoch [1/1], Step [7557/8897], Loss: 5.5532\n",
      "Epoch [1/1], Step [7558/8897], Loss: 5.2998\n",
      "Epoch [1/1], Step [7559/8897], Loss: 5.0913\n",
      "Epoch [1/1], Step [7560/8897], Loss: 5.4138\n",
      "Epoch [1/1], Step [7561/8897], Loss: 5.5797\n",
      "Epoch [1/1], Step [7562/8897], Loss: 5.1404\n",
      "Epoch [1/1], Step [7563/8897], Loss: 5.2395\n",
      "Epoch [1/1], Step [7564/8897], Loss: 5.3604\n",
      "Epoch [1/1], Step [7565/8897], Loss: 5.2712\n",
      "Epoch [1/1], Step [7566/8897], Loss: 5.1975\n",
      "Epoch [1/1], Step [7567/8897], Loss: 5.2652\n",
      "Epoch [1/1], Step [7568/8897], Loss: 5.2767\n",
      "Epoch [1/1], Step [7569/8897], Loss: 5.2519\n",
      "Epoch [1/1], Step [7570/8897], Loss: 5.1830\n",
      "Epoch [1/1], Step [7571/8897], Loss: 5.2005\n",
      "Epoch [1/1], Step [7572/8897], Loss: 5.2251\n",
      "Epoch [1/1], Step [7573/8897], Loss: 5.0830\n",
      "Epoch [1/1], Step [7574/8897], Loss: 5.4727\n",
      "Epoch [1/1], Step [7575/8897], Loss: 5.3767\n",
      "Epoch [1/1], Step [7576/8897], Loss: 5.7008\n",
      "Epoch [1/1], Step [7577/8897], Loss: 5.5345\n",
      "Epoch [1/1], Step [7578/8897], Loss: 5.5650\n",
      "Epoch [1/1], Step [7579/8897], Loss: 5.5675\n",
      "Epoch [1/1], Step [7580/8897], Loss: 5.3701\n",
      "Epoch [1/1], Step [7581/8897], Loss: 5.3474\n",
      "Epoch [1/1], Step [7582/8897], Loss: 5.0969\n",
      "Epoch [1/1], Step [7583/8897], Loss: 5.5474\n",
      "Epoch [1/1], Step [7584/8897], Loss: 5.1335\n",
      "Epoch [1/1], Step [7585/8897], Loss: 5.3045\n",
      "Epoch [1/1], Step [7586/8897], Loss: 5.2238\n",
      "Epoch [1/1], Step [7587/8897], Loss: 5.3166\n",
      "Epoch [1/1], Step [7588/8897], Loss: 5.4470\n",
      "Epoch [1/1], Step [7589/8897], Loss: 5.4791\n",
      "Epoch [1/1], Step [7590/8897], Loss: 5.4122\n",
      "Epoch [1/1], Step [7591/8897], Loss: 5.3289\n",
      "Epoch [1/1], Step [7592/8897], Loss: 5.5255\n",
      "Epoch [1/1], Step [7593/8897], Loss: 5.2512\n",
      "Epoch [1/1], Step [7594/8897], Loss: 5.0208\n",
      "Epoch [1/1], Step [7595/8897], Loss: 5.5416\n",
      "Epoch [1/1], Step [7596/8897], Loss: 5.3474\n",
      "Epoch [1/1], Step [7597/8897], Loss: 5.4393\n",
      "Epoch [1/1], Step [7598/8897], Loss: 5.2999\n",
      "Epoch [1/1], Step [7599/8897], Loss: 5.7355\n",
      "Epoch [1/1], Step [7600/8897], Loss: 5.4175\n",
      "Epoch [1/1], Step [7601/8897], Loss: 5.2780\n",
      "Epoch [1/1], Step [7602/8897], Loss: 5.3384\n",
      "Epoch [1/1], Step [7603/8897], Loss: 5.2527\n",
      "Epoch [1/1], Step [7604/8897], Loss: 5.3468\n",
      "Epoch [1/1], Step [7605/8897], Loss: 5.1992\n",
      "Epoch [1/1], Step [7606/8897], Loss: 5.3071\n",
      "Epoch [1/1], Step [7607/8897], Loss: 5.2599\n",
      "Epoch [1/1], Step [7608/8897], Loss: 5.2423\n",
      "Epoch [1/1], Step [7609/8897], Loss: 5.3493\n",
      "Epoch [1/1], Step [7610/8897], Loss: 5.3016\n",
      "Epoch [1/1], Step [7611/8897], Loss: 5.5347\n",
      "Epoch [1/1], Step [7612/8897], Loss: 5.1194\n",
      "Epoch [1/1], Step [7613/8897], Loss: 5.2926\n",
      "Epoch [1/1], Step [7614/8897], Loss: 5.6615\n",
      "Epoch [1/1], Step [7615/8897], Loss: 5.7123\n",
      "Epoch [1/1], Step [7616/8897], Loss: 5.0035\n",
      "Epoch [1/1], Step [7617/8897], Loss: 5.3544\n",
      "Epoch [1/1], Step [7618/8897], Loss: 5.2370\n",
      "Epoch [1/1], Step [7619/8897], Loss: 5.0945\n",
      "Epoch [1/1], Step [7620/8897], Loss: 5.4766\n",
      "Epoch [1/1], Step [7621/8897], Loss: 5.5850\n",
      "Epoch [1/1], Step [7622/8897], Loss: 5.6148\n",
      "Epoch [1/1], Step [7623/8897], Loss: 5.5023\n",
      "Epoch [1/1], Step [7624/8897], Loss: 5.5320\n",
      "Epoch [1/1], Step [7625/8897], Loss: 5.4370\n",
      "Epoch [1/1], Step [7626/8897], Loss: 5.6837\n",
      "Epoch [1/1], Step [7627/8897], Loss: 5.3261\n",
      "Epoch [1/1], Step [7628/8897], Loss: 5.3880\n",
      "Epoch [1/1], Step [7629/8897], Loss: 5.4954\n",
      "Epoch [1/1], Step [7630/8897], Loss: 5.5824\n",
      "Epoch [1/1], Step [7631/8897], Loss: 5.3495\n",
      "Epoch [1/1], Step [7632/8897], Loss: 5.4153\n",
      "Epoch [1/1], Step [7633/8897], Loss: 5.4165\n",
      "Epoch [1/1], Step [7634/8897], Loss: 5.3619\n",
      "Epoch [1/1], Step [7635/8897], Loss: 5.3370\n",
      "Epoch [1/1], Step [7636/8897], Loss: 5.3123\n",
      "Epoch [1/1], Step [7637/8897], Loss: 5.2206\n",
      "Epoch [1/1], Step [7638/8897], Loss: 5.1780\n",
      "Epoch [1/1], Step [7639/8897], Loss: 5.0588\n",
      "Epoch [1/1], Step [7640/8897], Loss: 5.4442\n",
      "Epoch [1/1], Step [7641/8897], Loss: 5.2509\n",
      "Epoch [1/1], Step [7642/8897], Loss: 4.9145\n",
      "Epoch [1/1], Step [7643/8897], Loss: 5.4440\n",
      "Epoch [1/1], Step [7644/8897], Loss: 5.6279\n",
      "Epoch [1/1], Step [7645/8897], Loss: 5.2649\n",
      "Epoch [1/1], Step [7646/8897], Loss: 5.4684\n",
      "Epoch [1/1], Step [7647/8897], Loss: 5.2370\n",
      "Epoch [1/1], Step [7648/8897], Loss: 5.3115\n",
      "Epoch [1/1], Step [7649/8897], Loss: 5.1034\n",
      "Epoch [1/1], Step [7650/8897], Loss: 5.0518\n",
      "Epoch [1/1], Step [7651/8897], Loss: 5.6121\n",
      "Epoch [1/1], Step [7652/8897], Loss: 5.3492\n",
      "Epoch [1/1], Step [7653/8897], Loss: 5.5649\n",
      "Epoch [1/1], Step [7654/8897], Loss: 4.9982\n",
      "Epoch [1/1], Step [7655/8897], Loss: 5.3880\n",
      "Epoch [1/1], Step [7656/8897], Loss: 5.1432\n",
      "Epoch [1/1], Step [7657/8897], Loss: 5.3451\n",
      "Epoch [1/1], Step [7658/8897], Loss: 5.5327\n",
      "Epoch [1/1], Step [7659/8897], Loss: 5.2147\n",
      "Epoch [1/1], Step [7660/8897], Loss: 5.2936\n",
      "Epoch [1/1], Step [7661/8897], Loss: 5.5044\n",
      "Epoch [1/1], Step [7662/8897], Loss: 5.3379\n",
      "Epoch [1/1], Step [7663/8897], Loss: 5.1333\n",
      "Epoch [1/1], Step [7664/8897], Loss: 5.5545\n",
      "Epoch [1/1], Step [7665/8897], Loss: 5.3949\n",
      "Epoch [1/1], Step [7666/8897], Loss: 5.5841\n",
      "Epoch [1/1], Step [7667/8897], Loss: 5.4610\n",
      "Epoch [1/1], Step [7668/8897], Loss: 5.2387\n",
      "Epoch [1/1], Step [7669/8897], Loss: 5.3289\n",
      "Epoch [1/1], Step [7670/8897], Loss: 5.1994\n",
      "Epoch [1/1], Step [7671/8897], Loss: 5.2018\n",
      "Epoch [1/1], Step [7672/8897], Loss: 5.8627\n",
      "Epoch [1/1], Step [7673/8897], Loss: 5.4835\n",
      "Epoch [1/1], Step [7674/8897], Loss: 5.3124\n",
      "Epoch [1/1], Step [7675/8897], Loss: 5.3334\n",
      "Epoch [1/1], Step [7676/8897], Loss: 5.2783\n",
      "Epoch [1/1], Step [7677/8897], Loss: 5.4724\n",
      "Epoch [1/1], Step [7678/8897], Loss: 5.1954\n",
      "Epoch [1/1], Step [7679/8897], Loss: 5.7679\n",
      "Epoch [1/1], Step [7680/8897], Loss: 5.2224\n",
      "Epoch [1/1], Step [7681/8897], Loss: 5.2375\n",
      "Epoch [1/1], Step [7682/8897], Loss: 5.7367\n",
      "Epoch [1/1], Step [7683/8897], Loss: 5.2953\n",
      "Epoch [1/1], Step [7684/8897], Loss: 5.1944\n",
      "Epoch [1/1], Step [7685/8897], Loss: 5.1293\n",
      "Epoch [1/1], Step [7686/8897], Loss: 5.2309\n",
      "Epoch [1/1], Step [7687/8897], Loss: 5.3365\n",
      "Epoch [1/1], Step [7688/8897], Loss: 5.2117\n",
      "Epoch [1/1], Step [7689/8897], Loss: 5.4586\n",
      "Epoch [1/1], Step [7690/8897], Loss: 5.5805\n",
      "Epoch [1/1], Step [7691/8897], Loss: 5.2569\n",
      "Epoch [1/1], Step [7692/8897], Loss: 5.2927\n",
      "Epoch [1/1], Step [7693/8897], Loss: 5.1122\n",
      "Epoch [1/1], Step [7694/8897], Loss: 5.3941\n",
      "Epoch [1/1], Step [7695/8897], Loss: 5.5088\n",
      "Epoch [1/1], Step [7696/8897], Loss: 5.1596\n",
      "Epoch [1/1], Step [7697/8897], Loss: 5.2351\n",
      "Epoch [1/1], Step [7698/8897], Loss: 5.4172\n",
      "Epoch [1/1], Step [7699/8897], Loss: 5.5640\n",
      "Epoch [1/1], Step [7700/8897], Loss: 5.4692\n",
      "Epoch [1/1], Step [7701/8897], Loss: 5.2529\n",
      "Epoch [1/1], Step [7702/8897], Loss: 5.1527\n",
      "Epoch [1/1], Step [7703/8897], Loss: 5.3448\n",
      "Epoch [1/1], Step [7704/8897], Loss: 5.2965\n",
      "Epoch [1/1], Step [7705/8897], Loss: 5.4224\n",
      "Epoch [1/1], Step [7706/8897], Loss: 5.3048\n",
      "Epoch [1/1], Step [7707/8897], Loss: 5.4126\n",
      "Epoch [1/1], Step [7708/8897], Loss: 5.3091\n",
      "Epoch [1/1], Step [7709/8897], Loss: 5.3465\n",
      "Epoch [1/1], Step [7710/8897], Loss: 5.2153\n",
      "Epoch [1/1], Step [7711/8897], Loss: 5.4924\n",
      "Epoch [1/1], Step [7712/8897], Loss: 5.4284\n",
      "Epoch [1/1], Step [7713/8897], Loss: 5.0839\n",
      "Epoch [1/1], Step [7714/8897], Loss: 5.2826\n",
      "Epoch [1/1], Step [7715/8897], Loss: 5.2540\n",
      "Epoch [1/1], Step [7716/8897], Loss: 5.3350\n",
      "Epoch [1/1], Step [7717/8897], Loss: 5.2644\n",
      "Epoch [1/1], Step [7718/8897], Loss: 5.1699\n",
      "Epoch [1/1], Step [7719/8897], Loss: 5.3636\n",
      "Epoch [1/1], Step [7720/8897], Loss: 5.2313\n",
      "Epoch [1/1], Step [7721/8897], Loss: 5.2582\n",
      "Epoch [1/1], Step [7722/8897], Loss: 5.4264\n",
      "Epoch [1/1], Step [7723/8897], Loss: 5.2772\n",
      "Epoch [1/1], Step [7724/8897], Loss: 5.2889\n",
      "Epoch [1/1], Step [7725/8897], Loss: 5.1336\n",
      "Epoch [1/1], Step [7726/8897], Loss: 5.4218\n",
      "Epoch [1/1], Step [7727/8897], Loss: 5.2961\n",
      "Epoch [1/1], Step [7728/8897], Loss: 5.3759\n",
      "Epoch [1/1], Step [7729/8897], Loss: 5.4296\n",
      "Epoch [1/1], Step [7730/8897], Loss: 5.4863\n",
      "Epoch [1/1], Step [7731/8897], Loss: 5.1374\n",
      "Epoch [1/1], Step [7732/8897], Loss: 5.1257\n",
      "Epoch [1/1], Step [7733/8897], Loss: 5.1384\n",
      "Epoch [1/1], Step [7734/8897], Loss: 5.3821\n",
      "Epoch [1/1], Step [7735/8897], Loss: 5.5173\n",
      "Epoch [1/1], Step [7736/8897], Loss: 5.6550\n",
      "Epoch [1/1], Step [7737/8897], Loss: 5.1714\n",
      "Epoch [1/1], Step [7738/8897], Loss: 5.3037\n",
      "Epoch [1/1], Step [7739/8897], Loss: 5.3137\n",
      "Epoch [1/1], Step [7740/8897], Loss: 5.2205\n",
      "Epoch [1/1], Step [7741/8897], Loss: 5.1932\n",
      "Epoch [1/1], Step [7742/8897], Loss: 5.2320\n",
      "Epoch [1/1], Step [7743/8897], Loss: 5.1263\n",
      "Epoch [1/1], Step [7744/8897], Loss: 5.4899\n",
      "Epoch [1/1], Step [7745/8897], Loss: 5.4796\n",
      "Epoch [1/1], Step [7746/8897], Loss: 5.1778\n",
      "Epoch [1/1], Step [7747/8897], Loss: 5.6179\n",
      "Epoch [1/1], Step [7748/8897], Loss: 5.5253\n",
      "Epoch [1/1], Step [7749/8897], Loss: 5.2087\n",
      "Epoch [1/1], Step [7750/8897], Loss: 5.2547\n",
      "Epoch [1/1], Step [7751/8897], Loss: 5.5420\n",
      "Epoch [1/1], Step [7752/8897], Loss: 5.4354\n",
      "Epoch [1/1], Step [7753/8897], Loss: 5.4182\n",
      "Epoch [1/1], Step [7754/8897], Loss: 5.3032\n",
      "Epoch [1/1], Step [7755/8897], Loss: 5.7132\n",
      "Epoch [1/1], Step [7756/8897], Loss: 5.5142\n",
      "Epoch [1/1], Step [7757/8897], Loss: 5.1821\n",
      "Epoch [1/1], Step [7758/8897], Loss: 5.3370\n",
      "Epoch [1/1], Step [7759/8897], Loss: 5.6860\n",
      "Epoch [1/1], Step [7760/8897], Loss: 5.5397\n",
      "Epoch [1/1], Step [7761/8897], Loss: 5.5321\n",
      "Epoch [1/1], Step [7762/8897], Loss: 5.4026\n",
      "Epoch [1/1], Step [7763/8897], Loss: 5.3160\n",
      "Epoch [1/1], Step [7764/8897], Loss: 5.3679\n",
      "Epoch [1/1], Step [7765/8897], Loss: 5.5014\n",
      "Epoch [1/1], Step [7766/8897], Loss: 5.2378\n",
      "Epoch [1/1], Step [7767/8897], Loss: 5.3510\n",
      "Epoch [1/1], Step [7768/8897], Loss: 5.3410\n",
      "Epoch [1/1], Step [7769/8897], Loss: 5.4644\n",
      "Epoch [1/1], Step [7770/8897], Loss: 5.4901\n",
      "Epoch [1/1], Step [7771/8897], Loss: 5.6822\n",
      "Epoch [1/1], Step [7772/8897], Loss: 5.1549\n",
      "Epoch [1/1], Step [7773/8897], Loss: 5.4875\n",
      "Epoch [1/1], Step [7774/8897], Loss: 5.2750\n",
      "Epoch [1/1], Step [7775/8897], Loss: 5.5817\n",
      "Epoch [1/1], Step [7776/8897], Loss: 5.2089\n",
      "Epoch [1/1], Step [7777/8897], Loss: 5.3322\n",
      "Epoch [1/1], Step [7778/8897], Loss: 5.3928\n",
      "Epoch [1/1], Step [7779/8897], Loss: 5.6300\n",
      "Epoch [1/1], Step [7780/8897], Loss: 5.1369\n",
      "Epoch [1/1], Step [7781/8897], Loss: 5.2422\n",
      "Epoch [1/1], Step [7782/8897], Loss: 5.3417\n",
      "Epoch [1/1], Step [7783/8897], Loss: 5.3803\n",
      "Epoch [1/1], Step [7784/8897], Loss: 5.3257\n",
      "Epoch [1/1], Step [7785/8897], Loss: 5.3902\n",
      "Epoch [1/1], Step [7786/8897], Loss: 5.2396\n",
      "Epoch [1/1], Step [7787/8897], Loss: 5.2988\n",
      "Epoch [1/1], Step [7788/8897], Loss: 5.4873\n",
      "Epoch [1/1], Step [7789/8897], Loss: 5.2054\n",
      "Epoch [1/1], Step [7790/8897], Loss: 5.1829\n",
      "Epoch [1/1], Step [7791/8897], Loss: 5.5074\n",
      "Epoch [1/1], Step [7792/8897], Loss: 5.4800\n",
      "Epoch [1/1], Step [7793/8897], Loss: 5.5373\n",
      "Epoch [1/1], Step [7794/8897], Loss: 5.2731\n",
      "Epoch [1/1], Step [7795/8897], Loss: 5.5749\n",
      "Epoch [1/1], Step [7796/8897], Loss: 5.3386\n",
      "Epoch [1/1], Step [7797/8897], Loss: 5.4932\n",
      "Epoch [1/1], Step [7798/8897], Loss: 5.1839\n",
      "Epoch [1/1], Step [7799/8897], Loss: 5.1569\n",
      "Epoch [1/1], Step [7800/8897], Loss: 5.2001\n",
      "Epoch [1/1], Step [7801/8897], Loss: 5.2535\n",
      "Epoch [1/1], Step [7802/8897], Loss: 5.7643\n",
      "Epoch [1/1], Step [7803/8897], Loss: 5.2633\n",
      "Epoch [1/1], Step [7804/8897], Loss: 5.3782\n",
      "Epoch [1/1], Step [7805/8897], Loss: 5.2138\n",
      "Epoch [1/1], Step [7806/8897], Loss: 5.2315\n",
      "Epoch [1/1], Step [7807/8897], Loss: 5.5103\n",
      "Epoch [1/1], Step [7808/8897], Loss: 5.3358\n",
      "Epoch [1/1], Step [7809/8897], Loss: 5.6831\n",
      "Epoch [1/1], Step [7810/8897], Loss: 5.0941\n",
      "Epoch [1/1], Step [7811/8897], Loss: 5.1778\n",
      "Epoch [1/1], Step [7812/8897], Loss: 5.4062\n",
      "Epoch [1/1], Step [7813/8897], Loss: 5.3550\n",
      "Epoch [1/1], Step [7814/8897], Loss: 5.3381\n",
      "Epoch [1/1], Step [7815/8897], Loss: 5.4165\n",
      "Epoch [1/1], Step [7816/8897], Loss: 5.1832\n",
      "Epoch [1/1], Step [7817/8897], Loss: 5.2758\n",
      "Epoch [1/1], Step [7818/8897], Loss: 5.3860\n",
      "Epoch [1/1], Step [7819/8897], Loss: 5.4203\n",
      "Epoch [1/1], Step [7820/8897], Loss: 5.1455\n",
      "Epoch [1/1], Step [7821/8897], Loss: 5.5449\n",
      "Epoch [1/1], Step [7822/8897], Loss: 5.2277\n",
      "Epoch [1/1], Step [7823/8897], Loss: 5.4213\n",
      "Epoch [1/1], Step [7824/8897], Loss: 5.5244\n",
      "Epoch [1/1], Step [7825/8897], Loss: 5.3849\n",
      "Epoch [1/1], Step [7826/8897], Loss: 5.3915\n",
      "Epoch [1/1], Step [7827/8897], Loss: 5.2896\n",
      "Epoch [1/1], Step [7828/8897], Loss: 5.2037\n",
      "Epoch [1/1], Step [7829/8897], Loss: 5.2849\n",
      "Epoch [1/1], Step [7830/8897], Loss: 5.4790\n",
      "Epoch [1/1], Step [7831/8897], Loss: 5.5546\n",
      "Epoch [1/1], Step [7832/8897], Loss: 5.2966\n",
      "Epoch [1/1], Step [7833/8897], Loss: 5.4392\n",
      "Epoch [1/1], Step [7834/8897], Loss: 5.2746\n",
      "Epoch [1/1], Step [7835/8897], Loss: 5.4115\n",
      "Epoch [1/1], Step [7836/8897], Loss: 5.1883\n",
      "Epoch [1/1], Step [7837/8897], Loss: 5.1876\n",
      "Epoch [1/1], Step [7838/8897], Loss: 5.2754\n",
      "Epoch [1/1], Step [7839/8897], Loss: 5.6442\n",
      "Epoch [1/1], Step [7840/8897], Loss: 5.3519\n",
      "Epoch [1/1], Step [7841/8897], Loss: 5.1446\n",
      "Epoch [1/1], Step [7842/8897], Loss: 5.3139\n",
      "Epoch [1/1], Step [7843/8897], Loss: 5.3762\n",
      "Epoch [1/1], Step [7844/8897], Loss: 5.2139\n",
      "Epoch [1/1], Step [7845/8897], Loss: 5.3684\n",
      "Epoch [1/1], Step [7846/8897], Loss: 5.3265\n",
      "Epoch [1/1], Step [7847/8897], Loss: 5.4496\n",
      "Epoch [1/1], Step [7848/8897], Loss: 5.4188\n",
      "Epoch [1/1], Step [7849/8897], Loss: 5.4094\n",
      "Epoch [1/1], Step [7850/8897], Loss: 5.4681\n",
      "Epoch [1/1], Step [7851/8897], Loss: 5.2021\n",
      "Epoch [1/1], Step [7852/8897], Loss: 5.5831\n",
      "Epoch [1/1], Step [7853/8897], Loss: 5.2585\n",
      "Epoch [1/1], Step [7854/8897], Loss: 5.4553\n",
      "Epoch [1/1], Step [7855/8897], Loss: 5.3291\n",
      "Epoch [1/1], Step [7856/8897], Loss: 5.3034\n",
      "Epoch [1/1], Step [7857/8897], Loss: 4.9709\n",
      "Epoch [1/1], Step [7858/8897], Loss: 5.3017\n",
      "Epoch [1/1], Step [7859/8897], Loss: 5.5569\n",
      "Epoch [1/1], Step [7860/8897], Loss: 5.5817\n",
      "Epoch [1/1], Step [7861/8897], Loss: 5.4416\n",
      "Epoch [1/1], Step [7862/8897], Loss: 5.5786\n",
      "Epoch [1/1], Step [7863/8897], Loss: 5.2921\n",
      "Epoch [1/1], Step [7864/8897], Loss: 5.4481\n",
      "Epoch [1/1], Step [7865/8897], Loss: 5.3873\n",
      "Epoch [1/1], Step [7866/8897], Loss: 5.4280\n",
      "Epoch [1/1], Step [7867/8897], Loss: 5.2694\n",
      "Epoch [1/1], Step [7868/8897], Loss: 5.4513\n",
      "Epoch [1/1], Step [7869/8897], Loss: 5.5043\n",
      "Epoch [1/1], Step [7870/8897], Loss: 5.4470\n",
      "Epoch [1/1], Step [7871/8897], Loss: 5.2635\n",
      "Epoch [1/1], Step [7872/8897], Loss: 5.2871\n",
      "Epoch [1/1], Step [7873/8897], Loss: 5.2685\n",
      "Epoch [1/1], Step [7874/8897], Loss: 5.3133\n",
      "Epoch [1/1], Step [7875/8897], Loss: 5.2269\n",
      "Epoch [1/1], Step [7876/8897], Loss: 5.2892\n",
      "Epoch [1/1], Step [7877/8897], Loss: 5.2040\n",
      "Epoch [1/1], Step [7878/8897], Loss: 5.3140\n",
      "Epoch [1/1], Step [7879/8897], Loss: 5.4049\n",
      "Epoch [1/1], Step [7880/8897], Loss: 5.3110\n",
      "Epoch [1/1], Step [7881/8897], Loss: 5.1722\n",
      "Epoch [1/1], Step [7882/8897], Loss: 5.3163\n",
      "Epoch [1/1], Step [7883/8897], Loss: 5.2986\n",
      "Epoch [1/1], Step [7884/8897], Loss: 5.0318\n",
      "Epoch [1/1], Step [7885/8897], Loss: 5.3830\n",
      "Epoch [1/1], Step [7886/8897], Loss: 5.2315\n",
      "Epoch [1/1], Step [7887/8897], Loss: 5.1783\n",
      "Epoch [1/1], Step [7888/8897], Loss: 5.3667\n",
      "Epoch [1/1], Step [7889/8897], Loss: 5.4011\n",
      "Epoch [1/1], Step [7890/8897], Loss: 5.4749\n",
      "Epoch [1/1], Step [7891/8897], Loss: 5.4529\n",
      "Epoch [1/1], Step [7892/8897], Loss: 5.3824\n",
      "Epoch [1/1], Step [7893/8897], Loss: 5.3108\n",
      "Epoch [1/1], Step [7894/8897], Loss: 5.4603\n",
      "Epoch [1/1], Step [7895/8897], Loss: 5.1299\n",
      "Epoch [1/1], Step [7896/8897], Loss: 5.5014\n",
      "Epoch [1/1], Step [7897/8897], Loss: 5.2289\n",
      "Epoch [1/1], Step [7898/8897], Loss: 5.3066\n",
      "Epoch [1/1], Step [7899/8897], Loss: 5.4394\n",
      "Epoch [1/1], Step [7900/8897], Loss: 5.2400\n",
      "Epoch [1/1], Step [7901/8897], Loss: 5.1982\n",
      "Epoch [1/1], Step [7902/8897], Loss: 5.5508\n",
      "Epoch [1/1], Step [7903/8897], Loss: 5.1948\n",
      "Epoch [1/1], Step [7904/8897], Loss: 5.4465\n",
      "Epoch [1/1], Step [7905/8897], Loss: 5.3314\n",
      "Epoch [1/1], Step [7906/8897], Loss: 5.4063\n",
      "Epoch [1/1], Step [7907/8897], Loss: 5.2011\n",
      "Epoch [1/1], Step [7908/8897], Loss: 5.1783\n",
      "Epoch [1/1], Step [7909/8897], Loss: 5.4333\n",
      "Epoch [1/1], Step [7910/8897], Loss: 5.3289\n",
      "Epoch [1/1], Step [7911/8897], Loss: 5.5759\n",
      "Epoch [1/1], Step [7912/8897], Loss: 5.1592\n",
      "Epoch [1/1], Step [7913/8897], Loss: 5.5762\n",
      "Epoch [1/1], Step [7914/8897], Loss: 5.1292\n",
      "Epoch [1/1], Step [7915/8897], Loss: 5.2264\n",
      "Epoch [1/1], Step [7916/8897], Loss: 5.0857\n",
      "Epoch [1/1], Step [7917/8897], Loss: 5.4508\n",
      "Epoch [1/1], Step [7918/8897], Loss: 5.5324\n",
      "Epoch [1/1], Step [7919/8897], Loss: 5.4276\n",
      "Epoch [1/1], Step [7920/8897], Loss: 5.3388\n",
      "Epoch [1/1], Step [7921/8897], Loss: 5.1938\n",
      "Epoch [1/1], Step [7922/8897], Loss: 5.2019\n",
      "Epoch [1/1], Step [7923/8897], Loss: 5.6231\n",
      "Epoch [1/1], Step [7924/8897], Loss: 5.3147\n",
      "Epoch [1/1], Step [7925/8897], Loss: 5.5488\n",
      "Epoch [1/1], Step [7926/8897], Loss: 5.3280\n",
      "Epoch [1/1], Step [7927/8897], Loss: 5.1093\n",
      "Epoch [1/1], Step [7928/8897], Loss: 5.2711\n",
      "Epoch [1/1], Step [7929/8897], Loss: 5.2656\n",
      "Epoch [1/1], Step [7930/8897], Loss: 5.2463\n",
      "Epoch [1/1], Step [7931/8897], Loss: 5.1360\n",
      "Epoch [1/1], Step [7932/8897], Loss: 5.2309\n",
      "Epoch [1/1], Step [7933/8897], Loss: 5.2599\n",
      "Epoch [1/1], Step [7934/8897], Loss: 5.2141\n",
      "Epoch [1/1], Step [7935/8897], Loss: 5.4909\n",
      "Epoch [1/1], Step [7936/8897], Loss: 5.3581\n",
      "Epoch [1/1], Step [7937/8897], Loss: 5.1477\n",
      "Epoch [1/1], Step [7938/8897], Loss: 5.3093\n",
      "Epoch [1/1], Step [7939/8897], Loss: 5.5486\n",
      "Epoch [1/1], Step [7940/8897], Loss: 5.3999\n",
      "Epoch [1/1], Step [7941/8897], Loss: 5.3959\n",
      "Epoch [1/1], Step [7942/8897], Loss: 5.3300\n",
      "Epoch [1/1], Step [7943/8897], Loss: 5.2631\n",
      "Epoch [1/1], Step [7944/8897], Loss: 5.2593\n",
      "Epoch [1/1], Step [7945/8897], Loss: 5.2331\n",
      "Epoch [1/1], Step [7946/8897], Loss: 5.0637\n",
      "Epoch [1/1], Step [7947/8897], Loss: 5.3577\n",
      "Epoch [1/1], Step [7948/8897], Loss: 5.1778\n",
      "Epoch [1/1], Step [7949/8897], Loss: 5.0593\n",
      "Epoch [1/1], Step [7950/8897], Loss: 5.2254\n",
      "Epoch [1/1], Step [7951/8897], Loss: 5.3908\n",
      "Epoch [1/1], Step [7952/8897], Loss: 5.3512\n",
      "Epoch [1/1], Step [7953/8897], Loss: 5.2030\n",
      "Epoch [1/1], Step [7954/8897], Loss: 5.3359\n",
      "Epoch [1/1], Step [7955/8897], Loss: 5.7016\n",
      "Epoch [1/1], Step [7956/8897], Loss: 5.2262\n",
      "Epoch [1/1], Step [7957/8897], Loss: 5.5295\n",
      "Epoch [1/1], Step [7958/8897], Loss: 5.4282\n",
      "Epoch [1/1], Step [7959/8897], Loss: 5.3870\n",
      "Epoch [1/1], Step [7960/8897], Loss: 5.3209\n",
      "Epoch [1/1], Step [7961/8897], Loss: 5.4726\n",
      "Epoch [1/1], Step [7962/8897], Loss: 5.4118\n",
      "Epoch [1/1], Step [7963/8897], Loss: 5.3611\n",
      "Epoch [1/1], Step [7964/8897], Loss: 5.3588\n",
      "Epoch [1/1], Step [7965/8897], Loss: 5.2322\n",
      "Epoch [1/1], Step [7966/8897], Loss: 5.3297\n",
      "Epoch [1/1], Step [7967/8897], Loss: 5.1994\n",
      "Epoch [1/1], Step [7968/8897], Loss: 5.4855\n",
      "Epoch [1/1], Step [7969/8897], Loss: 5.4316\n",
      "Epoch [1/1], Step [7970/8897], Loss: 5.0792\n",
      "Epoch [1/1], Step [7971/8897], Loss: 5.2291\n",
      "Epoch [1/1], Step [7972/8897], Loss: 5.4943\n",
      "Epoch [1/1], Step [7973/8897], Loss: 5.3599\n",
      "Epoch [1/1], Step [7974/8897], Loss: 5.3389\n",
      "Epoch [1/1], Step [7975/8897], Loss: 5.4282\n",
      "Epoch [1/1], Step [7976/8897], Loss: 5.2987\n",
      "Epoch [1/1], Step [7977/8897], Loss: 5.4898\n",
      "Epoch [1/1], Step [7978/8897], Loss: 5.5087\n",
      "Epoch [1/1], Step [7979/8897], Loss: 5.5047\n",
      "Epoch [1/1], Step [7980/8897], Loss: 5.3696\n",
      "Epoch [1/1], Step [7981/8897], Loss: 5.2072\n",
      "Epoch [1/1], Step [7982/8897], Loss: 5.4897\n",
      "Epoch [1/1], Step [7983/8897], Loss: 5.1793\n",
      "Epoch [1/1], Step [7984/8897], Loss: 5.4827\n",
      "Epoch [1/1], Step [7985/8897], Loss: 5.6190\n",
      "Epoch [1/1], Step [7986/8897], Loss: 5.4306\n",
      "Epoch [1/1], Step [7987/8897], Loss: 5.3343\n",
      "Epoch [1/1], Step [7988/8897], Loss: 5.1788\n",
      "Epoch [1/1], Step [7989/8897], Loss: 5.3595\n",
      "Epoch [1/1], Step [7990/8897], Loss: 5.3534\n",
      "Epoch [1/1], Step [7991/8897], Loss: 5.2649\n",
      "Epoch [1/1], Step [7992/8897], Loss: 5.1750\n",
      "Epoch [1/1], Step [7993/8897], Loss: 5.2237\n",
      "Epoch [1/1], Step [7994/8897], Loss: 5.3333\n",
      "Epoch [1/1], Step [7995/8897], Loss: 4.9530\n",
      "Epoch [1/1], Step [7996/8897], Loss: 5.5385\n",
      "Epoch [1/1], Step [7997/8897], Loss: 5.4173\n",
      "Epoch [1/1], Step [7998/8897], Loss: 5.4350\n",
      "Epoch [1/1], Step [7999/8897], Loss: 5.3109\n",
      "Epoch [1/1], Step [8000/8897], Loss: 5.4351\n",
      "Epoch [1/1], Step [8001/8897], Loss: 5.1486\n",
      "Epoch [1/1], Step [8002/8897], Loss: 5.5720\n",
      "Epoch [1/1], Step [8003/8897], Loss: 5.4833\n",
      "Epoch [1/1], Step [8004/8897], Loss: 5.4361\n",
      "Epoch [1/1], Step [8005/8897], Loss: 5.5424\n",
      "Epoch [1/1], Step [8006/8897], Loss: 5.3514\n",
      "Epoch [1/1], Step [8007/8897], Loss: 5.4828\n",
      "Epoch [1/1], Step [8008/8897], Loss: 5.2460\n",
      "Epoch [1/1], Step [8009/8897], Loss: 5.6683\n",
      "Epoch [1/1], Step [8010/8897], Loss: 5.2790\n",
      "Epoch [1/1], Step [8011/8897], Loss: 5.1891\n",
      "Epoch [1/1], Step [8012/8897], Loss: 5.2968\n",
      "Epoch [1/1], Step [8013/8897], Loss: 5.4510\n",
      "Epoch [1/1], Step [8014/8897], Loss: 5.2992\n",
      "Epoch [1/1], Step [8015/8897], Loss: 5.2851\n",
      "Epoch [1/1], Step [8016/8897], Loss: 5.3315\n",
      "Epoch [1/1], Step [8017/8897], Loss: 5.1675\n",
      "Epoch [1/1], Step [8018/8897], Loss: 5.4336\n",
      "Epoch [1/1], Step [8019/8897], Loss: 5.1201\n",
      "Epoch [1/1], Step [8020/8897], Loss: 5.1052\n",
      "Epoch [1/1], Step [8021/8897], Loss: 5.2012\n",
      "Epoch [1/1], Step [8022/8897], Loss: 5.3869\n",
      "Epoch [1/1], Step [8023/8897], Loss: 5.5216\n",
      "Epoch [1/1], Step [8024/8897], Loss: 5.3860\n",
      "Epoch [1/1], Step [8025/8897], Loss: 5.3904\n",
      "Epoch [1/1], Step [8026/8897], Loss: 5.0642\n",
      "Epoch [1/1], Step [8027/8897], Loss: 5.0538\n",
      "Epoch [1/1], Step [8028/8897], Loss: 5.2158\n",
      "Epoch [1/1], Step [8029/8897], Loss: 5.4230\n",
      "Epoch [1/1], Step [8030/8897], Loss: 5.3885\n",
      "Epoch [1/1], Step [8031/8897], Loss: 5.6009\n",
      "Epoch [1/1], Step [8032/8897], Loss: 5.3827\n",
      "Epoch [1/1], Step [8033/8897], Loss: 5.2641\n",
      "Epoch [1/1], Step [8034/8897], Loss: 5.2989\n",
      "Epoch [1/1], Step [8035/8897], Loss: 5.3803\n",
      "Epoch [1/1], Step [8036/8897], Loss: 5.3583\n",
      "Epoch [1/1], Step [8037/8897], Loss: 5.4294\n",
      "Epoch [1/1], Step [8038/8897], Loss: 5.1924\n",
      "Epoch [1/1], Step [8039/8897], Loss: 5.3015\n",
      "Epoch [1/1], Step [8040/8897], Loss: 5.4068\n",
      "Epoch [1/1], Step [8041/8897], Loss: 5.2715\n",
      "Epoch [1/1], Step [8042/8897], Loss: 5.0271\n",
      "Epoch [1/1], Step [8043/8897], Loss: 5.2692\n",
      "Epoch [1/1], Step [8044/8897], Loss: 5.4946\n",
      "Epoch [1/1], Step [8045/8897], Loss: 5.3149\n",
      "Epoch [1/1], Step [8046/8897], Loss: 5.3373\n",
      "Epoch [1/1], Step [8047/8897], Loss: 5.3687\n",
      "Epoch [1/1], Step [8048/8897], Loss: 5.3173\n",
      "Epoch [1/1], Step [8049/8897], Loss: 5.3326\n",
      "Epoch [1/1], Step [8050/8897], Loss: 5.4107\n",
      "Epoch [1/1], Step [8051/8897], Loss: 5.2995\n",
      "Epoch [1/1], Step [8052/8897], Loss: 5.5407\n",
      "Epoch [1/1], Step [8053/8897], Loss: 5.5041\n",
      "Epoch [1/1], Step [8054/8897], Loss: 5.5522\n",
      "Epoch [1/1], Step [8055/8897], Loss: 5.3665\n",
      "Epoch [1/1], Step [8056/8897], Loss: 5.2400\n",
      "Epoch [1/1], Step [8057/8897], Loss: 5.4906\n",
      "Epoch [1/1], Step [8058/8897], Loss: 5.2402\n",
      "Epoch [1/1], Step [8059/8897], Loss: 5.4893\n",
      "Epoch [1/1], Step [8060/8897], Loss: 5.2421\n",
      "Epoch [1/1], Step [8061/8897], Loss: 5.4019\n",
      "Epoch [1/1], Step [8062/8897], Loss: 5.2942\n",
      "Epoch [1/1], Step [8063/8897], Loss: 5.3206\n",
      "Epoch [1/1], Step [8064/8897], Loss: 5.0719\n",
      "Epoch [1/1], Step [8065/8897], Loss: 5.4588\n",
      "Epoch [1/1], Step [8066/8897], Loss: 5.4578\n",
      "Epoch [1/1], Step [8067/8897], Loss: 5.1558\n",
      "Epoch [1/1], Step [8068/8897], Loss: 5.5549\n",
      "Epoch [1/1], Step [8069/8897], Loss: 5.3323\n",
      "Epoch [1/1], Step [8070/8897], Loss: 5.2000\n",
      "Epoch [1/1], Step [8071/8897], Loss: 5.3386\n",
      "Epoch [1/1], Step [8072/8897], Loss: 5.2096\n",
      "Epoch [1/1], Step [8073/8897], Loss: 5.3030\n",
      "Epoch [1/1], Step [8074/8897], Loss: 5.2850\n",
      "Epoch [1/1], Step [8075/8897], Loss: 5.3688\n",
      "Epoch [1/1], Step [8076/8897], Loss: 5.2267\n",
      "Epoch [1/1], Step [8077/8897], Loss: 5.2950\n",
      "Epoch [1/1], Step [8078/8897], Loss: 5.1643\n",
      "Epoch [1/1], Step [8079/8897], Loss: 5.3245\n",
      "Epoch [1/1], Step [8080/8897], Loss: 5.1480\n",
      "Epoch [1/1], Step [8081/8897], Loss: 5.1790\n",
      "Epoch [1/1], Step [8082/8897], Loss: 5.3592\n",
      "Epoch [1/1], Step [8083/8897], Loss: 5.4432\n",
      "Epoch [1/1], Step [8084/8897], Loss: 5.2629\n",
      "Epoch [1/1], Step [8085/8897], Loss: 5.3433\n",
      "Epoch [1/1], Step [8086/8897], Loss: 5.3471\n",
      "Epoch [1/1], Step [8087/8897], Loss: 4.8941\n",
      "Epoch [1/1], Step [8088/8897], Loss: 5.2630\n",
      "Epoch [1/1], Step [8089/8897], Loss: 5.3496\n",
      "Epoch [1/1], Step [8090/8897], Loss: 5.3111\n",
      "Epoch [1/1], Step [8091/8897], Loss: 5.4959\n",
      "Epoch [1/1], Step [8092/8897], Loss: 5.4113\n",
      "Epoch [1/1], Step [8093/8897], Loss: 5.2557\n",
      "Epoch [1/1], Step [8094/8897], Loss: 5.5160\n",
      "Epoch [1/1], Step [8095/8897], Loss: 5.3638\n",
      "Epoch [1/1], Step [8096/8897], Loss: 5.3346\n",
      "Epoch [1/1], Step [8097/8897], Loss: 5.2030\n",
      "Epoch [1/1], Step [8098/8897], Loss: 5.5007\n",
      "Epoch [1/1], Step [8099/8897], Loss: 5.0728\n",
      "Epoch [1/1], Step [8100/8897], Loss: 5.2006\n",
      "Epoch [1/1], Step [8101/8897], Loss: 5.3122\n",
      "Epoch [1/1], Step [8102/8897], Loss: 5.5001\n",
      "Epoch [1/1], Step [8103/8897], Loss: 5.3127\n",
      "Epoch [1/1], Step [8104/8897], Loss: 5.2815\n",
      "Epoch [1/1], Step [8105/8897], Loss: 5.6914\n",
      "Epoch [1/1], Step [8106/8897], Loss: 5.6375\n",
      "Epoch [1/1], Step [8107/8897], Loss: 5.3155\n",
      "Epoch [1/1], Step [8108/8897], Loss: 5.5293\n",
      "Epoch [1/1], Step [8109/8897], Loss: 5.4232\n",
      "Epoch [1/1], Step [8110/8897], Loss: 5.3753\n",
      "Epoch [1/1], Step [8111/8897], Loss: 5.2922\n",
      "Epoch [1/1], Step [8112/8897], Loss: 5.2796\n",
      "Epoch [1/1], Step [8113/8897], Loss: 5.4529\n",
      "Epoch [1/1], Step [8114/8897], Loss: 5.2609\n",
      "Epoch [1/1], Step [8115/8897], Loss: 5.5466\n",
      "Epoch [1/1], Step [8116/8897], Loss: 5.3159\n",
      "Epoch [1/1], Step [8117/8897], Loss: 5.2007\n",
      "Epoch [1/1], Step [8118/8897], Loss: 5.3521\n",
      "Epoch [1/1], Step [8119/8897], Loss: 5.1894\n",
      "Epoch [1/1], Step [8120/8897], Loss: 5.1156\n",
      "Epoch [1/1], Step [8121/8897], Loss: 5.1541\n",
      "Epoch [1/1], Step [8122/8897], Loss: 5.2679\n",
      "Epoch [1/1], Step [8123/8897], Loss: 5.3777\n",
      "Epoch [1/1], Step [8124/8897], Loss: 5.6172\n",
      "Epoch [1/1], Step [8125/8897], Loss: 5.1559\n",
      "Epoch [1/1], Step [8126/8897], Loss: 5.2915\n",
      "Epoch [1/1], Step [8127/8897], Loss: 5.2828\n",
      "Epoch [1/1], Step [8128/8897], Loss: 5.4409\n",
      "Epoch [1/1], Step [8129/8897], Loss: 5.1170\n",
      "Epoch [1/1], Step [8130/8897], Loss: 5.5185\n",
      "Epoch [1/1], Step [8131/8897], Loss: 5.2600\n",
      "Epoch [1/1], Step [8132/8897], Loss: 5.1673\n",
      "Epoch [1/1], Step [8133/8897], Loss: 5.4655\n",
      "Epoch [1/1], Step [8134/8897], Loss: 5.4237\n",
      "Epoch [1/1], Step [8135/8897], Loss: 5.3028\n",
      "Epoch [1/1], Step [8136/8897], Loss: 5.4973\n",
      "Epoch [1/1], Step [8137/8897], Loss: 5.3870\n",
      "Epoch [1/1], Step [8138/8897], Loss: 5.5213\n",
      "Epoch [1/1], Step [8139/8897], Loss: 5.5562\n",
      "Epoch [1/1], Step [8140/8897], Loss: 5.2092\n",
      "Epoch [1/1], Step [8141/8897], Loss: 5.3459\n",
      "Epoch [1/1], Step [8142/8897], Loss: 5.2869\n",
      "Epoch [1/1], Step [8143/8897], Loss: 5.3757\n",
      "Epoch [1/1], Step [8144/8897], Loss: 5.3470\n",
      "Epoch [1/1], Step [8145/8897], Loss: 4.9969\n",
      "Epoch [1/1], Step [8146/8897], Loss: 5.3570\n",
      "Epoch [1/1], Step [8147/8897], Loss: 5.2491\n",
      "Epoch [1/1], Step [8148/8897], Loss: 5.5666\n",
      "Epoch [1/1], Step [8149/8897], Loss: 5.3757\n",
      "Epoch [1/1], Step [8150/8897], Loss: 5.1746\n",
      "Epoch [1/1], Step [8151/8897], Loss: 5.3748\n",
      "Epoch [1/1], Step [8152/8897], Loss: 5.2421\n",
      "Epoch [1/1], Step [8153/8897], Loss: 5.2699\n",
      "Epoch [1/1], Step [8154/8897], Loss: 5.3918\n",
      "Epoch [1/1], Step [8155/8897], Loss: 5.6975\n",
      "Epoch [1/1], Step [8156/8897], Loss: 5.3525\n",
      "Epoch [1/1], Step [8157/8897], Loss: 5.3335\n",
      "Epoch [1/1], Step [8158/8897], Loss: 5.5908\n",
      "Epoch [1/1], Step [8159/8897], Loss: 5.1889\n",
      "Epoch [1/1], Step [8160/8897], Loss: 5.3476\n",
      "Epoch [1/1], Step [8161/8897], Loss: 5.2428\n",
      "Epoch [1/1], Step [8162/8897], Loss: 5.4549\n",
      "Epoch [1/1], Step [8163/8897], Loss: 5.2798\n",
      "Epoch [1/1], Step [8164/8897], Loss: 5.3620\n",
      "Epoch [1/1], Step [8165/8897], Loss: 5.1632\n",
      "Epoch [1/1], Step [8166/8897], Loss: 5.1460\n",
      "Epoch [1/1], Step [8167/8897], Loss: 5.6082\n",
      "Epoch [1/1], Step [8168/8897], Loss: 5.3458\n",
      "Epoch [1/1], Step [8169/8897], Loss: 5.2181\n",
      "Epoch [1/1], Step [8170/8897], Loss: 5.6313\n",
      "Epoch [1/1], Step [8171/8897], Loss: 5.4137\n",
      "Epoch [1/1], Step [8172/8897], Loss: 5.3972\n",
      "Epoch [1/1], Step [8173/8897], Loss: 5.4301\n",
      "Epoch [1/1], Step [8174/8897], Loss: 5.4639\n",
      "Epoch [1/1], Step [8175/8897], Loss: 5.1705\n",
      "Epoch [1/1], Step [8176/8897], Loss: 5.2627\n",
      "Epoch [1/1], Step [8177/8897], Loss: 5.3404\n",
      "Epoch [1/1], Step [8178/8897], Loss: 5.3873\n",
      "Epoch [1/1], Step [8179/8897], Loss: 5.2991\n",
      "Epoch [1/1], Step [8180/8897], Loss: 5.1587\n",
      "Epoch [1/1], Step [8181/8897], Loss: 5.3157\n",
      "Epoch [1/1], Step [8182/8897], Loss: 5.1289\n",
      "Epoch [1/1], Step [8183/8897], Loss: 5.2375\n",
      "Epoch [1/1], Step [8184/8897], Loss: 5.3915\n",
      "Epoch [1/1], Step [8185/8897], Loss: 5.4856\n",
      "Epoch [1/1], Step [8186/8897], Loss: 5.4342\n",
      "Epoch [1/1], Step [8187/8897], Loss: 5.4207\n",
      "Epoch [1/1], Step [8188/8897], Loss: 5.5257\n",
      "Epoch [1/1], Step [8189/8897], Loss: 5.4517\n",
      "Epoch [1/1], Step [8190/8897], Loss: 5.5687\n",
      "Epoch [1/1], Step [8191/8897], Loss: 5.2776\n",
      "Epoch [1/1], Step [8192/8897], Loss: 5.4023\n",
      "Epoch [1/1], Step [8193/8897], Loss: 5.6256\n",
      "Epoch [1/1], Step [8194/8897], Loss: 5.4679\n",
      "Epoch [1/1], Step [8195/8897], Loss: 5.2952\n",
      "Epoch [1/1], Step [8196/8897], Loss: 5.2777\n",
      "Epoch [1/1], Step [8197/8897], Loss: 5.3843\n",
      "Epoch [1/1], Step [8198/8897], Loss: 5.5116\n",
      "Epoch [1/1], Step [8199/8897], Loss: 5.1934\n",
      "Epoch [1/1], Step [8200/8897], Loss: 5.4743\n",
      "Epoch [1/1], Step [8201/8897], Loss: 5.3269\n",
      "Epoch [1/1], Step [8202/8897], Loss: 5.4154\n",
      "Epoch [1/1], Step [8203/8897], Loss: 5.3755\n",
      "Epoch [1/1], Step [8204/8897], Loss: 5.4872\n",
      "Epoch [1/1], Step [8205/8897], Loss: 5.4039\n",
      "Epoch [1/1], Step [8206/8897], Loss: 5.3657\n",
      "Epoch [1/1], Step [8207/8897], Loss: 5.2836\n",
      "Epoch [1/1], Step [8208/8897], Loss: 5.4399\n",
      "Epoch [1/1], Step [8209/8897], Loss: 4.9943\n",
      "Epoch [1/1], Step [8210/8897], Loss: 5.1648\n",
      "Epoch [1/1], Step [8211/8897], Loss: 5.4097\n",
      "Epoch [1/1], Step [8212/8897], Loss: 5.2636\n",
      "Epoch [1/1], Step [8213/8897], Loss: 5.2865\n",
      "Epoch [1/1], Step [8214/8897], Loss: 5.1854\n",
      "Epoch [1/1], Step [8215/8897], Loss: 5.4222\n",
      "Epoch [1/1], Step [8216/8897], Loss: 5.3748\n",
      "Epoch [1/1], Step [8217/8897], Loss: 5.3204\n",
      "Epoch [1/1], Step [8218/8897], Loss: 5.3985\n",
      "Epoch [1/1], Step [8219/8897], Loss: 5.4128\n",
      "Epoch [1/1], Step [8220/8897], Loss: 5.4185\n",
      "Epoch [1/1], Step [8221/8897], Loss: 5.2287\n",
      "Epoch [1/1], Step [8222/8897], Loss: 5.4320\n",
      "Epoch [1/1], Step [8223/8897], Loss: 5.2562\n",
      "Epoch [1/1], Step [8224/8897], Loss: 5.4572\n",
      "Epoch [1/1], Step [8225/8897], Loss: 5.2963\n",
      "Epoch [1/1], Step [8226/8897], Loss: 5.2055\n",
      "Epoch [1/1], Step [8227/8897], Loss: 5.3962\n",
      "Epoch [1/1], Step [8228/8897], Loss: 5.4143\n",
      "Epoch [1/1], Step [8229/8897], Loss: 5.2757\n",
      "Epoch [1/1], Step [8230/8897], Loss: 5.5075\n",
      "Epoch [1/1], Step [8231/8897], Loss: 5.2342\n",
      "Epoch [1/1], Step [8232/8897], Loss: 5.3236\n",
      "Epoch [1/1], Step [8233/8897], Loss: 5.4423\n",
      "Epoch [1/1], Step [8234/8897], Loss: 5.0433\n",
      "Epoch [1/1], Step [8235/8897], Loss: 5.6522\n",
      "Epoch [1/1], Step [8236/8897], Loss: 5.1402\n",
      "Epoch [1/1], Step [8237/8897], Loss: 5.1431\n",
      "Epoch [1/1], Step [8238/8897], Loss: 5.4103\n",
      "Epoch [1/1], Step [8239/8897], Loss: 5.6250\n",
      "Epoch [1/1], Step [8240/8897], Loss: 5.5087\n",
      "Epoch [1/1], Step [8241/8897], Loss: 5.3497\n",
      "Epoch [1/1], Step [8242/8897], Loss: 5.4101\n",
      "Epoch [1/1], Step [8243/8897], Loss: 5.6714\n",
      "Epoch [1/1], Step [8244/8897], Loss: 5.4340\n",
      "Epoch [1/1], Step [8245/8897], Loss: 5.3894\n",
      "Epoch [1/1], Step [8246/8897], Loss: 5.4693\n",
      "Epoch [1/1], Step [8247/8897], Loss: 5.3106\n",
      "Epoch [1/1], Step [8248/8897], Loss: 5.2649\n",
      "Epoch [1/1], Step [8249/8897], Loss: 5.1163\n",
      "Epoch [1/1], Step [8250/8897], Loss: 5.3897\n",
      "Epoch [1/1], Step [8251/8897], Loss: 5.4190\n",
      "Epoch [1/1], Step [8252/8897], Loss: 5.2740\n",
      "Epoch [1/1], Step [8253/8897], Loss: 5.4125\n",
      "Epoch [1/1], Step [8254/8897], Loss: 5.2222\n",
      "Epoch [1/1], Step [8255/8897], Loss: 5.4652\n",
      "Epoch [1/1], Step [8256/8897], Loss: 5.1255\n",
      "Epoch [1/1], Step [8257/8897], Loss: 5.4188\n",
      "Epoch [1/1], Step [8258/8897], Loss: 5.2677\n",
      "Epoch [1/1], Step [8259/8897], Loss: 5.4294\n",
      "Epoch [1/1], Step [8260/8897], Loss: 5.4258\n",
      "Epoch [1/1], Step [8261/8897], Loss: 5.4159\n",
      "Epoch [1/1], Step [8262/8897], Loss: 5.1996\n",
      "Epoch [1/1], Step [8263/8897], Loss: 5.3896\n",
      "Epoch [1/1], Step [8264/8897], Loss: 5.3817\n",
      "Epoch [1/1], Step [8265/8897], Loss: 5.3239\n",
      "Epoch [1/1], Step [8266/8897], Loss: 5.2970\n",
      "Epoch [1/1], Step [8267/8897], Loss: 5.2325\n",
      "Epoch [1/1], Step [8268/8897], Loss: 5.3130\n",
      "Epoch [1/1], Step [8269/8897], Loss: 5.2080\n",
      "Epoch [1/1], Step [8270/8897], Loss: 5.2430\n",
      "Epoch [1/1], Step [8271/8897], Loss: 5.3864\n",
      "Epoch [1/1], Step [8272/8897], Loss: 5.2950\n",
      "Epoch [1/1], Step [8273/8897], Loss: 5.4002\n",
      "Epoch [1/1], Step [8274/8897], Loss: 5.5030\n",
      "Epoch [1/1], Step [8275/8897], Loss: 5.0859\n",
      "Epoch [1/1], Step [8276/8897], Loss: 5.1751\n",
      "Epoch [1/1], Step [8277/8897], Loss: 5.1494\n",
      "Epoch [1/1], Step [8278/8897], Loss: 5.1470\n",
      "Epoch [1/1], Step [8279/8897], Loss: 5.3100\n",
      "Epoch [1/1], Step [8280/8897], Loss: 5.3311\n",
      "Epoch [1/1], Step [8281/8897], Loss: 5.3680\n",
      "Epoch [1/1], Step [8282/8897], Loss: 5.3595\n",
      "Epoch [1/1], Step [8283/8897], Loss: 5.2587\n",
      "Epoch [1/1], Step [8284/8897], Loss: 4.9084\n",
      "Epoch [1/1], Step [8285/8897], Loss: 5.1697\n",
      "Epoch [1/1], Step [8286/8897], Loss: 5.4185\n",
      "Epoch [1/1], Step [8287/8897], Loss: 5.2608\n",
      "Epoch [1/1], Step [8288/8897], Loss: 5.3073\n",
      "Epoch [1/1], Step [8289/8897], Loss: 5.5360\n",
      "Epoch [1/1], Step [8290/8897], Loss: 5.2572\n",
      "Epoch [1/1], Step [8291/8897], Loss: 5.4618\n",
      "Epoch [1/1], Step [8292/8897], Loss: 5.3215\n",
      "Epoch [1/1], Step [8293/8897], Loss: 5.4024\n",
      "Epoch [1/1], Step [8294/8897], Loss: 5.2366\n",
      "Epoch [1/1], Step [8295/8897], Loss: 5.3910\n",
      "Epoch [1/1], Step [8296/8897], Loss: 5.2552\n",
      "Epoch [1/1], Step [8297/8897], Loss: 5.2470\n",
      "Epoch [1/1], Step [8298/8897], Loss: 5.4854\n",
      "Epoch [1/1], Step [8299/8897], Loss: 5.2396\n",
      "Epoch [1/1], Step [8300/8897], Loss: 5.3955\n",
      "Epoch [1/1], Step [8301/8897], Loss: 5.1944\n",
      "Epoch [1/1], Step [8302/8897], Loss: 5.3485\n",
      "Epoch [1/1], Step [8303/8897], Loss: 5.4092\n",
      "Epoch [1/1], Step [8304/8897], Loss: 5.2411\n",
      "Epoch [1/1], Step [8305/8897], Loss: 5.4648\n",
      "Epoch [1/1], Step [8306/8897], Loss: 5.2830\n",
      "Epoch [1/1], Step [8307/8897], Loss: 5.5648\n",
      "Epoch [1/1], Step [8308/8897], Loss: 5.2427\n",
      "Epoch [1/1], Step [8309/8897], Loss: 5.4558\n",
      "Epoch [1/1], Step [8310/8897], Loss: 5.3567\n",
      "Epoch [1/1], Step [8311/8897], Loss: 5.3284\n",
      "Epoch [1/1], Step [8312/8897], Loss: 5.3800\n",
      "Epoch [1/1], Step [8313/8897], Loss: 5.5102\n",
      "Epoch [1/1], Step [8314/8897], Loss: 5.7100\n",
      "Epoch [1/1], Step [8315/8897], Loss: 5.4602\n",
      "Epoch [1/1], Step [8316/8897], Loss: 5.3796\n",
      "Epoch [1/1], Step [8317/8897], Loss: 5.3921\n",
      "Epoch [1/1], Step [8318/8897], Loss: 5.2063\n",
      "Epoch [1/1], Step [8319/8897], Loss: 5.3526\n",
      "Epoch [1/1], Step [8320/8897], Loss: 5.2324\n",
      "Epoch [1/1], Step [8321/8897], Loss: 4.9779\n",
      "Epoch [1/1], Step [8322/8897], Loss: 5.3331\n",
      "Epoch [1/1], Step [8323/8897], Loss: 5.4257\n",
      "Epoch [1/1], Step [8324/8897], Loss: 5.4547\n",
      "Epoch [1/1], Step [8325/8897], Loss: 5.3700\n",
      "Epoch [1/1], Step [8326/8897], Loss: 5.0895\n",
      "Epoch [1/1], Step [8327/8897], Loss: 5.4277\n",
      "Epoch [1/1], Step [8328/8897], Loss: 5.2429\n",
      "Epoch [1/1], Step [8329/8897], Loss: 5.2814\n",
      "Epoch [1/1], Step [8330/8897], Loss: 5.2478\n",
      "Epoch [1/1], Step [8331/8897], Loss: 5.3552\n",
      "Epoch [1/1], Step [8332/8897], Loss: 5.4546\n",
      "Epoch [1/1], Step [8333/8897], Loss: 5.3448\n",
      "Epoch [1/1], Step [8334/8897], Loss: 5.3376\n",
      "Epoch [1/1], Step [8335/8897], Loss: 5.2117\n",
      "Epoch [1/1], Step [8336/8897], Loss: 5.2422\n",
      "Epoch [1/1], Step [8337/8897], Loss: 5.5050\n",
      "Epoch [1/1], Step [8338/8897], Loss: 5.3286\n",
      "Epoch [1/1], Step [8339/8897], Loss: 5.4843\n",
      "Epoch [1/1], Step [8340/8897], Loss: 5.2625\n",
      "Epoch [1/1], Step [8341/8897], Loss: 5.4345\n",
      "Epoch [1/1], Step [8342/8897], Loss: 5.2177\n",
      "Epoch [1/1], Step [8343/8897], Loss: 5.2417\n",
      "Epoch [1/1], Step [8344/8897], Loss: 5.3950\n",
      "Epoch [1/1], Step [8345/8897], Loss: 5.3721\n",
      "Epoch [1/1], Step [8346/8897], Loss: 5.6144\n",
      "Epoch [1/1], Step [8347/8897], Loss: 5.4205\n",
      "Epoch [1/1], Step [8348/8897], Loss: 5.4262\n",
      "Epoch [1/1], Step [8349/8897], Loss: 5.5713\n",
      "Epoch [1/1], Step [8350/8897], Loss: 5.5728\n",
      "Epoch [1/1], Step [8351/8897], Loss: 5.3637\n",
      "Epoch [1/1], Step [8352/8897], Loss: 5.1045\n",
      "Epoch [1/1], Step [8353/8897], Loss: 5.2923\n",
      "Epoch [1/1], Step [8354/8897], Loss: 5.2099\n",
      "Epoch [1/1], Step [8355/8897], Loss: 5.3801\n",
      "Epoch [1/1], Step [8356/8897], Loss: 5.3801\n",
      "Epoch [1/1], Step [8357/8897], Loss: 5.5113\n",
      "Epoch [1/1], Step [8358/8897], Loss: 5.2425\n",
      "Epoch [1/1], Step [8359/8897], Loss: 5.2912\n",
      "Epoch [1/1], Step [8360/8897], Loss: 5.5248\n",
      "Epoch [1/1], Step [8361/8897], Loss: 5.5644\n",
      "Epoch [1/1], Step [8362/8897], Loss: 5.5224\n",
      "Epoch [1/1], Step [8363/8897], Loss: 5.2000\n",
      "Epoch [1/1], Step [8364/8897], Loss: 5.4834\n",
      "Epoch [1/1], Step [8365/8897], Loss: 5.1292\n",
      "Epoch [1/1], Step [8366/8897], Loss: 5.1613\n",
      "Epoch [1/1], Step [8367/8897], Loss: 5.4653\n",
      "Epoch [1/1], Step [8368/8897], Loss: 5.4885\n",
      "Epoch [1/1], Step [8369/8897], Loss: 5.2051\n",
      "Epoch [1/1], Step [8370/8897], Loss: 5.0220\n",
      "Epoch [1/1], Step [8371/8897], Loss: 5.3190\n",
      "Epoch [1/1], Step [8372/8897], Loss: 5.3906\n",
      "Epoch [1/1], Step [8373/8897], Loss: 5.3613\n",
      "Epoch [1/1], Step [8374/8897], Loss: 5.0487\n",
      "Epoch [1/1], Step [8375/8897], Loss: 5.4031\n",
      "Epoch [1/1], Step [8376/8897], Loss: 5.3880\n",
      "Epoch [1/1], Step [8377/8897], Loss: 5.3432\n",
      "Epoch [1/1], Step [8378/8897], Loss: 5.2125\n",
      "Epoch [1/1], Step [8379/8897], Loss: 5.4182\n",
      "Epoch [1/1], Step [8380/8897], Loss: 5.2730\n",
      "Epoch [1/1], Step [8381/8897], Loss: 5.4403\n",
      "Epoch [1/1], Step [8382/8897], Loss: 5.6502\n",
      "Epoch [1/1], Step [8383/8897], Loss: 5.3129\n",
      "Epoch [1/1], Step [8384/8897], Loss: 5.0049\n",
      "Epoch [1/1], Step [8385/8897], Loss: 5.2304\n",
      "Epoch [1/1], Step [8386/8897], Loss: 5.4775\n",
      "Epoch [1/1], Step [8387/8897], Loss: 5.1470\n",
      "Epoch [1/1], Step [8388/8897], Loss: 5.1350\n",
      "Epoch [1/1], Step [8389/8897], Loss: 5.4923\n",
      "Epoch [1/1], Step [8390/8897], Loss: 5.2744\n",
      "Epoch [1/1], Step [8391/8897], Loss: 5.1997\n",
      "Epoch [1/1], Step [8392/8897], Loss: 5.5846\n",
      "Epoch [1/1], Step [8393/8897], Loss: 5.5246\n",
      "Epoch [1/1], Step [8394/8897], Loss: 5.2681\n",
      "Epoch [1/1], Step [8395/8897], Loss: 5.3664\n",
      "Epoch [1/1], Step [8396/8897], Loss: 5.2129\n",
      "Epoch [1/1], Step [8397/8897], Loss: 5.1421\n",
      "Epoch [1/1], Step [8398/8897], Loss: 5.1761\n",
      "Epoch [1/1], Step [8399/8897], Loss: 5.2980\n",
      "Epoch [1/1], Step [8400/8897], Loss: 5.3193\n",
      "Epoch [1/1], Step [8401/8897], Loss: 5.2782\n",
      "Epoch [1/1], Step [8402/8897], Loss: 5.2811\n",
      "Epoch [1/1], Step [8403/8897], Loss: 5.2708\n",
      "Epoch [1/1], Step [8404/8897], Loss: 5.1843\n",
      "Epoch [1/1], Step [8405/8897], Loss: 5.3125\n",
      "Epoch [1/1], Step [8406/8897], Loss: 5.1945\n",
      "Epoch [1/1], Step [8407/8897], Loss: 5.6241\n",
      "Epoch [1/1], Step [8408/8897], Loss: 5.3844\n",
      "Epoch [1/1], Step [8409/8897], Loss: 5.3990\n",
      "Epoch [1/1], Step [8410/8897], Loss: 5.3182\n",
      "Epoch [1/1], Step [8411/8897], Loss: 5.3434\n",
      "Epoch [1/1], Step [8412/8897], Loss: 5.0963\n",
      "Epoch [1/1], Step [8413/8897], Loss: 5.5205\n",
      "Epoch [1/1], Step [8414/8897], Loss: 5.4014\n",
      "Epoch [1/1], Step [8415/8897], Loss: 5.1377\n",
      "Epoch [1/1], Step [8416/8897], Loss: 5.4395\n",
      "Epoch [1/1], Step [8417/8897], Loss: 5.4123\n",
      "Epoch [1/1], Step [8418/8897], Loss: 5.4127\n",
      "Epoch [1/1], Step [8419/8897], Loss: 5.3495\n",
      "Epoch [1/1], Step [8420/8897], Loss: 5.2642\n",
      "Epoch [1/1], Step [8421/8897], Loss: 5.6252\n",
      "Epoch [1/1], Step [8422/8897], Loss: 5.5470\n",
      "Epoch [1/1], Step [8423/8897], Loss: 5.3015\n",
      "Epoch [1/1], Step [8424/8897], Loss: 5.1855\n",
      "Epoch [1/1], Step [8425/8897], Loss: 5.1983\n",
      "Epoch [1/1], Step [8426/8897], Loss: 5.2045\n",
      "Epoch [1/1], Step [8427/8897], Loss: 5.3063\n",
      "Epoch [1/1], Step [8428/8897], Loss: 5.3303\n",
      "Epoch [1/1], Step [8429/8897], Loss: 5.3980\n",
      "Epoch [1/1], Step [8430/8897], Loss: 5.3535\n",
      "Epoch [1/1], Step [8431/8897], Loss: 5.2542\n",
      "Epoch [1/1], Step [8432/8897], Loss: 5.4306\n",
      "Epoch [1/1], Step [8433/8897], Loss: 5.5188\n",
      "Epoch [1/1], Step [8434/8897], Loss: 5.3563\n",
      "Epoch [1/1], Step [8435/8897], Loss: 5.5223\n",
      "Epoch [1/1], Step [8436/8897], Loss: 5.1535\n",
      "Epoch [1/1], Step [8437/8897], Loss: 5.1726\n",
      "Epoch [1/1], Step [8438/8897], Loss: 5.3478\n",
      "Epoch [1/1], Step [8439/8897], Loss: 5.3470\n",
      "Epoch [1/1], Step [8440/8897], Loss: 5.3114\n",
      "Epoch [1/1], Step [8441/8897], Loss: 5.3731\n",
      "Epoch [1/1], Step [8442/8897], Loss: 5.2494\n",
      "Epoch [1/1], Step [8443/8897], Loss: 5.3700\n",
      "Epoch [1/1], Step [8444/8897], Loss: 5.3068\n",
      "Epoch [1/1], Step [8445/8897], Loss: 5.3222\n",
      "Epoch [1/1], Step [8446/8897], Loss: 5.3273\n",
      "Epoch [1/1], Step [8447/8897], Loss: 5.3445\n",
      "Epoch [1/1], Step [8448/8897], Loss: 5.1451\n",
      "Epoch [1/1], Step [8449/8897], Loss: 5.5594\n",
      "Epoch [1/1], Step [8450/8897], Loss: 5.5325\n",
      "Epoch [1/1], Step [8451/8897], Loss: 5.6235\n",
      "Epoch [1/1], Step [8452/8897], Loss: 5.3512\n",
      "Epoch [1/1], Step [8453/8897], Loss: 5.0810\n",
      "Epoch [1/1], Step [8454/8897], Loss: 5.4032\n",
      "Epoch [1/1], Step [8455/8897], Loss: 5.4873\n",
      "Epoch [1/1], Step [8456/8897], Loss: 5.1954\n",
      "Epoch [1/1], Step [8457/8897], Loss: 5.6461\n",
      "Epoch [1/1], Step [8458/8897], Loss: 5.4087\n",
      "Epoch [1/1], Step [8459/8897], Loss: 5.4095\n",
      "Epoch [1/1], Step [8460/8897], Loss: 5.4431\n",
      "Epoch [1/1], Step [8461/8897], Loss: 5.1705\n",
      "Epoch [1/1], Step [8462/8897], Loss: 5.4269\n",
      "Epoch [1/1], Step [8463/8897], Loss: 5.5854\n",
      "Epoch [1/1], Step [8464/8897], Loss: 5.2199\n",
      "Epoch [1/1], Step [8465/8897], Loss: 5.1106\n",
      "Epoch [1/1], Step [8466/8897], Loss: 5.6718\n",
      "Epoch [1/1], Step [8467/8897], Loss: 5.3509\n",
      "Epoch [1/1], Step [8468/8897], Loss: 5.3268\n",
      "Epoch [1/1], Step [8469/8897], Loss: 5.2470\n",
      "Epoch [1/1], Step [8470/8897], Loss: 5.2153\n",
      "Epoch [1/1], Step [8471/8897], Loss: 5.3580\n",
      "Epoch [1/1], Step [8472/8897], Loss: 5.0211\n",
      "Epoch [1/1], Step [8473/8897], Loss: 4.9925\n",
      "Epoch [1/1], Step [8474/8897], Loss: 5.2449\n",
      "Epoch [1/1], Step [8475/8897], Loss: 5.5134\n",
      "Epoch [1/1], Step [8476/8897], Loss: 5.3207\n",
      "Epoch [1/1], Step [8477/8897], Loss: 5.5813\n",
      "Epoch [1/1], Step [8478/8897], Loss: 5.1465\n",
      "Epoch [1/1], Step [8479/8897], Loss: 5.2713\n",
      "Epoch [1/1], Step [8480/8897], Loss: 5.2678\n",
      "Epoch [1/1], Step [8481/8897], Loss: 5.5066\n",
      "Epoch [1/1], Step [8482/8897], Loss: 5.5787\n",
      "Epoch [1/1], Step [8483/8897], Loss: 5.4895\n",
      "Epoch [1/1], Step [8484/8897], Loss: 5.2754\n",
      "Epoch [1/1], Step [8485/8897], Loss: 5.3625\n",
      "Epoch [1/1], Step [8486/8897], Loss: 5.5476\n",
      "Epoch [1/1], Step [8487/8897], Loss: 5.3313\n",
      "Epoch [1/1], Step [8488/8897], Loss: 5.5717\n",
      "Epoch [1/1], Step [8489/8897], Loss: 5.6405\n",
      "Epoch [1/1], Step [8490/8897], Loss: 5.2749\n",
      "Epoch [1/1], Step [8491/8897], Loss: 5.5507\n",
      "Epoch [1/1], Step [8492/8897], Loss: 5.2828\n",
      "Epoch [1/1], Step [8493/8897], Loss: 5.2869\n",
      "Epoch [1/1], Step [8494/8897], Loss: 5.5196\n",
      "Epoch [1/1], Step [8495/8897], Loss: 5.4151\n",
      "Epoch [1/1], Step [8496/8897], Loss: 5.3396\n",
      "Epoch [1/1], Step [8497/8897], Loss: 5.3718\n",
      "Epoch [1/1], Step [8498/8897], Loss: 5.5115\n",
      "Epoch [1/1], Step [8499/8897], Loss: 5.3582\n",
      "Epoch [1/1], Step [8500/8897], Loss: 5.2980\n",
      "Epoch [1/1], Step [8501/8897], Loss: 5.4549\n",
      "Epoch [1/1], Step [8502/8897], Loss: 5.4156\n",
      "Epoch [1/1], Step [8503/8897], Loss: 5.4032\n",
      "Epoch [1/1], Step [8504/8897], Loss: 5.4320\n",
      "Epoch [1/1], Step [8505/8897], Loss: 5.4230\n",
      "Epoch [1/1], Step [8506/8897], Loss: 5.3511\n",
      "Epoch [1/1], Step [8507/8897], Loss: 5.0746\n",
      "Epoch [1/1], Step [8508/8897], Loss: 5.3569\n",
      "Epoch [1/1], Step [8509/8897], Loss: 5.4541\n",
      "Epoch [1/1], Step [8510/8897], Loss: 5.4443\n",
      "Epoch [1/1], Step [8511/8897], Loss: 5.3173\n",
      "Epoch [1/1], Step [8512/8897], Loss: 5.2316\n",
      "Epoch [1/1], Step [8513/8897], Loss: 5.3394\n",
      "Epoch [1/1], Step [8514/8897], Loss: 5.3016\n",
      "Epoch [1/1], Step [8515/8897], Loss: 5.1993\n",
      "Epoch [1/1], Step [8516/8897], Loss: 5.3007\n",
      "Epoch [1/1], Step [8517/8897], Loss: 5.3616\n",
      "Epoch [1/1], Step [8518/8897], Loss: 5.3700\n",
      "Epoch [1/1], Step [8519/8897], Loss: 5.0460\n",
      "Epoch [1/1], Step [8520/8897], Loss: 5.3822\n",
      "Epoch [1/1], Step [8521/8897], Loss: 5.0720\n",
      "Epoch [1/1], Step [8522/8897], Loss: 5.5157\n",
      "Epoch [1/1], Step [8523/8897], Loss: 5.4375\n",
      "Epoch [1/1], Step [8524/8897], Loss: 5.1303\n",
      "Epoch [1/1], Step [8525/8897], Loss: 5.3623\n",
      "Epoch [1/1], Step [8526/8897], Loss: 5.3346\n",
      "Epoch [1/1], Step [8527/8897], Loss: 5.6404\n",
      "Epoch [1/1], Step [8528/8897], Loss: 5.4335\n",
      "Epoch [1/1], Step [8529/8897], Loss: 5.4936\n",
      "Epoch [1/1], Step [8530/8897], Loss: 5.2640\n",
      "Epoch [1/1], Step [8531/8897], Loss: 5.4692\n",
      "Epoch [1/1], Step [8532/8897], Loss: 5.1634\n",
      "Epoch [1/1], Step [8533/8897], Loss: 5.2668\n",
      "Epoch [1/1], Step [8534/8897], Loss: 5.3482\n",
      "Epoch [1/1], Step [8535/8897], Loss: 5.4720\n",
      "Epoch [1/1], Step [8536/8897], Loss: 5.4574\n",
      "Epoch [1/1], Step [8537/8897], Loss: 5.3875\n",
      "Epoch [1/1], Step [8538/8897], Loss: 5.4711\n",
      "Epoch [1/1], Step [8539/8897], Loss: 5.3269\n",
      "Epoch [1/1], Step [8540/8897], Loss: 5.5182\n",
      "Epoch [1/1], Step [8541/8897], Loss: 5.2260\n",
      "Epoch [1/1], Step [8542/8897], Loss: 5.1947\n",
      "Epoch [1/1], Step [8543/8897], Loss: 5.5374\n",
      "Epoch [1/1], Step [8544/8897], Loss: 5.4067\n",
      "Epoch [1/1], Step [8545/8897], Loss: 5.3997\n",
      "Epoch [1/1], Step [8546/8897], Loss: 5.6583\n",
      "Epoch [1/1], Step [8547/8897], Loss: 5.2289\n",
      "Epoch [1/1], Step [8548/8897], Loss: 5.3069\n",
      "Epoch [1/1], Step [8549/8897], Loss: 5.4548\n",
      "Epoch [1/1], Step [8550/8897], Loss: 5.2829\n",
      "Epoch [1/1], Step [8551/8897], Loss: 5.2107\n",
      "Epoch [1/1], Step [8552/8897], Loss: 5.4707\n",
      "Epoch [1/1], Step [8553/8897], Loss: 5.3339\n",
      "Epoch [1/1], Step [8554/8897], Loss: 5.6392\n",
      "Epoch [1/1], Step [8555/8897], Loss: 5.2572\n",
      "Epoch [1/1], Step [8556/8897], Loss: 5.1762\n",
      "Epoch [1/1], Step [8557/8897], Loss: 5.2902\n",
      "Epoch [1/1], Step [8558/8897], Loss: 5.3675\n",
      "Epoch [1/1], Step [8559/8897], Loss: 5.3751\n",
      "Epoch [1/1], Step [8560/8897], Loss: 5.3035\n",
      "Epoch [1/1], Step [8561/8897], Loss: 5.5039\n",
      "Epoch [1/1], Step [8562/8897], Loss: 5.5323\n",
      "Epoch [1/1], Step [8563/8897], Loss: 5.1445\n",
      "Epoch [1/1], Step [8564/8897], Loss: 5.3484\n",
      "Epoch [1/1], Step [8565/8897], Loss: 5.5348\n",
      "Epoch [1/1], Step [8566/8897], Loss: 4.9965\n",
      "Epoch [1/1], Step [8567/8897], Loss: 5.3313\n",
      "Epoch [1/1], Step [8568/8897], Loss: 5.6572\n",
      "Epoch [1/1], Step [8569/8897], Loss: 5.0802\n",
      "Epoch [1/1], Step [8570/8897], Loss: 5.4971\n",
      "Epoch [1/1], Step [8571/8897], Loss: 5.4230\n",
      "Epoch [1/1], Step [8572/8897], Loss: 5.2687\n",
      "Epoch [1/1], Step [8573/8897], Loss: 5.3568\n",
      "Epoch [1/1], Step [8574/8897], Loss: 5.4277\n",
      "Epoch [1/1], Step [8575/8897], Loss: 5.3523\n",
      "Epoch [1/1], Step [8576/8897], Loss: 5.3881\n",
      "Epoch [1/1], Step [8577/8897], Loss: 5.2103\n",
      "Epoch [1/1], Step [8578/8897], Loss: 5.3053\n",
      "Epoch [1/1], Step [8579/8897], Loss: 5.6335\n",
      "Epoch [1/1], Step [8580/8897], Loss: 5.1476\n",
      "Epoch [1/1], Step [8581/8897], Loss: 5.3494\n",
      "Epoch [1/1], Step [8582/8897], Loss: 5.4236\n",
      "Epoch [1/1], Step [8583/8897], Loss: 5.6260\n",
      "Epoch [1/1], Step [8584/8897], Loss: 5.4109\n",
      "Epoch [1/1], Step [8585/8897], Loss: 5.2737\n",
      "Epoch [1/1], Step [8586/8897], Loss: 5.3070\n",
      "Epoch [1/1], Step [8587/8897], Loss: 5.3388\n",
      "Epoch [1/1], Step [8588/8897], Loss: 5.2351\n",
      "Epoch [1/1], Step [8589/8897], Loss: 5.0337\n",
      "Epoch [1/1], Step [8590/8897], Loss: 5.3987\n",
      "Epoch [1/1], Step [8591/8897], Loss: 5.5943\n",
      "Epoch [1/1], Step [8592/8897], Loss: 5.4543\n",
      "Epoch [1/1], Step [8593/8897], Loss: 5.2006\n",
      "Epoch [1/1], Step [8594/8897], Loss: 5.6089\n",
      "Epoch [1/1], Step [8595/8897], Loss: 5.5187\n",
      "Epoch [1/1], Step [8596/8897], Loss: 5.3835\n",
      "Epoch [1/1], Step [8597/8897], Loss: 5.3934\n",
      "Epoch [1/1], Step [8598/8897], Loss: 5.4408\n",
      "Epoch [1/1], Step [8599/8897], Loss: 5.3898\n",
      "Epoch [1/1], Step [8600/8897], Loss: 5.2212\n",
      "Epoch [1/1], Step [8601/8897], Loss: 5.2219\n",
      "Epoch [1/1], Step [8602/8897], Loss: 5.2299\n",
      "Epoch [1/1], Step [8603/8897], Loss: 5.3597\n",
      "Epoch [1/1], Step [8604/8897], Loss: 5.4017\n",
      "Epoch [1/1], Step [8605/8897], Loss: 5.1101\n",
      "Epoch [1/1], Step [8606/8897], Loss: 5.3306\n",
      "Epoch [1/1], Step [8607/8897], Loss: 5.4125\n",
      "Epoch [1/1], Step [8608/8897], Loss: 5.1468\n",
      "Epoch [1/1], Step [8609/8897], Loss: 5.4466\n",
      "Epoch [1/1], Step [8610/8897], Loss: 5.4152\n",
      "Epoch [1/1], Step [8611/8897], Loss: 5.2162\n",
      "Epoch [1/1], Step [8612/8897], Loss: 5.2713\n",
      "Epoch [1/1], Step [8613/8897], Loss: 5.4285\n",
      "Epoch [1/1], Step [8614/8897], Loss: 5.5633\n",
      "Epoch [1/1], Step [8615/8897], Loss: 5.4050\n",
      "Epoch [1/1], Step [8616/8897], Loss: 5.4158\n",
      "Epoch [1/1], Step [8617/8897], Loss: 5.3378\n",
      "Epoch [1/1], Step [8618/8897], Loss: 5.4863\n",
      "Epoch [1/1], Step [8619/8897], Loss: 5.0874\n",
      "Epoch [1/1], Step [8620/8897], Loss: 5.2592\n",
      "Epoch [1/1], Step [8621/8897], Loss: 5.1309\n",
      "Epoch [1/1], Step [8622/8897], Loss: 5.7233\n",
      "Epoch [1/1], Step [8623/8897], Loss: 4.9031\n",
      "Epoch [1/1], Step [8624/8897], Loss: 5.4456\n",
      "Epoch [1/1], Step [8625/8897], Loss: 5.1661\n",
      "Epoch [1/1], Step [8626/8897], Loss: 5.3336\n",
      "Epoch [1/1], Step [8627/8897], Loss: 5.6518\n",
      "Epoch [1/1], Step [8628/8897], Loss: 5.1650\n",
      "Epoch [1/1], Step [8629/8897], Loss: 5.3852\n",
      "Epoch [1/1], Step [8630/8897], Loss: 5.4712\n",
      "Epoch [1/1], Step [8631/8897], Loss: 5.5754\n",
      "Epoch [1/1], Step [8632/8897], Loss: 5.5884\n",
      "Epoch [1/1], Step [8633/8897], Loss: 5.3637\n",
      "Epoch [1/1], Step [8634/8897], Loss: 5.3533\n",
      "Epoch [1/1], Step [8635/8897], Loss: 5.4005\n",
      "Epoch [1/1], Step [8636/8897], Loss: 5.1531\n",
      "Epoch [1/1], Step [8637/8897], Loss: 5.4459\n",
      "Epoch [1/1], Step [8638/8897], Loss: 5.1253\n",
      "Epoch [1/1], Step [8639/8897], Loss: 5.3575\n",
      "Epoch [1/1], Step [8640/8897], Loss: 5.1891\n",
      "Epoch [1/1], Step [8641/8897], Loss: 5.0121\n",
      "Epoch [1/1], Step [8642/8897], Loss: 5.3674\n",
      "Epoch [1/1], Step [8643/8897], Loss: 5.4221\n",
      "Epoch [1/1], Step [8644/8897], Loss: 5.6265\n",
      "Epoch [1/1], Step [8645/8897], Loss: 5.2306\n",
      "Epoch [1/1], Step [8646/8897], Loss: 5.2061\n",
      "Epoch [1/1], Step [8647/8897], Loss: 5.1758\n",
      "Epoch [1/1], Step [8648/8897], Loss: 5.1157\n",
      "Epoch [1/1], Step [8649/8897], Loss: 5.4719\n",
      "Epoch [1/1], Step [8650/8897], Loss: 5.2081\n",
      "Epoch [1/1], Step [8651/8897], Loss: 5.4711\n",
      "Epoch [1/1], Step [8652/8897], Loss: 5.6980\n",
      "Epoch [1/1], Step [8653/8897], Loss: 5.3230\n",
      "Epoch [1/1], Step [8654/8897], Loss: 5.2608\n",
      "Epoch [1/1], Step [8655/8897], Loss: 5.3934\n",
      "Epoch [1/1], Step [8656/8897], Loss: 5.3005\n",
      "Epoch [1/1], Step [8657/8897], Loss: 5.3348\n",
      "Epoch [1/1], Step [8658/8897], Loss: 5.4776\n",
      "Epoch [1/1], Step [8659/8897], Loss: 5.2337\n",
      "Epoch [1/1], Step [8660/8897], Loss: 5.3970\n",
      "Epoch [1/1], Step [8661/8897], Loss: 5.3203\n",
      "Epoch [1/1], Step [8662/8897], Loss: 5.4625\n",
      "Epoch [1/1], Step [8663/8897], Loss: 5.4472\n",
      "Epoch [1/1], Step [8664/8897], Loss: 5.4294\n",
      "Epoch [1/1], Step [8665/8897], Loss: 5.7696\n",
      "Epoch [1/1], Step [8666/8897], Loss: 5.3150\n",
      "Epoch [1/1], Step [8667/8897], Loss: 5.2663\n",
      "Epoch [1/1], Step [8668/8897], Loss: 5.1903\n",
      "Epoch [1/1], Step [8669/8897], Loss: 5.2189\n",
      "Epoch [1/1], Step [8670/8897], Loss: 5.4301\n",
      "Epoch [1/1], Step [8671/8897], Loss: 5.0149\n",
      "Epoch [1/1], Step [8672/8897], Loss: 5.4055\n",
      "Epoch [1/1], Step [8673/8897], Loss: 5.2115\n",
      "Epoch [1/1], Step [8674/8897], Loss: 5.3156\n",
      "Epoch [1/1], Step [8675/8897], Loss: 5.0809\n",
      "Epoch [1/1], Step [8676/8897], Loss: 5.4509\n",
      "Epoch [1/1], Step [8677/8897], Loss: 5.2442\n",
      "Epoch [1/1], Step [8678/8897], Loss: 5.2886\n",
      "Epoch [1/1], Step [8679/8897], Loss: 5.4653\n",
      "Epoch [1/1], Step [8680/8897], Loss: 5.1045\n",
      "Epoch [1/1], Step [8681/8897], Loss: 5.3678\n",
      "Epoch [1/1], Step [8682/8897], Loss: 5.3123\n",
      "Epoch [1/1], Step [8683/8897], Loss: 5.2722\n",
      "Epoch [1/1], Step [8684/8897], Loss: 5.5318\n",
      "Epoch [1/1], Step [8685/8897], Loss: 5.1772\n",
      "Epoch [1/1], Step [8686/8897], Loss: 5.6270\n",
      "Epoch [1/1], Step [8687/8897], Loss: 5.2700\n",
      "Epoch [1/1], Step [8688/8897], Loss: 5.4138\n",
      "Epoch [1/1], Step [8689/8897], Loss: 5.4313\n",
      "Epoch [1/1], Step [8690/8897], Loss: 5.2845\n",
      "Epoch [1/1], Step [8691/8897], Loss: 5.2929\n",
      "Epoch [1/1], Step [8692/8897], Loss: 5.3689\n",
      "Epoch [1/1], Step [8693/8897], Loss: 5.5722\n",
      "Epoch [1/1], Step [8694/8897], Loss: 5.3753\n",
      "Epoch [1/1], Step [8695/8897], Loss: 5.3624\n",
      "Epoch [1/1], Step [8696/8897], Loss: 5.2733\n",
      "Epoch [1/1], Step [8697/8897], Loss: 5.1641\n",
      "Epoch [1/1], Step [8698/8897], Loss: 5.3935\n",
      "Epoch [1/1], Step [8699/8897], Loss: 5.2168\n",
      "Epoch [1/1], Step [8700/8897], Loss: 5.2557\n",
      "Epoch [1/1], Step [8701/8897], Loss: 5.6608\n",
      "Epoch [1/1], Step [8702/8897], Loss: 5.5515\n",
      "Epoch [1/1], Step [8703/8897], Loss: 5.5155\n",
      "Epoch [1/1], Step [8704/8897], Loss: 5.6072\n",
      "Epoch [1/1], Step [8705/8897], Loss: 5.4553\n",
      "Epoch [1/1], Step [8706/8897], Loss: 5.3847\n",
      "Epoch [1/1], Step [8707/8897], Loss: 5.4120\n",
      "Epoch [1/1], Step [8708/8897], Loss: 5.3430\n",
      "Epoch [1/1], Step [8709/8897], Loss: 5.3617\n",
      "Epoch [1/1], Step [8710/8897], Loss: 5.0113\n",
      "Epoch [1/1], Step [8711/8897], Loss: 5.5908\n",
      "Epoch [1/1], Step [8712/8897], Loss: 5.4407\n",
      "Epoch [1/1], Step [8713/8897], Loss: 5.5189\n",
      "Epoch [1/1], Step [8714/8897], Loss: 5.5874\n",
      "Epoch [1/1], Step [8715/8897], Loss: 5.2861\n",
      "Epoch [1/1], Step [8716/8897], Loss: 5.3438\n",
      "Epoch [1/1], Step [8717/8897], Loss: 5.0157\n",
      "Epoch [1/1], Step [8718/8897], Loss: 5.3200\n",
      "Epoch [1/1], Step [8719/8897], Loss: 5.2019\n",
      "Epoch [1/1], Step [8720/8897], Loss: 5.3357\n",
      "Epoch [1/1], Step [8721/8897], Loss: 5.2242\n",
      "Epoch [1/1], Step [8722/8897], Loss: 5.6764\n",
      "Epoch [1/1], Step [8723/8897], Loss: 5.6746\n",
      "Epoch [1/1], Step [8724/8897], Loss: 5.0550\n",
      "Epoch [1/1], Step [8725/8897], Loss: 5.2638\n",
      "Epoch [1/1], Step [8726/8897], Loss: 5.4076\n",
      "Epoch [1/1], Step [8727/8897], Loss: 5.4361\n",
      "Epoch [1/1], Step [8728/8897], Loss: 5.1534\n",
      "Epoch [1/1], Step [8729/8897], Loss: 5.2651\n",
      "Epoch [1/1], Step [8730/8897], Loss: 5.3587\n",
      "Epoch [1/1], Step [8731/8897], Loss: 5.4616\n",
      "Epoch [1/1], Step [8732/8897], Loss: 5.3088\n",
      "Epoch [1/1], Step [8733/8897], Loss: 5.2722\n",
      "Epoch [1/1], Step [8734/8897], Loss: 5.2586\n",
      "Epoch [1/1], Step [8735/8897], Loss: 5.0450\n",
      "Epoch [1/1], Step [8736/8897], Loss: 5.4375\n",
      "Epoch [1/1], Step [8737/8897], Loss: 5.2276\n",
      "Epoch [1/1], Step [8738/8897], Loss: 5.2624\n",
      "Epoch [1/1], Step [8739/8897], Loss: 5.2610\n",
      "Epoch [1/1], Step [8740/8897], Loss: 5.5044\n",
      "Epoch [1/1], Step [8741/8897], Loss: 5.4075\n",
      "Epoch [1/1], Step [8742/8897], Loss: 5.6063\n",
      "Epoch [1/1], Step [8743/8897], Loss: 5.5726\n",
      "Epoch [1/1], Step [8744/8897], Loss: 5.2936\n",
      "Epoch [1/1], Step [8745/8897], Loss: 5.3832\n",
      "Epoch [1/1], Step [8746/8897], Loss: 5.2752\n",
      "Epoch [1/1], Step [8747/8897], Loss: 5.0578\n",
      "Epoch [1/1], Step [8748/8897], Loss: 5.3852\n",
      "Epoch [1/1], Step [8749/8897], Loss: 5.1649\n",
      "Epoch [1/1], Step [8750/8897], Loss: 5.3596\n",
      "Epoch [1/1], Step [8751/8897], Loss: 5.1591\n",
      "Epoch [1/1], Step [8752/8897], Loss: 5.6302\n",
      "Epoch [1/1], Step [8753/8897], Loss: 5.0863\n",
      "Epoch [1/1], Step [8754/8897], Loss: 5.5196\n",
      "Epoch [1/1], Step [8755/8897], Loss: 5.2987\n",
      "Epoch [1/1], Step [8756/8897], Loss: 5.3548\n",
      "Epoch [1/1], Step [8757/8897], Loss: 5.2406\n",
      "Epoch [1/1], Step [8758/8897], Loss: 5.2744\n",
      "Epoch [1/1], Step [8759/8897], Loss: 5.4106\n",
      "Epoch [1/1], Step [8760/8897], Loss: 5.3835\n",
      "Epoch [1/1], Step [8761/8897], Loss: 5.3306\n",
      "Epoch [1/1], Step [8762/8897], Loss: 5.2033\n",
      "Epoch [1/1], Step [8763/8897], Loss: 5.3488\n",
      "Epoch [1/1], Step [8764/8897], Loss: 5.5223\n",
      "Epoch [1/1], Step [8765/8897], Loss: 5.5693\n",
      "Epoch [1/1], Step [8766/8897], Loss: 5.2539\n",
      "Epoch [1/1], Step [8767/8897], Loss: 5.2889\n",
      "Epoch [1/1], Step [8768/8897], Loss: 5.5791\n",
      "Epoch [1/1], Step [8769/8897], Loss: 5.2765\n",
      "Epoch [1/1], Step [8770/8897], Loss: 5.3430\n",
      "Epoch [1/1], Step [8771/8897], Loss: 5.0634\n",
      "Epoch [1/1], Step [8772/8897], Loss: 5.5929\n",
      "Epoch [1/1], Step [8773/8897], Loss: 5.2249\n",
      "Epoch [1/1], Step [8774/8897], Loss: 5.2243\n",
      "Epoch [1/1], Step [8775/8897], Loss: 5.2822\n",
      "Epoch [1/1], Step [8776/8897], Loss: 5.4663\n",
      "Epoch [1/1], Step [8777/8897], Loss: 5.3601\n",
      "Epoch [1/1], Step [8778/8897], Loss: 5.4437\n",
      "Epoch [1/1], Step [8779/8897], Loss: 5.6304\n",
      "Epoch [1/1], Step [8780/8897], Loss: 5.3535\n",
      "Epoch [1/1], Step [8781/8897], Loss: 5.1863\n",
      "Epoch [1/1], Step [8782/8897], Loss: 5.4741\n",
      "Epoch [1/1], Step [8783/8897], Loss: 5.3076\n",
      "Epoch [1/1], Step [8784/8897], Loss: 5.3240\n",
      "Epoch [1/1], Step [8785/8897], Loss: 5.1478\n",
      "Epoch [1/1], Step [8786/8897], Loss: 5.3552\n",
      "Epoch [1/1], Step [8787/8897], Loss: 5.3565\n",
      "Epoch [1/1], Step [8788/8897], Loss: 5.2574\n",
      "Epoch [1/1], Step [8789/8897], Loss: 5.3755\n",
      "Epoch [1/1], Step [8790/8897], Loss: 5.4408\n",
      "Epoch [1/1], Step [8791/8897], Loss: 5.1427\n",
      "Epoch [1/1], Step [8792/8897], Loss: 5.4361\n",
      "Epoch [1/1], Step [8793/8897], Loss: 5.1943\n",
      "Epoch [1/1], Step [8794/8897], Loss: 5.0852\n",
      "Epoch [1/1], Step [8795/8897], Loss: 5.3300\n",
      "Epoch [1/1], Step [8796/8897], Loss: 5.4660\n",
      "Epoch [1/1], Step [8797/8897], Loss: 5.7264\n",
      "Epoch [1/1], Step [8798/8897], Loss: 5.0957\n",
      "Epoch [1/1], Step [8799/8897], Loss: 5.6028\n",
      "Epoch [1/1], Step [8800/8897], Loss: 5.4131\n",
      "Epoch [1/1], Step [8801/8897], Loss: 5.3675\n",
      "Epoch [1/1], Step [8802/8897], Loss: 5.0766\n",
      "Epoch [1/1], Step [8803/8897], Loss: 5.0850\n",
      "Epoch [1/1], Step [8804/8897], Loss: 5.6514\n",
      "Epoch [1/1], Step [8805/8897], Loss: 5.1369\n",
      "Epoch [1/1], Step [8806/8897], Loss: 5.3364\n",
      "Epoch [1/1], Step [8807/8897], Loss: 5.4166\n",
      "Epoch [1/1], Step [8808/8897], Loss: 5.7192\n",
      "Epoch [1/1], Step [8809/8897], Loss: 5.1002\n",
      "Epoch [1/1], Step [8810/8897], Loss: 5.2913\n",
      "Epoch [1/1], Step [8811/8897], Loss: 5.1461\n",
      "Epoch [1/1], Step [8812/8897], Loss: 5.4194\n",
      "Epoch [1/1], Step [8813/8897], Loss: 5.6353\n",
      "Epoch [1/1], Step [8814/8897], Loss: 5.2149\n",
      "Epoch [1/1], Step [8815/8897], Loss: 5.4108\n",
      "Epoch [1/1], Step [8816/8897], Loss: 5.3908\n",
      "Epoch [1/1], Step [8817/8897], Loss: 5.3096\n",
      "Epoch [1/1], Step [8818/8897], Loss: 5.2647\n",
      "Epoch [1/1], Step [8819/8897], Loss: 5.2548\n",
      "Epoch [1/1], Step [8820/8897], Loss: 5.1778\n",
      "Epoch [1/1], Step [8821/8897], Loss: 5.2903\n",
      "Epoch [1/1], Step [8822/8897], Loss: 5.2592\n",
      "Epoch [1/1], Step [8823/8897], Loss: 5.3408\n",
      "Epoch [1/1], Step [8824/8897], Loss: 5.3366\n",
      "Epoch [1/1], Step [8825/8897], Loss: 5.3808\n",
      "Epoch [1/1], Step [8826/8897], Loss: 5.4208\n",
      "Epoch [1/1], Step [8827/8897], Loss: 5.5324\n",
      "Epoch [1/1], Step [8828/8897], Loss: 5.2449\n",
      "Epoch [1/1], Step [8829/8897], Loss: 5.1334\n",
      "Epoch [1/1], Step [8830/8897], Loss: 5.0406\n",
      "Epoch [1/1], Step [8831/8897], Loss: 5.5884\n",
      "Epoch [1/1], Step [8832/8897], Loss: 5.3335\n",
      "Epoch [1/1], Step [8833/8897], Loss: 5.3283\n",
      "Epoch [1/1], Step [8834/8897], Loss: 5.4983\n",
      "Epoch [1/1], Step [8835/8897], Loss: 5.4729\n",
      "Epoch [1/1], Step [8836/8897], Loss: 5.2762\n",
      "Epoch [1/1], Step [8837/8897], Loss: 5.3532\n",
      "Epoch [1/1], Step [8838/8897], Loss: 5.3207\n",
      "Epoch [1/1], Step [8839/8897], Loss: 5.4775\n",
      "Epoch [1/1], Step [8840/8897], Loss: 5.1329\n",
      "Epoch [1/1], Step [8841/8897], Loss: 5.2366\n",
      "Epoch [1/1], Step [8842/8897], Loss: 5.3872\n",
      "Epoch [1/1], Step [8843/8897], Loss: 5.4501\n",
      "Epoch [1/1], Step [8844/8897], Loss: 5.5421\n",
      "Epoch [1/1], Step [8845/8897], Loss: 5.1969\n",
      "Epoch [1/1], Step [8846/8897], Loss: 5.5210\n",
      "Epoch [1/1], Step [8847/8897], Loss: 5.4660\n",
      "Epoch [1/1], Step [8848/8897], Loss: 5.3048\n",
      "Epoch [1/1], Step [8849/8897], Loss: 5.4049\n",
      "Epoch [1/1], Step [8850/8897], Loss: 5.3065\n",
      "Epoch [1/1], Step [8851/8897], Loss: 5.1581\n",
      "Epoch [1/1], Step [8852/8897], Loss: 5.4455\n",
      "Epoch [1/1], Step [8853/8897], Loss: 5.2959\n",
      "Epoch [1/1], Step [8854/8897], Loss: 5.2938\n",
      "Epoch [1/1], Step [8855/8897], Loss: 5.2444\n",
      "Epoch [1/1], Step [8856/8897], Loss: 5.4934\n",
      "Epoch [1/1], Step [8857/8897], Loss: 5.0283\n",
      "Epoch [1/1], Step [8858/8897], Loss: 5.6228\n",
      "Epoch [1/1], Step [8859/8897], Loss: 5.2289\n",
      "Epoch [1/1], Step [8860/8897], Loss: 5.5135\n",
      "Epoch [1/1], Step [8861/8897], Loss: 5.0771\n",
      "Epoch [1/1], Step [8862/8897], Loss: 5.3374\n",
      "Epoch [1/1], Step [8863/8897], Loss: 5.6419\n",
      "Epoch [1/1], Step [8864/8897], Loss: 5.1423\n",
      "Epoch [1/1], Step [8865/8897], Loss: 5.6168\n",
      "Epoch [1/1], Step [8866/8897], Loss: 5.5667\n",
      "Epoch [1/1], Step [8867/8897], Loss: 5.2979\n",
      "Epoch [1/1], Step [8868/8897], Loss: 5.0465\n",
      "Epoch [1/1], Step [8869/8897], Loss: 5.4280\n",
      "Epoch [1/1], Step [8870/8897], Loss: 5.6039\n",
      "Epoch [1/1], Step [8871/8897], Loss: 5.1687\n",
      "Epoch [1/1], Step [8872/8897], Loss: 5.5469\n",
      "Epoch [1/1], Step [8873/8897], Loss: 5.2133\n",
      "Epoch [1/1], Step [8874/8897], Loss: 5.2086\n",
      "Epoch [1/1], Step [8875/8897], Loss: 5.4295\n",
      "Epoch [1/1], Step [8876/8897], Loss: 5.3063\n",
      "Epoch [1/1], Step [8877/8897], Loss: 5.4450\n",
      "Epoch [1/1], Step [8878/8897], Loss: 5.3275\n",
      "Epoch [1/1], Step [8879/8897], Loss: 5.2659\n",
      "Epoch [1/1], Step [8880/8897], Loss: 5.3517\n",
      "Epoch [1/1], Step [8881/8897], Loss: 5.2156\n",
      "Epoch [1/1], Step [8882/8897], Loss: 5.0506\n",
      "Epoch [1/1], Step [8883/8897], Loss: 5.2043\n",
      "Epoch [1/1], Step [8884/8897], Loss: 5.5060\n",
      "Epoch [1/1], Step [8885/8897], Loss: 5.5242\n",
      "Epoch [1/1], Step [8886/8897], Loss: 5.1484\n",
      "Epoch [1/1], Step [8887/8897], Loss: 5.3865\n",
      "Epoch [1/1], Step [8888/8897], Loss: 5.2425\n",
      "Epoch [1/1], Step [8889/8897], Loss: 5.2210\n",
      "Epoch [1/1], Step [8890/8897], Loss: 5.3702\n",
      "Epoch [1/1], Step [8891/8897], Loss: 5.4128\n",
      "Epoch [1/1], Step [8892/8897], Loss: 5.1637\n",
      "Epoch [1/1], Step [8893/8897], Loss: 5.3965\n",
      "Epoch [1/1], Step [8894/8897], Loss: 5.4713\n",
      "Epoch [1/1], Step [8895/8897], Loss: 5.1092\n",
      "Epoch [1/1], Step [8896/8897], Loss: 5.3781\n",
      "Epoch [1/1], Step [8897/8897], Loss: 5.3107\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "learning_rates = []\n",
    "\n",
    "# Training Loop\n",
    "for epoch_idx in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (train_images, train_labels) in enumerate(train_loader1k):\n",
    "        train_images, train_labels = train_images.to(device), train_labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        train_outputs = model(train_images)\n",
    "\n",
    "        # Compute the loss\n",
    "        train_loss = criterion(train_outputs, train_labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Gradient Clip\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Step optimizer and the scheduler\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss.item())\n",
    "        learning_rates.append(scheduler.get_last_lr()[0])  # Assumes optimizer has a single param group\n",
    "\n",
    "        print(f\"Epoch [{epoch_idx+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader1k)}], Loss: {train_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the validation images: 7.578%\n"
     ]
    }
   ],
   "source": [
    "# Validation Loop\n",
    "# NOTE: LOGITS TO MAX LOGIT FUNCTION MIGHT CHANGE DUE TO SPECIFIC NATURE OF VISION TRANSFORMER ALGORITHM\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for val_images, val_labels in val_loader1k:\n",
    "        val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "\n",
    "        # Logits\n",
    "        val_outputs = model(val_images)\n",
    "\n",
    "        # Let the index of the highest logit be the predicted class \n",
    "        _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "\n",
    "        # Update counts from this batch's values\n",
    "        total_count += val_labels.size(0)\n",
    "        correct_count += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "    # Print accuracy score\n",
    "    print(f'Accuracy of the model on the validation images: {100 * correct_count / total_count}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save(model, './models/vit_16p_1280d_12l.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit21k_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
