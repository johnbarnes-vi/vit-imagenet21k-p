{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure you follow the preprocessing instructions in the README.md file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Lets see the directory structure of imagenet1k\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        jpeg_files = [f for f in files if f.endswith('.JPEG')]\n",
    "        if jpeg_files:  # if the list is not empty\n",
    "            print('{}Number of JPEG files: {}'.format(subindent, len(jpeg_files)))\n",
    "        for f in files:\n",
    "            if f.endswith('.txt'):\n",
    "                print('{}{}'.format(subindent, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet1k_resized/ILSVRC2012_img_train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet1k_resized/ILSVRC2012_img_val/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet21k_resized_new/imagenet21k_train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet21k_resized_new/imagenet21k_val/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is clear from the output of the above cells that preprocessing worked!\n",
    "\n",
    "We are looking to see if the validation and training sets are organized in the same manner and that they are ordered the same.\n",
    "\n",
    "This makes input into the `torchvision.datasets.ImageFolder` class work without a hitch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries to unzip `tiny-imagenet-200.zip`\n",
    "import zipfile\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "# Importing pytorch libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy class, import real class when vit.py is done\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb Cell 10\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39;49mSGD(model\u001b[39m.\u001b[39;49mparameters(), lr\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m, momentum\u001b[39m=\u001b[39;49m\u001b[39m0.9\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Define a transform for training data\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/vit-imagenet21k-p/vit21k_env/lib/python3.10/site-packages/torch/optim/sgd.py:27\u001b[0m, in \u001b[0;36mSGD.__init__\u001b[0;34m(self, params, lr, momentum, dampening, weight_decay, nesterov, maximize, foreach, differentiable)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mif\u001b[39;00m nesterov \u001b[39mand\u001b[39;00m (momentum \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m dampening \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m     26\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNesterov momentum requires a momentum and zero dampening\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(params, defaults)\n",
      "File \u001b[0;32m~/repos/vit-imagenet21k-p/vit21k_env/lib/python3.10/site-packages/torch/optim/optimizer.py:187\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    185\u001b[0m param_groups \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(params)\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(param_groups) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39moptimizer got an empty parameter list\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(param_groups[\u001b[39m0\u001b[39m], \u001b[39mdict\u001b[39m):\n\u001b[1;32m    189\u001b[0m     param_groups \u001b[39m=\u001b[39m [{\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: param_groups}]\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VisionTransformer()\n",
    "model.to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "num_epochs = 10\n",
    "\n",
    "# Define a transform for training data\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Pad(4),  # Pad the image by 4 pixels\n",
    "    transforms.RandomCrop(224),  # Randomly crop a 224x224 region from the padded image\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Define a transform for validation data\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to range [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CPU cores: 24\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of available CPU cores:\", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageNet-1k has 1,281,200 training images and 50,000 validation images!\n"
     ]
    }
   ],
   "source": [
    "# Load ImageNet1k dataset and make DataLoaders\n",
    "train_dataset1k = datasets.ImageFolder(root='/mnt/imagenet1k_resized/ILSVRC2012_img_train', transform=train_transform)\n",
    "val_dataset1k = datasets.ImageFolder(root='/mnt/imagenet1k_resized/ILSVRC2012_img_val', transform=val_transform)\n",
    "\n",
    "train_loader1k = DataLoader(dataset=train_dataset1k, batch_size=batch_size, shuffle=True, num_workers=20, pin_memory=True)\n",
    "val_loader1k = DataLoader(dataset=val_dataset1k, batch_size=batch_size, shuffle=False, num_workers=20, pin_memory=True)\n",
    "\n",
    "print(f\"ImageNet-1k has {len(train_loader1k)*batch_size:,} training images and {len(val_loader1k)*batch_size:,} validation images!\")\n",
    "\n",
    "# Load ImageNet21k dataset and make DataLoaders\n",
    "#train_dataset21k = datasets.ImageFolder(root='/mnt/imagenet21k_resized_new/imagenet21k_train', transform=train_transform)\n",
    "#val_dataset21k = datasets.ImageFolder(root='/mnt/imagenet21k_resized_new/imagenet21k_val', transform=val_transform)\n",
    "\n",
    "#train_loader21k = DataLoader(dataset=train_dataset21k, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "#val_loader21k = DataLoader(dataset=val_dataset21k, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "#print(f\"ImageNet-21k has {len(train_loader21k)*batch_size:,} training images and {len(val_loader21k)*batch_size:,} validation images!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images batch shape: torch.Size([100, 3, 224, 224])\n",
      "Train labels batch shape: torch.Size([100])\n",
      "Train images data type: torch.float32\n",
      "Train labels data type: torch.int64\n",
      "Validation images batch shape: torch.Size([100, 3, 224, 224])\n",
      "Validation labels batch shape: torch.Size([100])\n",
      "Validation images data type: torch.float32\n",
      "Validation labels data type: torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Inspect a batch from train_loader1k\n",
    "train_images, train_labels = next(iter(train_loader1k))\n",
    "print(\"Train images batch shape:\", train_images.shape)\n",
    "print(\"Train labels batch shape:\", train_labels.shape)\n",
    "print(\"Train images data type:\", train_images.dtype)\n",
    "print(\"Train labels data type:\", train_labels.dtype)\n",
    "\n",
    "# Inspect a batch from val_loader1k\n",
    "val_images, val_labels = next(iter(val_loader1k))\n",
    "print(\"Validation images batch shape:\", val_images.shape)\n",
    "print(\"Validation labels batch shape:\", val_labels.shape)\n",
    "print(\"Validation images data type:\", val_images.dtype)\n",
    "print(\"Validation labels data type:\", val_labels.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Training Loop\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, (train_images, train_labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader1k):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (train_images, train_labels) in enumerate(train_loader1k):\n",
    "        train_images, train_labels = train_images.to(device), train_labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        train_outputs = model(train_images)\n",
    "\n",
    "        # Compute the loss\n",
    "        train_loss = criterion(train_outputs, train_labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch_idx+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader1k)}], Loss: {train_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Module [VisionTransformer] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m val_images, val_labels \u001b[39m=\u001b[39m val_images\u001b[39m.\u001b[39mto(device), val_labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Logits\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb#X25sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m val_outputs \u001b[39m=\u001b[39m model(val_images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Let the index of the highest logit be the predicted class \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jabarne6/repos/vit-imagenet21k-p/main.ipynb#X25sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m _, val_predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(val_outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/repos/vit-imagenet21k-p/vit21k_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/vit-imagenet21k-p/vit21k_env/lib/python3.10/site-packages/torch/nn/modules/module.py:363\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_unimplemented\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39minput\u001b[39m: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Defines the computation performed at every call.\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \n\u001b[1;32m    355\u001b[0m \u001b[39m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[39m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModule [\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m] is missing the required \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mforward\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m function\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Module [VisionTransformer] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "# Validation Loop\n",
    "# NOTE: LOGITS TO MAX LOGIT FUNCTION MIGHT CHANGE DUE TO SPECIFIC NATURE OF VISION TRANSFORMER ALGORITHM\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for val_images, val_labels in val_loader1k:\n",
    "        val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "\n",
    "        # Logits\n",
    "        val_outputs = model(val_images)\n",
    "\n",
    "        # Let the index of the highest logit be the predicted class \n",
    "        _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "\n",
    "        # Update counts from this batch's values\n",
    "        total_count += val_labels.size(0)\n",
    "        correct_count += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "    # Print accuracy score\n",
    "    print(f'Accuracy of the model on the validation images: {100 * correct_count / total_count}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit21k_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
