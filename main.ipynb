{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure you follow the preprocessing instructions in the README.md file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Lets see the directory structure of imagenet1k\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        jpeg_files = [f for f in files if f.endswith('.JPEG')]\n",
    "        if jpeg_files:  # if the list is not empty\n",
    "            print('{}Number of JPEG files: {}'.format(subindent, len(jpeg_files)))\n",
    "        for f in files:\n",
    "            if f.endswith('.txt'):\n",
    "                print('{}{}'.format(subindent, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet1k_resized/ILSVRC2012_img_train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet1k_resized/ILSVRC2012_img_val/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet21k_resized_new/imagenet21k_train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_files('/mnt/imagenet21k_resized_new/imagenet21k_val/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is clear from the output of the above cells that preprocessing worked!\n",
    "\n",
    "We are looking to see if the validation and training sets are organized in the same manner and that they are ordered the same.\n",
    "\n",
    "This makes input into the `torchvision.datasets.ImageFolder` class work without a hitch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries to unzip `tiny-imagenet-200.zip`\n",
    "import zipfile\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "# Importing pytorch libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing custom VisionTransformer Model\n",
    "\n",
    "from models.vit import VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cuda')\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 384 # should be 4096 for ViT paper\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "patch_size_ = 32\n",
    "\n",
    "num_layers_ = 12\n",
    "D_ = 768\n",
    "mlp_size_ = 3072\n",
    "num_heads_ = 12\n",
    "\n",
    "num_classes_ = 1000\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "model = VisionTransformer(patch_size=patch_size_, D=D_, num_layers=num_layers_, num_classes=num_classes_, num_heads=num_heads_, mlp_size=mlp_size_)\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "\n",
    "# Define a transform for training data\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Pad(4),  # Pad the image by 4 pixels\n",
    "    transforms.RandomCrop(224),  # Randomly crop a 224x224 region from the padded image\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Define a transform for validation data\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to range [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CPU cores: 24\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of available CPU cores:\", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageNet-1k has 1,281,408 training images and 50,304 validation images!\n"
     ]
    }
   ],
   "source": [
    "# Load ImageNet1k dataset and make DataLoaders\n",
    "train_dataset1k = datasets.ImageFolder(root='/mnt/imagenet1k_resized/ILSVRC2012_img_train', transform=train_transform)\n",
    "val_dataset1k = datasets.ImageFolder(root='/mnt/imagenet1k_resized/ILSVRC2012_img_val', transform=val_transform)\n",
    "\n",
    "train_loader1k = DataLoader(dataset=train_dataset1k, batch_size=batch_size, shuffle=True, num_workers=20, pin_memory=True)\n",
    "val_loader1k = DataLoader(dataset=val_dataset1k, batch_size=batch_size, shuffle=False, num_workers=20, pin_memory=True)\n",
    "\n",
    "#Calculate total steps\n",
    "total_steps = len(train_loader1k) * num_epochs\n",
    "\n",
    "# StepLR that decays the learning rate every 30 epochs\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.9)\n",
    "\n",
    "print(f\"ImageNet-1k has {len(train_loader1k)*batch_size:,} training images and {len(val_loader1k)*batch_size:,} validation images!\")\n",
    "\n",
    "# Load ImageNet21k dataset and make DataLoaders\n",
    "#train_dataset21k = datasets.ImageFolder(root='/mnt/imagenet21k_resized_new/imagenet21k_train', transform=train_transform)\n",
    "#val_dataset21k = datasets.ImageFolder(root='/mnt/imagenet21k_resized_new/imagenet21k_val', transform=val_transform)\n",
    "\n",
    "#train_loader21k = DataLoader(dataset=train_dataset21k, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "#val_loader21k = DataLoader(dataset=val_dataset21k, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "#print(f\"ImageNet-21k has {len(train_loader21k)*batch_size:,} training images and {len(val_loader21k)*batch_size:,} validation images!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3337"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a batch from train_loader1k\n",
    "train_images, train_labels = next(iter(train_loader1k))\n",
    "train_images, train_labels = train_images.to(device), train_labels.to(device)\n",
    "\n",
    "print(\"Train images batch shape:\", train_images.shape)\n",
    "print(\"Train labels batch shape:\", train_labels.shape)\n",
    "print(\"Train images data type:\", train_images.dtype)\n",
    "print(\"Train labels data type:\", train_labels.dtype)\n",
    "\n",
    "# Inspect a batch from val_loader1k\n",
    "val_images, val_labels = next(iter(val_loader1k))\n",
    "\n",
    "print(\"Validation images batch shape:\", val_images.shape)\n",
    "print(\"Validation labels batch shape:\", val_labels.shape)\n",
    "print(\"Validation images data type:\", val_images.dtype)\n",
    "print(\"Validation labels data type:\", val_labels.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING COMPONENTS OF vit.py IN IPYNB BEFORE MOVING TO .PY FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Class for Image Preprocessing\n",
    "class ImagePreprocessor(nn.Module):\n",
    "    def __init__(self, patch_size):\n",
    "        super(ImagePreprocessor, self).__init__()\n",
    "        self.patch_size = patch_size  # Size of each patch\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Dynamically get the batch size and channel dimensions\n",
    "        batch_size, channel, _, _ = x.size()\n",
    "\n",
    "        # Using unfold to create patches\n",
    "        x_p = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "\n",
    "        # Reshape into the desired shape\n",
    "        x_p = x_p.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "        x_p = x_p.view(batch_size, -1, self.patch_size * self.patch_size * channel)\n",
    "\n",
    "        # Now x_p should have shape [batch_size, (Height * Width) / (patch_size * patch_size), (patch_size * patch_size * channel)]\n",
    "        \n",
    "        return x_p\n",
    "\n",
    "# Class for Patch Embedding\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_dim, D):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.D = D  # Dimension to project to\n",
    "        self.linear = nn.Linear(patch_dim, D)  # Linear projection layer\n",
    "\n",
    "    def forward(self, x_p):\n",
    "        # Project patches to D dimensions\n",
    "        x_emb = self.linear(x_p)\n",
    "        return x_emb\n",
    "\n",
    "# Class for adding a Class Token\n",
    "class ClassToken(nn.Module):\n",
    "    def __init__(self, D):\n",
    "        super(ClassToken, self).__init__()\n",
    "        self.class_token_embedding = nn.Parameter(torch.randn(1, 1, D))  # Learnable class token\n",
    "\n",
    "    def forward(self, x_emb):\n",
    "        # Prepend class token to patch embeddings\n",
    "        batch_size = x_emb.size(0)\n",
    "        class_token = self.class_token_embedding.repeat(batch_size, 1, 1)\n",
    "        x_class = torch.cat([class_token, x_emb], dim=1)\n",
    "        return x_class\n",
    "\n",
    "# Class for Position Embeddings\n",
    "class PositionEmbedding(nn.Module):\n",
    "    def __init__(self, seq_len, D):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(1, seq_len, D))  # Learnable position embeddings\n",
    "\n",
    "    def forward(self, x_class):\n",
    "        # Add position embeddings\n",
    "        x_pos = x_class + self.position_embeddings\n",
    "        return x_pos\n",
    "\n",
    "# Class for Transformer Encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, D, num_layers):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_norm = nn.LayerNorm(D)\n",
    "        self.multihead_attention = nn.MultiheadAttention(D, num_heads=4, batch_first=True)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(D, D),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(D, D)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_pos):\n",
    "        # Transformer Encoder Logic\n",
    "        for _ in range(self.num_layers):\n",
    "            x_norm = self.layer_norm(x_pos)\n",
    "            x_att, _ = self.multihead_attention(x_norm, x_norm, x_norm)\n",
    "            x_pos = x_pos + x_att\n",
    "            x_pos = x_pos + self.mlp(self.layer_norm(x_pos))\n",
    "        return x_pos\n",
    "\n",
    "# Class for Classification Head\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, D, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.linear = nn.Linear(D, num_classes)  # Linear layer for classification\n",
    "\n",
    "    def forward(self, x_transformed):\n",
    "        # Take the class token and perform classification\n",
    "        x_class_token = x_transformed[:, 0, :]\n",
    "        print(\"x_class_token.shape: \", x_class_token.shape)\n",
    "        output = self.linear(x_class_token)\n",
    "        return output\n",
    "\n",
    "# Main Vision Transformer Class\n",
    "class VisionTransformerTest(nn.Module):\n",
    "    def __init__(self, patch_size, D, num_layers, num_classes):\n",
    "        super(VisionTransformerTest, self).__init__()\n",
    "        self.image_preprocessor = ImagePreprocessor(patch_size)\n",
    "        self.patch_embedding = PatchEmbedding(patch_size * patch_size * 3, D)  # 3 channels, patch_size x patch_size patches\n",
    "        self.class_token = ClassToken(D)\n",
    "        self.position_embedding = PositionEmbedding(197, D)  # 196 patches + 1 class token\n",
    "        self.transformer_encoder = TransformerEncoder(D, num_layers)\n",
    "        self.classification_head = ClassificationHead(D, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"x.shape\",x.shape)\n",
    "        \n",
    "        # Preprocess the image into patches\n",
    "        x_p = self.image_preprocessor(x)\n",
    "        print(\"x_p.shape: \", x_p.shape)\n",
    "\n",
    "        # Generate patch embeddings\n",
    "        x_emb = self.patch_embedding(x_p)\n",
    "        print(\"x_emb.shape: \", x_emb.shape)\n",
    "\n",
    "        # Prepend the class token\n",
    "        x_class = self.class_token(x_emb)\n",
    "        print(\"x_class.shape: \", x_class.shape)\n",
    "\n",
    "        # Add position embeddings\n",
    "        x_pos = self.position_embedding(x_class)\n",
    "        print(\"x_pos.shape: \", x_pos.shape)\n",
    "\n",
    "        # Pass through the Transformer Encoder\n",
    "        x_transformed = self.transformer_encoder(x_pos)\n",
    "        print(\"x_transformed.shape: \", x_transformed.shape)\n",
    "    \n",
    "        # Perform classification\n",
    "        output = self.classification_head(x_transformed)\n",
    "        print(\"output.shape: \", output.shape)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = VisionTransformerTest(patch_size=patch_size_, D=D_, num_layers=num_layers_, num_classes=num_classes_)\n",
    "model_test.to(device)\n",
    "output = model_test(train_images)\n",
    "print(\"Above was the transformation path of the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING COMPONENTS OF vit.py IN IPYNB BEFORE MOVING TO .PY FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1/3337], Loss: 8.9804\n",
      "Epoch [1/1], Step [2/3337], Loss: 8.5813\n",
      "Epoch [1/1], Step [3/3337], Loss: 8.2338\n",
      "Epoch [1/1], Step [4/3337], Loss: 8.0848\n",
      "Epoch [1/1], Step [5/3337], Loss: 7.9801\n",
      "Epoch [1/1], Step [6/3337], Loss: 7.7715\n",
      "Epoch [1/1], Step [7/3337], Loss: 7.6221\n",
      "Epoch [1/1], Step [8/3337], Loss: 7.7203\n",
      "Epoch [1/1], Step [9/3337], Loss: 7.5934\n",
      "Epoch [1/1], Step [10/3337], Loss: 7.5391\n",
      "Epoch [1/1], Step [11/3337], Loss: 7.4459\n",
      "Epoch [1/1], Step [12/3337], Loss: 7.5244\n",
      "Epoch [1/1], Step [13/3337], Loss: 7.3936\n",
      "Epoch [1/1], Step [14/3337], Loss: 7.3887\n",
      "Epoch [1/1], Step [15/3337], Loss: 7.3373\n",
      "Epoch [1/1], Step [16/3337], Loss: 7.3250\n",
      "Epoch [1/1], Step [17/3337], Loss: 7.2787\n",
      "Epoch [1/1], Step [18/3337], Loss: 7.3096\n",
      "Epoch [1/1], Step [19/3337], Loss: 7.3543\n",
      "Epoch [1/1], Step [20/3337], Loss: 7.2564\n",
      "Epoch [1/1], Step [21/3337], Loss: 7.2403\n",
      "Epoch [1/1], Step [22/3337], Loss: 7.2475\n",
      "Epoch [1/1], Step [23/3337], Loss: 7.1873\n",
      "Epoch [1/1], Step [24/3337], Loss: 7.2112\n",
      "Epoch [1/1], Step [25/3337], Loss: 7.2569\n",
      "Epoch [1/1], Step [26/3337], Loss: 7.2115\n",
      "Epoch [1/1], Step [27/3337], Loss: 7.2550\n",
      "Epoch [1/1], Step [28/3337], Loss: 7.2007\n",
      "Epoch [1/1], Step [29/3337], Loss: 7.2926\n",
      "Epoch [1/1], Step [30/3337], Loss: 7.2282\n",
      "Epoch [1/1], Step [31/3337], Loss: 7.2045\n",
      "Epoch [1/1], Step [32/3337], Loss: 7.2902\n",
      "Epoch [1/1], Step [33/3337], Loss: 7.1906\n",
      "Epoch [1/1], Step [34/3337], Loss: 7.2462\n",
      "Epoch [1/1], Step [35/3337], Loss: 7.2805\n",
      "Epoch [1/1], Step [36/3337], Loss: 7.2531\n",
      "Epoch [1/1], Step [37/3337], Loss: 7.1761\n",
      "Epoch [1/1], Step [38/3337], Loss: 7.1506\n",
      "Epoch [1/1], Step [39/3337], Loss: 7.1478\n",
      "Epoch [1/1], Step [40/3337], Loss: 7.1521\n",
      "Epoch [1/1], Step [41/3337], Loss: 7.1588\n",
      "Epoch [1/1], Step [42/3337], Loss: 7.2050\n",
      "Epoch [1/1], Step [43/3337], Loss: 7.0577\n",
      "Epoch [1/1], Step [44/3337], Loss: 7.1521\n",
      "Epoch [1/1], Step [45/3337], Loss: 7.1297\n",
      "Epoch [1/1], Step [46/3337], Loss: 7.1227\n",
      "Epoch [1/1], Step [47/3337], Loss: 7.0956\n",
      "Epoch [1/1], Step [48/3337], Loss: 7.1331\n",
      "Epoch [1/1], Step [49/3337], Loss: 7.1682\n",
      "Epoch [1/1], Step [50/3337], Loss: 7.0932\n",
      "Epoch [1/1], Step [51/3337], Loss: 7.1171\n",
      "Epoch [1/1], Step [52/3337], Loss: 7.1028\n",
      "Epoch [1/1], Step [53/3337], Loss: 7.0889\n",
      "Epoch [1/1], Step [54/3337], Loss: 7.1354\n",
      "Epoch [1/1], Step [55/3337], Loss: 7.1010\n",
      "Epoch [1/1], Step [56/3337], Loss: 7.1408\n",
      "Epoch [1/1], Step [57/3337], Loss: 7.1410\n",
      "Epoch [1/1], Step [58/3337], Loss: 7.1216\n",
      "Epoch [1/1], Step [59/3337], Loss: 7.1261\n",
      "Epoch [1/1], Step [60/3337], Loss: 7.1833\n",
      "Epoch [1/1], Step [61/3337], Loss: 7.1680\n",
      "Epoch [1/1], Step [62/3337], Loss: 7.1831\n",
      "Epoch [1/1], Step [63/3337], Loss: 7.1574\n",
      "Epoch [1/1], Step [64/3337], Loss: 7.0450\n",
      "Epoch [1/1], Step [65/3337], Loss: 7.0683\n",
      "Epoch [1/1], Step [66/3337], Loss: 7.0861\n",
      "Epoch [1/1], Step [67/3337], Loss: 7.0322\n",
      "Epoch [1/1], Step [68/3337], Loss: 7.0556\n",
      "Epoch [1/1], Step [69/3337], Loss: 7.1903\n",
      "Epoch [1/1], Step [70/3337], Loss: 7.1721\n",
      "Epoch [1/1], Step [71/3337], Loss: 7.1432\n",
      "Epoch [1/1], Step [72/3337], Loss: 7.0959\n",
      "Epoch [1/1], Step [73/3337], Loss: 7.0065\n",
      "Epoch [1/1], Step [74/3337], Loss: 7.1038\n",
      "Epoch [1/1], Step [75/3337], Loss: 7.0647\n",
      "Epoch [1/1], Step [76/3337], Loss: 6.9949\n",
      "Epoch [1/1], Step [77/3337], Loss: 7.0654\n",
      "Epoch [1/1], Step [78/3337], Loss: 7.1170\n",
      "Epoch [1/1], Step [79/3337], Loss: 7.1282\n",
      "Epoch [1/1], Step [80/3337], Loss: 7.1881\n",
      "Epoch [1/1], Step [81/3337], Loss: 7.1171\n",
      "Epoch [1/1], Step [82/3337], Loss: 7.1144\n",
      "Epoch [1/1], Step [83/3337], Loss: 6.9961\n",
      "Epoch [1/1], Step [84/3337], Loss: 7.0901\n",
      "Epoch [1/1], Step [85/3337], Loss: 7.1317\n",
      "Epoch [1/1], Step [86/3337], Loss: 7.1136\n",
      "Epoch [1/1], Step [87/3337], Loss: 7.0350\n",
      "Epoch [1/1], Step [88/3337], Loss: 7.1444\n",
      "Epoch [1/1], Step [89/3337], Loss: 7.1372\n",
      "Epoch [1/1], Step [90/3337], Loss: 7.0856\n",
      "Epoch [1/1], Step [91/3337], Loss: 7.0449\n",
      "Epoch [1/1], Step [92/3337], Loss: 7.1172\n",
      "Epoch [1/1], Step [93/3337], Loss: 7.1056\n",
      "Epoch [1/1], Step [94/3337], Loss: 7.0006\n",
      "Epoch [1/1], Step [95/3337], Loss: 6.9867\n",
      "Epoch [1/1], Step [96/3337], Loss: 7.1061\n",
      "Epoch [1/1], Step [97/3337], Loss: 7.0341\n",
      "Epoch [1/1], Step [98/3337], Loss: 7.0241\n",
      "Epoch [1/1], Step [99/3337], Loss: 7.0346\n",
      "Epoch [1/1], Step [100/3337], Loss: 7.1051\n",
      "Epoch [1/1], Step [101/3337], Loss: 7.0676\n",
      "Epoch [1/1], Step [102/3337], Loss: 7.0977\n",
      "Epoch [1/1], Step [103/3337], Loss: 7.0741\n",
      "Epoch [1/1], Step [104/3337], Loss: 6.9977\n",
      "Epoch [1/1], Step [105/3337], Loss: 7.0219\n",
      "Epoch [1/1], Step [106/3337], Loss: 7.0462\n",
      "Epoch [1/1], Step [107/3337], Loss: 7.0476\n",
      "Epoch [1/1], Step [108/3337], Loss: 7.0129\n",
      "Epoch [1/1], Step [109/3337], Loss: 6.9707\n",
      "Epoch [1/1], Step [110/3337], Loss: 7.0716\n",
      "Epoch [1/1], Step [111/3337], Loss: 7.0548\n",
      "Epoch [1/1], Step [112/3337], Loss: 7.0565\n",
      "Epoch [1/1], Step [113/3337], Loss: 6.9804\n",
      "Epoch [1/1], Step [114/3337], Loss: 6.9802\n",
      "Epoch [1/1], Step [115/3337], Loss: 7.0057\n",
      "Epoch [1/1], Step [116/3337], Loss: 7.0816\n",
      "Epoch [1/1], Step [117/3337], Loss: 7.0689\n",
      "Epoch [1/1], Step [118/3337], Loss: 7.0048\n",
      "Epoch [1/1], Step [119/3337], Loss: 6.9456\n",
      "Epoch [1/1], Step [120/3337], Loss: 6.9329\n",
      "Epoch [1/1], Step [121/3337], Loss: 7.0016\n",
      "Epoch [1/1], Step [122/3337], Loss: 6.9439\n",
      "Epoch [1/1], Step [123/3337], Loss: 6.9367\n",
      "Epoch [1/1], Step [124/3337], Loss: 6.9971\n",
      "Epoch [1/1], Step [125/3337], Loss: 7.0962\n",
      "Epoch [1/1], Step [126/3337], Loss: 7.0500\n",
      "Epoch [1/1], Step [127/3337], Loss: 7.0096\n",
      "Epoch [1/1], Step [128/3337], Loss: 7.0186\n",
      "Epoch [1/1], Step [129/3337], Loss: 7.0443\n",
      "Epoch [1/1], Step [130/3337], Loss: 7.0378\n",
      "Epoch [1/1], Step [131/3337], Loss: 7.0327\n",
      "Epoch [1/1], Step [132/3337], Loss: 6.9178\n",
      "Epoch [1/1], Step [133/3337], Loss: 7.1105\n",
      "Epoch [1/1], Step [134/3337], Loss: 7.0097\n",
      "Epoch [1/1], Step [135/3337], Loss: 6.9952\n",
      "Epoch [1/1], Step [136/3337], Loss: 7.0304\n",
      "Epoch [1/1], Step [137/3337], Loss: 7.0157\n",
      "Epoch [1/1], Step [138/3337], Loss: 7.0246\n",
      "Epoch [1/1], Step [139/3337], Loss: 6.9895\n",
      "Epoch [1/1], Step [140/3337], Loss: 6.9441\n",
      "Epoch [1/1], Step [141/3337], Loss: 6.9455\n",
      "Epoch [1/1], Step [142/3337], Loss: 6.9169\n",
      "Epoch [1/1], Step [143/3337], Loss: 6.9756\n",
      "Epoch [1/1], Step [144/3337], Loss: 7.0698\n",
      "Epoch [1/1], Step [145/3337], Loss: 7.0322\n",
      "Epoch [1/1], Step [146/3337], Loss: 6.9232\n",
      "Epoch [1/1], Step [147/3337], Loss: 6.9367\n",
      "Epoch [1/1], Step [148/3337], Loss: 6.9787\n",
      "Epoch [1/1], Step [149/3337], Loss: 6.9477\n",
      "Epoch [1/1], Step [150/3337], Loss: 6.9942\n",
      "Epoch [1/1], Step [151/3337], Loss: 6.9433\n",
      "Epoch [1/1], Step [152/3337], Loss: 7.0028\n",
      "Epoch [1/1], Step [153/3337], Loss: 6.9734\n",
      "Epoch [1/1], Step [154/3337], Loss: 6.8777\n",
      "Epoch [1/1], Step [155/3337], Loss: 7.0106\n",
      "Epoch [1/1], Step [156/3337], Loss: 6.9659\n",
      "Epoch [1/1], Step [157/3337], Loss: 6.9759\n",
      "Epoch [1/1], Step [158/3337], Loss: 6.9561\n",
      "Epoch [1/1], Step [159/3337], Loss: 6.9188\n",
      "Epoch [1/1], Step [160/3337], Loss: 6.9723\n",
      "Epoch [1/1], Step [161/3337], Loss: 6.9134\n",
      "Epoch [1/1], Step [162/3337], Loss: 6.9521\n",
      "Epoch [1/1], Step [163/3337], Loss: 6.9488\n",
      "Epoch [1/1], Step [164/3337], Loss: 6.8948\n",
      "Epoch [1/1], Step [165/3337], Loss: 6.9246\n",
      "Epoch [1/1], Step [166/3337], Loss: 7.0215\n",
      "Epoch [1/1], Step [167/3337], Loss: 6.9493\n",
      "Epoch [1/1], Step [168/3337], Loss: 7.0451\n",
      "Epoch [1/1], Step [169/3337], Loss: 6.9496\n",
      "Epoch [1/1], Step [170/3337], Loss: 6.9706\n",
      "Epoch [1/1], Step [171/3337], Loss: 7.0374\n",
      "Epoch [1/1], Step [172/3337], Loss: 6.9048\n",
      "Epoch [1/1], Step [173/3337], Loss: 6.9237\n",
      "Epoch [1/1], Step [174/3337], Loss: 6.8975\n",
      "Epoch [1/1], Step [175/3337], Loss: 7.0277\n",
      "Epoch [1/1], Step [176/3337], Loss: 6.9320\n",
      "Epoch [1/1], Step [177/3337], Loss: 6.9292\n",
      "Epoch [1/1], Step [178/3337], Loss: 6.8883\n",
      "Epoch [1/1], Step [179/3337], Loss: 6.9354\n",
      "Epoch [1/1], Step [180/3337], Loss: 6.9382\n",
      "Epoch [1/1], Step [181/3337], Loss: 6.9791\n",
      "Epoch [1/1], Step [182/3337], Loss: 6.9003\n",
      "Epoch [1/1], Step [183/3337], Loss: 6.9047\n",
      "Epoch [1/1], Step [184/3337], Loss: 6.8498\n",
      "Epoch [1/1], Step [185/3337], Loss: 6.9316\n",
      "Epoch [1/1], Step [186/3337], Loss: 6.9887\n",
      "Epoch [1/1], Step [187/3337], Loss: 6.9758\n",
      "Epoch [1/1], Step [188/3337], Loss: 6.9743\n",
      "Epoch [1/1], Step [189/3337], Loss: 6.8469\n",
      "Epoch [1/1], Step [190/3337], Loss: 6.8703\n",
      "Epoch [1/1], Step [191/3337], Loss: 6.9622\n",
      "Epoch [1/1], Step [192/3337], Loss: 6.8315\n",
      "Epoch [1/1], Step [193/3337], Loss: 6.8790\n",
      "Epoch [1/1], Step [194/3337], Loss: 6.9228\n",
      "Epoch [1/1], Step [195/3337], Loss: 6.9277\n",
      "Epoch [1/1], Step [196/3337], Loss: 6.9443\n",
      "Epoch [1/1], Step [197/3337], Loss: 6.8447\n",
      "Epoch [1/1], Step [198/3337], Loss: 6.9861\n",
      "Epoch [1/1], Step [199/3337], Loss: 6.9207\n",
      "Epoch [1/1], Step [200/3337], Loss: 6.9555\n",
      "Epoch [1/1], Step [201/3337], Loss: 6.9346\n",
      "Epoch [1/1], Step [202/3337], Loss: 6.9034\n",
      "Epoch [1/1], Step [203/3337], Loss: 6.8675\n",
      "Epoch [1/1], Step [204/3337], Loss: 6.9223\n",
      "Epoch [1/1], Step [205/3337], Loss: 6.9376\n",
      "Epoch [1/1], Step [206/3337], Loss: 6.8087\n",
      "Epoch [1/1], Step [207/3337], Loss: 6.9137\n",
      "Epoch [1/1], Step [208/3337], Loss: 6.9507\n",
      "Epoch [1/1], Step [209/3337], Loss: 6.8313\n",
      "Epoch [1/1], Step [210/3337], Loss: 6.9462\n",
      "Epoch [1/1], Step [211/3337], Loss: 6.7653\n",
      "Epoch [1/1], Step [212/3337], Loss: 6.9501\n",
      "Epoch [1/1], Step [213/3337], Loss: 6.8929\n",
      "Epoch [1/1], Step [214/3337], Loss: 6.9188\n",
      "Epoch [1/1], Step [215/3337], Loss: 6.8754\n",
      "Epoch [1/1], Step [216/3337], Loss: 6.9186\n",
      "Epoch [1/1], Step [217/3337], Loss: 6.9892\n",
      "Epoch [1/1], Step [218/3337], Loss: 6.8688\n",
      "Epoch [1/1], Step [219/3337], Loss: 6.8142\n",
      "Epoch [1/1], Step [220/3337], Loss: 6.8404\n",
      "Epoch [1/1], Step [221/3337], Loss: 6.8269\n",
      "Epoch [1/1], Step [222/3337], Loss: 6.8314\n",
      "Epoch [1/1], Step [223/3337], Loss: 6.8397\n",
      "Epoch [1/1], Step [224/3337], Loss: 6.8593\n",
      "Epoch [1/1], Step [225/3337], Loss: 6.8666\n",
      "Epoch [1/1], Step [226/3337], Loss: 6.7993\n",
      "Epoch [1/1], Step [227/3337], Loss: 6.9301\n",
      "Epoch [1/1], Step [228/3337], Loss: 6.8675\n",
      "Epoch [1/1], Step [229/3337], Loss: 6.8830\n",
      "Epoch [1/1], Step [230/3337], Loss: 6.8290\n",
      "Epoch [1/1], Step [231/3337], Loss: 6.9044\n",
      "Epoch [1/1], Step [232/3337], Loss: 6.8028\n",
      "Epoch [1/1], Step [233/3337], Loss: 6.8983\n",
      "Epoch [1/1], Step [234/3337], Loss: 6.8525\n",
      "Epoch [1/1], Step [235/3337], Loss: 6.8511\n",
      "Epoch [1/1], Step [236/3337], Loss: 6.9305\n",
      "Epoch [1/1], Step [237/3337], Loss: 6.8376\n",
      "Epoch [1/1], Step [238/3337], Loss: 6.9088\n",
      "Epoch [1/1], Step [239/3337], Loss: 6.8129\n",
      "Epoch [1/1], Step [240/3337], Loss: 6.7517\n",
      "Epoch [1/1], Step [241/3337], Loss: 6.9218\n",
      "Epoch [1/1], Step [242/3337], Loss: 6.8368\n",
      "Epoch [1/1], Step [243/3337], Loss: 6.8975\n",
      "Epoch [1/1], Step [244/3337], Loss: 6.8957\n",
      "Epoch [1/1], Step [245/3337], Loss: 6.8765\n",
      "Epoch [1/1], Step [246/3337], Loss: 6.8821\n",
      "Epoch [1/1], Step [247/3337], Loss: 6.9168\n",
      "Epoch [1/1], Step [248/3337], Loss: 6.8801\n",
      "Epoch [1/1], Step [249/3337], Loss: 6.7839\n",
      "Epoch [1/1], Step [250/3337], Loss: 6.8070\n",
      "Epoch [1/1], Step [251/3337], Loss: 6.7779\n",
      "Epoch [1/1], Step [252/3337], Loss: 6.8085\n",
      "Epoch [1/1], Step [253/3337], Loss: 6.7405\n",
      "Epoch [1/1], Step [254/3337], Loss: 6.8096\n",
      "Epoch [1/1], Step [255/3337], Loss: 6.8827\n",
      "Epoch [1/1], Step [256/3337], Loss: 6.8334\n",
      "Epoch [1/1], Step [257/3337], Loss: 6.8702\n",
      "Epoch [1/1], Step [258/3337], Loss: 6.7899\n",
      "Epoch [1/1], Step [259/3337], Loss: 6.8323\n",
      "Epoch [1/1], Step [260/3337], Loss: 6.8941\n",
      "Epoch [1/1], Step [261/3337], Loss: 6.8224\n",
      "Epoch [1/1], Step [262/3337], Loss: 6.8404\n",
      "Epoch [1/1], Step [263/3337], Loss: 6.7995\n",
      "Epoch [1/1], Step [264/3337], Loss: 6.9014\n",
      "Epoch [1/1], Step [265/3337], Loss: 6.8281\n",
      "Epoch [1/1], Step [266/3337], Loss: 6.8170\n",
      "Epoch [1/1], Step [267/3337], Loss: 6.8604\n",
      "Epoch [1/1], Step [268/3337], Loss: 6.8025\n",
      "Epoch [1/1], Step [269/3337], Loss: 6.7514\n",
      "Epoch [1/1], Step [270/3337], Loss: 6.8146\n",
      "Epoch [1/1], Step [271/3337], Loss: 6.8294\n",
      "Epoch [1/1], Step [272/3337], Loss: 6.8969\n",
      "Epoch [1/1], Step [273/3337], Loss: 6.8591\n",
      "Epoch [1/1], Step [274/3337], Loss: 6.8900\n",
      "Epoch [1/1], Step [275/3337], Loss: 6.8426\n",
      "Epoch [1/1], Step [276/3337], Loss: 6.7766\n",
      "Epoch [1/1], Step [277/3337], Loss: 6.8300\n",
      "Epoch [1/1], Step [278/3337], Loss: 6.8347\n",
      "Epoch [1/1], Step [279/3337], Loss: 6.7891\n",
      "Epoch [1/1], Step [280/3337], Loss: 6.7760\n",
      "Epoch [1/1], Step [281/3337], Loss: 6.8621\n",
      "Epoch [1/1], Step [282/3337], Loss: 6.8065\n",
      "Epoch [1/1], Step [283/3337], Loss: 6.7672\n",
      "Epoch [1/1], Step [284/3337], Loss: 6.8450\n",
      "Epoch [1/1], Step [285/3337], Loss: 6.8010\n",
      "Epoch [1/1], Step [286/3337], Loss: 6.7678\n",
      "Epoch [1/1], Step [287/3337], Loss: 6.6651\n",
      "Epoch [1/1], Step [288/3337], Loss: 6.8083\n",
      "Epoch [1/1], Step [289/3337], Loss: 6.8206\n",
      "Epoch [1/1], Step [290/3337], Loss: 6.7640\n",
      "Epoch [1/1], Step [291/3337], Loss: 6.7402\n",
      "Epoch [1/1], Step [292/3337], Loss: 6.8643\n",
      "Epoch [1/1], Step [293/3337], Loss: 6.8318\n",
      "Epoch [1/1], Step [294/3337], Loss: 6.8421\n",
      "Epoch [1/1], Step [295/3337], Loss: 6.8239\n",
      "Epoch [1/1], Step [296/3337], Loss: 6.7791\n",
      "Epoch [1/1], Step [297/3337], Loss: 6.8088\n",
      "Epoch [1/1], Step [298/3337], Loss: 6.6867\n",
      "Epoch [1/1], Step [299/3337], Loss: 6.6790\n",
      "Epoch [1/1], Step [300/3337], Loss: 6.8372\n",
      "Epoch [1/1], Step [301/3337], Loss: 6.7741\n",
      "Epoch [1/1], Step [302/3337], Loss: 6.6987\n",
      "Epoch [1/1], Step [303/3337], Loss: 6.8707\n",
      "Epoch [1/1], Step [304/3337], Loss: 6.7895\n",
      "Epoch [1/1], Step [305/3337], Loss: 6.7260\n",
      "Epoch [1/1], Step [306/3337], Loss: 6.8038\n",
      "Epoch [1/1], Step [307/3337], Loss: 6.7167\n",
      "Epoch [1/1], Step [308/3337], Loss: 6.8827\n",
      "Epoch [1/1], Step [309/3337], Loss: 6.7970\n",
      "Epoch [1/1], Step [310/3337], Loss: 6.8272\n",
      "Epoch [1/1], Step [311/3337], Loss: 6.8358\n",
      "Epoch [1/1], Step [312/3337], Loss: 6.7739\n",
      "Epoch [1/1], Step [313/3337], Loss: 6.7888\n",
      "Epoch [1/1], Step [314/3337], Loss: 6.7738\n",
      "Epoch [1/1], Step [315/3337], Loss: 6.7438\n",
      "Epoch [1/1], Step [316/3337], Loss: 6.7710\n",
      "Epoch [1/1], Step [317/3337], Loss: 6.8294\n",
      "Epoch [1/1], Step [318/3337], Loss: 6.8195\n",
      "Epoch [1/1], Step [319/3337], Loss: 6.8286\n",
      "Epoch [1/1], Step [320/3337], Loss: 6.6760\n",
      "Epoch [1/1], Step [321/3337], Loss: 6.7634\n",
      "Epoch [1/1], Step [322/3337], Loss: 6.8072\n",
      "Epoch [1/1], Step [323/3337], Loss: 6.7429\n",
      "Epoch [1/1], Step [324/3337], Loss: 6.8201\n",
      "Epoch [1/1], Step [325/3337], Loss: 6.7618\n",
      "Epoch [1/1], Step [326/3337], Loss: 6.7303\n",
      "Epoch [1/1], Step [327/3337], Loss: 6.7576\n",
      "Epoch [1/1], Step [328/3337], Loss: 6.7523\n",
      "Epoch [1/1], Step [329/3337], Loss: 6.7645\n",
      "Epoch [1/1], Step [330/3337], Loss: 6.6857\n",
      "Epoch [1/1], Step [331/3337], Loss: 6.7376\n",
      "Epoch [1/1], Step [332/3337], Loss: 6.8102\n",
      "Epoch [1/1], Step [333/3337], Loss: 6.7532\n",
      "Epoch [1/1], Step [334/3337], Loss: 6.7781\n",
      "Epoch [1/1], Step [335/3337], Loss: 6.7395\n",
      "Epoch [1/1], Step [336/3337], Loss: 6.8003\n",
      "Epoch [1/1], Step [337/3337], Loss: 6.7569\n",
      "Epoch [1/1], Step [338/3337], Loss: 6.6282\n",
      "Epoch [1/1], Step [339/3337], Loss: 6.7000\n",
      "Epoch [1/1], Step [340/3337], Loss: 6.7622\n",
      "Epoch [1/1], Step [341/3337], Loss: 6.7919\n",
      "Epoch [1/1], Step [342/3337], Loss: 6.6842\n",
      "Epoch [1/1], Step [343/3337], Loss: 6.7575\n",
      "Epoch [1/1], Step [344/3337], Loss: 6.7897\n",
      "Epoch [1/1], Step [345/3337], Loss: 6.7131\n",
      "Epoch [1/1], Step [346/3337], Loss: 6.6342\n",
      "Epoch [1/1], Step [347/3337], Loss: 6.8349\n",
      "Epoch [1/1], Step [348/3337], Loss: 6.7498\n",
      "Epoch [1/1], Step [349/3337], Loss: 6.7985\n",
      "Epoch [1/1], Step [350/3337], Loss: 6.6899\n",
      "Epoch [1/1], Step [351/3337], Loss: 6.6785\n",
      "Epoch [1/1], Step [352/3337], Loss: 6.7779\n",
      "Epoch [1/1], Step [353/3337], Loss: 6.7734\n",
      "Epoch [1/1], Step [354/3337], Loss: 6.6807\n",
      "Epoch [1/1], Step [355/3337], Loss: 6.7960\n",
      "Epoch [1/1], Step [356/3337], Loss: 6.7606\n",
      "Epoch [1/1], Step [357/3337], Loss: 6.7270\n",
      "Epoch [1/1], Step [358/3337], Loss: 6.7589\n",
      "Epoch [1/1], Step [359/3337], Loss: 6.6650\n",
      "Epoch [1/1], Step [360/3337], Loss: 6.5980\n",
      "Epoch [1/1], Step [361/3337], Loss: 6.7272\n",
      "Epoch [1/1], Step [362/3337], Loss: 6.6869\n",
      "Epoch [1/1], Step [363/3337], Loss: 6.8014\n",
      "Epoch [1/1], Step [364/3337], Loss: 6.6725\n",
      "Epoch [1/1], Step [365/3337], Loss: 6.7945\n",
      "Epoch [1/1], Step [366/3337], Loss: 6.7595\n",
      "Epoch [1/1], Step [367/3337], Loss: 6.7770\n",
      "Epoch [1/1], Step [368/3337], Loss: 6.6499\n",
      "Epoch [1/1], Step [369/3337], Loss: 6.7383\n",
      "Epoch [1/1], Step [370/3337], Loss: 6.7108\n",
      "Epoch [1/1], Step [371/3337], Loss: 6.6873\n",
      "Epoch [1/1], Step [372/3337], Loss: 6.7093\n",
      "Epoch [1/1], Step [373/3337], Loss: 6.7055\n",
      "Epoch [1/1], Step [374/3337], Loss: 6.7014\n",
      "Epoch [1/1], Step [375/3337], Loss: 6.6145\n",
      "Epoch [1/1], Step [376/3337], Loss: 6.6790\n",
      "Epoch [1/1], Step [377/3337], Loss: 6.6589\n",
      "Epoch [1/1], Step [378/3337], Loss: 6.6648\n",
      "Epoch [1/1], Step [379/3337], Loss: 6.7070\n",
      "Epoch [1/1], Step [380/3337], Loss: 6.6634\n",
      "Epoch [1/1], Step [381/3337], Loss: 6.7814\n",
      "Epoch [1/1], Step [382/3337], Loss: 6.7185\n",
      "Epoch [1/1], Step [383/3337], Loss: 6.7278\n",
      "Epoch [1/1], Step [384/3337], Loss: 6.7833\n",
      "Epoch [1/1], Step [385/3337], Loss: 6.7590\n",
      "Epoch [1/1], Step [386/3337], Loss: 6.6931\n",
      "Epoch [1/1], Step [387/3337], Loss: 6.7510\n",
      "Epoch [1/1], Step [388/3337], Loss: 6.6600\n",
      "Epoch [1/1], Step [389/3337], Loss: 6.7443\n",
      "Epoch [1/1], Step [390/3337], Loss: 6.6456\n",
      "Epoch [1/1], Step [391/3337], Loss: 6.6802\n",
      "Epoch [1/1], Step [392/3337], Loss: 6.6828\n",
      "Epoch [1/1], Step [393/3337], Loss: 6.7117\n",
      "Epoch [1/1], Step [394/3337], Loss: 6.7053\n",
      "Epoch [1/1], Step [395/3337], Loss: 6.7061\n",
      "Epoch [1/1], Step [396/3337], Loss: 6.6556\n",
      "Epoch [1/1], Step [397/3337], Loss: 6.7081\n",
      "Epoch [1/1], Step [398/3337], Loss: 6.7083\n",
      "Epoch [1/1], Step [399/3337], Loss: 6.7431\n",
      "Epoch [1/1], Step [400/3337], Loss: 6.7396\n",
      "Epoch [1/1], Step [401/3337], Loss: 6.6741\n",
      "Epoch [1/1], Step [402/3337], Loss: 6.7554\n",
      "Epoch [1/1], Step [403/3337], Loss: 6.6346\n",
      "Epoch [1/1], Step [404/3337], Loss: 6.7404\n",
      "Epoch [1/1], Step [405/3337], Loss: 6.7182\n",
      "Epoch [1/1], Step [406/3337], Loss: 6.7973\n",
      "Epoch [1/1], Step [407/3337], Loss: 6.7251\n",
      "Epoch [1/1], Step [408/3337], Loss: 6.7103\n",
      "Epoch [1/1], Step [409/3337], Loss: 6.7713\n",
      "Epoch [1/1], Step [410/3337], Loss: 6.7674\n",
      "Epoch [1/1], Step [411/3337], Loss: 6.8219\n",
      "Epoch [1/1], Step [412/3337], Loss: 6.6688\n",
      "Epoch [1/1], Step [413/3337], Loss: 6.7111\n",
      "Epoch [1/1], Step [414/3337], Loss: 6.6991\n",
      "Epoch [1/1], Step [415/3337], Loss: 6.6403\n",
      "Epoch [1/1], Step [416/3337], Loss: 6.6179\n",
      "Epoch [1/1], Step [417/3337], Loss: 6.6271\n",
      "Epoch [1/1], Step [418/3337], Loss: 6.6514\n",
      "Epoch [1/1], Step [419/3337], Loss: 6.7476\n",
      "Epoch [1/1], Step [420/3337], Loss: 6.7087\n",
      "Epoch [1/1], Step [421/3337], Loss: 6.7022\n",
      "Epoch [1/1], Step [422/3337], Loss: 6.6975\n",
      "Epoch [1/1], Step [423/3337], Loss: 6.6996\n",
      "Epoch [1/1], Step [424/3337], Loss: 6.6961\n",
      "Epoch [1/1], Step [425/3337], Loss: 6.6868\n",
      "Epoch [1/1], Step [426/3337], Loss: 6.6932\n",
      "Epoch [1/1], Step [427/3337], Loss: 6.6673\n",
      "Epoch [1/1], Step [428/3337], Loss: 6.6349\n",
      "Epoch [1/1], Step [429/3337], Loss: 6.6659\n",
      "Epoch [1/1], Step [430/3337], Loss: 6.6465\n",
      "Epoch [1/1], Step [431/3337], Loss: 6.6759\n",
      "Epoch [1/1], Step [432/3337], Loss: 6.7189\n",
      "Epoch [1/1], Step [433/3337], Loss: 6.7725\n",
      "Epoch [1/1], Step [434/3337], Loss: 6.6595\n",
      "Epoch [1/1], Step [435/3337], Loss: 6.6611\n",
      "Epoch [1/1], Step [436/3337], Loss: 6.7623\n",
      "Epoch [1/1], Step [437/3337], Loss: 6.6567\n",
      "Epoch [1/1], Step [438/3337], Loss: 6.6604\n",
      "Epoch [1/1], Step [439/3337], Loss: 6.5681\n",
      "Epoch [1/1], Step [440/3337], Loss: 6.6605\n",
      "Epoch [1/1], Step [441/3337], Loss: 6.7205\n",
      "Epoch [1/1], Step [442/3337], Loss: 6.5864\n",
      "Epoch [1/1], Step [443/3337], Loss: 6.6195\n",
      "Epoch [1/1], Step [444/3337], Loss: 6.6200\n",
      "Epoch [1/1], Step [445/3337], Loss: 6.6421\n",
      "Epoch [1/1], Step [446/3337], Loss: 6.6838\n",
      "Epoch [1/1], Step [447/3337], Loss: 6.6887\n",
      "Epoch [1/1], Step [448/3337], Loss: 6.7325\n",
      "Epoch [1/1], Step [449/3337], Loss: 6.5574\n",
      "Epoch [1/1], Step [450/3337], Loss: 6.6852\n",
      "Epoch [1/1], Step [451/3337], Loss: 6.6735\n",
      "Epoch [1/1], Step [452/3337], Loss: 6.6740\n",
      "Epoch [1/1], Step [453/3337], Loss: 6.6608\n",
      "Epoch [1/1], Step [454/3337], Loss: 6.5886\n",
      "Epoch [1/1], Step [455/3337], Loss: 6.6391\n",
      "Epoch [1/1], Step [456/3337], Loss: 6.6886\n",
      "Epoch [1/1], Step [457/3337], Loss: 6.6441\n",
      "Epoch [1/1], Step [458/3337], Loss: 6.6715\n",
      "Epoch [1/1], Step [459/3337], Loss: 6.6625\n",
      "Epoch [1/1], Step [460/3337], Loss: 6.6536\n",
      "Epoch [1/1], Step [461/3337], Loss: 6.6763\n",
      "Epoch [1/1], Step [462/3337], Loss: 6.6862\n",
      "Epoch [1/1], Step [463/3337], Loss: 6.7142\n",
      "Epoch [1/1], Step [464/3337], Loss: 6.6960\n",
      "Epoch [1/1], Step [465/3337], Loss: 6.6216\n",
      "Epoch [1/1], Step [466/3337], Loss: 6.6508\n",
      "Epoch [1/1], Step [467/3337], Loss: 6.5956\n",
      "Epoch [1/1], Step [468/3337], Loss: 6.6820\n",
      "Epoch [1/1], Step [469/3337], Loss: 6.6560\n",
      "Epoch [1/1], Step [470/3337], Loss: 6.6082\n",
      "Epoch [1/1], Step [471/3337], Loss: 6.6468\n",
      "Epoch [1/1], Step [472/3337], Loss: 6.6754\n",
      "Epoch [1/1], Step [473/3337], Loss: 6.6696\n",
      "Epoch [1/1], Step [474/3337], Loss: 6.6338\n",
      "Epoch [1/1], Step [475/3337], Loss: 6.6362\n",
      "Epoch [1/1], Step [476/3337], Loss: 6.6251\n",
      "Epoch [1/1], Step [477/3337], Loss: 6.7245\n",
      "Epoch [1/1], Step [478/3337], Loss: 6.6279\n",
      "Epoch [1/1], Step [479/3337], Loss: 6.7532\n",
      "Epoch [1/1], Step [480/3337], Loss: 6.6518\n",
      "Epoch [1/1], Step [481/3337], Loss: 6.6877\n",
      "Epoch [1/1], Step [482/3337], Loss: 6.7034\n",
      "Epoch [1/1], Step [483/3337], Loss: 6.7094\n",
      "Epoch [1/1], Step [484/3337], Loss: 6.7220\n",
      "Epoch [1/1], Step [485/3337], Loss: 6.5952\n",
      "Epoch [1/1], Step [486/3337], Loss: 6.7569\n",
      "Epoch [1/1], Step [487/3337], Loss: 6.6785\n",
      "Epoch [1/1], Step [488/3337], Loss: 6.6669\n",
      "Epoch [1/1], Step [489/3337], Loss: 6.5916\n",
      "Epoch [1/1], Step [490/3337], Loss: 6.6170\n",
      "Epoch [1/1], Step [491/3337], Loss: 6.6139\n",
      "Epoch [1/1], Step [492/3337], Loss: 6.6846\n",
      "Epoch [1/1], Step [493/3337], Loss: 6.7022\n",
      "Epoch [1/1], Step [494/3337], Loss: 6.6709\n",
      "Epoch [1/1], Step [495/3337], Loss: 6.6480\n",
      "Epoch [1/1], Step [496/3337], Loss: 6.6106\n",
      "Epoch [1/1], Step [497/3337], Loss: 6.6058\n",
      "Epoch [1/1], Step [498/3337], Loss: 6.5344\n",
      "Epoch [1/1], Step [499/3337], Loss: 6.6974\n",
      "Epoch [1/1], Step [500/3337], Loss: 6.6443\n",
      "Epoch [1/1], Step [501/3337], Loss: 6.6442\n",
      "Epoch [1/1], Step [502/3337], Loss: 6.5646\n",
      "Epoch [1/1], Step [503/3337], Loss: 6.5990\n",
      "Epoch [1/1], Step [504/3337], Loss: 6.6849\n",
      "Epoch [1/1], Step [505/3337], Loss: 6.6051\n",
      "Epoch [1/1], Step [506/3337], Loss: 6.6184\n",
      "Epoch [1/1], Step [507/3337], Loss: 6.7282\n",
      "Epoch [1/1], Step [508/3337], Loss: 6.5534\n",
      "Epoch [1/1], Step [509/3337], Loss: 6.5993\n",
      "Epoch [1/1], Step [510/3337], Loss: 6.5300\n",
      "Epoch [1/1], Step [511/3337], Loss: 6.5790\n",
      "Epoch [1/1], Step [512/3337], Loss: 6.6230\n",
      "Epoch [1/1], Step [513/3337], Loss: 6.5988\n",
      "Epoch [1/1], Step [514/3337], Loss: 6.6063\n",
      "Epoch [1/1], Step [515/3337], Loss: 6.6107\n",
      "Epoch [1/1], Step [516/3337], Loss: 6.6148\n",
      "Epoch [1/1], Step [517/3337], Loss: 6.6072\n",
      "Epoch [1/1], Step [518/3337], Loss: 6.6931\n",
      "Epoch [1/1], Step [519/3337], Loss: 6.5845\n",
      "Epoch [1/1], Step [520/3337], Loss: 6.5735\n",
      "Epoch [1/1], Step [521/3337], Loss: 6.6273\n",
      "Epoch [1/1], Step [522/3337], Loss: 6.5280\n",
      "Epoch [1/1], Step [523/3337], Loss: 6.5566\n",
      "Epoch [1/1], Step [524/3337], Loss: 6.5207\n",
      "Epoch [1/1], Step [525/3337], Loss: 6.6779\n",
      "Epoch [1/1], Step [526/3337], Loss: 6.6370\n",
      "Epoch [1/1], Step [527/3337], Loss: 6.4403\n",
      "Epoch [1/1], Step [528/3337], Loss: 6.6194\n",
      "Epoch [1/1], Step [529/3337], Loss: 6.6074\n",
      "Epoch [1/1], Step [530/3337], Loss: 6.6228\n",
      "Epoch [1/1], Step [531/3337], Loss: 6.5949\n",
      "Epoch [1/1], Step [532/3337], Loss: 6.5637\n",
      "Epoch [1/1], Step [533/3337], Loss: 6.6071\n",
      "Epoch [1/1], Step [534/3337], Loss: 6.5204\n",
      "Epoch [1/1], Step [535/3337], Loss: 6.6452\n",
      "Epoch [1/1], Step [536/3337], Loss: 6.6202\n",
      "Epoch [1/1], Step [537/3337], Loss: 6.5625\n",
      "Epoch [1/1], Step [538/3337], Loss: 6.6070\n",
      "Epoch [1/1], Step [539/3337], Loss: 6.6137\n",
      "Epoch [1/1], Step [540/3337], Loss: 6.6773\n",
      "Epoch [1/1], Step [541/3337], Loss: 6.4817\n",
      "Epoch [1/1], Step [542/3337], Loss: 6.5991\n",
      "Epoch [1/1], Step [543/3337], Loss: 6.5947\n",
      "Epoch [1/1], Step [544/3337], Loss: 6.4737\n",
      "Epoch [1/1], Step [545/3337], Loss: 6.5708\n",
      "Epoch [1/1], Step [546/3337], Loss: 6.5643\n",
      "Epoch [1/1], Step [547/3337], Loss: 6.6473\n",
      "Epoch [1/1], Step [548/3337], Loss: 6.5098\n",
      "Epoch [1/1], Step [549/3337], Loss: 6.6373\n",
      "Epoch [1/1], Step [550/3337], Loss: 6.5898\n",
      "Epoch [1/1], Step [551/3337], Loss: 6.5143\n",
      "Epoch [1/1], Step [552/3337], Loss: 6.6446\n",
      "Epoch [1/1], Step [553/3337], Loss: 6.6076\n",
      "Epoch [1/1], Step [554/3337], Loss: 6.5349\n",
      "Epoch [1/1], Step [555/3337], Loss: 6.5858\n",
      "Epoch [1/1], Step [556/3337], Loss: 6.5452\n",
      "Epoch [1/1], Step [557/3337], Loss: 6.5833\n",
      "Epoch [1/1], Step [558/3337], Loss: 6.5615\n",
      "Epoch [1/1], Step [559/3337], Loss: 6.5713\n",
      "Epoch [1/1], Step [560/3337], Loss: 6.7005\n",
      "Epoch [1/1], Step [561/3337], Loss: 6.5229\n",
      "Epoch [1/1], Step [562/3337], Loss: 6.6116\n",
      "Epoch [1/1], Step [563/3337], Loss: 6.5619\n",
      "Epoch [1/1], Step [564/3337], Loss: 6.5959\n",
      "Epoch [1/1], Step [565/3337], Loss: 6.5418\n",
      "Epoch [1/1], Step [566/3337], Loss: 6.4660\n",
      "Epoch [1/1], Step [567/3337], Loss: 6.5281\n",
      "Epoch [1/1], Step [568/3337], Loss: 6.6148\n",
      "Epoch [1/1], Step [569/3337], Loss: 6.5069\n",
      "Epoch [1/1], Step [570/3337], Loss: 6.4939\n",
      "Epoch [1/1], Step [571/3337], Loss: 6.5633\n",
      "Epoch [1/1], Step [572/3337], Loss: 6.5457\n",
      "Epoch [1/1], Step [573/3337], Loss: 6.4888\n",
      "Epoch [1/1], Step [574/3337], Loss: 6.5732\n",
      "Epoch [1/1], Step [575/3337], Loss: 6.5916\n",
      "Epoch [1/1], Step [576/3337], Loss: 6.6029\n",
      "Epoch [1/1], Step [577/3337], Loss: 6.4756\n",
      "Epoch [1/1], Step [578/3337], Loss: 6.6165\n",
      "Epoch [1/1], Step [579/3337], Loss: 6.5821\n",
      "Epoch [1/1], Step [580/3337], Loss: 6.5478\n",
      "Epoch [1/1], Step [581/3337], Loss: 6.6073\n",
      "Epoch [1/1], Step [582/3337], Loss: 6.6021\n",
      "Epoch [1/1], Step [583/3337], Loss: 6.6248\n",
      "Epoch [1/1], Step [584/3337], Loss: 6.4888\n",
      "Epoch [1/1], Step [585/3337], Loss: 6.6063\n",
      "Epoch [1/1], Step [586/3337], Loss: 6.5236\n",
      "Epoch [1/1], Step [587/3337], Loss: 6.5164\n",
      "Epoch [1/1], Step [588/3337], Loss: 6.5211\n",
      "Epoch [1/1], Step [589/3337], Loss: 6.5664\n",
      "Epoch [1/1], Step [590/3337], Loss: 6.4767\n",
      "Epoch [1/1], Step [591/3337], Loss: 6.6005\n",
      "Epoch [1/1], Step [592/3337], Loss: 6.5143\n",
      "Epoch [1/1], Step [593/3337], Loss: 6.6500\n",
      "Epoch [1/1], Step [594/3337], Loss: 6.5549\n",
      "Epoch [1/1], Step [595/3337], Loss: 6.5315\n",
      "Epoch [1/1], Step [596/3337], Loss: 6.4915\n",
      "Epoch [1/1], Step [597/3337], Loss: 6.5374\n",
      "Epoch [1/1], Step [598/3337], Loss: 6.4863\n",
      "Epoch [1/1], Step [599/3337], Loss: 6.5680\n",
      "Epoch [1/1], Step [600/3337], Loss: 6.6070\n",
      "Epoch [1/1], Step [601/3337], Loss: 6.4865\n",
      "Epoch [1/1], Step [602/3337], Loss: 6.6094\n",
      "Epoch [1/1], Step [603/3337], Loss: 6.5429\n",
      "Epoch [1/1], Step [604/3337], Loss: 6.5634\n",
      "Epoch [1/1], Step [605/3337], Loss: 6.4774\n",
      "Epoch [1/1], Step [606/3337], Loss: 6.4838\n",
      "Epoch [1/1], Step [607/3337], Loss: 6.4850\n",
      "Epoch [1/1], Step [608/3337], Loss: 6.6033\n",
      "Epoch [1/1], Step [609/3337], Loss: 6.5162\n",
      "Epoch [1/1], Step [610/3337], Loss: 6.5697\n",
      "Epoch [1/1], Step [611/3337], Loss: 6.5987\n",
      "Epoch [1/1], Step [612/3337], Loss: 6.5871\n",
      "Epoch [1/1], Step [613/3337], Loss: 6.5646\n",
      "Epoch [1/1], Step [614/3337], Loss: 6.5611\n",
      "Epoch [1/1], Step [615/3337], Loss: 6.5381\n",
      "Epoch [1/1], Step [616/3337], Loss: 6.5111\n",
      "Epoch [1/1], Step [617/3337], Loss: 6.4961\n",
      "Epoch [1/1], Step [618/3337], Loss: 6.5683\n",
      "Epoch [1/1], Step [619/3337], Loss: 6.5114\n",
      "Epoch [1/1], Step [620/3337], Loss: 6.5917\n",
      "Epoch [1/1], Step [621/3337], Loss: 6.5037\n",
      "Epoch [1/1], Step [622/3337], Loss: 6.5134\n",
      "Epoch [1/1], Step [623/3337], Loss: 6.6180\n",
      "Epoch [1/1], Step [624/3337], Loss: 6.5696\n",
      "Epoch [1/1], Step [625/3337], Loss: 6.4295\n",
      "Epoch [1/1], Step [626/3337], Loss: 6.4627\n",
      "Epoch [1/1], Step [627/3337], Loss: 6.5635\n",
      "Epoch [1/1], Step [628/3337], Loss: 6.5058\n",
      "Epoch [1/1], Step [629/3337], Loss: 6.5370\n",
      "Epoch [1/1], Step [630/3337], Loss: 6.4462\n",
      "Epoch [1/1], Step [631/3337], Loss: 6.5813\n",
      "Epoch [1/1], Step [632/3337], Loss: 6.5347\n",
      "Epoch [1/1], Step [633/3337], Loss: 6.6055\n",
      "Epoch [1/1], Step [634/3337], Loss: 6.4798\n",
      "Epoch [1/1], Step [635/3337], Loss: 6.5814\n",
      "Epoch [1/1], Step [636/3337], Loss: 6.5257\n",
      "Epoch [1/1], Step [637/3337], Loss: 6.5300\n",
      "Epoch [1/1], Step [638/3337], Loss: 6.4231\n",
      "Epoch [1/1], Step [639/3337], Loss: 6.5859\n",
      "Epoch [1/1], Step [640/3337], Loss: 6.5196\n",
      "Epoch [1/1], Step [641/3337], Loss: 6.4809\n",
      "Epoch [1/1], Step [642/3337], Loss: 6.5277\n",
      "Epoch [1/1], Step [643/3337], Loss: 6.6228\n",
      "Epoch [1/1], Step [644/3337], Loss: 6.5546\n",
      "Epoch [1/1], Step [645/3337], Loss: 6.4851\n",
      "Epoch [1/1], Step [646/3337], Loss: 6.5341\n",
      "Epoch [1/1], Step [647/3337], Loss: 6.4729\n",
      "Epoch [1/1], Step [648/3337], Loss: 6.5493\n",
      "Epoch [1/1], Step [649/3337], Loss: 6.4895\n",
      "Epoch [1/1], Step [650/3337], Loss: 6.5240\n",
      "Epoch [1/1], Step [651/3337], Loss: 6.4327\n",
      "Epoch [1/1], Step [652/3337], Loss: 6.5379\n",
      "Epoch [1/1], Step [653/3337], Loss: 6.5274\n",
      "Epoch [1/1], Step [654/3337], Loss: 6.5385\n",
      "Epoch [1/1], Step [655/3337], Loss: 6.5175\n",
      "Epoch [1/1], Step [656/3337], Loss: 6.3891\n",
      "Epoch [1/1], Step [657/3337], Loss: 6.5268\n",
      "Epoch [1/1], Step [658/3337], Loss: 6.5364\n",
      "Epoch [1/1], Step [659/3337], Loss: 6.5156\n",
      "Epoch [1/1], Step [660/3337], Loss: 6.4527\n",
      "Epoch [1/1], Step [661/3337], Loss: 6.5160\n",
      "Epoch [1/1], Step [662/3337], Loss: 6.5267\n",
      "Epoch [1/1], Step [663/3337], Loss: 6.5109\n",
      "Epoch [1/1], Step [664/3337], Loss: 6.5053\n",
      "Epoch [1/1], Step [665/3337], Loss: 6.5981\n",
      "Epoch [1/1], Step [666/3337], Loss: 6.5333\n",
      "Epoch [1/1], Step [667/3337], Loss: 6.5251\n",
      "Epoch [1/1], Step [668/3337], Loss: 6.4410\n",
      "Epoch [1/1], Step [669/3337], Loss: 6.5246\n",
      "Epoch [1/1], Step [670/3337], Loss: 6.5247\n",
      "Epoch [1/1], Step [671/3337], Loss: 6.5716\n",
      "Epoch [1/1], Step [672/3337], Loss: 6.5305\n",
      "Epoch [1/1], Step [673/3337], Loss: 6.5009\n",
      "Epoch [1/1], Step [674/3337], Loss: 6.5150\n",
      "Epoch [1/1], Step [675/3337], Loss: 6.4667\n",
      "Epoch [1/1], Step [676/3337], Loss: 6.4581\n",
      "Epoch [1/1], Step [677/3337], Loss: 6.5143\n",
      "Epoch [1/1], Step [678/3337], Loss: 6.4573\n",
      "Epoch [1/1], Step [679/3337], Loss: 6.6020\n",
      "Epoch [1/1], Step [680/3337], Loss: 6.4668\n",
      "Epoch [1/1], Step [681/3337], Loss: 6.5539\n",
      "Epoch [1/1], Step [682/3337], Loss: 6.4960\n",
      "Epoch [1/1], Step [683/3337], Loss: 6.4939\n",
      "Epoch [1/1], Step [684/3337], Loss: 6.4978\n",
      "Epoch [1/1], Step [685/3337], Loss: 6.4198\n",
      "Epoch [1/1], Step [686/3337], Loss: 6.5979\n",
      "Epoch [1/1], Step [687/3337], Loss: 6.5747\n",
      "Epoch [1/1], Step [688/3337], Loss: 6.5466\n",
      "Epoch [1/1], Step [689/3337], Loss: 6.4814\n",
      "Epoch [1/1], Step [690/3337], Loss: 6.4482\n",
      "Epoch [1/1], Step [691/3337], Loss: 6.5131\n",
      "Epoch [1/1], Step [692/3337], Loss: 6.5959\n",
      "Epoch [1/1], Step [693/3337], Loss: 6.4656\n",
      "Epoch [1/1], Step [694/3337], Loss: 6.4878\n",
      "Epoch [1/1], Step [695/3337], Loss: 6.4831\n",
      "Epoch [1/1], Step [696/3337], Loss: 6.5467\n",
      "Epoch [1/1], Step [697/3337], Loss: 6.5120\n",
      "Epoch [1/1], Step [698/3337], Loss: 6.4649\n",
      "Epoch [1/1], Step [699/3337], Loss: 6.4452\n",
      "Epoch [1/1], Step [700/3337], Loss: 6.4690\n",
      "Epoch [1/1], Step [701/3337], Loss: 6.4852\n",
      "Epoch [1/1], Step [702/3337], Loss: 6.5597\n",
      "Epoch [1/1], Step [703/3337], Loss: 6.4259\n",
      "Epoch [1/1], Step [704/3337], Loss: 6.5332\n",
      "Epoch [1/1], Step [705/3337], Loss: 6.4160\n",
      "Epoch [1/1], Step [706/3337], Loss: 6.3959\n",
      "Epoch [1/1], Step [707/3337], Loss: 6.5339\n",
      "Epoch [1/1], Step [708/3337], Loss: 6.4697\n",
      "Epoch [1/1], Step [709/3337], Loss: 6.4243\n",
      "Epoch [1/1], Step [710/3337], Loss: 6.5391\n",
      "Epoch [1/1], Step [711/3337], Loss: 6.5488\n",
      "Epoch [1/1], Step [712/3337], Loss: 6.5441\n",
      "Epoch [1/1], Step [713/3337], Loss: 6.4695\n",
      "Epoch [1/1], Step [714/3337], Loss: 6.5454\n",
      "Epoch [1/1], Step [715/3337], Loss: 6.4843\n",
      "Epoch [1/1], Step [716/3337], Loss: 6.5264\n",
      "Epoch [1/1], Step [717/3337], Loss: 6.4737\n",
      "Epoch [1/1], Step [718/3337], Loss: 6.5738\n",
      "Epoch [1/1], Step [719/3337], Loss: 6.4506\n",
      "Epoch [1/1], Step [720/3337], Loss: 6.4672\n",
      "Epoch [1/1], Step [721/3337], Loss: 6.5087\n",
      "Epoch [1/1], Step [722/3337], Loss: 6.4701\n",
      "Epoch [1/1], Step [723/3337], Loss: 6.3856\n",
      "Epoch [1/1], Step [724/3337], Loss: 6.4747\n",
      "Epoch [1/1], Step [725/3337], Loss: 6.4248\n",
      "Epoch [1/1], Step [726/3337], Loss: 6.4023\n",
      "Epoch [1/1], Step [727/3337], Loss: 6.4783\n",
      "Epoch [1/1], Step [728/3337], Loss: 6.4224\n",
      "Epoch [1/1], Step [729/3337], Loss: 6.4518\n",
      "Epoch [1/1], Step [730/3337], Loss: 6.4819\n",
      "Epoch [1/1], Step [731/3337], Loss: 6.4657\n",
      "Epoch [1/1], Step [732/3337], Loss: 6.3795\n",
      "Epoch [1/1], Step [733/3337], Loss: 6.5106\n",
      "Epoch [1/1], Step [734/3337], Loss: 6.4964\n",
      "Epoch [1/1], Step [735/3337], Loss: 6.5727\n",
      "Epoch [1/1], Step [736/3337], Loss: 6.4980\n",
      "Epoch [1/1], Step [737/3337], Loss: 6.4333\n",
      "Epoch [1/1], Step [738/3337], Loss: 6.4838\n",
      "Epoch [1/1], Step [739/3337], Loss: 6.6009\n",
      "Epoch [1/1], Step [740/3337], Loss: 6.4678\n",
      "Epoch [1/1], Step [741/3337], Loss: 6.3817\n",
      "Epoch [1/1], Step [742/3337], Loss: 6.5465\n",
      "Epoch [1/1], Step [743/3337], Loss: 6.3613\n",
      "Epoch [1/1], Step [744/3337], Loss: 6.5157\n",
      "Epoch [1/1], Step [745/3337], Loss: 6.3818\n",
      "Epoch [1/1], Step [746/3337], Loss: 6.4637\n",
      "Epoch [1/1], Step [747/3337], Loss: 6.4524\n",
      "Epoch [1/1], Step [748/3337], Loss: 6.4596\n",
      "Epoch [1/1], Step [749/3337], Loss: 6.4223\n",
      "Epoch [1/1], Step [750/3337], Loss: 6.4722\n",
      "Epoch [1/1], Step [751/3337], Loss: 6.4555\n",
      "Epoch [1/1], Step [752/3337], Loss: 6.4457\n",
      "Epoch [1/1], Step [753/3337], Loss: 6.3990\n",
      "Epoch [1/1], Step [754/3337], Loss: 6.6101\n",
      "Epoch [1/1], Step [755/3337], Loss: 6.4301\n",
      "Epoch [1/1], Step [756/3337], Loss: 6.5959\n",
      "Epoch [1/1], Step [757/3337], Loss: 6.4395\n",
      "Epoch [1/1], Step [758/3337], Loss: 6.4794\n",
      "Epoch [1/1], Step [759/3337], Loss: 6.4902\n",
      "Epoch [1/1], Step [760/3337], Loss: 6.4191\n",
      "Epoch [1/1], Step [761/3337], Loss: 6.3814\n",
      "Epoch [1/1], Step [762/3337], Loss: 6.4483\n",
      "Epoch [1/1], Step [763/3337], Loss: 6.5939\n",
      "Epoch [1/1], Step [764/3337], Loss: 6.4210\n",
      "Epoch [1/1], Step [765/3337], Loss: 6.5099\n",
      "Epoch [1/1], Step [766/3337], Loss: 6.3607\n",
      "Epoch [1/1], Step [767/3337], Loss: 6.5259\n",
      "Epoch [1/1], Step [768/3337], Loss: 6.3264\n",
      "Epoch [1/1], Step [769/3337], Loss: 6.4405\n",
      "Epoch [1/1], Step [770/3337], Loss: 6.4869\n",
      "Epoch [1/1], Step [771/3337], Loss: 6.4199\n",
      "Epoch [1/1], Step [772/3337], Loss: 6.4414\n",
      "Epoch [1/1], Step [773/3337], Loss: 6.4839\n",
      "Epoch [1/1], Step [774/3337], Loss: 6.4363\n",
      "Epoch [1/1], Step [775/3337], Loss: 6.4055\n",
      "Epoch [1/1], Step [776/3337], Loss: 6.3813\n",
      "Epoch [1/1], Step [777/3337], Loss: 6.3723\n",
      "Epoch [1/1], Step [778/3337], Loss: 6.4581\n",
      "Epoch [1/1], Step [779/3337], Loss: 6.2918\n",
      "Epoch [1/1], Step [780/3337], Loss: 6.4674\n",
      "Epoch [1/1], Step [781/3337], Loss: 6.3612\n",
      "Epoch [1/1], Step [782/3337], Loss: 6.4862\n",
      "Epoch [1/1], Step [783/3337], Loss: 6.5044\n",
      "Epoch [1/1], Step [784/3337], Loss: 6.4169\n",
      "Epoch [1/1], Step [785/3337], Loss: 6.5281\n",
      "Epoch [1/1], Step [786/3337], Loss: 6.5128\n",
      "Epoch [1/1], Step [787/3337], Loss: 6.5484\n",
      "Epoch [1/1], Step [788/3337], Loss: 6.3940\n",
      "Epoch [1/1], Step [789/3337], Loss: 6.3789\n",
      "Epoch [1/1], Step [790/3337], Loss: 6.4741\n",
      "Epoch [1/1], Step [791/3337], Loss: 6.4169\n",
      "Epoch [1/1], Step [792/3337], Loss: 6.4555\n",
      "Epoch [1/1], Step [793/3337], Loss: 6.4561\n",
      "Epoch [1/1], Step [794/3337], Loss: 6.4166\n",
      "Epoch [1/1], Step [795/3337], Loss: 6.3688\n",
      "Epoch [1/1], Step [796/3337], Loss: 6.4238\n",
      "Epoch [1/1], Step [797/3337], Loss: 6.4201\n",
      "Epoch [1/1], Step [798/3337], Loss: 6.4095\n",
      "Epoch [1/1], Step [799/3337], Loss: 6.4615\n",
      "Epoch [1/1], Step [800/3337], Loss: 6.2839\n",
      "Epoch [1/1], Step [801/3337], Loss: 6.3746\n",
      "Epoch [1/1], Step [802/3337], Loss: 6.5784\n",
      "Epoch [1/1], Step [803/3337], Loss: 6.2933\n",
      "Epoch [1/1], Step [804/3337], Loss: 6.4956\n",
      "Epoch [1/1], Step [805/3337], Loss: 6.4316\n",
      "Epoch [1/1], Step [806/3337], Loss: 6.4285\n",
      "Epoch [1/1], Step [807/3337], Loss: 6.5240\n",
      "Epoch [1/1], Step [808/3337], Loss: 6.4740\n",
      "Epoch [1/1], Step [809/3337], Loss: 6.4473\n",
      "Epoch [1/1], Step [810/3337], Loss: 6.3935\n",
      "Epoch [1/1], Step [811/3337], Loss: 6.4526\n",
      "Epoch [1/1], Step [812/3337], Loss: 6.4527\n",
      "Epoch [1/1], Step [813/3337], Loss: 6.3206\n",
      "Epoch [1/1], Step [814/3337], Loss: 6.3956\n",
      "Epoch [1/1], Step [815/3337], Loss: 6.4284\n",
      "Epoch [1/1], Step [816/3337], Loss: 6.4387\n",
      "Epoch [1/1], Step [817/3337], Loss: 6.4031\n",
      "Epoch [1/1], Step [818/3337], Loss: 6.4893\n",
      "Epoch [1/1], Step [819/3337], Loss: 6.5507\n",
      "Epoch [1/1], Step [820/3337], Loss: 6.4722\n",
      "Epoch [1/1], Step [821/3337], Loss: 6.3869\n",
      "Epoch [1/1], Step [822/3337], Loss: 6.4547\n",
      "Epoch [1/1], Step [823/3337], Loss: 6.5237\n",
      "Epoch [1/1], Step [824/3337], Loss: 6.4214\n",
      "Epoch [1/1], Step [825/3337], Loss: 6.4248\n",
      "Epoch [1/1], Step [826/3337], Loss: 6.5087\n",
      "Epoch [1/1], Step [827/3337], Loss: 6.3492\n",
      "Epoch [1/1], Step [828/3337], Loss: 6.3940\n",
      "Epoch [1/1], Step [829/3337], Loss: 6.5693\n",
      "Epoch [1/1], Step [830/3337], Loss: 6.3779\n",
      "Epoch [1/1], Step [831/3337], Loss: 6.4663\n",
      "Epoch [1/1], Step [832/3337], Loss: 6.3618\n",
      "Epoch [1/1], Step [833/3337], Loss: 6.4418\n",
      "Epoch [1/1], Step [834/3337], Loss: 6.4761\n",
      "Epoch [1/1], Step [835/3337], Loss: 6.4334\n",
      "Epoch [1/1], Step [836/3337], Loss: 6.3961\n",
      "Epoch [1/1], Step [837/3337], Loss: 6.4387\n",
      "Epoch [1/1], Step [838/3337], Loss: 6.3769\n",
      "Epoch [1/1], Step [839/3337], Loss: 6.4374\n",
      "Epoch [1/1], Step [840/3337], Loss: 6.3933\n",
      "Epoch [1/1], Step [841/3337], Loss: 6.3674\n",
      "Epoch [1/1], Step [842/3337], Loss: 6.3719\n",
      "Epoch [1/1], Step [843/3337], Loss: 6.4032\n",
      "Epoch [1/1], Step [844/3337], Loss: 6.5301\n",
      "Epoch [1/1], Step [845/3337], Loss: 6.3910\n",
      "Epoch [1/1], Step [846/3337], Loss: 6.4462\n",
      "Epoch [1/1], Step [847/3337], Loss: 6.4385\n",
      "Epoch [1/1], Step [848/3337], Loss: 6.4419\n",
      "Epoch [1/1], Step [849/3337], Loss: 6.4353\n",
      "Epoch [1/1], Step [850/3337], Loss: 6.3919\n",
      "Epoch [1/1], Step [851/3337], Loss: 6.5265\n",
      "Epoch [1/1], Step [852/3337], Loss: 6.3437\n",
      "Epoch [1/1], Step [853/3337], Loss: 6.4601\n",
      "Epoch [1/1], Step [854/3337], Loss: 6.2077\n",
      "Epoch [1/1], Step [855/3337], Loss: 6.3887\n",
      "Epoch [1/1], Step [856/3337], Loss: 6.3924\n",
      "Epoch [1/1], Step [857/3337], Loss: 6.3148\n",
      "Epoch [1/1], Step [858/3337], Loss: 6.3633\n",
      "Epoch [1/1], Step [859/3337], Loss: 6.3980\n",
      "Epoch [1/1], Step [860/3337], Loss: 6.3867\n",
      "Epoch [1/1], Step [861/3337], Loss: 6.4776\n",
      "Epoch [1/1], Step [862/3337], Loss: 6.4996\n",
      "Epoch [1/1], Step [863/3337], Loss: 6.3983\n",
      "Epoch [1/1], Step [864/3337], Loss: 6.4668\n",
      "Epoch [1/1], Step [865/3337], Loss: 6.3354\n",
      "Epoch [1/1], Step [866/3337], Loss: 6.4261\n",
      "Epoch [1/1], Step [867/3337], Loss: 6.4274\n",
      "Epoch [1/1], Step [868/3337], Loss: 6.5078\n",
      "Epoch [1/1], Step [869/3337], Loss: 6.3276\n",
      "Epoch [1/1], Step [870/3337], Loss: 6.3573\n",
      "Epoch [1/1], Step [871/3337], Loss: 6.3979\n",
      "Epoch [1/1], Step [872/3337], Loss: 6.3724\n",
      "Epoch [1/1], Step [873/3337], Loss: 6.4285\n",
      "Epoch [1/1], Step [874/3337], Loss: 6.3403\n",
      "Epoch [1/1], Step [875/3337], Loss: 6.4086\n",
      "Epoch [1/1], Step [876/3337], Loss: 6.4862\n",
      "Epoch [1/1], Step [877/3337], Loss: 6.4351\n",
      "Epoch [1/1], Step [878/3337], Loss: 6.4797\n",
      "Epoch [1/1], Step [879/3337], Loss: 6.3061\n",
      "Epoch [1/1], Step [880/3337], Loss: 6.4245\n",
      "Epoch [1/1], Step [881/3337], Loss: 6.4313\n",
      "Epoch [1/1], Step [882/3337], Loss: 6.4093\n",
      "Epoch [1/1], Step [883/3337], Loss: 6.3675\n",
      "Epoch [1/1], Step [884/3337], Loss: 6.4310\n",
      "Epoch [1/1], Step [885/3337], Loss: 6.4187\n",
      "Epoch [1/1], Step [886/3337], Loss: 6.4419\n",
      "Epoch [1/1], Step [887/3337], Loss: 6.3269\n",
      "Epoch [1/1], Step [888/3337], Loss: 6.4415\n",
      "Epoch [1/1], Step [889/3337], Loss: 6.3675\n",
      "Epoch [1/1], Step [890/3337], Loss: 6.4210\n",
      "Epoch [1/1], Step [891/3337], Loss: 6.2007\n",
      "Epoch [1/1], Step [892/3337], Loss: 6.4291\n",
      "Epoch [1/1], Step [893/3337], Loss: 6.2770\n",
      "Epoch [1/1], Step [894/3337], Loss: 6.3959\n",
      "Epoch [1/1], Step [895/3337], Loss: 6.3211\n",
      "Epoch [1/1], Step [896/3337], Loss: 6.4623\n",
      "Epoch [1/1], Step [897/3337], Loss: 6.3318\n",
      "Epoch [1/1], Step [898/3337], Loss: 6.2896\n",
      "Epoch [1/1], Step [899/3337], Loss: 6.4046\n",
      "Epoch [1/1], Step [900/3337], Loss: 6.3530\n",
      "Epoch [1/1], Step [901/3337], Loss: 6.3499\n",
      "Epoch [1/1], Step [902/3337], Loss: 6.4432\n",
      "Epoch [1/1], Step [903/3337], Loss: 6.3257\n",
      "Epoch [1/1], Step [904/3337], Loss: 6.4614\n",
      "Epoch [1/1], Step [905/3337], Loss: 6.3059\n",
      "Epoch [1/1], Step [906/3337], Loss: 6.3436\n",
      "Epoch [1/1], Step [907/3337], Loss: 6.4608\n",
      "Epoch [1/1], Step [908/3337], Loss: 6.2688\n",
      "Epoch [1/1], Step [909/3337], Loss: 6.4440\n",
      "Epoch [1/1], Step [910/3337], Loss: 6.4732\n",
      "Epoch [1/1], Step [911/3337], Loss: 6.3316\n",
      "Epoch [1/1], Step [912/3337], Loss: 6.2670\n",
      "Epoch [1/1], Step [913/3337], Loss: 6.3792\n",
      "Epoch [1/1], Step [914/3337], Loss: 6.5085\n",
      "Epoch [1/1], Step [915/3337], Loss: 6.3212\n",
      "Epoch [1/1], Step [916/3337], Loss: 6.4343\n",
      "Epoch [1/1], Step [917/3337], Loss: 6.3194\n",
      "Epoch [1/1], Step [918/3337], Loss: 6.3052\n",
      "Epoch [1/1], Step [919/3337], Loss: 6.3835\n",
      "Epoch [1/1], Step [920/3337], Loss: 6.3248\n",
      "Epoch [1/1], Step [921/3337], Loss: 6.3900\n",
      "Epoch [1/1], Step [922/3337], Loss: 6.2614\n",
      "Epoch [1/1], Step [923/3337], Loss: 6.3431\n",
      "Epoch [1/1], Step [924/3337], Loss: 6.4074\n",
      "Epoch [1/1], Step [925/3337], Loss: 6.3641\n",
      "Epoch [1/1], Step [926/3337], Loss: 6.2466\n",
      "Epoch [1/1], Step [927/3337], Loss: 6.2790\n",
      "Epoch [1/1], Step [928/3337], Loss: 6.3005\n",
      "Epoch [1/1], Step [929/3337], Loss: 6.3177\n",
      "Epoch [1/1], Step [930/3337], Loss: 6.3958\n",
      "Epoch [1/1], Step [931/3337], Loss: 6.3970\n",
      "Epoch [1/1], Step [932/3337], Loss: 6.3195\n",
      "Epoch [1/1], Step [933/3337], Loss: 6.3432\n",
      "Epoch [1/1], Step [934/3337], Loss: 6.3848\n",
      "Epoch [1/1], Step [935/3337], Loss: 6.3630\n",
      "Epoch [1/1], Step [936/3337], Loss: 6.3638\n",
      "Epoch [1/1], Step [937/3337], Loss: 6.4521\n",
      "Epoch [1/1], Step [938/3337], Loss: 6.3912\n",
      "Epoch [1/1], Step [939/3337], Loss: 6.3088\n",
      "Epoch [1/1], Step [940/3337], Loss: 6.3117\n",
      "Epoch [1/1], Step [941/3337], Loss: 6.3583\n",
      "Epoch [1/1], Step [942/3337], Loss: 6.2655\n",
      "Epoch [1/1], Step [943/3337], Loss: 6.3957\n",
      "Epoch [1/1], Step [944/3337], Loss: 6.4395\n",
      "Epoch [1/1], Step [945/3337], Loss: 6.3221\n",
      "Epoch [1/1], Step [946/3337], Loss: 6.3169\n",
      "Epoch [1/1], Step [947/3337], Loss: 6.4146\n",
      "Epoch [1/1], Step [948/3337], Loss: 6.4121\n",
      "Epoch [1/1], Step [949/3337], Loss: 6.3796\n",
      "Epoch [1/1], Step [950/3337], Loss: 6.3470\n",
      "Epoch [1/1], Step [951/3337], Loss: 6.2063\n",
      "Epoch [1/1], Step [952/3337], Loss: 6.3468\n",
      "Epoch [1/1], Step [953/3337], Loss: 6.3317\n",
      "Epoch [1/1], Step [954/3337], Loss: 6.3401\n",
      "Epoch [1/1], Step [955/3337], Loss: 6.3455\n",
      "Epoch [1/1], Step [956/3337], Loss: 6.2759\n",
      "Epoch [1/1], Step [957/3337], Loss: 6.4031\n",
      "Epoch [1/1], Step [958/3337], Loss: 6.3001\n",
      "Epoch [1/1], Step [959/3337], Loss: 6.3065\n",
      "Epoch [1/1], Step [960/3337], Loss: 6.3486\n",
      "Epoch [1/1], Step [961/3337], Loss: 6.3987\n",
      "Epoch [1/1], Step [962/3337], Loss: 6.3809\n",
      "Epoch [1/1], Step [963/3337], Loss: 6.3573\n",
      "Epoch [1/1], Step [964/3337], Loss: 6.3504\n",
      "Epoch [1/1], Step [965/3337], Loss: 6.3118\n",
      "Epoch [1/1], Step [966/3337], Loss: 6.2421\n",
      "Epoch [1/1], Step [967/3337], Loss: 6.4159\n",
      "Epoch [1/1], Step [968/3337], Loss: 6.3337\n",
      "Epoch [1/1], Step [969/3337], Loss: 6.4344\n",
      "Epoch [1/1], Step [970/3337], Loss: 6.4190\n",
      "Epoch [1/1], Step [971/3337], Loss: 6.2552\n",
      "Epoch [1/1], Step [972/3337], Loss: 6.3974\n",
      "Epoch [1/1], Step [973/3337], Loss: 6.3605\n",
      "Epoch [1/1], Step [974/3337], Loss: 6.4258\n",
      "Epoch [1/1], Step [975/3337], Loss: 6.3866\n",
      "Epoch [1/1], Step [976/3337], Loss: 6.2907\n",
      "Epoch [1/1], Step [977/3337], Loss: 6.4159\n",
      "Epoch [1/1], Step [978/3337], Loss: 6.3153\n",
      "Epoch [1/1], Step [979/3337], Loss: 6.2336\n",
      "Epoch [1/1], Step [980/3337], Loss: 6.3456\n",
      "Epoch [1/1], Step [981/3337], Loss: 6.4407\n",
      "Epoch [1/1], Step [982/3337], Loss: 6.2764\n",
      "Epoch [1/1], Step [983/3337], Loss: 6.3234\n",
      "Epoch [1/1], Step [984/3337], Loss: 6.3194\n",
      "Epoch [1/1], Step [985/3337], Loss: 6.4174\n",
      "Epoch [1/1], Step [986/3337], Loss: 6.3235\n",
      "Epoch [1/1], Step [987/3337], Loss: 6.3541\n",
      "Epoch [1/1], Step [988/3337], Loss: 6.3106\n",
      "Epoch [1/1], Step [989/3337], Loss: 6.4436\n",
      "Epoch [1/1], Step [990/3337], Loss: 6.4966\n",
      "Epoch [1/1], Step [991/3337], Loss: 6.2366\n",
      "Epoch [1/1], Step [992/3337], Loss: 6.4095\n",
      "Epoch [1/1], Step [993/3337], Loss: 6.2623\n",
      "Epoch [1/1], Step [994/3337], Loss: 6.4105\n",
      "Epoch [1/1], Step [995/3337], Loss: 6.3005\n",
      "Epoch [1/1], Step [996/3337], Loss: 6.2518\n",
      "Epoch [1/1], Step [997/3337], Loss: 6.3173\n",
      "Epoch [1/1], Step [998/3337], Loss: 6.3905\n",
      "Epoch [1/1], Step [999/3337], Loss: 6.4630\n",
      "Epoch [1/1], Step [1000/3337], Loss: 6.3232\n",
      "Epoch [1/1], Step [1001/3337], Loss: 6.3287\n",
      "Epoch [1/1], Step [1002/3337], Loss: 6.2161\n",
      "Epoch [1/1], Step [1003/3337], Loss: 6.4895\n",
      "Epoch [1/1], Step [1004/3337], Loss: 6.2521\n",
      "Epoch [1/1], Step [1005/3337], Loss: 6.2907\n",
      "Epoch [1/1], Step [1006/3337], Loss: 6.3531\n",
      "Epoch [1/1], Step [1007/3337], Loss: 6.4074\n",
      "Epoch [1/1], Step [1008/3337], Loss: 6.3278\n",
      "Epoch [1/1], Step [1009/3337], Loss: 6.4009\n",
      "Epoch [1/1], Step [1010/3337], Loss: 6.3215\n",
      "Epoch [1/1], Step [1011/3337], Loss: 6.2483\n",
      "Epoch [1/1], Step [1012/3337], Loss: 6.3192\n",
      "Epoch [1/1], Step [1013/3337], Loss: 6.3241\n",
      "Epoch [1/1], Step [1014/3337], Loss: 6.3234\n",
      "Epoch [1/1], Step [1015/3337], Loss: 6.3439\n",
      "Epoch [1/1], Step [1016/3337], Loss: 6.2284\n",
      "Epoch [1/1], Step [1017/3337], Loss: 6.2028\n",
      "Epoch [1/1], Step [1018/3337], Loss: 6.2848\n",
      "Epoch [1/1], Step [1019/3337], Loss: 6.2951\n",
      "Epoch [1/1], Step [1020/3337], Loss: 6.2815\n",
      "Epoch [1/1], Step [1021/3337], Loss: 6.3753\n",
      "Epoch [1/1], Step [1022/3337], Loss: 6.3020\n",
      "Epoch [1/1], Step [1023/3337], Loss: 6.4517\n",
      "Epoch [1/1], Step [1024/3337], Loss: 6.3116\n",
      "Epoch [1/1], Step [1025/3337], Loss: 6.2510\n",
      "Epoch [1/1], Step [1026/3337], Loss: 6.2732\n",
      "Epoch [1/1], Step [1027/3337], Loss: 6.3373\n",
      "Epoch [1/1], Step [1028/3337], Loss: 6.3201\n",
      "Epoch [1/1], Step [1029/3337], Loss: 6.2900\n",
      "Epoch [1/1], Step [1030/3337], Loss: 6.2736\n",
      "Epoch [1/1], Step [1031/3337], Loss: 6.2940\n",
      "Epoch [1/1], Step [1032/3337], Loss: 6.2270\n",
      "Epoch [1/1], Step [1033/3337], Loss: 6.2572\n",
      "Epoch [1/1], Step [1034/3337], Loss: 6.2655\n",
      "Epoch [1/1], Step [1035/3337], Loss: 6.3638\n",
      "Epoch [1/1], Step [1036/3337], Loss: 6.3152\n",
      "Epoch [1/1], Step [1037/3337], Loss: 6.2995\n",
      "Epoch [1/1], Step [1038/3337], Loss: 6.1667\n",
      "Epoch [1/1], Step [1039/3337], Loss: 6.2955\n",
      "Epoch [1/1], Step [1040/3337], Loss: 6.2514\n",
      "Epoch [1/1], Step [1041/3337], Loss: 6.3565\n",
      "Epoch [1/1], Step [1042/3337], Loss: 6.1709\n",
      "Epoch [1/1], Step [1043/3337], Loss: 6.2208\n",
      "Epoch [1/1], Step [1044/3337], Loss: 6.4032\n",
      "Epoch [1/1], Step [1045/3337], Loss: 6.3452\n",
      "Epoch [1/1], Step [1046/3337], Loss: 6.2586\n",
      "Epoch [1/1], Step [1047/3337], Loss: 6.2739\n",
      "Epoch [1/1], Step [1048/3337], Loss: 6.2567\n",
      "Epoch [1/1], Step [1049/3337], Loss: 6.2000\n",
      "Epoch [1/1], Step [1050/3337], Loss: 6.2683\n",
      "Epoch [1/1], Step [1051/3337], Loss: 6.3389\n",
      "Epoch [1/1], Step [1052/3337], Loss: 6.3757\n",
      "Epoch [1/1], Step [1053/3337], Loss: 6.3348\n",
      "Epoch [1/1], Step [1054/3337], Loss: 6.2563\n",
      "Epoch [1/1], Step [1055/3337], Loss: 6.2469\n",
      "Epoch [1/1], Step [1056/3337], Loss: 6.2853\n",
      "Epoch [1/1], Step [1057/3337], Loss: 6.4007\n",
      "Epoch [1/1], Step [1058/3337], Loss: 6.3083\n",
      "Epoch [1/1], Step [1059/3337], Loss: 6.4284\n",
      "Epoch [1/1], Step [1060/3337], Loss: 6.3203\n",
      "Epoch [1/1], Step [1061/3337], Loss: 6.2083\n",
      "Epoch [1/1], Step [1062/3337], Loss: 6.3200\n",
      "Epoch [1/1], Step [1063/3337], Loss: 6.2895\n",
      "Epoch [1/1], Step [1064/3337], Loss: 6.3022\n",
      "Epoch [1/1], Step [1065/3337], Loss: 6.2762\n",
      "Epoch [1/1], Step [1066/3337], Loss: 6.3142\n",
      "Epoch [1/1], Step [1067/3337], Loss: 6.2268\n",
      "Epoch [1/1], Step [1068/3337], Loss: 6.4574\n",
      "Epoch [1/1], Step [1069/3337], Loss: 6.3868\n",
      "Epoch [1/1], Step [1070/3337], Loss: 6.3056\n",
      "Epoch [1/1], Step [1071/3337], Loss: 6.2762\n",
      "Epoch [1/1], Step [1072/3337], Loss: 6.2522\n",
      "Epoch [1/1], Step [1073/3337], Loss: 6.2157\n",
      "Epoch [1/1], Step [1074/3337], Loss: 6.2356\n",
      "Epoch [1/1], Step [1075/3337], Loss: 6.3216\n",
      "Epoch [1/1], Step [1076/3337], Loss: 6.4168\n",
      "Epoch [1/1], Step [1077/3337], Loss: 6.2591\n",
      "Epoch [1/1], Step [1078/3337], Loss: 6.3591\n",
      "Epoch [1/1], Step [1079/3337], Loss: 6.2155\n",
      "Epoch [1/1], Step [1080/3337], Loss: 6.2451\n",
      "Epoch [1/1], Step [1081/3337], Loss: 6.3535\n",
      "Epoch [1/1], Step [1082/3337], Loss: 6.3294\n",
      "Epoch [1/1], Step [1083/3337], Loss: 6.3144\n",
      "Epoch [1/1], Step [1084/3337], Loss: 6.2805\n",
      "Epoch [1/1], Step [1085/3337], Loss: 6.2573\n",
      "Epoch [1/1], Step [1086/3337], Loss: 6.2868\n",
      "Epoch [1/1], Step [1087/3337], Loss: 6.4432\n",
      "Epoch [1/1], Step [1088/3337], Loss: 6.2187\n",
      "Epoch [1/1], Step [1089/3337], Loss: 6.3565\n",
      "Epoch [1/1], Step [1090/3337], Loss: 6.3095\n",
      "Epoch [1/1], Step [1091/3337], Loss: 6.3625\n",
      "Epoch [1/1], Step [1092/3337], Loss: 6.3113\n",
      "Epoch [1/1], Step [1093/3337], Loss: 6.2749\n",
      "Epoch [1/1], Step [1094/3337], Loss: 6.3380\n",
      "Epoch [1/1], Step [1095/3337], Loss: 6.2790\n",
      "Epoch [1/1], Step [1096/3337], Loss: 6.2171\n",
      "Epoch [1/1], Step [1097/3337], Loss: 6.3511\n",
      "Epoch [1/1], Step [1098/3337], Loss: 6.3037\n",
      "Epoch [1/1], Step [1099/3337], Loss: 6.2675\n",
      "Epoch [1/1], Step [1100/3337], Loss: 6.3201\n",
      "Epoch [1/1], Step [1101/3337], Loss: 6.3434\n",
      "Epoch [1/1], Step [1102/3337], Loss: 6.2295\n",
      "Epoch [1/1], Step [1103/3337], Loss: 6.2660\n",
      "Epoch [1/1], Step [1104/3337], Loss: 6.2911\n",
      "Epoch [1/1], Step [1105/3337], Loss: 6.2780\n",
      "Epoch [1/1], Step [1106/3337], Loss: 6.2523\n",
      "Epoch [1/1], Step [1107/3337], Loss: 6.3030\n",
      "Epoch [1/1], Step [1108/3337], Loss: 6.1991\n",
      "Epoch [1/1], Step [1109/3337], Loss: 6.2829\n",
      "Epoch [1/1], Step [1110/3337], Loss: 6.2071\n",
      "Epoch [1/1], Step [1111/3337], Loss: 6.2333\n",
      "Epoch [1/1], Step [1112/3337], Loss: 6.2375\n",
      "Epoch [1/1], Step [1113/3337], Loss: 6.2173\n",
      "Epoch [1/1], Step [1114/3337], Loss: 6.2251\n",
      "Epoch [1/1], Step [1115/3337], Loss: 6.2443\n",
      "Epoch [1/1], Step [1116/3337], Loss: 6.2748\n",
      "Epoch [1/1], Step [1117/3337], Loss: 6.2881\n",
      "Epoch [1/1], Step [1118/3337], Loss: 6.2528\n",
      "Epoch [1/1], Step [1119/3337], Loss: 6.2966\n",
      "Epoch [1/1], Step [1120/3337], Loss: 6.2554\n",
      "Epoch [1/1], Step [1121/3337], Loss: 6.3235\n",
      "Epoch [1/1], Step [1122/3337], Loss: 6.2004\n",
      "Epoch [1/1], Step [1123/3337], Loss: 6.2973\n",
      "Epoch [1/1], Step [1124/3337], Loss: 6.2445\n",
      "Epoch [1/1], Step [1125/3337], Loss: 6.3097\n",
      "Epoch [1/1], Step [1126/3337], Loss: 6.3353\n",
      "Epoch [1/1], Step [1127/3337], Loss: 6.2819\n",
      "Epoch [1/1], Step [1128/3337], Loss: 6.3391\n",
      "Epoch [1/1], Step [1129/3337], Loss: 6.1863\n",
      "Epoch [1/1], Step [1130/3337], Loss: 6.3004\n",
      "Epoch [1/1], Step [1131/3337], Loss: 6.2888\n",
      "Epoch [1/1], Step [1132/3337], Loss: 6.3345\n",
      "Epoch [1/1], Step [1133/3337], Loss: 6.2022\n",
      "Epoch [1/1], Step [1134/3337], Loss: 6.3234\n",
      "Epoch [1/1], Step [1135/3337], Loss: 6.2373\n",
      "Epoch [1/1], Step [1136/3337], Loss: 6.4097\n",
      "Epoch [1/1], Step [1137/3337], Loss: 6.3223\n",
      "Epoch [1/1], Step [1138/3337], Loss: 6.1616\n",
      "Epoch [1/1], Step [1139/3337], Loss: 6.2863\n",
      "Epoch [1/1], Step [1140/3337], Loss: 6.2177\n",
      "Epoch [1/1], Step [1141/3337], Loss: 6.2329\n",
      "Epoch [1/1], Step [1142/3337], Loss: 6.1330\n",
      "Epoch [1/1], Step [1143/3337], Loss: 6.2292\n",
      "Epoch [1/1], Step [1144/3337], Loss: 6.2518\n",
      "Epoch [1/1], Step [1145/3337], Loss: 6.2692\n",
      "Epoch [1/1], Step [1146/3337], Loss: 6.2437\n",
      "Epoch [1/1], Step [1147/3337], Loss: 6.3179\n",
      "Epoch [1/1], Step [1148/3337], Loss: 6.2455\n",
      "Epoch [1/1], Step [1149/3337], Loss: 6.2631\n",
      "Epoch [1/1], Step [1150/3337], Loss: 6.2966\n",
      "Epoch [1/1], Step [1151/3337], Loss: 6.2531\n",
      "Epoch [1/1], Step [1152/3337], Loss: 6.3473\n",
      "Epoch [1/1], Step [1153/3337], Loss: 6.2852\n",
      "Epoch [1/1], Step [1154/3337], Loss: 6.1764\n",
      "Epoch [1/1], Step [1155/3337], Loss: 6.1609\n",
      "Epoch [1/1], Step [1156/3337], Loss: 6.3186\n",
      "Epoch [1/1], Step [1157/3337], Loss: 6.2921\n",
      "Epoch [1/1], Step [1158/3337], Loss: 6.3938\n",
      "Epoch [1/1], Step [1159/3337], Loss: 6.3298\n",
      "Epoch [1/1], Step [1160/3337], Loss: 6.2465\n",
      "Epoch [1/1], Step [1161/3337], Loss: 6.2144\n",
      "Epoch [1/1], Step [1162/3337], Loss: 6.2597\n",
      "Epoch [1/1], Step [1163/3337], Loss: 6.3246\n",
      "Epoch [1/1], Step [1164/3337], Loss: 6.3088\n",
      "Epoch [1/1], Step [1165/3337], Loss: 6.2422\n",
      "Epoch [1/1], Step [1166/3337], Loss: 6.3083\n",
      "Epoch [1/1], Step [1167/3337], Loss: 6.2689\n",
      "Epoch [1/1], Step [1168/3337], Loss: 6.2901\n",
      "Epoch [1/1], Step [1169/3337], Loss: 6.2864\n",
      "Epoch [1/1], Step [1170/3337], Loss: 6.3434\n",
      "Epoch [1/1], Step [1171/3337], Loss: 6.2505\n",
      "Epoch [1/1], Step [1172/3337], Loss: 6.2613\n",
      "Epoch [1/1], Step [1173/3337], Loss: 6.1817\n",
      "Epoch [1/1], Step [1174/3337], Loss: 6.2076\n",
      "Epoch [1/1], Step [1175/3337], Loss: 6.1882\n",
      "Epoch [1/1], Step [1176/3337], Loss: 6.2366\n",
      "Epoch [1/1], Step [1177/3337], Loss: 6.2015\n",
      "Epoch [1/1], Step [1178/3337], Loss: 6.1968\n",
      "Epoch [1/1], Step [1179/3337], Loss: 6.3013\n",
      "Epoch [1/1], Step [1180/3337], Loss: 6.2675\n",
      "Epoch [1/1], Step [1181/3337], Loss: 6.2669\n",
      "Epoch [1/1], Step [1182/3337], Loss: 6.3061\n",
      "Epoch [1/1], Step [1183/3337], Loss: 6.2102\n",
      "Epoch [1/1], Step [1184/3337], Loss: 6.2248\n",
      "Epoch [1/1], Step [1185/3337], Loss: 6.2774\n",
      "Epoch [1/1], Step [1186/3337], Loss: 6.1367\n",
      "Epoch [1/1], Step [1187/3337], Loss: 6.2311\n",
      "Epoch [1/1], Step [1188/3337], Loss: 6.2477\n",
      "Epoch [1/1], Step [1189/3337], Loss: 6.3531\n",
      "Epoch [1/1], Step [1190/3337], Loss: 6.1766\n",
      "Epoch [1/1], Step [1191/3337], Loss: 6.3082\n",
      "Epoch [1/1], Step [1192/3337], Loss: 6.3937\n",
      "Epoch [1/1], Step [1193/3337], Loss: 6.2990\n",
      "Epoch [1/1], Step [1194/3337], Loss: 6.3989\n",
      "Epoch [1/1], Step [1195/3337], Loss: 6.2491\n",
      "Epoch [1/1], Step [1196/3337], Loss: 6.3190\n",
      "Epoch [1/1], Step [1197/3337], Loss: 6.2239\n",
      "Epoch [1/1], Step [1198/3337], Loss: 6.2353\n",
      "Epoch [1/1], Step [1199/3337], Loss: 6.2653\n",
      "Epoch [1/1], Step [1200/3337], Loss: 6.2677\n",
      "Epoch [1/1], Step [1201/3337], Loss: 6.2630\n",
      "Epoch [1/1], Step [1202/3337], Loss: 6.3650\n",
      "Epoch [1/1], Step [1203/3337], Loss: 6.2265\n",
      "Epoch [1/1], Step [1204/3337], Loss: 6.2296\n",
      "Epoch [1/1], Step [1205/3337], Loss: 6.1976\n",
      "Epoch [1/1], Step [1206/3337], Loss: 6.3624\n",
      "Epoch [1/1], Step [1207/3337], Loss: 6.3462\n",
      "Epoch [1/1], Step [1208/3337], Loss: 6.3207\n",
      "Epoch [1/1], Step [1209/3337], Loss: 6.2363\n",
      "Epoch [1/1], Step [1210/3337], Loss: 6.3238\n",
      "Epoch [1/1], Step [1211/3337], Loss: 6.2396\n",
      "Epoch [1/1], Step [1212/3337], Loss: 6.1922\n",
      "Epoch [1/1], Step [1213/3337], Loss: 6.1956\n",
      "Epoch [1/1], Step [1214/3337], Loss: 6.2184\n",
      "Epoch [1/1], Step [1215/3337], Loss: 6.2759\n",
      "Epoch [1/1], Step [1216/3337], Loss: 6.3718\n",
      "Epoch [1/1], Step [1217/3337], Loss: 6.2861\n",
      "Epoch [1/1], Step [1218/3337], Loss: 6.2519\n",
      "Epoch [1/1], Step [1219/3337], Loss: 6.2392\n",
      "Epoch [1/1], Step [1220/3337], Loss: 6.2205\n",
      "Epoch [1/1], Step [1221/3337], Loss: 6.1513\n",
      "Epoch [1/1], Step [1222/3337], Loss: 6.3325\n",
      "Epoch [1/1], Step [1223/3337], Loss: 6.3266\n",
      "Epoch [1/1], Step [1224/3337], Loss: 6.2770\n",
      "Epoch [1/1], Step [1225/3337], Loss: 6.1838\n",
      "Epoch [1/1], Step [1226/3337], Loss: 6.2398\n",
      "Epoch [1/1], Step [1227/3337], Loss: 6.4014\n",
      "Epoch [1/1], Step [1228/3337], Loss: 6.0968\n",
      "Epoch [1/1], Step [1229/3337], Loss: 6.0496\n",
      "Epoch [1/1], Step [1230/3337], Loss: 6.2375\n",
      "Epoch [1/1], Step [1231/3337], Loss: 6.2804\n",
      "Epoch [1/1], Step [1232/3337], Loss: 6.2261\n",
      "Epoch [1/1], Step [1233/3337], Loss: 6.2390\n",
      "Epoch [1/1], Step [1234/3337], Loss: 6.2185\n",
      "Epoch [1/1], Step [1235/3337], Loss: 6.2920\n",
      "Epoch [1/1], Step [1236/3337], Loss: 6.2933\n",
      "Epoch [1/1], Step [1237/3337], Loss: 6.2983\n",
      "Epoch [1/1], Step [1238/3337], Loss: 6.2617\n",
      "Epoch [1/1], Step [1239/3337], Loss: 6.2681\n",
      "Epoch [1/1], Step [1240/3337], Loss: 6.1986\n",
      "Epoch [1/1], Step [1241/3337], Loss: 6.1966\n",
      "Epoch [1/1], Step [1242/3337], Loss: 6.1970\n",
      "Epoch [1/1], Step [1243/3337], Loss: 6.1819\n",
      "Epoch [1/1], Step [1244/3337], Loss: 6.2732\n",
      "Epoch [1/1], Step [1245/3337], Loss: 6.1900\n",
      "Epoch [1/1], Step [1246/3337], Loss: 6.2681\n",
      "Epoch [1/1], Step [1247/3337], Loss: 6.3367\n",
      "Epoch [1/1], Step [1248/3337], Loss: 6.2534\n",
      "Epoch [1/1], Step [1249/3337], Loss: 6.0614\n",
      "Epoch [1/1], Step [1250/3337], Loss: 6.1826\n",
      "Epoch [1/1], Step [1251/3337], Loss: 6.1998\n",
      "Epoch [1/1], Step [1252/3337], Loss: 6.1363\n",
      "Epoch [1/1], Step [1253/3337], Loss: 6.2676\n",
      "Epoch [1/1], Step [1254/3337], Loss: 6.2233\n",
      "Epoch [1/1], Step [1255/3337], Loss: 6.2149\n",
      "Epoch [1/1], Step [1256/3337], Loss: 6.2069\n",
      "Epoch [1/1], Step [1257/3337], Loss: 6.2409\n",
      "Epoch [1/1], Step [1258/3337], Loss: 6.1608\n",
      "Epoch [1/1], Step [1259/3337], Loss: 6.1838\n",
      "Epoch [1/1], Step [1260/3337], Loss: 6.1706\n",
      "Epoch [1/1], Step [1261/3337], Loss: 6.2344\n",
      "Epoch [1/1], Step [1262/3337], Loss: 6.1707\n",
      "Epoch [1/1], Step [1263/3337], Loss: 6.1805\n",
      "Epoch [1/1], Step [1264/3337], Loss: 6.1730\n",
      "Epoch [1/1], Step [1265/3337], Loss: 6.1492\n",
      "Epoch [1/1], Step [1266/3337], Loss: 6.2781\n",
      "Epoch [1/1], Step [1267/3337], Loss: 6.1825\n",
      "Epoch [1/1], Step [1268/3337], Loss: 6.2973\n",
      "Epoch [1/1], Step [1269/3337], Loss: 6.2919\n",
      "Epoch [1/1], Step [1270/3337], Loss: 6.2353\n",
      "Epoch [1/1], Step [1271/3337], Loss: 6.1894\n",
      "Epoch [1/1], Step [1272/3337], Loss: 6.2272\n",
      "Epoch [1/1], Step [1273/3337], Loss: 6.2507\n",
      "Epoch [1/1], Step [1274/3337], Loss: 6.2284\n",
      "Epoch [1/1], Step [1275/3337], Loss: 6.2132\n",
      "Epoch [1/1], Step [1276/3337], Loss: 6.2032\n",
      "Epoch [1/1], Step [1277/3337], Loss: 6.2560\n",
      "Epoch [1/1], Step [1278/3337], Loss: 6.3491\n",
      "Epoch [1/1], Step [1279/3337], Loss: 6.2567\n",
      "Epoch [1/1], Step [1280/3337], Loss: 6.2087\n",
      "Epoch [1/1], Step [1281/3337], Loss: 6.3097\n",
      "Epoch [1/1], Step [1282/3337], Loss: 6.1931\n",
      "Epoch [1/1], Step [1283/3337], Loss: 6.1885\n",
      "Epoch [1/1], Step [1284/3337], Loss: 6.2703\n",
      "Epoch [1/1], Step [1285/3337], Loss: 6.2205\n",
      "Epoch [1/1], Step [1286/3337], Loss: 6.2020\n",
      "Epoch [1/1], Step [1287/3337], Loss: 6.2978\n",
      "Epoch [1/1], Step [1288/3337], Loss: 6.2211\n",
      "Epoch [1/1], Step [1289/3337], Loss: 6.2129\n",
      "Epoch [1/1], Step [1290/3337], Loss: 6.2100\n",
      "Epoch [1/1], Step [1291/3337], Loss: 6.3199\n",
      "Epoch [1/1], Step [1292/3337], Loss: 6.1934\n",
      "Epoch [1/1], Step [1293/3337], Loss: 6.1526\n",
      "Epoch [1/1], Step [1294/3337], Loss: 6.1394\n",
      "Epoch [1/1], Step [1295/3337], Loss: 6.0864\n",
      "Epoch [1/1], Step [1296/3337], Loss: 6.0652\n",
      "Epoch [1/1], Step [1297/3337], Loss: 6.2408\n",
      "Epoch [1/1], Step [1298/3337], Loss: 6.2248\n",
      "Epoch [1/1], Step [1299/3337], Loss: 6.1977\n",
      "Epoch [1/1], Step [1300/3337], Loss: 6.1704\n",
      "Epoch [1/1], Step [1301/3337], Loss: 6.1422\n",
      "Epoch [1/1], Step [1302/3337], Loss: 6.2066\n",
      "Epoch [1/1], Step [1303/3337], Loss: 6.1680\n",
      "Epoch [1/1], Step [1304/3337], Loss: 6.1671\n",
      "Epoch [1/1], Step [1305/3337], Loss: 6.2114\n",
      "Epoch [1/1], Step [1306/3337], Loss: 6.2832\n",
      "Epoch [1/1], Step [1307/3337], Loss: 6.2351\n",
      "Epoch [1/1], Step [1308/3337], Loss: 6.1893\n",
      "Epoch [1/1], Step [1309/3337], Loss: 6.2020\n",
      "Epoch [1/1], Step [1310/3337], Loss: 6.1816\n",
      "Epoch [1/1], Step [1311/3337], Loss: 6.2369\n",
      "Epoch [1/1], Step [1312/3337], Loss: 6.2683\n",
      "Epoch [1/1], Step [1313/3337], Loss: 6.2225\n",
      "Epoch [1/1], Step [1314/3337], Loss: 6.1728\n",
      "Epoch [1/1], Step [1315/3337], Loss: 6.1820\n",
      "Epoch [1/1], Step [1316/3337], Loss: 6.1114\n",
      "Epoch [1/1], Step [1317/3337], Loss: 6.1907\n",
      "Epoch [1/1], Step [1318/3337], Loss: 6.2324\n",
      "Epoch [1/1], Step [1319/3337], Loss: 6.2256\n",
      "Epoch [1/1], Step [1320/3337], Loss: 6.2298\n",
      "Epoch [1/1], Step [1321/3337], Loss: 6.2768\n",
      "Epoch [1/1], Step [1322/3337], Loss: 6.2625\n",
      "Epoch [1/1], Step [1323/3337], Loss: 6.2492\n",
      "Epoch [1/1], Step [1324/3337], Loss: 6.2196\n",
      "Epoch [1/1], Step [1325/3337], Loss: 6.2013\n",
      "Epoch [1/1], Step [1326/3337], Loss: 6.1591\n",
      "Epoch [1/1], Step [1327/3337], Loss: 6.3452\n",
      "Epoch [1/1], Step [1328/3337], Loss: 6.1809\n",
      "Epoch [1/1], Step [1329/3337], Loss: 6.2572\n",
      "Epoch [1/1], Step [1330/3337], Loss: 6.2040\n",
      "Epoch [1/1], Step [1331/3337], Loss: 6.1622\n",
      "Epoch [1/1], Step [1332/3337], Loss: 6.2396\n",
      "Epoch [1/1], Step [1333/3337], Loss: 6.2569\n",
      "Epoch [1/1], Step [1334/3337], Loss: 6.1981\n",
      "Epoch [1/1], Step [1335/3337], Loss: 6.1680\n",
      "Epoch [1/1], Step [1336/3337], Loss: 6.2561\n",
      "Epoch [1/1], Step [1337/3337], Loss: 6.1977\n",
      "Epoch [1/1], Step [1338/3337], Loss: 6.1966\n",
      "Epoch [1/1], Step [1339/3337], Loss: 6.2635\n",
      "Epoch [1/1], Step [1340/3337], Loss: 6.2316\n",
      "Epoch [1/1], Step [1341/3337], Loss: 6.2514\n",
      "Epoch [1/1], Step [1342/3337], Loss: 6.1312\n",
      "Epoch [1/1], Step [1343/3337], Loss: 6.1284\n",
      "Epoch [1/1], Step [1344/3337], Loss: 6.2646\n",
      "Epoch [1/1], Step [1345/3337], Loss: 6.1444\n",
      "Epoch [1/1], Step [1346/3337], Loss: 6.2207\n",
      "Epoch [1/1], Step [1347/3337], Loss: 6.2532\n",
      "Epoch [1/1], Step [1348/3337], Loss: 6.1073\n",
      "Epoch [1/1], Step [1349/3337], Loss: 6.2937\n",
      "Epoch [1/1], Step [1350/3337], Loss: 6.2910\n",
      "Epoch [1/1], Step [1351/3337], Loss: 6.1477\n",
      "Epoch [1/1], Step [1352/3337], Loss: 6.2581\n",
      "Epoch [1/1], Step [1353/3337], Loss: 6.1581\n",
      "Epoch [1/1], Step [1354/3337], Loss: 6.1887\n",
      "Epoch [1/1], Step [1355/3337], Loss: 6.1360\n",
      "Epoch [1/1], Step [1356/3337], Loss: 6.3455\n",
      "Epoch [1/1], Step [1357/3337], Loss: 6.2453\n",
      "Epoch [1/1], Step [1358/3337], Loss: 6.2117\n",
      "Epoch [1/1], Step [1359/3337], Loss: 6.1173\n",
      "Epoch [1/1], Step [1360/3337], Loss: 6.1431\n",
      "Epoch [1/1], Step [1361/3337], Loss: 6.3618\n",
      "Epoch [1/1], Step [1362/3337], Loss: 6.2149\n",
      "Epoch [1/1], Step [1363/3337], Loss: 6.1538\n",
      "Epoch [1/1], Step [1364/3337], Loss: 6.2125\n",
      "Epoch [1/1], Step [1365/3337], Loss: 6.3013\n",
      "Epoch [1/1], Step [1366/3337], Loss: 6.2235\n",
      "Epoch [1/1], Step [1367/3337], Loss: 6.2040\n",
      "Epoch [1/1], Step [1368/3337], Loss: 6.1638\n",
      "Epoch [1/1], Step [1369/3337], Loss: 6.2668\n",
      "Epoch [1/1], Step [1370/3337], Loss: 6.1209\n",
      "Epoch [1/1], Step [1371/3337], Loss: 6.1084\n",
      "Epoch [1/1], Step [1372/3337], Loss: 6.2548\n",
      "Epoch [1/1], Step [1373/3337], Loss: 6.2851\n",
      "Epoch [1/1], Step [1374/3337], Loss: 6.0872\n",
      "Epoch [1/1], Step [1375/3337], Loss: 6.0677\n",
      "Epoch [1/1], Step [1376/3337], Loss: 6.3190\n",
      "Epoch [1/1], Step [1377/3337], Loss: 6.0258\n",
      "Epoch [1/1], Step [1378/3337], Loss: 6.2388\n",
      "Epoch [1/1], Step [1379/3337], Loss: 6.1774\n",
      "Epoch [1/1], Step [1380/3337], Loss: 6.1285\n",
      "Epoch [1/1], Step [1381/3337], Loss: 6.1695\n",
      "Epoch [1/1], Step [1382/3337], Loss: 6.3441\n",
      "Epoch [1/1], Step [1383/3337], Loss: 6.2177\n",
      "Epoch [1/1], Step [1384/3337], Loss: 6.1157\n",
      "Epoch [1/1], Step [1385/3337], Loss: 6.2127\n",
      "Epoch [1/1], Step [1386/3337], Loss: 6.2580\n",
      "Epoch [1/1], Step [1387/3337], Loss: 6.0785\n",
      "Epoch [1/1], Step [1388/3337], Loss: 6.1996\n",
      "Epoch [1/1], Step [1389/3337], Loss: 6.1249\n",
      "Epoch [1/1], Step [1390/3337], Loss: 6.1826\n",
      "Epoch [1/1], Step [1391/3337], Loss: 6.1045\n",
      "Epoch [1/1], Step [1392/3337], Loss: 6.3220\n",
      "Epoch [1/1], Step [1393/3337], Loss: 6.1407\n",
      "Epoch [1/1], Step [1394/3337], Loss: 6.1046\n",
      "Epoch [1/1], Step [1395/3337], Loss: 6.2394\n",
      "Epoch [1/1], Step [1396/3337], Loss: 6.2016\n",
      "Epoch [1/1], Step [1397/3337], Loss: 6.1819\n",
      "Epoch [1/1], Step [1398/3337], Loss: 6.0761\n",
      "Epoch [1/1], Step [1399/3337], Loss: 6.1494\n",
      "Epoch [1/1], Step [1400/3337], Loss: 6.2309\n",
      "Epoch [1/1], Step [1401/3337], Loss: 6.2454\n",
      "Epoch [1/1], Step [1402/3337], Loss: 6.2835\n",
      "Epoch [1/1], Step [1403/3337], Loss: 6.1265\n",
      "Epoch [1/1], Step [1404/3337], Loss: 6.2150\n",
      "Epoch [1/1], Step [1405/3337], Loss: 6.1028\n",
      "Epoch [1/1], Step [1406/3337], Loss: 6.1710\n",
      "Epoch [1/1], Step [1407/3337], Loss: 6.1770\n",
      "Epoch [1/1], Step [1408/3337], Loss: 6.1348\n",
      "Epoch [1/1], Step [1409/3337], Loss: 6.1977\n",
      "Epoch [1/1], Step [1410/3337], Loss: 6.2147\n",
      "Epoch [1/1], Step [1411/3337], Loss: 6.1785\n",
      "Epoch [1/1], Step [1412/3337], Loss: 6.1612\n",
      "Epoch [1/1], Step [1413/3337], Loss: 6.1058\n",
      "Epoch [1/1], Step [1414/3337], Loss: 6.1379\n",
      "Epoch [1/1], Step [1415/3337], Loss: 6.1841\n",
      "Epoch [1/1], Step [1416/3337], Loss: 6.1921\n",
      "Epoch [1/1], Step [1417/3337], Loss: 6.2057\n",
      "Epoch [1/1], Step [1418/3337], Loss: 6.1335\n",
      "Epoch [1/1], Step [1419/3337], Loss: 6.2013\n",
      "Epoch [1/1], Step [1420/3337], Loss: 6.1328\n",
      "Epoch [1/1], Step [1421/3337], Loss: 6.2381\n",
      "Epoch [1/1], Step [1422/3337], Loss: 6.2702\n",
      "Epoch [1/1], Step [1423/3337], Loss: 6.1685\n",
      "Epoch [1/1], Step [1424/3337], Loss: 6.1935\n",
      "Epoch [1/1], Step [1425/3337], Loss: 6.2404\n",
      "Epoch [1/1], Step [1426/3337], Loss: 6.1863\n",
      "Epoch [1/1], Step [1427/3337], Loss: 6.0407\n",
      "Epoch [1/1], Step [1428/3337], Loss: 6.1363\n",
      "Epoch [1/1], Step [1429/3337], Loss: 6.1900\n",
      "Epoch [1/1], Step [1430/3337], Loss: 6.1837\n",
      "Epoch [1/1], Step [1431/3337], Loss: 6.1286\n",
      "Epoch [1/1], Step [1432/3337], Loss: 6.1222\n",
      "Epoch [1/1], Step [1433/3337], Loss: 6.1054\n",
      "Epoch [1/1], Step [1434/3337], Loss: 6.1145\n",
      "Epoch [1/1], Step [1435/3337], Loss: 6.2055\n",
      "Epoch [1/1], Step [1436/3337], Loss: 6.1818\n",
      "Epoch [1/1], Step [1437/3337], Loss: 6.2387\n",
      "Epoch [1/1], Step [1438/3337], Loss: 6.1429\n",
      "Epoch [1/1], Step [1439/3337], Loss: 6.1017\n",
      "Epoch [1/1], Step [1440/3337], Loss: 6.1229\n",
      "Epoch [1/1], Step [1441/3337], Loss: 6.0664\n",
      "Epoch [1/1], Step [1442/3337], Loss: 6.0524\n",
      "Epoch [1/1], Step [1443/3337], Loss: 6.1241\n",
      "Epoch [1/1], Step [1444/3337], Loss: 6.2357\n",
      "Epoch [1/1], Step [1445/3337], Loss: 6.1808\n",
      "Epoch [1/1], Step [1446/3337], Loss: 6.2172\n",
      "Epoch [1/1], Step [1447/3337], Loss: 6.1561\n",
      "Epoch [1/1], Step [1448/3337], Loss: 6.1402\n",
      "Epoch [1/1], Step [1449/3337], Loss: 6.1459\n",
      "Epoch [1/1], Step [1450/3337], Loss: 6.1979\n",
      "Epoch [1/1], Step [1451/3337], Loss: 6.1267\n",
      "Epoch [1/1], Step [1452/3337], Loss: 6.2176\n",
      "Epoch [1/1], Step [1453/3337], Loss: 6.2216\n",
      "Epoch [1/1], Step [1454/3337], Loss: 6.0949\n",
      "Epoch [1/1], Step [1455/3337], Loss: 6.1300\n",
      "Epoch [1/1], Step [1456/3337], Loss: 6.1817\n",
      "Epoch [1/1], Step [1457/3337], Loss: 6.2163\n",
      "Epoch [1/1], Step [1458/3337], Loss: 6.1550\n",
      "Epoch [1/1], Step [1459/3337], Loss: 6.2016\n",
      "Epoch [1/1], Step [1460/3337], Loss: 6.2943\n",
      "Epoch [1/1], Step [1461/3337], Loss: 6.1895\n",
      "Epoch [1/1], Step [1462/3337], Loss: 6.1347\n",
      "Epoch [1/1], Step [1463/3337], Loss: 6.0447\n",
      "Epoch [1/1], Step [1464/3337], Loss: 6.1573\n",
      "Epoch [1/1], Step [1465/3337], Loss: 6.1639\n",
      "Epoch [1/1], Step [1466/3337], Loss: 6.1032\n",
      "Epoch [1/1], Step [1467/3337], Loss: 6.3184\n",
      "Epoch [1/1], Step [1468/3337], Loss: 6.1931\n",
      "Epoch [1/1], Step [1469/3337], Loss: 6.1745\n",
      "Epoch [1/1], Step [1470/3337], Loss: 6.0825\n",
      "Epoch [1/1], Step [1471/3337], Loss: 6.2179\n",
      "Epoch [1/1], Step [1472/3337], Loss: 6.1795\n",
      "Epoch [1/1], Step [1473/3337], Loss: 6.2483\n",
      "Epoch [1/1], Step [1474/3337], Loss: 6.1324\n",
      "Epoch [1/1], Step [1475/3337], Loss: 6.1668\n",
      "Epoch [1/1], Step [1476/3337], Loss: 6.0860\n",
      "Epoch [1/1], Step [1477/3337], Loss: 6.1030\n",
      "Epoch [1/1], Step [1478/3337], Loss: 6.2333\n",
      "Epoch [1/1], Step [1479/3337], Loss: 6.1056\n",
      "Epoch [1/1], Step [1480/3337], Loss: 6.1113\n",
      "Epoch [1/1], Step [1481/3337], Loss: 6.1790\n",
      "Epoch [1/1], Step [1482/3337], Loss: 6.1706\n",
      "Epoch [1/1], Step [1483/3337], Loss: 6.0458\n",
      "Epoch [1/1], Step [1484/3337], Loss: 6.1534\n",
      "Epoch [1/1], Step [1485/3337], Loss: 6.1331\n",
      "Epoch [1/1], Step [1486/3337], Loss: 6.1200\n",
      "Epoch [1/1], Step [1487/3337], Loss: 6.1081\n",
      "Epoch [1/1], Step [1488/3337], Loss: 6.1224\n",
      "Epoch [1/1], Step [1489/3337], Loss: 6.2231\n",
      "Epoch [1/1], Step [1490/3337], Loss: 6.2488\n",
      "Epoch [1/1], Step [1491/3337], Loss: 6.1529\n",
      "Epoch [1/1], Step [1492/3337], Loss: 6.1832\n",
      "Epoch [1/1], Step [1493/3337], Loss: 6.1068\n",
      "Epoch [1/1], Step [1494/3337], Loss: 6.1509\n",
      "Epoch [1/1], Step [1495/3337], Loss: 6.0644\n",
      "Epoch [1/1], Step [1496/3337], Loss: 6.1180\n",
      "Epoch [1/1], Step [1497/3337], Loss: 6.2043\n",
      "Epoch [1/1], Step [1498/3337], Loss: 6.1825\n",
      "Epoch [1/1], Step [1499/3337], Loss: 6.1989\n",
      "Epoch [1/1], Step [1500/3337], Loss: 6.2201\n",
      "Epoch [1/1], Step [1501/3337], Loss: 6.2190\n",
      "Epoch [1/1], Step [1502/3337], Loss: 6.0722\n",
      "Epoch [1/1], Step [1503/3337], Loss: 6.3109\n",
      "Epoch [1/1], Step [1504/3337], Loss: 6.1743\n",
      "Epoch [1/1], Step [1505/3337], Loss: 6.0986\n",
      "Epoch [1/1], Step [1506/3337], Loss: 6.1556\n",
      "Epoch [1/1], Step [1507/3337], Loss: 6.1718\n",
      "Epoch [1/1], Step [1508/3337], Loss: 6.0830\n",
      "Epoch [1/1], Step [1509/3337], Loss: 6.0404\n",
      "Epoch [1/1], Step [1510/3337], Loss: 6.1516\n",
      "Epoch [1/1], Step [1511/3337], Loss: 6.1499\n",
      "Epoch [1/1], Step [1512/3337], Loss: 6.0441\n",
      "Epoch [1/1], Step [1513/3337], Loss: 6.1383\n",
      "Epoch [1/1], Step [1514/3337], Loss: 6.0026\n",
      "Epoch [1/1], Step [1515/3337], Loss: 6.1015\n",
      "Epoch [1/1], Step [1516/3337], Loss: 6.1842\n",
      "Epoch [1/1], Step [1517/3337], Loss: 6.1451\n",
      "Epoch [1/1], Step [1518/3337], Loss: 6.1229\n",
      "Epoch [1/1], Step [1519/3337], Loss: 6.1666\n",
      "Epoch [1/1], Step [1520/3337], Loss: 6.1751\n",
      "Epoch [1/1], Step [1521/3337], Loss: 6.0078\n",
      "Epoch [1/1], Step [1522/3337], Loss: 6.1103\n",
      "Epoch [1/1], Step [1523/3337], Loss: 6.2913\n",
      "Epoch [1/1], Step [1524/3337], Loss: 6.1140\n",
      "Epoch [1/1], Step [1525/3337], Loss: 6.1984\n",
      "Epoch [1/1], Step [1526/3337], Loss: 6.1186\n",
      "Epoch [1/1], Step [1527/3337], Loss: 6.1012\n",
      "Epoch [1/1], Step [1528/3337], Loss: 6.1712\n",
      "Epoch [1/1], Step [1529/3337], Loss: 6.1403\n",
      "Epoch [1/1], Step [1530/3337], Loss: 6.1842\n",
      "Epoch [1/1], Step [1531/3337], Loss: 6.1867\n",
      "Epoch [1/1], Step [1532/3337], Loss: 6.0881\n",
      "Epoch [1/1], Step [1533/3337], Loss: 6.0153\n",
      "Epoch [1/1], Step [1534/3337], Loss: 6.0329\n",
      "Epoch [1/1], Step [1535/3337], Loss: 6.1430\n",
      "Epoch [1/1], Step [1536/3337], Loss: 6.0659\n",
      "Epoch [1/1], Step [1537/3337], Loss: 6.1272\n",
      "Epoch [1/1], Step [1538/3337], Loss: 6.0489\n",
      "Epoch [1/1], Step [1539/3337], Loss: 6.1030\n",
      "Epoch [1/1], Step [1540/3337], Loss: 6.0265\n",
      "Epoch [1/1], Step [1541/3337], Loss: 6.1184\n",
      "Epoch [1/1], Step [1542/3337], Loss: 6.0220\n",
      "Epoch [1/1], Step [1543/3337], Loss: 6.1031\n",
      "Epoch [1/1], Step [1544/3337], Loss: 6.0303\n",
      "Epoch [1/1], Step [1545/3337], Loss: 6.0721\n",
      "Epoch [1/1], Step [1546/3337], Loss: 6.1477\n",
      "Epoch [1/1], Step [1547/3337], Loss: 5.9643\n",
      "Epoch [1/1], Step [1548/3337], Loss: 6.1335\n",
      "Epoch [1/1], Step [1549/3337], Loss: 6.1643\n",
      "Epoch [1/1], Step [1550/3337], Loss: 6.0963\n",
      "Epoch [1/1], Step [1551/3337], Loss: 6.1030\n",
      "Epoch [1/1], Step [1552/3337], Loss: 6.1098\n",
      "Epoch [1/1], Step [1553/3337], Loss: 6.1101\n",
      "Epoch [1/1], Step [1554/3337], Loss: 6.2134\n",
      "Epoch [1/1], Step [1555/3337], Loss: 6.0147\n",
      "Epoch [1/1], Step [1556/3337], Loss: 6.0478\n",
      "Epoch [1/1], Step [1557/3337], Loss: 6.0804\n",
      "Epoch [1/1], Step [1558/3337], Loss: 6.1304\n",
      "Epoch [1/1], Step [1559/3337], Loss: 6.1851\n",
      "Epoch [1/1], Step [1560/3337], Loss: 6.1058\n",
      "Epoch [1/1], Step [1561/3337], Loss: 6.1793\n",
      "Epoch [1/1], Step [1562/3337], Loss: 6.0860\n",
      "Epoch [1/1], Step [1563/3337], Loss: 6.0100\n",
      "Epoch [1/1], Step [1564/3337], Loss: 6.1574\n",
      "Epoch [1/1], Step [1565/3337], Loss: 6.1307\n",
      "Epoch [1/1], Step [1566/3337], Loss: 6.1446\n",
      "Epoch [1/1], Step [1567/3337], Loss: 6.1911\n",
      "Epoch [1/1], Step [1568/3337], Loss: 6.0652\n",
      "Epoch [1/1], Step [1569/3337], Loss: 6.0821\n",
      "Epoch [1/1], Step [1570/3337], Loss: 6.0260\n",
      "Epoch [1/1], Step [1571/3337], Loss: 6.0753\n",
      "Epoch [1/1], Step [1572/3337], Loss: 6.1017\n",
      "Epoch [1/1], Step [1573/3337], Loss: 6.0688\n",
      "Epoch [1/1], Step [1574/3337], Loss: 6.2087\n",
      "Epoch [1/1], Step [1575/3337], Loss: 6.1345\n",
      "Epoch [1/1], Step [1576/3337], Loss: 6.0352\n",
      "Epoch [1/1], Step [1577/3337], Loss: 6.1819\n",
      "Epoch [1/1], Step [1578/3337], Loss: 6.0501\n",
      "Epoch [1/1], Step [1579/3337], Loss: 6.0749\n",
      "Epoch [1/1], Step [1580/3337], Loss: 6.0931\n",
      "Epoch [1/1], Step [1581/3337], Loss: 6.1826\n",
      "Epoch [1/1], Step [1582/3337], Loss: 6.0824\n",
      "Epoch [1/1], Step [1583/3337], Loss: 6.0975\n",
      "Epoch [1/1], Step [1584/3337], Loss: 6.2462\n",
      "Epoch [1/1], Step [1585/3337], Loss: 6.1718\n",
      "Epoch [1/1], Step [1586/3337], Loss: 6.0338\n",
      "Epoch [1/1], Step [1587/3337], Loss: 6.0335\n",
      "Epoch [1/1], Step [1588/3337], Loss: 6.1397\n",
      "Epoch [1/1], Step [1589/3337], Loss: 6.0625\n",
      "Epoch [1/1], Step [1590/3337], Loss: 6.2652\n",
      "Epoch [1/1], Step [1591/3337], Loss: 6.1340\n",
      "Epoch [1/1], Step [1592/3337], Loss: 6.0049\n",
      "Epoch [1/1], Step [1593/3337], Loss: 6.1345\n",
      "Epoch [1/1], Step [1594/3337], Loss: 5.9864\n",
      "Epoch [1/1], Step [1595/3337], Loss: 6.0751\n",
      "Epoch [1/1], Step [1596/3337], Loss: 5.8548\n",
      "Epoch [1/1], Step [1597/3337], Loss: 6.0906\n",
      "Epoch [1/1], Step [1598/3337], Loss: 6.0775\n",
      "Epoch [1/1], Step [1599/3337], Loss: 6.1277\n",
      "Epoch [1/1], Step [1600/3337], Loss: 6.1583\n",
      "Epoch [1/1], Step [1601/3337], Loss: 6.0780\n",
      "Epoch [1/1], Step [1602/3337], Loss: 6.0912\n",
      "Epoch [1/1], Step [1603/3337], Loss: 6.0647\n",
      "Epoch [1/1], Step [1604/3337], Loss: 6.1666\n",
      "Epoch [1/1], Step [1605/3337], Loss: 6.0938\n",
      "Epoch [1/1], Step [1606/3337], Loss: 6.1762\n",
      "Epoch [1/1], Step [1607/3337], Loss: 6.2496\n",
      "Epoch [1/1], Step [1608/3337], Loss: 6.0012\n",
      "Epoch [1/1], Step [1609/3337], Loss: 6.1001\n",
      "Epoch [1/1], Step [1610/3337], Loss: 6.2949\n",
      "Epoch [1/1], Step [1611/3337], Loss: 6.0857\n",
      "Epoch [1/1], Step [1612/3337], Loss: 6.2541\n",
      "Epoch [1/1], Step [1613/3337], Loss: 6.0618\n",
      "Epoch [1/1], Step [1614/3337], Loss: 6.0994\n",
      "Epoch [1/1], Step [1615/3337], Loss: 6.0835\n",
      "Epoch [1/1], Step [1616/3337], Loss: 6.0302\n",
      "Epoch [1/1], Step [1617/3337], Loss: 6.1594\n",
      "Epoch [1/1], Step [1618/3337], Loss: 6.1572\n",
      "Epoch [1/1], Step [1619/3337], Loss: 6.0535\n",
      "Epoch [1/1], Step [1620/3337], Loss: 6.1716\n",
      "Epoch [1/1], Step [1621/3337], Loss: 6.2502\n",
      "Epoch [1/1], Step [1622/3337], Loss: 6.1485\n",
      "Epoch [1/1], Step [1623/3337], Loss: 6.1084\n",
      "Epoch [1/1], Step [1624/3337], Loss: 6.1309\n",
      "Epoch [1/1], Step [1625/3337], Loss: 6.1418\n",
      "Epoch [1/1], Step [1626/3337], Loss: 6.1997\n",
      "Epoch [1/1], Step [1627/3337], Loss: 6.1137\n",
      "Epoch [1/1], Step [1628/3337], Loss: 6.1281\n",
      "Epoch [1/1], Step [1629/3337], Loss: 6.0757\n",
      "Epoch [1/1], Step [1630/3337], Loss: 6.0872\n",
      "Epoch [1/1], Step [1631/3337], Loss: 6.2431\n",
      "Epoch [1/1], Step [1632/3337], Loss: 6.0685\n",
      "Epoch [1/1], Step [1633/3337], Loss: 6.1924\n",
      "Epoch [1/1], Step [1634/3337], Loss: 6.0919\n",
      "Epoch [1/1], Step [1635/3337], Loss: 6.1346\n",
      "Epoch [1/1], Step [1636/3337], Loss: 6.1883\n",
      "Epoch [1/1], Step [1637/3337], Loss: 6.0911\n",
      "Epoch [1/1], Step [1638/3337], Loss: 6.1016\n",
      "Epoch [1/1], Step [1639/3337], Loss: 6.2210\n",
      "Epoch [1/1], Step [1640/3337], Loss: 6.0951\n",
      "Epoch [1/1], Step [1641/3337], Loss: 5.9598\n",
      "Epoch [1/1], Step [1642/3337], Loss: 6.0715\n",
      "Epoch [1/1], Step [1643/3337], Loss: 6.0471\n",
      "Epoch [1/1], Step [1644/3337], Loss: 6.0641\n",
      "Epoch [1/1], Step [1645/3337], Loss: 6.2172\n",
      "Epoch [1/1], Step [1646/3337], Loss: 6.0901\n",
      "Epoch [1/1], Step [1647/3337], Loss: 6.1996\n",
      "Epoch [1/1], Step [1648/3337], Loss: 6.1489\n",
      "Epoch [1/1], Step [1649/3337], Loss: 6.1112\n",
      "Epoch [1/1], Step [1650/3337], Loss: 6.1334\n",
      "Epoch [1/1], Step [1651/3337], Loss: 6.1715\n",
      "Epoch [1/1], Step [1652/3337], Loss: 6.1353\n",
      "Epoch [1/1], Step [1653/3337], Loss: 6.0586\n",
      "Epoch [1/1], Step [1654/3337], Loss: 6.1068\n",
      "Epoch [1/1], Step [1655/3337], Loss: 6.2111\n",
      "Epoch [1/1], Step [1656/3337], Loss: 6.1223\n",
      "Epoch [1/1], Step [1657/3337], Loss: 6.0522\n",
      "Epoch [1/1], Step [1658/3337], Loss: 6.0774\n",
      "Epoch [1/1], Step [1659/3337], Loss: 6.0489\n",
      "Epoch [1/1], Step [1660/3337], Loss: 6.0322\n",
      "Epoch [1/1], Step [1661/3337], Loss: 6.0846\n",
      "Epoch [1/1], Step [1662/3337], Loss: 5.9892\n",
      "Epoch [1/1], Step [1663/3337], Loss: 6.0441\n",
      "Epoch [1/1], Step [1664/3337], Loss: 6.0818\n",
      "Epoch [1/1], Step [1665/3337], Loss: 6.0056\n",
      "Epoch [1/1], Step [1666/3337], Loss: 6.1576\n",
      "Epoch [1/1], Step [1667/3337], Loss: 6.1869\n",
      "Epoch [1/1], Step [1668/3337], Loss: 6.0680\n",
      "Epoch [1/1], Step [1669/3337], Loss: 6.0272\n",
      "Epoch [1/1], Step [1670/3337], Loss: 6.1550\n",
      "Epoch [1/1], Step [1671/3337], Loss: 6.1044\n",
      "Epoch [1/1], Step [1672/3337], Loss: 6.1675\n",
      "Epoch [1/1], Step [1673/3337], Loss: 6.0706\n",
      "Epoch [1/1], Step [1674/3337], Loss: 6.0605\n",
      "Epoch [1/1], Step [1675/3337], Loss: 6.1763\n",
      "Epoch [1/1], Step [1676/3337], Loss: 6.1043\n",
      "Epoch [1/1], Step [1677/3337], Loss: 6.0827\n",
      "Epoch [1/1], Step [1678/3337], Loss: 6.0135\n",
      "Epoch [1/1], Step [1679/3337], Loss: 6.0897\n",
      "Epoch [1/1], Step [1680/3337], Loss: 6.1261\n",
      "Epoch [1/1], Step [1681/3337], Loss: 6.1453\n",
      "Epoch [1/1], Step [1682/3337], Loss: 5.9860\n",
      "Epoch [1/1], Step [1683/3337], Loss: 6.0145\n",
      "Epoch [1/1], Step [1684/3337], Loss: 6.0566\n",
      "Epoch [1/1], Step [1685/3337], Loss: 6.0734\n",
      "Epoch [1/1], Step [1686/3337], Loss: 6.0434\n",
      "Epoch [1/1], Step [1687/3337], Loss: 6.0244\n",
      "Epoch [1/1], Step [1688/3337], Loss: 6.2079\n",
      "Epoch [1/1], Step [1689/3337], Loss: 6.0082\n",
      "Epoch [1/1], Step [1690/3337], Loss: 6.1190\n",
      "Epoch [1/1], Step [1691/3337], Loss: 6.0630\n",
      "Epoch [1/1], Step [1692/3337], Loss: 5.9912\n",
      "Epoch [1/1], Step [1693/3337], Loss: 6.0503\n",
      "Epoch [1/1], Step [1694/3337], Loss: 5.9376\n",
      "Epoch [1/1], Step [1695/3337], Loss: 6.1342\n",
      "Epoch [1/1], Step [1696/3337], Loss: 6.1314\n",
      "Epoch [1/1], Step [1697/3337], Loss: 6.0829\n",
      "Epoch [1/1], Step [1698/3337], Loss: 5.9537\n",
      "Epoch [1/1], Step [1699/3337], Loss: 6.1743\n",
      "Epoch [1/1], Step [1700/3337], Loss: 6.2427\n",
      "Epoch [1/1], Step [1701/3337], Loss: 6.0451\n",
      "Epoch [1/1], Step [1702/3337], Loss: 6.0173\n",
      "Epoch [1/1], Step [1703/3337], Loss: 6.0497\n",
      "Epoch [1/1], Step [1704/3337], Loss: 6.1731\n",
      "Epoch [1/1], Step [1705/3337], Loss: 6.1417\n",
      "Epoch [1/1], Step [1706/3337], Loss: 6.0614\n",
      "Epoch [1/1], Step [1707/3337], Loss: 6.0386\n",
      "Epoch [1/1], Step [1708/3337], Loss: 6.1132\n",
      "Epoch [1/1], Step [1709/3337], Loss: 6.1392\n",
      "Epoch [1/1], Step [1710/3337], Loss: 6.0721\n",
      "Epoch [1/1], Step [1711/3337], Loss: 6.0969\n",
      "Epoch [1/1], Step [1712/3337], Loss: 6.0499\n",
      "Epoch [1/1], Step [1713/3337], Loss: 6.2012\n",
      "Epoch [1/1], Step [1714/3337], Loss: 6.1720\n",
      "Epoch [1/1], Step [1715/3337], Loss: 6.0257\n",
      "Epoch [1/1], Step [1716/3337], Loss: 6.1260\n",
      "Epoch [1/1], Step [1717/3337], Loss: 6.0478\n",
      "Epoch [1/1], Step [1718/3337], Loss: 6.0768\n",
      "Epoch [1/1], Step [1719/3337], Loss: 6.1677\n",
      "Epoch [1/1], Step [1720/3337], Loss: 6.0618\n",
      "Epoch [1/1], Step [1721/3337], Loss: 6.1702\n",
      "Epoch [1/1], Step [1722/3337], Loss: 6.0520\n",
      "Epoch [1/1], Step [1723/3337], Loss: 6.2351\n",
      "Epoch [1/1], Step [1724/3337], Loss: 6.1275\n",
      "Epoch [1/1], Step [1725/3337], Loss: 6.1791\n",
      "Epoch [1/1], Step [1726/3337], Loss: 6.2534\n",
      "Epoch [1/1], Step [1727/3337], Loss: 5.9154\n",
      "Epoch [1/1], Step [1728/3337], Loss: 5.9636\n",
      "Epoch [1/1], Step [1729/3337], Loss: 6.0960\n",
      "Epoch [1/1], Step [1730/3337], Loss: 6.0124\n",
      "Epoch [1/1], Step [1731/3337], Loss: 5.9025\n",
      "Epoch [1/1], Step [1732/3337], Loss: 6.0750\n",
      "Epoch [1/1], Step [1733/3337], Loss: 6.0003\n",
      "Epoch [1/1], Step [1734/3337], Loss: 6.0482\n",
      "Epoch [1/1], Step [1735/3337], Loss: 6.0968\n",
      "Epoch [1/1], Step [1736/3337], Loss: 6.1673\n",
      "Epoch [1/1], Step [1737/3337], Loss: 6.0479\n",
      "Epoch [1/1], Step [1738/3337], Loss: 6.0376\n",
      "Epoch [1/1], Step [1739/3337], Loss: 6.1894\n",
      "Epoch [1/1], Step [1740/3337], Loss: 6.0788\n",
      "Epoch [1/1], Step [1741/3337], Loss: 6.0920\n",
      "Epoch [1/1], Step [1742/3337], Loss: 6.0223\n",
      "Epoch [1/1], Step [1743/3337], Loss: 6.0631\n",
      "Epoch [1/1], Step [1744/3337], Loss: 6.0375\n",
      "Epoch [1/1], Step [1745/3337], Loss: 6.0205\n",
      "Epoch [1/1], Step [1746/3337], Loss: 5.9578\n",
      "Epoch [1/1], Step [1747/3337], Loss: 6.1178\n",
      "Epoch [1/1], Step [1748/3337], Loss: 5.9307\n",
      "Epoch [1/1], Step [1749/3337], Loss: 6.0189\n",
      "Epoch [1/1], Step [1750/3337], Loss: 5.8974\n",
      "Epoch [1/1], Step [1751/3337], Loss: 6.1516\n",
      "Epoch [1/1], Step [1752/3337], Loss: 6.1311\n",
      "Epoch [1/1], Step [1753/3337], Loss: 6.0538\n",
      "Epoch [1/1], Step [1754/3337], Loss: 6.0614\n",
      "Epoch [1/1], Step [1755/3337], Loss: 6.0254\n",
      "Epoch [1/1], Step [1756/3337], Loss: 5.9321\n",
      "Epoch [1/1], Step [1757/3337], Loss: 6.2275\n",
      "Epoch [1/1], Step [1758/3337], Loss: 6.0645\n",
      "Epoch [1/1], Step [1759/3337], Loss: 6.0613\n",
      "Epoch [1/1], Step [1760/3337], Loss: 6.0475\n",
      "Epoch [1/1], Step [1761/3337], Loss: 5.9938\n",
      "Epoch [1/1], Step [1762/3337], Loss: 6.0701\n",
      "Epoch [1/1], Step [1763/3337], Loss: 6.1116\n",
      "Epoch [1/1], Step [1764/3337], Loss: 6.1006\n",
      "Epoch [1/1], Step [1765/3337], Loss: 6.0601\n",
      "Epoch [1/1], Step [1766/3337], Loss: 6.0679\n",
      "Epoch [1/1], Step [1767/3337], Loss: 6.0875\n",
      "Epoch [1/1], Step [1768/3337], Loss: 6.1019\n",
      "Epoch [1/1], Step [1769/3337], Loss: 6.0941\n",
      "Epoch [1/1], Step [1770/3337], Loss: 6.0505\n",
      "Epoch [1/1], Step [1771/3337], Loss: 5.9899\n",
      "Epoch [1/1], Step [1772/3337], Loss: 6.0732\n",
      "Epoch [1/1], Step [1773/3337], Loss: 6.0678\n",
      "Epoch [1/1], Step [1774/3337], Loss: 6.0570\n",
      "Epoch [1/1], Step [1775/3337], Loss: 6.2099\n",
      "Epoch [1/1], Step [1776/3337], Loss: 6.1197\n",
      "Epoch [1/1], Step [1777/3337], Loss: 5.9962\n",
      "Epoch [1/1], Step [1778/3337], Loss: 5.9864\n",
      "Epoch [1/1], Step [1779/3337], Loss: 6.0464\n",
      "Epoch [1/1], Step [1780/3337], Loss: 6.0967\n",
      "Epoch [1/1], Step [1781/3337], Loss: 5.9896\n",
      "Epoch [1/1], Step [1782/3337], Loss: 6.0522\n",
      "Epoch [1/1], Step [1783/3337], Loss: 6.1258\n",
      "Epoch [1/1], Step [1784/3337], Loss: 6.1333\n",
      "Epoch [1/1], Step [1785/3337], Loss: 6.0108\n",
      "Epoch [1/1], Step [1786/3337], Loss: 6.0081\n",
      "Epoch [1/1], Step [1787/3337], Loss: 6.1303\n",
      "Epoch [1/1], Step [1788/3337], Loss: 6.0451\n",
      "Epoch [1/1], Step [1789/3337], Loss: 5.9834\n",
      "Epoch [1/1], Step [1790/3337], Loss: 6.1130\n",
      "Epoch [1/1], Step [1791/3337], Loss: 6.0280\n",
      "Epoch [1/1], Step [1792/3337], Loss: 6.0884\n",
      "Epoch [1/1], Step [1793/3337], Loss: 6.0468\n",
      "Epoch [1/1], Step [1794/3337], Loss: 6.0867\n",
      "Epoch [1/1], Step [1795/3337], Loss: 6.0683\n",
      "Epoch [1/1], Step [1796/3337], Loss: 6.1102\n",
      "Epoch [1/1], Step [1797/3337], Loss: 6.0049\n",
      "Epoch [1/1], Step [1798/3337], Loss: 6.0243\n",
      "Epoch [1/1], Step [1799/3337], Loss: 6.0584\n",
      "Epoch [1/1], Step [1800/3337], Loss: 6.0363\n",
      "Epoch [1/1], Step [1801/3337], Loss: 6.0399\n",
      "Epoch [1/1], Step [1802/3337], Loss: 6.0293\n",
      "Epoch [1/1], Step [1803/3337], Loss: 6.1592\n",
      "Epoch [1/1], Step [1804/3337], Loss: 6.0067\n",
      "Epoch [1/1], Step [1805/3337], Loss: 6.1303\n",
      "Epoch [1/1], Step [1806/3337], Loss: 6.0341\n",
      "Epoch [1/1], Step [1807/3337], Loss: 6.0178\n",
      "Epoch [1/1], Step [1808/3337], Loss: 6.1086\n",
      "Epoch [1/1], Step [1809/3337], Loss: 6.0942\n",
      "Epoch [1/1], Step [1810/3337], Loss: 6.1638\n",
      "Epoch [1/1], Step [1811/3337], Loss: 6.0141\n",
      "Epoch [1/1], Step [1812/3337], Loss: 6.0558\n",
      "Epoch [1/1], Step [1813/3337], Loss: 6.0052\n",
      "Epoch [1/1], Step [1814/3337], Loss: 6.0570\n",
      "Epoch [1/1], Step [1815/3337], Loss: 6.0624\n",
      "Epoch [1/1], Step [1816/3337], Loss: 6.0696\n",
      "Epoch [1/1], Step [1817/3337], Loss: 6.0451\n",
      "Epoch [1/1], Step [1818/3337], Loss: 6.0296\n",
      "Epoch [1/1], Step [1819/3337], Loss: 5.9473\n",
      "Epoch [1/1], Step [1820/3337], Loss: 6.0397\n",
      "Epoch [1/1], Step [1821/3337], Loss: 5.9069\n",
      "Epoch [1/1], Step [1822/3337], Loss: 5.9551\n",
      "Epoch [1/1], Step [1823/3337], Loss: 6.0102\n",
      "Epoch [1/1], Step [1824/3337], Loss: 6.1663\n",
      "Epoch [1/1], Step [1825/3337], Loss: 6.1050\n",
      "Epoch [1/1], Step [1826/3337], Loss: 6.1059\n",
      "Epoch [1/1], Step [1827/3337], Loss: 6.0822\n",
      "Epoch [1/1], Step [1828/3337], Loss: 5.9751\n",
      "Epoch [1/1], Step [1829/3337], Loss: 5.9995\n",
      "Epoch [1/1], Step [1830/3337], Loss: 6.0053\n",
      "Epoch [1/1], Step [1831/3337], Loss: 6.0558\n",
      "Epoch [1/1], Step [1832/3337], Loss: 6.0270\n",
      "Epoch [1/1], Step [1833/3337], Loss: 6.0872\n",
      "Epoch [1/1], Step [1834/3337], Loss: 6.0268\n",
      "Epoch [1/1], Step [1835/3337], Loss: 6.0448\n",
      "Epoch [1/1], Step [1836/3337], Loss: 6.0665\n",
      "Epoch [1/1], Step [1837/3337], Loss: 6.0657\n",
      "Epoch [1/1], Step [1838/3337], Loss: 5.9536\n",
      "Epoch [1/1], Step [1839/3337], Loss: 6.0167\n",
      "Epoch [1/1], Step [1840/3337], Loss: 6.0674\n",
      "Epoch [1/1], Step [1841/3337], Loss: 5.9956\n",
      "Epoch [1/1], Step [1842/3337], Loss: 6.0631\n",
      "Epoch [1/1], Step [1843/3337], Loss: 5.9565\n",
      "Epoch [1/1], Step [1844/3337], Loss: 5.9793\n",
      "Epoch [1/1], Step [1845/3337], Loss: 6.0843\n",
      "Epoch [1/1], Step [1846/3337], Loss: 5.9698\n",
      "Epoch [1/1], Step [1847/3337], Loss: 5.9730\n",
      "Epoch [1/1], Step [1848/3337], Loss: 5.9287\n",
      "Epoch [1/1], Step [1849/3337], Loss: 6.1026\n",
      "Epoch [1/1], Step [1850/3337], Loss: 6.0270\n",
      "Epoch [1/1], Step [1851/3337], Loss: 5.8923\n",
      "Epoch [1/1], Step [1852/3337], Loss: 6.1426\n",
      "Epoch [1/1], Step [1853/3337], Loss: 6.0652\n",
      "Epoch [1/1], Step [1854/3337], Loss: 6.0265\n",
      "Epoch [1/1], Step [1855/3337], Loss: 6.0739\n",
      "Epoch [1/1], Step [1856/3337], Loss: 5.9223\n",
      "Epoch [1/1], Step [1857/3337], Loss: 5.9819\n",
      "Epoch [1/1], Step [1858/3337], Loss: 5.9620\n",
      "Epoch [1/1], Step [1859/3337], Loss: 6.0354\n",
      "Epoch [1/1], Step [1860/3337], Loss: 6.0054\n",
      "Epoch [1/1], Step [1861/3337], Loss: 6.0755\n",
      "Epoch [1/1], Step [1862/3337], Loss: 5.8960\n",
      "Epoch [1/1], Step [1863/3337], Loss: 6.0009\n",
      "Epoch [1/1], Step [1864/3337], Loss: 5.8954\n",
      "Epoch [1/1], Step [1865/3337], Loss: 6.0274\n",
      "Epoch [1/1], Step [1866/3337], Loss: 6.0462\n",
      "Epoch [1/1], Step [1867/3337], Loss: 6.0080\n",
      "Epoch [1/1], Step [1868/3337], Loss: 6.0182\n",
      "Epoch [1/1], Step [1869/3337], Loss: 6.0239\n",
      "Epoch [1/1], Step [1870/3337], Loss: 6.0531\n",
      "Epoch [1/1], Step [1871/3337], Loss: 5.9107\n",
      "Epoch [1/1], Step [1872/3337], Loss: 6.1874\n",
      "Epoch [1/1], Step [1873/3337], Loss: 6.0113\n",
      "Epoch [1/1], Step [1874/3337], Loss: 5.9591\n",
      "Epoch [1/1], Step [1875/3337], Loss: 6.0363\n",
      "Epoch [1/1], Step [1876/3337], Loss: 6.1336\n",
      "Epoch [1/1], Step [1877/3337], Loss: 6.0440\n",
      "Epoch [1/1], Step [1878/3337], Loss: 6.1322\n",
      "Epoch [1/1], Step [1879/3337], Loss: 6.0887\n",
      "Epoch [1/1], Step [1880/3337], Loss: 6.0668\n",
      "Epoch [1/1], Step [1881/3337], Loss: 6.0909\n",
      "Epoch [1/1], Step [1882/3337], Loss: 5.9415\n",
      "Epoch [1/1], Step [1883/3337], Loss: 6.0929\n",
      "Epoch [1/1], Step [1884/3337], Loss: 5.9582\n",
      "Epoch [1/1], Step [1885/3337], Loss: 5.9096\n",
      "Epoch [1/1], Step [1886/3337], Loss: 5.9005\n",
      "Epoch [1/1], Step [1887/3337], Loss: 6.0957\n",
      "Epoch [1/1], Step [1888/3337], Loss: 6.0011\n",
      "Epoch [1/1], Step [1889/3337], Loss: 6.1121\n",
      "Epoch [1/1], Step [1890/3337], Loss: 5.9914\n",
      "Epoch [1/1], Step [1891/3337], Loss: 5.9954\n",
      "Epoch [1/1], Step [1892/3337], Loss: 6.0689\n",
      "Epoch [1/1], Step [1893/3337], Loss: 6.0844\n",
      "Epoch [1/1], Step [1894/3337], Loss: 6.0016\n",
      "Epoch [1/1], Step [1895/3337], Loss: 6.0616\n",
      "Epoch [1/1], Step [1896/3337], Loss: 6.0206\n",
      "Epoch [1/1], Step [1897/3337], Loss: 5.9671\n",
      "Epoch [1/1], Step [1898/3337], Loss: 5.9882\n",
      "Epoch [1/1], Step [1899/3337], Loss: 5.8828\n",
      "Epoch [1/1], Step [1900/3337], Loss: 6.0034\n",
      "Epoch [1/1], Step [1901/3337], Loss: 5.8502\n",
      "Epoch [1/1], Step [1902/3337], Loss: 5.9591\n",
      "Epoch [1/1], Step [1903/3337], Loss: 6.0882\n",
      "Epoch [1/1], Step [1904/3337], Loss: 5.8729\n",
      "Epoch [1/1], Step [1905/3337], Loss: 6.0370\n",
      "Epoch [1/1], Step [1906/3337], Loss: 6.0608\n",
      "Epoch [1/1], Step [1907/3337], Loss: 5.9670\n",
      "Epoch [1/1], Step [1908/3337], Loss: 5.9832\n",
      "Epoch [1/1], Step [1909/3337], Loss: 5.9833\n",
      "Epoch [1/1], Step [1910/3337], Loss: 6.0222\n",
      "Epoch [1/1], Step [1911/3337], Loss: 6.0576\n",
      "Epoch [1/1], Step [1912/3337], Loss: 5.9967\n",
      "Epoch [1/1], Step [1913/3337], Loss: 6.0428\n",
      "Epoch [1/1], Step [1914/3337], Loss: 5.9506\n",
      "Epoch [1/1], Step [1915/3337], Loss: 5.9887\n",
      "Epoch [1/1], Step [1916/3337], Loss: 5.9955\n",
      "Epoch [1/1], Step [1917/3337], Loss: 6.0511\n",
      "Epoch [1/1], Step [1918/3337], Loss: 5.8684\n",
      "Epoch [1/1], Step [1919/3337], Loss: 6.1120\n",
      "Epoch [1/1], Step [1920/3337], Loss: 6.0334\n",
      "Epoch [1/1], Step [1921/3337], Loss: 6.1534\n",
      "Epoch [1/1], Step [1922/3337], Loss: 5.9423\n",
      "Epoch [1/1], Step [1923/3337], Loss: 6.0702\n",
      "Epoch [1/1], Step [1924/3337], Loss: 5.8915\n",
      "Epoch [1/1], Step [1925/3337], Loss: 6.0441\n",
      "Epoch [1/1], Step [1926/3337], Loss: 5.9185\n",
      "Epoch [1/1], Step [1927/3337], Loss: 5.9803\n",
      "Epoch [1/1], Step [1928/3337], Loss: 5.9778\n",
      "Epoch [1/1], Step [1929/3337], Loss: 6.1343\n",
      "Epoch [1/1], Step [1930/3337], Loss: 6.0556\n",
      "Epoch [1/1], Step [1931/3337], Loss: 6.1000\n",
      "Epoch [1/1], Step [1932/3337], Loss: 6.0414\n",
      "Epoch [1/1], Step [1933/3337], Loss: 6.0949\n",
      "Epoch [1/1], Step [1934/3337], Loss: 6.0905\n",
      "Epoch [1/1], Step [1935/3337], Loss: 5.9355\n",
      "Epoch [1/1], Step [1936/3337], Loss: 6.1421\n",
      "Epoch [1/1], Step [1937/3337], Loss: 6.0367\n",
      "Epoch [1/1], Step [1938/3337], Loss: 5.9962\n",
      "Epoch [1/1], Step [1939/3337], Loss: 6.0266\n",
      "Epoch [1/1], Step [1940/3337], Loss: 6.1628\n",
      "Epoch [1/1], Step [1941/3337], Loss: 6.0478\n",
      "Epoch [1/1], Step [1942/3337], Loss: 5.9477\n",
      "Epoch [1/1], Step [1943/3337], Loss: 6.0698\n",
      "Epoch [1/1], Step [1944/3337], Loss: 6.1029\n",
      "Epoch [1/1], Step [1945/3337], Loss: 6.1298\n",
      "Epoch [1/1], Step [1946/3337], Loss: 5.9238\n",
      "Epoch [1/1], Step [1947/3337], Loss: 5.9971\n",
      "Epoch [1/1], Step [1948/3337], Loss: 5.9536\n",
      "Epoch [1/1], Step [1949/3337], Loss: 6.0100\n",
      "Epoch [1/1], Step [1950/3337], Loss: 5.9466\n",
      "Epoch [1/1], Step [1951/3337], Loss: 6.0293\n",
      "Epoch [1/1], Step [1952/3337], Loss: 6.0526\n",
      "Epoch [1/1], Step [1953/3337], Loss: 6.0427\n",
      "Epoch [1/1], Step [1954/3337], Loss: 6.1955\n",
      "Epoch [1/1], Step [1955/3337], Loss: 5.9035\n",
      "Epoch [1/1], Step [1956/3337], Loss: 6.0401\n",
      "Epoch [1/1], Step [1957/3337], Loss: 5.9732\n",
      "Epoch [1/1], Step [1958/3337], Loss: 5.9262\n",
      "Epoch [1/1], Step [1959/3337], Loss: 6.0111\n",
      "Epoch [1/1], Step [1960/3337], Loss: 5.9298\n",
      "Epoch [1/1], Step [1961/3337], Loss: 5.9760\n",
      "Epoch [1/1], Step [1962/3337], Loss: 6.0564\n",
      "Epoch [1/1], Step [1963/3337], Loss: 6.1622\n",
      "Epoch [1/1], Step [1964/3337], Loss: 5.9461\n",
      "Epoch [1/1], Step [1965/3337], Loss: 5.9907\n",
      "Epoch [1/1], Step [1966/3337], Loss: 5.9972\n",
      "Epoch [1/1], Step [1967/3337], Loss: 6.0274\n",
      "Epoch [1/1], Step [1968/3337], Loss: 5.9616\n",
      "Epoch [1/1], Step [1969/3337], Loss: 6.0133\n",
      "Epoch [1/1], Step [1970/3337], Loss: 6.0688\n",
      "Epoch [1/1], Step [1971/3337], Loss: 5.9915\n",
      "Epoch [1/1], Step [1972/3337], Loss: 6.0880\n",
      "Epoch [1/1], Step [1973/3337], Loss: 6.1242\n",
      "Epoch [1/1], Step [1974/3337], Loss: 6.0302\n",
      "Epoch [1/1], Step [1975/3337], Loss: 5.9391\n",
      "Epoch [1/1], Step [1976/3337], Loss: 6.1085\n",
      "Epoch [1/1], Step [1977/3337], Loss: 6.0200\n",
      "Epoch [1/1], Step [1978/3337], Loss: 5.8966\n",
      "Epoch [1/1], Step [1979/3337], Loss: 5.9872\n",
      "Epoch [1/1], Step [1980/3337], Loss: 6.0263\n",
      "Epoch [1/1], Step [1981/3337], Loss: 6.0353\n",
      "Epoch [1/1], Step [1982/3337], Loss: 5.9639\n",
      "Epoch [1/1], Step [1983/3337], Loss: 5.8970\n",
      "Epoch [1/1], Step [1984/3337], Loss: 5.9569\n",
      "Epoch [1/1], Step [1985/3337], Loss: 5.9207\n",
      "Epoch [1/1], Step [1986/3337], Loss: 5.9429\n",
      "Epoch [1/1], Step [1987/3337], Loss: 6.0360\n",
      "Epoch [1/1], Step [1988/3337], Loss: 5.9708\n",
      "Epoch [1/1], Step [1989/3337], Loss: 6.0429\n",
      "Epoch [1/1], Step [1990/3337], Loss: 5.8893\n",
      "Epoch [1/1], Step [1991/3337], Loss: 5.8714\n",
      "Epoch [1/1], Step [1992/3337], Loss: 5.9791\n",
      "Epoch [1/1], Step [1993/3337], Loss: 6.0226\n",
      "Epoch [1/1], Step [1994/3337], Loss: 5.9256\n",
      "Epoch [1/1], Step [1995/3337], Loss: 6.0391\n",
      "Epoch [1/1], Step [1996/3337], Loss: 6.0421\n",
      "Epoch [1/1], Step [1997/3337], Loss: 6.1408\n",
      "Epoch [1/1], Step [1998/3337], Loss: 5.9352\n",
      "Epoch [1/1], Step [1999/3337], Loss: 5.9252\n",
      "Epoch [1/1], Step [2000/3337], Loss: 6.1053\n",
      "Epoch [1/1], Step [2001/3337], Loss: 5.8991\n",
      "Epoch [1/1], Step [2002/3337], Loss: 6.0324\n",
      "Epoch [1/1], Step [2003/3337], Loss: 6.0861\n",
      "Epoch [1/1], Step [2004/3337], Loss: 6.0859\n",
      "Epoch [1/1], Step [2005/3337], Loss: 5.8674\n",
      "Epoch [1/1], Step [2006/3337], Loss: 6.0000\n",
      "Epoch [1/1], Step [2007/3337], Loss: 6.0570\n",
      "Epoch [1/1], Step [2008/3337], Loss: 5.9526\n",
      "Epoch [1/1], Step [2009/3337], Loss: 5.8972\n",
      "Epoch [1/1], Step [2010/3337], Loss: 5.9951\n",
      "Epoch [1/1], Step [2011/3337], Loss: 6.0193\n",
      "Epoch [1/1], Step [2012/3337], Loss: 5.8808\n",
      "Epoch [1/1], Step [2013/3337], Loss: 5.9168\n",
      "Epoch [1/1], Step [2014/3337], Loss: 6.0572\n",
      "Epoch [1/1], Step [2015/3337], Loss: 5.9585\n",
      "Epoch [1/1], Step [2016/3337], Loss: 6.1111\n",
      "Epoch [1/1], Step [2017/3337], Loss: 5.9867\n",
      "Epoch [1/1], Step [2018/3337], Loss: 5.9473\n",
      "Epoch [1/1], Step [2019/3337], Loss: 5.8933\n",
      "Epoch [1/1], Step [2020/3337], Loss: 5.9636\n",
      "Epoch [1/1], Step [2021/3337], Loss: 6.0025\n",
      "Epoch [1/1], Step [2022/3337], Loss: 5.8937\n",
      "Epoch [1/1], Step [2023/3337], Loss: 6.0587\n",
      "Epoch [1/1], Step [2024/3337], Loss: 6.1358\n",
      "Epoch [1/1], Step [2025/3337], Loss: 6.1160\n",
      "Epoch [1/1], Step [2026/3337], Loss: 5.9632\n",
      "Epoch [1/1], Step [2027/3337], Loss: 5.9582\n",
      "Epoch [1/1], Step [2028/3337], Loss: 5.9446\n",
      "Epoch [1/1], Step [2029/3337], Loss: 6.0798\n",
      "Epoch [1/1], Step [2030/3337], Loss: 5.8370\n",
      "Epoch [1/1], Step [2031/3337], Loss: 6.0317\n",
      "Epoch [1/1], Step [2032/3337], Loss: 5.9900\n",
      "Epoch [1/1], Step [2033/3337], Loss: 5.9603\n",
      "Epoch [1/1], Step [2034/3337], Loss: 6.0126\n",
      "Epoch [1/1], Step [2035/3337], Loss: 5.9512\n",
      "Epoch [1/1], Step [2036/3337], Loss: 6.0435\n",
      "Epoch [1/1], Step [2037/3337], Loss: 6.0095\n",
      "Epoch [1/1], Step [2038/3337], Loss: 5.9371\n",
      "Epoch [1/1], Step [2039/3337], Loss: 6.0164\n",
      "Epoch [1/1], Step [2040/3337], Loss: 5.9563\n",
      "Epoch [1/1], Step [2041/3337], Loss: 5.9866\n",
      "Epoch [1/1], Step [2042/3337], Loss: 5.9934\n",
      "Epoch [1/1], Step [2043/3337], Loss: 6.0321\n",
      "Epoch [1/1], Step [2044/3337], Loss: 5.8284\n",
      "Epoch [1/1], Step [2045/3337], Loss: 5.9365\n",
      "Epoch [1/1], Step [2046/3337], Loss: 5.9648\n",
      "Epoch [1/1], Step [2047/3337], Loss: 6.1520\n",
      "Epoch [1/1], Step [2048/3337], Loss: 5.8961\n",
      "Epoch [1/1], Step [2049/3337], Loss: 5.9780\n",
      "Epoch [1/1], Step [2050/3337], Loss: 5.8994\n",
      "Epoch [1/1], Step [2051/3337], Loss: 5.8773\n",
      "Epoch [1/1], Step [2052/3337], Loss: 5.9557\n",
      "Epoch [1/1], Step [2053/3337], Loss: 6.0952\n",
      "Epoch [1/1], Step [2054/3337], Loss: 6.0411\n",
      "Epoch [1/1], Step [2055/3337], Loss: 6.0637\n",
      "Epoch [1/1], Step [2056/3337], Loss: 5.9181\n",
      "Epoch [1/1], Step [2057/3337], Loss: 6.0646\n",
      "Epoch [1/1], Step [2058/3337], Loss: 5.9103\n",
      "Epoch [1/1], Step [2059/3337], Loss: 5.9616\n",
      "Epoch [1/1], Step [2060/3337], Loss: 5.8768\n",
      "Epoch [1/1], Step [2061/3337], Loss: 5.8604\n",
      "Epoch [1/1], Step [2062/3337], Loss: 5.9303\n",
      "Epoch [1/1], Step [2063/3337], Loss: 5.9865\n",
      "Epoch [1/1], Step [2064/3337], Loss: 5.9766\n",
      "Epoch [1/1], Step [2065/3337], Loss: 6.1141\n",
      "Epoch [1/1], Step [2066/3337], Loss: 6.0468\n",
      "Epoch [1/1], Step [2067/3337], Loss: 5.9136\n",
      "Epoch [1/1], Step [2068/3337], Loss: 5.8500\n",
      "Epoch [1/1], Step [2069/3337], Loss: 5.9828\n",
      "Epoch [1/1], Step [2070/3337], Loss: 6.0512\n",
      "Epoch [1/1], Step [2071/3337], Loss: 5.9032\n",
      "Epoch [1/1], Step [2072/3337], Loss: 6.0034\n",
      "Epoch [1/1], Step [2073/3337], Loss: 6.0715\n",
      "Epoch [1/1], Step [2074/3337], Loss: 5.9779\n",
      "Epoch [1/1], Step [2075/3337], Loss: 5.9708\n",
      "Epoch [1/1], Step [2076/3337], Loss: 5.9449\n",
      "Epoch [1/1], Step [2077/3337], Loss: 5.8540\n",
      "Epoch [1/1], Step [2078/3337], Loss: 6.0657\n",
      "Epoch [1/1], Step [2079/3337], Loss: 5.9189\n",
      "Epoch [1/1], Step [2080/3337], Loss: 5.8887\n",
      "Epoch [1/1], Step [2081/3337], Loss: 5.8101\n",
      "Epoch [1/1], Step [2082/3337], Loss: 6.0051\n",
      "Epoch [1/1], Step [2083/3337], Loss: 6.0005\n",
      "Epoch [1/1], Step [2084/3337], Loss: 6.0477\n",
      "Epoch [1/1], Step [2085/3337], Loss: 6.0491\n",
      "Epoch [1/1], Step [2086/3337], Loss: 5.7976\n",
      "Epoch [1/1], Step [2087/3337], Loss: 5.9888\n",
      "Epoch [1/1], Step [2088/3337], Loss: 5.8551\n",
      "Epoch [1/1], Step [2089/3337], Loss: 6.0077\n",
      "Epoch [1/1], Step [2090/3337], Loss: 5.9316\n",
      "Epoch [1/1], Step [2091/3337], Loss: 5.7530\n",
      "Epoch [1/1], Step [2092/3337], Loss: 6.0233\n",
      "Epoch [1/1], Step [2093/3337], Loss: 6.0050\n",
      "Epoch [1/1], Step [2094/3337], Loss: 6.0549\n",
      "Epoch [1/1], Step [2095/3337], Loss: 5.9728\n",
      "Epoch [1/1], Step [2096/3337], Loss: 6.0011\n",
      "Epoch [1/1], Step [2097/3337], Loss: 5.9049\n",
      "Epoch [1/1], Step [2098/3337], Loss: 5.8810\n",
      "Epoch [1/1], Step [2099/3337], Loss: 5.8893\n",
      "Epoch [1/1], Step [2100/3337], Loss: 6.0101\n",
      "Epoch [1/1], Step [2101/3337], Loss: 5.9489\n",
      "Epoch [1/1], Step [2102/3337], Loss: 6.0587\n",
      "Epoch [1/1], Step [2103/3337], Loss: 5.8482\n",
      "Epoch [1/1], Step [2104/3337], Loss: 5.9142\n",
      "Epoch [1/1], Step [2105/3337], Loss: 5.9563\n",
      "Epoch [1/1], Step [2106/3337], Loss: 5.9829\n",
      "Epoch [1/1], Step [2107/3337], Loss: 5.9265\n",
      "Epoch [1/1], Step [2108/3337], Loss: 5.9670\n",
      "Epoch [1/1], Step [2109/3337], Loss: 6.0433\n",
      "Epoch [1/1], Step [2110/3337], Loss: 5.9897\n",
      "Epoch [1/1], Step [2111/3337], Loss: 5.9387\n",
      "Epoch [1/1], Step [2112/3337], Loss: 5.9552\n",
      "Epoch [1/1], Step [2113/3337], Loss: 6.0592\n",
      "Epoch [1/1], Step [2114/3337], Loss: 5.8955\n",
      "Epoch [1/1], Step [2115/3337], Loss: 5.9846\n",
      "Epoch [1/1], Step [2116/3337], Loss: 5.9227\n",
      "Epoch [1/1], Step [2117/3337], Loss: 5.8952\n",
      "Epoch [1/1], Step [2118/3337], Loss: 6.0166\n",
      "Epoch [1/1], Step [2119/3337], Loss: 5.8988\n",
      "Epoch [1/1], Step [2120/3337], Loss: 6.0642\n",
      "Epoch [1/1], Step [2121/3337], Loss: 6.0197\n",
      "Epoch [1/1], Step [2122/3337], Loss: 5.8623\n",
      "Epoch [1/1], Step [2123/3337], Loss: 5.9615\n",
      "Epoch [1/1], Step [2124/3337], Loss: 5.9746\n",
      "Epoch [1/1], Step [2125/3337], Loss: 6.0264\n",
      "Epoch [1/1], Step [2126/3337], Loss: 5.9669\n",
      "Epoch [1/1], Step [2127/3337], Loss: 6.0505\n",
      "Epoch [1/1], Step [2128/3337], Loss: 5.9717\n",
      "Epoch [1/1], Step [2129/3337], Loss: 6.0014\n",
      "Epoch [1/1], Step [2130/3337], Loss: 5.9954\n",
      "Epoch [1/1], Step [2131/3337], Loss: 5.9922\n",
      "Epoch [1/1], Step [2132/3337], Loss: 5.9088\n",
      "Epoch [1/1], Step [2133/3337], Loss: 5.9875\n",
      "Epoch [1/1], Step [2134/3337], Loss: 5.9493\n",
      "Epoch [1/1], Step [2135/3337], Loss: 5.9024\n",
      "Epoch [1/1], Step [2136/3337], Loss: 6.0534\n",
      "Epoch [1/1], Step [2137/3337], Loss: 6.1489\n",
      "Epoch [1/1], Step [2138/3337], Loss: 5.9978\n",
      "Epoch [1/1], Step [2139/3337], Loss: 5.9420\n",
      "Epoch [1/1], Step [2140/3337], Loss: 5.9039\n",
      "Epoch [1/1], Step [2141/3337], Loss: 5.9319\n",
      "Epoch [1/1], Step [2142/3337], Loss: 5.8797\n",
      "Epoch [1/1], Step [2143/3337], Loss: 5.9475\n",
      "Epoch [1/1], Step [2144/3337], Loss: 6.0605\n",
      "Epoch [1/1], Step [2145/3337], Loss: 6.0008\n",
      "Epoch [1/1], Step [2146/3337], Loss: 5.9472\n",
      "Epoch [1/1], Step [2147/3337], Loss: 5.9274\n",
      "Epoch [1/1], Step [2148/3337], Loss: 5.9509\n",
      "Epoch [1/1], Step [2149/3337], Loss: 5.9240\n",
      "Epoch [1/1], Step [2150/3337], Loss: 5.8976\n",
      "Epoch [1/1], Step [2151/3337], Loss: 5.9893\n",
      "Epoch [1/1], Step [2152/3337], Loss: 5.9747\n",
      "Epoch [1/1], Step [2153/3337], Loss: 5.8789\n",
      "Epoch [1/1], Step [2154/3337], Loss: 5.9548\n",
      "Epoch [1/1], Step [2155/3337], Loss: 5.9029\n",
      "Epoch [1/1], Step [2156/3337], Loss: 5.9786\n",
      "Epoch [1/1], Step [2157/3337], Loss: 6.0105\n",
      "Epoch [1/1], Step [2158/3337], Loss: 5.8936\n",
      "Epoch [1/1], Step [2159/3337], Loss: 5.9302\n",
      "Epoch [1/1], Step [2160/3337], Loss: 5.8981\n",
      "Epoch [1/1], Step [2161/3337], Loss: 6.0444\n",
      "Epoch [1/1], Step [2162/3337], Loss: 5.9455\n",
      "Epoch [1/1], Step [2163/3337], Loss: 5.8657\n",
      "Epoch [1/1], Step [2164/3337], Loss: 5.8265\n",
      "Epoch [1/1], Step [2165/3337], Loss: 5.8075\n",
      "Epoch [1/1], Step [2166/3337], Loss: 5.9268\n",
      "Epoch [1/1], Step [2167/3337], Loss: 5.9354\n",
      "Epoch [1/1], Step [2168/3337], Loss: 5.9500\n",
      "Epoch [1/1], Step [2169/3337], Loss: 5.7338\n",
      "Epoch [1/1], Step [2170/3337], Loss: 5.9124\n",
      "Epoch [1/1], Step [2171/3337], Loss: 6.0206\n",
      "Epoch [1/1], Step [2172/3337], Loss: 6.0497\n",
      "Epoch [1/1], Step [2173/3337], Loss: 5.8663\n",
      "Epoch [1/1], Step [2174/3337], Loss: 6.0386\n",
      "Epoch [1/1], Step [2175/3337], Loss: 5.9262\n",
      "Epoch [1/1], Step [2176/3337], Loss: 5.9529\n",
      "Epoch [1/1], Step [2177/3337], Loss: 5.8155\n",
      "Epoch [1/1], Step [2178/3337], Loss: 5.9337\n",
      "Epoch [1/1], Step [2179/3337], Loss: 6.0665\n",
      "Epoch [1/1], Step [2180/3337], Loss: 5.8615\n",
      "Epoch [1/1], Step [2181/3337], Loss: 5.8826\n",
      "Epoch [1/1], Step [2182/3337], Loss: 5.9135\n",
      "Epoch [1/1], Step [2183/3337], Loss: 6.0078\n",
      "Epoch [1/1], Step [2184/3337], Loss: 5.9968\n",
      "Epoch [1/1], Step [2185/3337], Loss: 5.9513\n",
      "Epoch [1/1], Step [2186/3337], Loss: 5.8947\n",
      "Epoch [1/1], Step [2187/3337], Loss: 6.0817\n",
      "Epoch [1/1], Step [2188/3337], Loss: 6.0686\n",
      "Epoch [1/1], Step [2189/3337], Loss: 5.8886\n",
      "Epoch [1/1], Step [2190/3337], Loss: 5.9126\n",
      "Epoch [1/1], Step [2191/3337], Loss: 5.9392\n",
      "Epoch [1/1], Step [2192/3337], Loss: 5.9726\n",
      "Epoch [1/1], Step [2193/3337], Loss: 5.8402\n",
      "Epoch [1/1], Step [2194/3337], Loss: 5.8430\n",
      "Epoch [1/1], Step [2195/3337], Loss: 5.8504\n",
      "Epoch [1/1], Step [2196/3337], Loss: 5.9675\n",
      "Epoch [1/1], Step [2197/3337], Loss: 6.0388\n",
      "Epoch [1/1], Step [2198/3337], Loss: 5.9057\n",
      "Epoch [1/1], Step [2199/3337], Loss: 5.7866\n",
      "Epoch [1/1], Step [2200/3337], Loss: 5.9769\n",
      "Epoch [1/1], Step [2201/3337], Loss: 6.0731\n",
      "Epoch [1/1], Step [2202/3337], Loss: 5.8045\n",
      "Epoch [1/1], Step [2203/3337], Loss: 5.9238\n",
      "Epoch [1/1], Step [2204/3337], Loss: 5.8661\n",
      "Epoch [1/1], Step [2205/3337], Loss: 5.7962\n",
      "Epoch [1/1], Step [2206/3337], Loss: 5.9197\n",
      "Epoch [1/1], Step [2207/3337], Loss: 6.0372\n",
      "Epoch [1/1], Step [2208/3337], Loss: 5.9423\n",
      "Epoch [1/1], Step [2209/3337], Loss: 5.7893\n",
      "Epoch [1/1], Step [2210/3337], Loss: 5.8721\n",
      "Epoch [1/1], Step [2211/3337], Loss: 5.9722\n",
      "Epoch [1/1], Step [2212/3337], Loss: 5.8877\n",
      "Epoch [1/1], Step [2213/3337], Loss: 6.0335\n",
      "Epoch [1/1], Step [2214/3337], Loss: 5.8932\n",
      "Epoch [1/1], Step [2215/3337], Loss: 6.0278\n",
      "Epoch [1/1], Step [2216/3337], Loss: 5.8101\n",
      "Epoch [1/1], Step [2217/3337], Loss: 6.0671\n",
      "Epoch [1/1], Step [2218/3337], Loss: 5.7954\n",
      "Epoch [1/1], Step [2219/3337], Loss: 5.9828\n",
      "Epoch [1/1], Step [2220/3337], Loss: 5.8966\n",
      "Epoch [1/1], Step [2221/3337], Loss: 5.9230\n",
      "Epoch [1/1], Step [2222/3337], Loss: 6.0055\n",
      "Epoch [1/1], Step [2223/3337], Loss: 5.9653\n",
      "Epoch [1/1], Step [2224/3337], Loss: 5.9211\n",
      "Epoch [1/1], Step [2225/3337], Loss: 5.9315\n",
      "Epoch [1/1], Step [2226/3337], Loss: 6.0055\n",
      "Epoch [1/1], Step [2227/3337], Loss: 5.9578\n",
      "Epoch [1/1], Step [2228/3337], Loss: 5.9835\n",
      "Epoch [1/1], Step [2229/3337], Loss: 6.0079\n",
      "Epoch [1/1], Step [2230/3337], Loss: 5.9851\n",
      "Epoch [1/1], Step [2231/3337], Loss: 5.9200\n",
      "Epoch [1/1], Step [2232/3337], Loss: 5.8812\n",
      "Epoch [1/1], Step [2233/3337], Loss: 6.0063\n",
      "Epoch [1/1], Step [2234/3337], Loss: 5.8952\n",
      "Epoch [1/1], Step [2235/3337], Loss: 5.8487\n",
      "Epoch [1/1], Step [2236/3337], Loss: 5.9407\n",
      "Epoch [1/1], Step [2237/3337], Loss: 5.8593\n",
      "Epoch [1/1], Step [2238/3337], Loss: 5.8754\n",
      "Epoch [1/1], Step [2239/3337], Loss: 5.7846\n",
      "Epoch [1/1], Step [2240/3337], Loss: 5.9550\n",
      "Epoch [1/1], Step [2241/3337], Loss: 5.8585\n",
      "Epoch [1/1], Step [2242/3337], Loss: 5.9541\n",
      "Epoch [1/1], Step [2243/3337], Loss: 6.0467\n",
      "Epoch [1/1], Step [2244/3337], Loss: 5.9200\n",
      "Epoch [1/1], Step [2245/3337], Loss: 5.9631\n",
      "Epoch [1/1], Step [2246/3337], Loss: 6.0542\n",
      "Epoch [1/1], Step [2247/3337], Loss: 5.9282\n",
      "Epoch [1/1], Step [2248/3337], Loss: 5.9722\n",
      "Epoch [1/1], Step [2249/3337], Loss: 5.9062\n",
      "Epoch [1/1], Step [2250/3337], Loss: 5.9492\n",
      "Epoch [1/1], Step [2251/3337], Loss: 6.0002\n",
      "Epoch [1/1], Step [2252/3337], Loss: 6.0437\n",
      "Epoch [1/1], Step [2253/3337], Loss: 5.9378\n",
      "Epoch [1/1], Step [2254/3337], Loss: 5.9619\n",
      "Epoch [1/1], Step [2255/3337], Loss: 5.9509\n",
      "Epoch [1/1], Step [2256/3337], Loss: 5.9884\n",
      "Epoch [1/1], Step [2257/3337], Loss: 5.9010\n",
      "Epoch [1/1], Step [2258/3337], Loss: 6.0054\n",
      "Epoch [1/1], Step [2259/3337], Loss: 5.8116\n",
      "Epoch [1/1], Step [2260/3337], Loss: 5.8636\n",
      "Epoch [1/1], Step [2261/3337], Loss: 5.9124\n",
      "Epoch [1/1], Step [2262/3337], Loss: 5.9109\n",
      "Epoch [1/1], Step [2263/3337], Loss: 5.9682\n",
      "Epoch [1/1], Step [2264/3337], Loss: 5.8582\n",
      "Epoch [1/1], Step [2265/3337], Loss: 5.9312\n",
      "Epoch [1/1], Step [2266/3337], Loss: 6.0853\n",
      "Epoch [1/1], Step [2267/3337], Loss: 5.9201\n",
      "Epoch [1/1], Step [2268/3337], Loss: 5.9311\n",
      "Epoch [1/1], Step [2269/3337], Loss: 5.8620\n",
      "Epoch [1/1], Step [2270/3337], Loss: 5.9647\n",
      "Epoch [1/1], Step [2271/3337], Loss: 5.9904\n",
      "Epoch [1/1], Step [2272/3337], Loss: 5.9444\n",
      "Epoch [1/1], Step [2273/3337], Loss: 5.9795\n",
      "Epoch [1/1], Step [2274/3337], Loss: 6.0267\n",
      "Epoch [1/1], Step [2275/3337], Loss: 5.9479\n",
      "Epoch [1/1], Step [2276/3337], Loss: 5.7815\n",
      "Epoch [1/1], Step [2277/3337], Loss: 5.8970\n",
      "Epoch [1/1], Step [2278/3337], Loss: 5.8997\n",
      "Epoch [1/1], Step [2279/3337], Loss: 5.9321\n",
      "Epoch [1/1], Step [2280/3337], Loss: 5.8729\n",
      "Epoch [1/1], Step [2281/3337], Loss: 5.9254\n",
      "Epoch [1/1], Step [2282/3337], Loss: 5.8855\n",
      "Epoch [1/1], Step [2283/3337], Loss: 5.8968\n",
      "Epoch [1/1], Step [2284/3337], Loss: 5.8654\n",
      "Epoch [1/1], Step [2285/3337], Loss: 6.0860\n",
      "Epoch [1/1], Step [2286/3337], Loss: 5.8856\n",
      "Epoch [1/1], Step [2287/3337], Loss: 5.9541\n",
      "Epoch [1/1], Step [2288/3337], Loss: 5.9344\n",
      "Epoch [1/1], Step [2289/3337], Loss: 5.9827\n",
      "Epoch [1/1], Step [2290/3337], Loss: 5.9096\n",
      "Epoch [1/1], Step [2291/3337], Loss: 5.9034\n",
      "Epoch [1/1], Step [2292/3337], Loss: 5.8000\n",
      "Epoch [1/1], Step [2293/3337], Loss: 5.8180\n",
      "Epoch [1/1], Step [2294/3337], Loss: 5.8834\n",
      "Epoch [1/1], Step [2295/3337], Loss: 5.9592\n",
      "Epoch [1/1], Step [2296/3337], Loss: 5.9489\n",
      "Epoch [1/1], Step [2297/3337], Loss: 5.7978\n",
      "Epoch [1/1], Step [2298/3337], Loss: 5.9200\n",
      "Epoch [1/1], Step [2299/3337], Loss: 5.9008\n",
      "Epoch [1/1], Step [2300/3337], Loss: 5.9430\n",
      "Epoch [1/1], Step [2301/3337], Loss: 5.9240\n",
      "Epoch [1/1], Step [2302/3337], Loss: 6.0553\n",
      "Epoch [1/1], Step [2303/3337], Loss: 5.9378\n",
      "Epoch [1/1], Step [2304/3337], Loss: 5.9818\n",
      "Epoch [1/1], Step [2305/3337], Loss: 5.8991\n",
      "Epoch [1/1], Step [2306/3337], Loss: 6.0746\n",
      "Epoch [1/1], Step [2307/3337], Loss: 5.8859\n",
      "Epoch [1/1], Step [2308/3337], Loss: 5.9637\n",
      "Epoch [1/1], Step [2309/3337], Loss: 5.9031\n",
      "Epoch [1/1], Step [2310/3337], Loss: 5.9667\n",
      "Epoch [1/1], Step [2311/3337], Loss: 5.9948\n",
      "Epoch [1/1], Step [2312/3337], Loss: 5.9953\n",
      "Epoch [1/1], Step [2313/3337], Loss: 6.0001\n",
      "Epoch [1/1], Step [2314/3337], Loss: 5.9606\n",
      "Epoch [1/1], Step [2315/3337], Loss: 5.7535\n",
      "Epoch [1/1], Step [2316/3337], Loss: 5.8497\n",
      "Epoch [1/1], Step [2317/3337], Loss: 5.9060\n",
      "Epoch [1/1], Step [2318/3337], Loss: 6.0986\n",
      "Epoch [1/1], Step [2319/3337], Loss: 5.9913\n",
      "Epoch [1/1], Step [2320/3337], Loss: 5.8046\n",
      "Epoch [1/1], Step [2321/3337], Loss: 6.1051\n",
      "Epoch [1/1], Step [2322/3337], Loss: 5.7762\n",
      "Epoch [1/1], Step [2323/3337], Loss: 5.9360\n",
      "Epoch [1/1], Step [2324/3337], Loss: 5.7146\n",
      "Epoch [1/1], Step [2325/3337], Loss: 5.8264\n",
      "Epoch [1/1], Step [2326/3337], Loss: 5.8464\n",
      "Epoch [1/1], Step [2327/3337], Loss: 5.9304\n",
      "Epoch [1/1], Step [2328/3337], Loss: 5.9978\n",
      "Epoch [1/1], Step [2329/3337], Loss: 5.9419\n",
      "Epoch [1/1], Step [2330/3337], Loss: 5.8940\n",
      "Epoch [1/1], Step [2331/3337], Loss: 5.9221\n",
      "Epoch [1/1], Step [2332/3337], Loss: 5.8808\n",
      "Epoch [1/1], Step [2333/3337], Loss: 5.8946\n",
      "Epoch [1/1], Step [2334/3337], Loss: 5.9017\n",
      "Epoch [1/1], Step [2335/3337], Loss: 5.8218\n",
      "Epoch [1/1], Step [2336/3337], Loss: 6.0062\n",
      "Epoch [1/1], Step [2337/3337], Loss: 5.9146\n",
      "Epoch [1/1], Step [2338/3337], Loss: 5.9353\n",
      "Epoch [1/1], Step [2339/3337], Loss: 5.7614\n",
      "Epoch [1/1], Step [2340/3337], Loss: 5.9694\n",
      "Epoch [1/1], Step [2341/3337], Loss: 5.9076\n",
      "Epoch [1/1], Step [2342/3337], Loss: 6.1143\n",
      "Epoch [1/1], Step [2343/3337], Loss: 5.9673\n",
      "Epoch [1/1], Step [2344/3337], Loss: 5.9177\n",
      "Epoch [1/1], Step [2345/3337], Loss: 5.9387\n",
      "Epoch [1/1], Step [2346/3337], Loss: 5.9880\n",
      "Epoch [1/1], Step [2347/3337], Loss: 6.0191\n",
      "Epoch [1/1], Step [2348/3337], Loss: 5.8567\n",
      "Epoch [1/1], Step [2349/3337], Loss: 6.0538\n",
      "Epoch [1/1], Step [2350/3337], Loss: 5.7852\n",
      "Epoch [1/1], Step [2351/3337], Loss: 5.9011\n",
      "Epoch [1/1], Step [2352/3337], Loss: 5.8816\n",
      "Epoch [1/1], Step [2353/3337], Loss: 5.9070\n",
      "Epoch [1/1], Step [2354/3337], Loss: 5.9741\n",
      "Epoch [1/1], Step [2355/3337], Loss: 5.9113\n",
      "Epoch [1/1], Step [2356/3337], Loss: 5.8072\n",
      "Epoch [1/1], Step [2357/3337], Loss: 5.9518\n",
      "Epoch [1/1], Step [2358/3337], Loss: 5.9928\n",
      "Epoch [1/1], Step [2359/3337], Loss: 5.8643\n",
      "Epoch [1/1], Step [2360/3337], Loss: 6.0745\n",
      "Epoch [1/1], Step [2361/3337], Loss: 5.8856\n",
      "Epoch [1/1], Step [2362/3337], Loss: 5.9681\n",
      "Epoch [1/1], Step [2363/3337], Loss: 5.8691\n",
      "Epoch [1/1], Step [2364/3337], Loss: 5.8214\n",
      "Epoch [1/1], Step [2365/3337], Loss: 5.8613\n",
      "Epoch [1/1], Step [2366/3337], Loss: 5.9375\n",
      "Epoch [1/1], Step [2367/3337], Loss: 5.9679\n",
      "Epoch [1/1], Step [2368/3337], Loss: 6.0539\n",
      "Epoch [1/1], Step [2369/3337], Loss: 5.9992\n",
      "Epoch [1/1], Step [2370/3337], Loss: 5.8096\n",
      "Epoch [1/1], Step [2371/3337], Loss: 5.8800\n",
      "Epoch [1/1], Step [2372/3337], Loss: 5.9074\n",
      "Epoch [1/1], Step [2373/3337], Loss: 5.8589\n",
      "Epoch [1/1], Step [2374/3337], Loss: 5.9108\n",
      "Epoch [1/1], Step [2375/3337], Loss: 5.9973\n",
      "Epoch [1/1], Step [2376/3337], Loss: 5.9031\n",
      "Epoch [1/1], Step [2377/3337], Loss: 5.8760\n",
      "Epoch [1/1], Step [2378/3337], Loss: 5.9421\n",
      "Epoch [1/1], Step [2379/3337], Loss: 5.9765\n",
      "Epoch [1/1], Step [2380/3337], Loss: 5.9755\n",
      "Epoch [1/1], Step [2381/3337], Loss: 5.8794\n",
      "Epoch [1/1], Step [2382/3337], Loss: 5.9131\n",
      "Epoch [1/1], Step [2383/3337], Loss: 5.9337\n",
      "Epoch [1/1], Step [2384/3337], Loss: 5.9193\n",
      "Epoch [1/1], Step [2385/3337], Loss: 5.8053\n",
      "Epoch [1/1], Step [2386/3337], Loss: 6.1277\n",
      "Epoch [1/1], Step [2387/3337], Loss: 5.8384\n",
      "Epoch [1/1], Step [2388/3337], Loss: 5.9128\n",
      "Epoch [1/1], Step [2389/3337], Loss: 5.9407\n",
      "Epoch [1/1], Step [2390/3337], Loss: 6.0262\n",
      "Epoch [1/1], Step [2391/3337], Loss: 5.9775\n",
      "Epoch [1/1], Step [2392/3337], Loss: 5.8275\n",
      "Epoch [1/1], Step [2393/3337], Loss: 6.0012\n",
      "Epoch [1/1], Step [2394/3337], Loss: 5.9479\n",
      "Epoch [1/1], Step [2395/3337], Loss: 5.8918\n",
      "Epoch [1/1], Step [2396/3337], Loss: 5.8684\n",
      "Epoch [1/1], Step [2397/3337], Loss: 5.9545\n",
      "Epoch [1/1], Step [2398/3337], Loss: 5.9225\n",
      "Epoch [1/1], Step [2399/3337], Loss: 5.9929\n",
      "Epoch [1/1], Step [2400/3337], Loss: 5.7495\n",
      "Epoch [1/1], Step [2401/3337], Loss: 5.8005\n",
      "Epoch [1/1], Step [2402/3337], Loss: 5.9470\n",
      "Epoch [1/1], Step [2403/3337], Loss: 5.9076\n",
      "Epoch [1/1], Step [2404/3337], Loss: 5.9229\n",
      "Epoch [1/1], Step [2405/3337], Loss: 5.8410\n",
      "Epoch [1/1], Step [2406/3337], Loss: 5.8197\n",
      "Epoch [1/1], Step [2407/3337], Loss: 5.8829\n",
      "Epoch [1/1], Step [2408/3337], Loss: 5.8950\n",
      "Epoch [1/1], Step [2409/3337], Loss: 5.9027\n",
      "Epoch [1/1], Step [2410/3337], Loss: 5.9458\n",
      "Epoch [1/1], Step [2411/3337], Loss: 5.8796\n",
      "Epoch [1/1], Step [2412/3337], Loss: 5.8823\n",
      "Epoch [1/1], Step [2413/3337], Loss: 5.7765\n",
      "Epoch [1/1], Step [2414/3337], Loss: 5.8635\n",
      "Epoch [1/1], Step [2415/3337], Loss: 5.8760\n",
      "Epoch [1/1], Step [2416/3337], Loss: 5.8773\n",
      "Epoch [1/1], Step [2417/3337], Loss: 5.8784\n",
      "Epoch [1/1], Step [2418/3337], Loss: 5.8506\n",
      "Epoch [1/1], Step [2419/3337], Loss: 5.9077\n",
      "Epoch [1/1], Step [2420/3337], Loss: 6.0490\n",
      "Epoch [1/1], Step [2421/3337], Loss: 5.9758\n",
      "Epoch [1/1], Step [2422/3337], Loss: 5.8497\n",
      "Epoch [1/1], Step [2423/3337], Loss: 5.9707\n",
      "Epoch [1/1], Step [2424/3337], Loss: 5.9143\n",
      "Epoch [1/1], Step [2425/3337], Loss: 5.9541\n",
      "Epoch [1/1], Step [2426/3337], Loss: 6.0278\n",
      "Epoch [1/1], Step [2427/3337], Loss: 5.8470\n",
      "Epoch [1/1], Step [2428/3337], Loss: 5.8878\n",
      "Epoch [1/1], Step [2429/3337], Loss: 5.8811\n",
      "Epoch [1/1], Step [2430/3337], Loss: 5.7346\n",
      "Epoch [1/1], Step [2431/3337], Loss: 5.8267\n",
      "Epoch [1/1], Step [2432/3337], Loss: 5.9943\n",
      "Epoch [1/1], Step [2433/3337], Loss: 5.9146\n",
      "Epoch [1/1], Step [2434/3337], Loss: 5.8395\n",
      "Epoch [1/1], Step [2435/3337], Loss: 5.9582\n",
      "Epoch [1/1], Step [2436/3337], Loss: 5.8520\n",
      "Epoch [1/1], Step [2437/3337], Loss: 5.8263\n",
      "Epoch [1/1], Step [2438/3337], Loss: 5.8007\n",
      "Epoch [1/1], Step [2439/3337], Loss: 5.8268\n",
      "Epoch [1/1], Step [2440/3337], Loss: 6.0063\n",
      "Epoch [1/1], Step [2441/3337], Loss: 5.8153\n",
      "Epoch [1/1], Step [2442/3337], Loss: 5.8357\n",
      "Epoch [1/1], Step [2443/3337], Loss: 5.9046\n",
      "Epoch [1/1], Step [2444/3337], Loss: 6.0628\n",
      "Epoch [1/1], Step [2445/3337], Loss: 5.9351\n",
      "Epoch [1/1], Step [2446/3337], Loss: 5.9397\n",
      "Epoch [1/1], Step [2447/3337], Loss: 6.0870\n",
      "Epoch [1/1], Step [2448/3337], Loss: 5.8643\n",
      "Epoch [1/1], Step [2449/3337], Loss: 6.0082\n",
      "Epoch [1/1], Step [2450/3337], Loss: 5.7807\n",
      "Epoch [1/1], Step [2451/3337], Loss: 5.7706\n",
      "Epoch [1/1], Step [2452/3337], Loss: 5.8274\n",
      "Epoch [1/1], Step [2453/3337], Loss: 5.8947\n",
      "Epoch [1/1], Step [2454/3337], Loss: 5.8976\n",
      "Epoch [1/1], Step [2455/3337], Loss: 5.7888\n",
      "Epoch [1/1], Step [2456/3337], Loss: 5.9152\n",
      "Epoch [1/1], Step [2457/3337], Loss: 5.9332\n",
      "Epoch [1/1], Step [2458/3337], Loss: 5.8804\n",
      "Epoch [1/1], Step [2459/3337], Loss: 5.8565\n",
      "Epoch [1/1], Step [2460/3337], Loss: 5.8770\n",
      "Epoch [1/1], Step [2461/3337], Loss: 5.8265\n",
      "Epoch [1/1], Step [2462/3337], Loss: 5.8833\n",
      "Epoch [1/1], Step [2463/3337], Loss: 5.8624\n",
      "Epoch [1/1], Step [2464/3337], Loss: 5.9096\n",
      "Epoch [1/1], Step [2465/3337], Loss: 5.8714\n",
      "Epoch [1/1], Step [2466/3337], Loss: 5.8965\n",
      "Epoch [1/1], Step [2467/3337], Loss: 5.8163\n",
      "Epoch [1/1], Step [2468/3337], Loss: 5.8867\n",
      "Epoch [1/1], Step [2469/3337], Loss: 5.8983\n",
      "Epoch [1/1], Step [2470/3337], Loss: 5.8765\n",
      "Epoch [1/1], Step [2471/3337], Loss: 5.9098\n",
      "Epoch [1/1], Step [2472/3337], Loss: 5.8248\n",
      "Epoch [1/1], Step [2473/3337], Loss: 5.7843\n",
      "Epoch [1/1], Step [2474/3337], Loss: 5.9608\n",
      "Epoch [1/1], Step [2475/3337], Loss: 5.9402\n",
      "Epoch [1/1], Step [2476/3337], Loss: 5.9562\n",
      "Epoch [1/1], Step [2477/3337], Loss: 5.9605\n",
      "Epoch [1/1], Step [2478/3337], Loss: 5.7692\n",
      "Epoch [1/1], Step [2479/3337], Loss: 5.8844\n",
      "Epoch [1/1], Step [2480/3337], Loss: 5.8639\n",
      "Epoch [1/1], Step [2481/3337], Loss: 5.8901\n",
      "Epoch [1/1], Step [2482/3337], Loss: 5.7633\n",
      "Epoch [1/1], Step [2483/3337], Loss: 5.8831\n",
      "Epoch [1/1], Step [2484/3337], Loss: 5.9351\n",
      "Epoch [1/1], Step [2485/3337], Loss: 5.8746\n",
      "Epoch [1/1], Step [2486/3337], Loss: 5.9692\n",
      "Epoch [1/1], Step [2487/3337], Loss: 5.9878\n",
      "Epoch [1/1], Step [2488/3337], Loss: 5.8284\n",
      "Epoch [1/1], Step [2489/3337], Loss: 6.0138\n",
      "Epoch [1/1], Step [2490/3337], Loss: 5.8572\n",
      "Epoch [1/1], Step [2491/3337], Loss: 5.8901\n",
      "Epoch [1/1], Step [2492/3337], Loss: 5.7510\n",
      "Epoch [1/1], Step [2493/3337], Loss: 5.9111\n",
      "Epoch [1/1], Step [2494/3337], Loss: 5.9588\n",
      "Epoch [1/1], Step [2495/3337], Loss: 5.9819\n",
      "Epoch [1/1], Step [2496/3337], Loss: 5.9173\n",
      "Epoch [1/1], Step [2497/3337], Loss: 5.8594\n",
      "Epoch [1/1], Step [2498/3337], Loss: 5.8479\n",
      "Epoch [1/1], Step [2499/3337], Loss: 5.9117\n",
      "Epoch [1/1], Step [2500/3337], Loss: 5.9520\n",
      "Epoch [1/1], Step [2501/3337], Loss: 5.8732\n",
      "Epoch [1/1], Step [2502/3337], Loss: 5.7742\n",
      "Epoch [1/1], Step [2503/3337], Loss: 5.9485\n",
      "Epoch [1/1], Step [2504/3337], Loss: 5.9726\n",
      "Epoch [1/1], Step [2505/3337], Loss: 6.0432\n",
      "Epoch [1/1], Step [2506/3337], Loss: 5.9242\n",
      "Epoch [1/1], Step [2507/3337], Loss: 5.8418\n",
      "Epoch [1/1], Step [2508/3337], Loss: 5.9968\n",
      "Epoch [1/1], Step [2509/3337], Loss: 5.9595\n",
      "Epoch [1/1], Step [2510/3337], Loss: 6.0283\n",
      "Epoch [1/1], Step [2511/3337], Loss: 5.8979\n",
      "Epoch [1/1], Step [2512/3337], Loss: 5.9338\n",
      "Epoch [1/1], Step [2513/3337], Loss: 5.8621\n",
      "Epoch [1/1], Step [2514/3337], Loss: 5.8676\n",
      "Epoch [1/1], Step [2515/3337], Loss: 5.8090\n",
      "Epoch [1/1], Step [2516/3337], Loss: 5.8592\n",
      "Epoch [1/1], Step [2517/3337], Loss: 5.8757\n",
      "Epoch [1/1], Step [2518/3337], Loss: 5.8612\n",
      "Epoch [1/1], Step [2519/3337], Loss: 5.9678\n",
      "Epoch [1/1], Step [2520/3337], Loss: 5.8700\n",
      "Epoch [1/1], Step [2521/3337], Loss: 5.9133\n",
      "Epoch [1/1], Step [2522/3337], Loss: 5.9485\n",
      "Epoch [1/1], Step [2523/3337], Loss: 5.8832\n",
      "Epoch [1/1], Step [2524/3337], Loss: 5.8163\n",
      "Epoch [1/1], Step [2525/3337], Loss: 5.8626\n",
      "Epoch [1/1], Step [2526/3337], Loss: 5.7399\n",
      "Epoch [1/1], Step [2527/3337], Loss: 6.0080\n",
      "Epoch [1/1], Step [2528/3337], Loss: 5.6816\n",
      "Epoch [1/1], Step [2529/3337], Loss: 5.9039\n",
      "Epoch [1/1], Step [2530/3337], Loss: 6.0005\n",
      "Epoch [1/1], Step [2531/3337], Loss: 5.7509\n",
      "Epoch [1/1], Step [2532/3337], Loss: 5.8676\n",
      "Epoch [1/1], Step [2533/3337], Loss: 5.8547\n",
      "Epoch [1/1], Step [2534/3337], Loss: 5.9354\n",
      "Epoch [1/1], Step [2535/3337], Loss: 5.7998\n",
      "Epoch [1/1], Step [2536/3337], Loss: 5.8706\n",
      "Epoch [1/1], Step [2537/3337], Loss: 5.9415\n",
      "Epoch [1/1], Step [2538/3337], Loss: 5.7685\n",
      "Epoch [1/1], Step [2539/3337], Loss: 5.7410\n",
      "Epoch [1/1], Step [2540/3337], Loss: 5.8431\n",
      "Epoch [1/1], Step [2541/3337], Loss: 5.7338\n",
      "Epoch [1/1], Step [2542/3337], Loss: 5.8068\n",
      "Epoch [1/1], Step [2543/3337], Loss: 6.0071\n",
      "Epoch [1/1], Step [2544/3337], Loss: 5.8965\n",
      "Epoch [1/1], Step [2545/3337], Loss: 5.9176\n",
      "Epoch [1/1], Step [2546/3337], Loss: 5.9282\n",
      "Epoch [1/1], Step [2547/3337], Loss: 5.7259\n",
      "Epoch [1/1], Step [2548/3337], Loss: 6.0199\n",
      "Epoch [1/1], Step [2549/3337], Loss: 5.8961\n",
      "Epoch [1/1], Step [2550/3337], Loss: 5.7771\n",
      "Epoch [1/1], Step [2551/3337], Loss: 5.8331\n",
      "Epoch [1/1], Step [2552/3337], Loss: 5.8921\n",
      "Epoch [1/1], Step [2553/3337], Loss: 5.9934\n",
      "Epoch [1/1], Step [2554/3337], Loss: 5.8068\n",
      "Epoch [1/1], Step [2555/3337], Loss: 5.8849\n",
      "Epoch [1/1], Step [2556/3337], Loss: 5.9798\n",
      "Epoch [1/1], Step [2557/3337], Loss: 5.7646\n",
      "Epoch [1/1], Step [2558/3337], Loss: 5.8382\n",
      "Epoch [1/1], Step [2559/3337], Loss: 5.7741\n",
      "Epoch [1/1], Step [2560/3337], Loss: 5.8146\n",
      "Epoch [1/1], Step [2561/3337], Loss: 5.8535\n",
      "Epoch [1/1], Step [2562/3337], Loss: 5.7853\n",
      "Epoch [1/1], Step [2563/3337], Loss: 5.9684\n",
      "Epoch [1/1], Step [2564/3337], Loss: 5.8608\n",
      "Epoch [1/1], Step [2565/3337], Loss: 5.9968\n",
      "Epoch [1/1], Step [2566/3337], Loss: 5.9536\n",
      "Epoch [1/1], Step [2567/3337], Loss: 5.8663\n",
      "Epoch [1/1], Step [2568/3337], Loss: 6.0333\n",
      "Epoch [1/1], Step [2569/3337], Loss: 5.7053\n",
      "Epoch [1/1], Step [2570/3337], Loss: 5.8529\n",
      "Epoch [1/1], Step [2571/3337], Loss: 5.8575\n",
      "Epoch [1/1], Step [2572/3337], Loss: 5.8875\n",
      "Epoch [1/1], Step [2573/3337], Loss: 5.8340\n",
      "Epoch [1/1], Step [2574/3337], Loss: 5.7975\n",
      "Epoch [1/1], Step [2575/3337], Loss: 5.7959\n",
      "Epoch [1/1], Step [2576/3337], Loss: 5.8919\n",
      "Epoch [1/1], Step [2577/3337], Loss: 5.9291\n",
      "Epoch [1/1], Step [2578/3337], Loss: 5.9289\n",
      "Epoch [1/1], Step [2579/3337], Loss: 6.0342\n",
      "Epoch [1/1], Step [2580/3337], Loss: 5.7848\n",
      "Epoch [1/1], Step [2581/3337], Loss: 5.8145\n",
      "Epoch [1/1], Step [2582/3337], Loss: 5.8275\n",
      "Epoch [1/1], Step [2583/3337], Loss: 5.9218\n",
      "Epoch [1/1], Step [2584/3337], Loss: 5.8796\n",
      "Epoch [1/1], Step [2585/3337], Loss: 5.8550\n",
      "Epoch [1/1], Step [2586/3337], Loss: 5.8973\n",
      "Epoch [1/1], Step [2587/3337], Loss: 5.8824\n",
      "Epoch [1/1], Step [2588/3337], Loss: 5.9483\n",
      "Epoch [1/1], Step [2589/3337], Loss: 5.9690\n",
      "Epoch [1/1], Step [2590/3337], Loss: 5.9506\n",
      "Epoch [1/1], Step [2591/3337], Loss: 5.8336\n",
      "Epoch [1/1], Step [2592/3337], Loss: 5.9156\n",
      "Epoch [1/1], Step [2593/3337], Loss: 5.8399\n",
      "Epoch [1/1], Step [2594/3337], Loss: 5.8648\n",
      "Epoch [1/1], Step [2595/3337], Loss: 5.8241\n",
      "Epoch [1/1], Step [2596/3337], Loss: 5.8756\n",
      "Epoch [1/1], Step [2597/3337], Loss: 5.8218\n",
      "Epoch [1/1], Step [2598/3337], Loss: 5.9803\n",
      "Epoch [1/1], Step [2599/3337], Loss: 5.8780\n",
      "Epoch [1/1], Step [2600/3337], Loss: 5.8740\n",
      "Epoch [1/1], Step [2601/3337], Loss: 5.7370\n",
      "Epoch [1/1], Step [2602/3337], Loss: 5.8337\n",
      "Epoch [1/1], Step [2603/3337], Loss: 5.8180\n",
      "Epoch [1/1], Step [2604/3337], Loss: 5.8614\n",
      "Epoch [1/1], Step [2605/3337], Loss: 5.9032\n",
      "Epoch [1/1], Step [2606/3337], Loss: 5.9195\n",
      "Epoch [1/1], Step [2607/3337], Loss: 5.8122\n",
      "Epoch [1/1], Step [2608/3337], Loss: 5.7111\n",
      "Epoch [1/1], Step [2609/3337], Loss: 6.0196\n",
      "Epoch [1/1], Step [2610/3337], Loss: 5.8192\n",
      "Epoch [1/1], Step [2611/3337], Loss: 5.9877\n",
      "Epoch [1/1], Step [2612/3337], Loss: 5.8457\n",
      "Epoch [1/1], Step [2613/3337], Loss: 5.8149\n",
      "Epoch [1/1], Step [2614/3337], Loss: 5.8198\n",
      "Epoch [1/1], Step [2615/3337], Loss: 5.8548\n",
      "Epoch [1/1], Step [2616/3337], Loss: 5.9846\n",
      "Epoch [1/1], Step [2617/3337], Loss: 5.6946\n",
      "Epoch [1/1], Step [2618/3337], Loss: 5.7240\n",
      "Epoch [1/1], Step [2619/3337], Loss: 5.7731\n",
      "Epoch [1/1], Step [2620/3337], Loss: 5.9063\n",
      "Epoch [1/1], Step [2621/3337], Loss: 5.7898\n",
      "Epoch [1/1], Step [2622/3337], Loss: 5.9500\n",
      "Epoch [1/1], Step [2623/3337], Loss: 5.8667\n",
      "Epoch [1/1], Step [2624/3337], Loss: 5.9256\n",
      "Epoch [1/1], Step [2625/3337], Loss: 5.7952\n",
      "Epoch [1/1], Step [2626/3337], Loss: 5.8679\n",
      "Epoch [1/1], Step [2627/3337], Loss: 5.8418\n",
      "Epoch [1/1], Step [2628/3337], Loss: 5.7619\n",
      "Epoch [1/1], Step [2629/3337], Loss: 5.7300\n",
      "Epoch [1/1], Step [2630/3337], Loss: 5.8501\n",
      "Epoch [1/1], Step [2631/3337], Loss: 5.7801\n",
      "Epoch [1/1], Step [2632/3337], Loss: 5.8494\n",
      "Epoch [1/1], Step [2633/3337], Loss: 5.9070\n",
      "Epoch [1/1], Step [2634/3337], Loss: 5.9555\n",
      "Epoch [1/1], Step [2635/3337], Loss: 5.9190\n",
      "Epoch [1/1], Step [2636/3337], Loss: 5.9393\n",
      "Epoch [1/1], Step [2637/3337], Loss: 5.9174\n",
      "Epoch [1/1], Step [2638/3337], Loss: 5.8296\n",
      "Epoch [1/1], Step [2639/3337], Loss: 5.8998\n",
      "Epoch [1/1], Step [2640/3337], Loss: 5.6460\n",
      "Epoch [1/1], Step [2641/3337], Loss: 5.7808\n",
      "Epoch [1/1], Step [2642/3337], Loss: 5.8377\n",
      "Epoch [1/1], Step [2643/3337], Loss: 5.8943\n",
      "Epoch [1/1], Step [2644/3337], Loss: 5.9195\n",
      "Epoch [1/1], Step [2645/3337], Loss: 6.0834\n",
      "Epoch [1/1], Step [2646/3337], Loss: 5.8905\n",
      "Epoch [1/1], Step [2647/3337], Loss: 5.8454\n",
      "Epoch [1/1], Step [2648/3337], Loss: 5.7813\n",
      "Epoch [1/1], Step [2649/3337], Loss: 5.6600\n",
      "Epoch [1/1], Step [2650/3337], Loss: 5.9368\n",
      "Epoch [1/1], Step [2651/3337], Loss: 5.8935\n",
      "Epoch [1/1], Step [2652/3337], Loss: 5.8105\n",
      "Epoch [1/1], Step [2653/3337], Loss: 6.0385\n",
      "Epoch [1/1], Step [2654/3337], Loss: 5.7451\n",
      "Epoch [1/1], Step [2655/3337], Loss: 5.7912\n",
      "Epoch [1/1], Step [2656/3337], Loss: 5.7200\n",
      "Epoch [1/1], Step [2657/3337], Loss: 5.8659\n",
      "Epoch [1/1], Step [2658/3337], Loss: 5.8213\n",
      "Epoch [1/1], Step [2659/3337], Loss: 5.8978\n",
      "Epoch [1/1], Step [2660/3337], Loss: 5.9333\n",
      "Epoch [1/1], Step [2661/3337], Loss: 5.9389\n",
      "Epoch [1/1], Step [2662/3337], Loss: 5.8370\n",
      "Epoch [1/1], Step [2663/3337], Loss: 5.8343\n",
      "Epoch [1/1], Step [2664/3337], Loss: 5.8034\n",
      "Epoch [1/1], Step [2665/3337], Loss: 5.8988\n",
      "Epoch [1/1], Step [2666/3337], Loss: 5.8720\n",
      "Epoch [1/1], Step [2667/3337], Loss: 5.9235\n",
      "Epoch [1/1], Step [2668/3337], Loss: 5.7021\n",
      "Epoch [1/1], Step [2669/3337], Loss: 5.9556\n",
      "Epoch [1/1], Step [2670/3337], Loss: 5.8142\n",
      "Epoch [1/1], Step [2671/3337], Loss: 5.8919\n",
      "Epoch [1/1], Step [2672/3337], Loss: 5.9040\n",
      "Epoch [1/1], Step [2673/3337], Loss: 5.9808\n",
      "Epoch [1/1], Step [2674/3337], Loss: 5.7891\n",
      "Epoch [1/1], Step [2675/3337], Loss: 5.7426\n",
      "Epoch [1/1], Step [2676/3337], Loss: 5.8041\n",
      "Epoch [1/1], Step [2677/3337], Loss: 5.7605\n",
      "Epoch [1/1], Step [2678/3337], Loss: 5.8462\n",
      "Epoch [1/1], Step [2679/3337], Loss: 5.7989\n",
      "Epoch [1/1], Step [2680/3337], Loss: 5.7637\n",
      "Epoch [1/1], Step [2681/3337], Loss: 5.7942\n",
      "Epoch [1/1], Step [2682/3337], Loss: 5.8032\n",
      "Epoch [1/1], Step [2683/3337], Loss: 5.9407\n",
      "Epoch [1/1], Step [2684/3337], Loss: 5.8815\n",
      "Epoch [1/1], Step [2685/3337], Loss: 5.8130\n",
      "Epoch [1/1], Step [2686/3337], Loss: 5.9275\n",
      "Epoch [1/1], Step [2687/3337], Loss: 5.9915\n",
      "Epoch [1/1], Step [2688/3337], Loss: 5.8087\n",
      "Epoch [1/1], Step [2689/3337], Loss: 5.8969\n",
      "Epoch [1/1], Step [2690/3337], Loss: 5.8617\n",
      "Epoch [1/1], Step [2691/3337], Loss: 5.8267\n",
      "Epoch [1/1], Step [2692/3337], Loss: 5.7965\n",
      "Epoch [1/1], Step [2693/3337], Loss: 5.8922\n",
      "Epoch [1/1], Step [2694/3337], Loss: 5.8573\n",
      "Epoch [1/1], Step [2695/3337], Loss: 5.8942\n",
      "Epoch [1/1], Step [2696/3337], Loss: 5.8026\n",
      "Epoch [1/1], Step [2697/3337], Loss: 5.7266\n",
      "Epoch [1/1], Step [2698/3337], Loss: 5.8584\n",
      "Epoch [1/1], Step [2699/3337], Loss: 5.8249\n",
      "Epoch [1/1], Step [2700/3337], Loss: 5.8247\n",
      "Epoch [1/1], Step [2701/3337], Loss: 5.9212\n",
      "Epoch [1/1], Step [2702/3337], Loss: 5.6483\n",
      "Epoch [1/1], Step [2703/3337], Loss: 5.9236\n",
      "Epoch [1/1], Step [2704/3337], Loss: 5.8279\n",
      "Epoch [1/1], Step [2705/3337], Loss: 5.9179\n",
      "Epoch [1/1], Step [2706/3337], Loss: 5.5871\n",
      "Epoch [1/1], Step [2707/3337], Loss: 5.8456\n",
      "Epoch [1/1], Step [2708/3337], Loss: 5.9860\n",
      "Epoch [1/1], Step [2709/3337], Loss: 5.8690\n",
      "Epoch [1/1], Step [2710/3337], Loss: 5.7283\n",
      "Epoch [1/1], Step [2711/3337], Loss: 5.8789\n",
      "Epoch [1/1], Step [2712/3337], Loss: 5.9210\n",
      "Epoch [1/1], Step [2713/3337], Loss: 5.6554\n",
      "Epoch [1/1], Step [2714/3337], Loss: 5.8051\n",
      "Epoch [1/1], Step [2715/3337], Loss: 5.7914\n",
      "Epoch [1/1], Step [2716/3337], Loss: 5.7709\n",
      "Epoch [1/1], Step [2717/3337], Loss: 5.8319\n",
      "Epoch [1/1], Step [2718/3337], Loss: 5.8552\n",
      "Epoch [1/1], Step [2719/3337], Loss: 5.8641\n",
      "Epoch [1/1], Step [2720/3337], Loss: 5.9660\n",
      "Epoch [1/1], Step [2721/3337], Loss: 5.8645\n",
      "Epoch [1/1], Step [2722/3337], Loss: 5.9904\n",
      "Epoch [1/1], Step [2723/3337], Loss: 5.8563\n",
      "Epoch [1/1], Step [2724/3337], Loss: 5.9145\n",
      "Epoch [1/1], Step [2725/3337], Loss: 5.9350\n",
      "Epoch [1/1], Step [2726/3337], Loss: 5.8480\n",
      "Epoch [1/1], Step [2727/3337], Loss: 5.9072\n",
      "Epoch [1/1], Step [2728/3337], Loss: 5.9159\n",
      "Epoch [1/1], Step [2729/3337], Loss: 5.8328\n",
      "Epoch [1/1], Step [2730/3337], Loss: 5.7922\n",
      "Epoch [1/1], Step [2731/3337], Loss: 5.8761\n",
      "Epoch [1/1], Step [2732/3337], Loss: 5.9157\n",
      "Epoch [1/1], Step [2733/3337], Loss: 5.8443\n",
      "Epoch [1/1], Step [2734/3337], Loss: 5.8936\n",
      "Epoch [1/1], Step [2735/3337], Loss: 5.8563\n",
      "Epoch [1/1], Step [2736/3337], Loss: 5.9203\n",
      "Epoch [1/1], Step [2737/3337], Loss: 5.7790\n",
      "Epoch [1/1], Step [2738/3337], Loss: 5.8408\n",
      "Epoch [1/1], Step [2739/3337], Loss: 5.7574\n",
      "Epoch [1/1], Step [2740/3337], Loss: 5.8683\n",
      "Epoch [1/1], Step [2741/3337], Loss: 5.8962\n",
      "Epoch [1/1], Step [2742/3337], Loss: 5.9495\n",
      "Epoch [1/1], Step [2743/3337], Loss: 5.8192\n",
      "Epoch [1/1], Step [2744/3337], Loss: 5.9453\n",
      "Epoch [1/1], Step [2745/3337], Loss: 5.8041\n",
      "Epoch [1/1], Step [2746/3337], Loss: 5.9676\n",
      "Epoch [1/1], Step [2747/3337], Loss: 5.8275\n",
      "Epoch [1/1], Step [2748/3337], Loss: 5.8789\n",
      "Epoch [1/1], Step [2749/3337], Loss: 5.9485\n",
      "Epoch [1/1], Step [2750/3337], Loss: 6.0293\n",
      "Epoch [1/1], Step [2751/3337], Loss: 5.7426\n",
      "Epoch [1/1], Step [2752/3337], Loss: 5.7674\n",
      "Epoch [1/1], Step [2753/3337], Loss: 5.8386\n",
      "Epoch [1/1], Step [2754/3337], Loss: 5.8411\n",
      "Epoch [1/1], Step [2755/3337], Loss: 5.9428\n",
      "Epoch [1/1], Step [2756/3337], Loss: 5.9714\n",
      "Epoch [1/1], Step [2757/3337], Loss: 5.9411\n",
      "Epoch [1/1], Step [2758/3337], Loss: 5.8224\n",
      "Epoch [1/1], Step [2759/3337], Loss: 5.8526\n",
      "Epoch [1/1], Step [2760/3337], Loss: 5.8583\n",
      "Epoch [1/1], Step [2761/3337], Loss: 5.7986\n",
      "Epoch [1/1], Step [2762/3337], Loss: 5.8208\n",
      "Epoch [1/1], Step [2763/3337], Loss: 5.8751\n",
      "Epoch [1/1], Step [2764/3337], Loss: 5.9048\n",
      "Epoch [1/1], Step [2765/3337], Loss: 5.9786\n",
      "Epoch [1/1], Step [2766/3337], Loss: 5.7936\n",
      "Epoch [1/1], Step [2767/3337], Loss: 5.8845\n",
      "Epoch [1/1], Step [2768/3337], Loss: 5.7579\n",
      "Epoch [1/1], Step [2769/3337], Loss: 5.8289\n",
      "Epoch [1/1], Step [2770/3337], Loss: 5.8748\n",
      "Epoch [1/1], Step [2771/3337], Loss: 5.8837\n",
      "Epoch [1/1], Step [2772/3337], Loss: 5.8072\n",
      "Epoch [1/1], Step [2773/3337], Loss: 5.8687\n",
      "Epoch [1/1], Step [2774/3337], Loss: 5.8288\n",
      "Epoch [1/1], Step [2775/3337], Loss: 5.7026\n",
      "Epoch [1/1], Step [2776/3337], Loss: 5.8128\n",
      "Epoch [1/1], Step [2777/3337], Loss: 5.8014\n",
      "Epoch [1/1], Step [2778/3337], Loss: 5.7302\n",
      "Epoch [1/1], Step [2779/3337], Loss: 5.8754\n",
      "Epoch [1/1], Step [2780/3337], Loss: 5.7896\n",
      "Epoch [1/1], Step [2781/3337], Loss: 5.8003\n",
      "Epoch [1/1], Step [2782/3337], Loss: 5.8091\n",
      "Epoch [1/1], Step [2783/3337], Loss: 5.9538\n",
      "Epoch [1/1], Step [2784/3337], Loss: 5.7040\n",
      "Epoch [1/1], Step [2785/3337], Loss: 5.7236\n",
      "Epoch [1/1], Step [2786/3337], Loss: 5.8280\n",
      "Epoch [1/1], Step [2787/3337], Loss: 5.8780\n",
      "Epoch [1/1], Step [2788/3337], Loss: 5.6782\n",
      "Epoch [1/1], Step [2789/3337], Loss: 5.8357\n",
      "Epoch [1/1], Step [2790/3337], Loss: 5.8915\n",
      "Epoch [1/1], Step [2791/3337], Loss: 5.7361\n",
      "Epoch [1/1], Step [2792/3337], Loss: 5.9235\n",
      "Epoch [1/1], Step [2793/3337], Loss: 5.8232\n",
      "Epoch [1/1], Step [2794/3337], Loss: 5.7665\n",
      "Epoch [1/1], Step [2795/3337], Loss: 5.8501\n",
      "Epoch [1/1], Step [2796/3337], Loss: 5.9846\n",
      "Epoch [1/1], Step [2797/3337], Loss: 5.9834\n",
      "Epoch [1/1], Step [2798/3337], Loss: 5.8518\n",
      "Epoch [1/1], Step [2799/3337], Loss: 5.7696\n",
      "Epoch [1/1], Step [2800/3337], Loss: 5.8752\n",
      "Epoch [1/1], Step [2801/3337], Loss: 5.8658\n",
      "Epoch [1/1], Step [2802/3337], Loss: 5.8712\n",
      "Epoch [1/1], Step [2803/3337], Loss: 5.8453\n",
      "Epoch [1/1], Step [2804/3337], Loss: 5.8511\n",
      "Epoch [1/1], Step [2805/3337], Loss: 5.9322\n",
      "Epoch [1/1], Step [2806/3337], Loss: 5.8946\n",
      "Epoch [1/1], Step [2807/3337], Loss: 5.7599\n",
      "Epoch [1/1], Step [2808/3337], Loss: 5.8463\n",
      "Epoch [1/1], Step [2809/3337], Loss: 5.8934\n",
      "Epoch [1/1], Step [2810/3337], Loss: 5.6084\n",
      "Epoch [1/1], Step [2811/3337], Loss: 5.9332\n",
      "Epoch [1/1], Step [2812/3337], Loss: 5.7933\n",
      "Epoch [1/1], Step [2813/3337], Loss: 5.9213\n",
      "Epoch [1/1], Step [2814/3337], Loss: 5.7818\n",
      "Epoch [1/1], Step [2815/3337], Loss: 5.7634\n",
      "Epoch [1/1], Step [2816/3337], Loss: 5.8675\n",
      "Epoch [1/1], Step [2817/3337], Loss: 5.8218\n",
      "Epoch [1/1], Step [2818/3337], Loss: 5.8134\n",
      "Epoch [1/1], Step [2819/3337], Loss: 5.8994\n",
      "Epoch [1/1], Step [2820/3337], Loss: 5.7888\n",
      "Epoch [1/1], Step [2821/3337], Loss: 5.7994\n",
      "Epoch [1/1], Step [2822/3337], Loss: 5.9973\n",
      "Epoch [1/1], Step [2823/3337], Loss: 5.8491\n",
      "Epoch [1/1], Step [2824/3337], Loss: 5.7374\n",
      "Epoch [1/1], Step [2825/3337], Loss: 5.6862\n",
      "Epoch [1/1], Step [2826/3337], Loss: 5.8612\n",
      "Epoch [1/1], Step [2827/3337], Loss: 5.8310\n",
      "Epoch [1/1], Step [2828/3337], Loss: 5.8236\n",
      "Epoch [1/1], Step [2829/3337], Loss: 5.9273\n",
      "Epoch [1/1], Step [2830/3337], Loss: 5.7338\n",
      "Epoch [1/1], Step [2831/3337], Loss: 5.8273\n",
      "Epoch [1/1], Step [2832/3337], Loss: 5.8466\n",
      "Epoch [1/1], Step [2833/3337], Loss: 5.7790\n",
      "Epoch [1/1], Step [2834/3337], Loss: 5.6825\n",
      "Epoch [1/1], Step [2835/3337], Loss: 5.7111\n",
      "Epoch [1/1], Step [2836/3337], Loss: 5.7337\n",
      "Epoch [1/1], Step [2837/3337], Loss: 5.9650\n",
      "Epoch [1/1], Step [2838/3337], Loss: 5.8111\n",
      "Epoch [1/1], Step [2839/3337], Loss: 5.7146\n",
      "Epoch [1/1], Step [2840/3337], Loss: 5.7918\n",
      "Epoch [1/1], Step [2841/3337], Loss: 5.8136\n",
      "Epoch [1/1], Step [2842/3337], Loss: 5.8500\n",
      "Epoch [1/1], Step [2843/3337], Loss: 5.8276\n",
      "Epoch [1/1], Step [2844/3337], Loss: 5.6698\n",
      "Epoch [1/1], Step [2845/3337], Loss: 5.9280\n",
      "Epoch [1/1], Step [2846/3337], Loss: 5.8342\n",
      "Epoch [1/1], Step [2847/3337], Loss: 5.7858\n",
      "Epoch [1/1], Step [2848/3337], Loss: 5.8218\n",
      "Epoch [1/1], Step [2849/3337], Loss: 5.6407\n",
      "Epoch [1/1], Step [2850/3337], Loss: 5.7740\n",
      "Epoch [1/1], Step [2851/3337], Loss: 5.7204\n",
      "Epoch [1/1], Step [2852/3337], Loss: 5.8169\n",
      "Epoch [1/1], Step [2853/3337], Loss: 5.8816\n",
      "Epoch [1/1], Step [2854/3337], Loss: 5.8531\n",
      "Epoch [1/1], Step [2855/3337], Loss: 5.6910\n",
      "Epoch [1/1], Step [2856/3337], Loss: 5.8054\n",
      "Epoch [1/1], Step [2857/3337], Loss: 5.7724\n",
      "Epoch [1/1], Step [2858/3337], Loss: 5.8053\n",
      "Epoch [1/1], Step [2859/3337], Loss: 5.8573\n",
      "Epoch [1/1], Step [2860/3337], Loss: 5.9370\n",
      "Epoch [1/1], Step [2861/3337], Loss: 5.7030\n",
      "Epoch [1/1], Step [2862/3337], Loss: 5.6944\n",
      "Epoch [1/1], Step [2863/3337], Loss: 5.7496\n",
      "Epoch [1/1], Step [2864/3337], Loss: 5.8390\n",
      "Epoch [1/1], Step [2865/3337], Loss: 5.8240\n",
      "Epoch [1/1], Step [2866/3337], Loss: 5.7074\n",
      "Epoch [1/1], Step [2867/3337], Loss: 5.9277\n",
      "Epoch [1/1], Step [2868/3337], Loss: 5.6676\n",
      "Epoch [1/1], Step [2869/3337], Loss: 5.8777\n",
      "Epoch [1/1], Step [2870/3337], Loss: 5.8031\n",
      "Epoch [1/1], Step [2871/3337], Loss: 5.8182\n",
      "Epoch [1/1], Step [2872/3337], Loss: 5.8760\n",
      "Epoch [1/1], Step [2873/3337], Loss: 5.8482\n",
      "Epoch [1/1], Step [2874/3337], Loss: 5.8135\n",
      "Epoch [1/1], Step [2875/3337], Loss: 5.7384\n",
      "Epoch [1/1], Step [2876/3337], Loss: 5.7245\n",
      "Epoch [1/1], Step [2877/3337], Loss: 5.9738\n",
      "Epoch [1/1], Step [2878/3337], Loss: 5.7294\n",
      "Epoch [1/1], Step [2879/3337], Loss: 5.8002\n",
      "Epoch [1/1], Step [2880/3337], Loss: 5.7552\n",
      "Epoch [1/1], Step [2881/3337], Loss: 5.8828\n",
      "Epoch [1/1], Step [2882/3337], Loss: 5.7662\n",
      "Epoch [1/1], Step [2883/3337], Loss: 5.8684\n",
      "Epoch [1/1], Step [2884/3337], Loss: 5.7466\n",
      "Epoch [1/1], Step [2885/3337], Loss: 5.7678\n",
      "Epoch [1/1], Step [2886/3337], Loss: 5.7393\n",
      "Epoch [1/1], Step [2887/3337], Loss: 5.9362\n",
      "Epoch [1/1], Step [2888/3337], Loss: 5.9445\n",
      "Epoch [1/1], Step [2889/3337], Loss: 5.8245\n",
      "Epoch [1/1], Step [2890/3337], Loss: 5.7284\n",
      "Epoch [1/1], Step [2891/3337], Loss: 5.7681\n",
      "Epoch [1/1], Step [2892/3337], Loss: 5.8343\n",
      "Epoch [1/1], Step [2893/3337], Loss: 5.9080\n",
      "Epoch [1/1], Step [2894/3337], Loss: 5.8397\n",
      "Epoch [1/1], Step [2895/3337], Loss: 5.7933\n",
      "Epoch [1/1], Step [2896/3337], Loss: 5.8990\n",
      "Epoch [1/1], Step [2897/3337], Loss: 5.7860\n",
      "Epoch [1/1], Step [2898/3337], Loss: 5.8026\n",
      "Epoch [1/1], Step [2899/3337], Loss: 5.8581\n",
      "Epoch [1/1], Step [2900/3337], Loss: 5.8590\n",
      "Epoch [1/1], Step [2901/3337], Loss: 5.7435\n",
      "Epoch [1/1], Step [2902/3337], Loss: 5.8235\n",
      "Epoch [1/1], Step [2903/3337], Loss: 5.7878\n",
      "Epoch [1/1], Step [2904/3337], Loss: 5.8323\n",
      "Epoch [1/1], Step [2905/3337], Loss: 5.7622\n",
      "Epoch [1/1], Step [2906/3337], Loss: 5.8290\n",
      "Epoch [1/1], Step [2907/3337], Loss: 5.8874\n",
      "Epoch [1/1], Step [2908/3337], Loss: 5.7036\n",
      "Epoch [1/1], Step [2909/3337], Loss: 5.7156\n",
      "Epoch [1/1], Step [2910/3337], Loss: 5.7028\n",
      "Epoch [1/1], Step [2911/3337], Loss: 5.7415\n",
      "Epoch [1/1], Step [2912/3337], Loss: 6.0194\n",
      "Epoch [1/1], Step [2913/3337], Loss: 5.8718\n",
      "Epoch [1/1], Step [2914/3337], Loss: 5.9427\n",
      "Epoch [1/1], Step [2915/3337], Loss: 5.8894\n",
      "Epoch [1/1], Step [2916/3337], Loss: 5.6963\n",
      "Epoch [1/1], Step [2917/3337], Loss: 5.9149\n",
      "Epoch [1/1], Step [2918/3337], Loss: 5.8341\n",
      "Epoch [1/1], Step [2919/3337], Loss: 5.9593\n",
      "Epoch [1/1], Step [2920/3337], Loss: 5.8817\n",
      "Epoch [1/1], Step [2921/3337], Loss: 5.7692\n",
      "Epoch [1/1], Step [2922/3337], Loss: 5.8535\n",
      "Epoch [1/1], Step [2923/3337], Loss: 5.8623\n",
      "Epoch [1/1], Step [2924/3337], Loss: 5.7261\n",
      "Epoch [1/1], Step [2925/3337], Loss: 5.7531\n",
      "Epoch [1/1], Step [2926/3337], Loss: 5.8658\n",
      "Epoch [1/1], Step [2927/3337], Loss: 5.8555\n",
      "Epoch [1/1], Step [2928/3337], Loss: 5.7978\n",
      "Epoch [1/1], Step [2929/3337], Loss: 5.6826\n",
      "Epoch [1/1], Step [2930/3337], Loss: 5.9032\n",
      "Epoch [1/1], Step [2931/3337], Loss: 5.8030\n",
      "Epoch [1/1], Step [2932/3337], Loss: 5.7857\n",
      "Epoch [1/1], Step [2933/3337], Loss: 5.9384\n",
      "Epoch [1/1], Step [2934/3337], Loss: 5.7434\n",
      "Epoch [1/1], Step [2935/3337], Loss: 5.7533\n",
      "Epoch [1/1], Step [2936/3337], Loss: 5.8417\n",
      "Epoch [1/1], Step [2937/3337], Loss: 5.8262\n",
      "Epoch [1/1], Step [2938/3337], Loss: 5.9837\n",
      "Epoch [1/1], Step [2939/3337], Loss: 5.8679\n",
      "Epoch [1/1], Step [2940/3337], Loss: 5.8753\n",
      "Epoch [1/1], Step [2941/3337], Loss: 5.7944\n",
      "Epoch [1/1], Step [2942/3337], Loss: 5.9572\n",
      "Epoch [1/1], Step [2943/3337], Loss: 5.7538\n",
      "Epoch [1/1], Step [2944/3337], Loss: 5.7522\n",
      "Epoch [1/1], Step [2945/3337], Loss: 5.6835\n",
      "Epoch [1/1], Step [2946/3337], Loss: 5.8061\n",
      "Epoch [1/1], Step [2947/3337], Loss: 5.8778\n",
      "Epoch [1/1], Step [2948/3337], Loss: 5.6896\n",
      "Epoch [1/1], Step [2949/3337], Loss: 5.7630\n",
      "Epoch [1/1], Step [2950/3337], Loss: 5.9121\n",
      "Epoch [1/1], Step [2951/3337], Loss: 5.7353\n",
      "Epoch [1/1], Step [2952/3337], Loss: 5.7597\n",
      "Epoch [1/1], Step [2953/3337], Loss: 5.7034\n",
      "Epoch [1/1], Step [2954/3337], Loss: 5.8152\n",
      "Epoch [1/1], Step [2955/3337], Loss: 5.7640\n",
      "Epoch [1/1], Step [2956/3337], Loss: 5.9337\n",
      "Epoch [1/1], Step [2957/3337], Loss: 5.8221\n",
      "Epoch [1/1], Step [2958/3337], Loss: 5.7995\n",
      "Epoch [1/1], Step [2959/3337], Loss: 5.7374\n",
      "Epoch [1/1], Step [2960/3337], Loss: 5.7217\n",
      "Epoch [1/1], Step [2961/3337], Loss: 5.7912\n",
      "Epoch [1/1], Step [2962/3337], Loss: 5.8078\n",
      "Epoch [1/1], Step [2963/3337], Loss: 5.8540\n",
      "Epoch [1/1], Step [2964/3337], Loss: 5.6947\n",
      "Epoch [1/1], Step [2965/3337], Loss: 5.8007\n",
      "Epoch [1/1], Step [2966/3337], Loss: 5.8170\n",
      "Epoch [1/1], Step [2967/3337], Loss: 5.7425\n",
      "Epoch [1/1], Step [2968/3337], Loss: 5.7975\n",
      "Epoch [1/1], Step [2969/3337], Loss: 5.8100\n",
      "Epoch [1/1], Step [2970/3337], Loss: 5.6375\n",
      "Epoch [1/1], Step [2971/3337], Loss: 5.7584\n",
      "Epoch [1/1], Step [2972/3337], Loss: 5.7771\n",
      "Epoch [1/1], Step [2973/3337], Loss: 5.9194\n",
      "Epoch [1/1], Step [2974/3337], Loss: 5.7049\n",
      "Epoch [1/1], Step [2975/3337], Loss: 5.7228\n",
      "Epoch [1/1], Step [2976/3337], Loss: 5.7973\n",
      "Epoch [1/1], Step [2977/3337], Loss: 5.7981\n",
      "Epoch [1/1], Step [2978/3337], Loss: 5.7963\n",
      "Epoch [1/1], Step [2979/3337], Loss: 5.7939\n",
      "Epoch [1/1], Step [2980/3337], Loss: 5.6897\n",
      "Epoch [1/1], Step [2981/3337], Loss: 5.6699\n",
      "Epoch [1/1], Step [2982/3337], Loss: 5.7334\n",
      "Epoch [1/1], Step [2983/3337], Loss: 5.9175\n",
      "Epoch [1/1], Step [2984/3337], Loss: 5.8646\n",
      "Epoch [1/1], Step [2985/3337], Loss: 5.6686\n",
      "Epoch [1/1], Step [2986/3337], Loss: 5.8080\n",
      "Epoch [1/1], Step [2987/3337], Loss: 5.7968\n",
      "Epoch [1/1], Step [2988/3337], Loss: 5.8647\n",
      "Epoch [1/1], Step [2989/3337], Loss: 5.9718\n",
      "Epoch [1/1], Step [2990/3337], Loss: 5.6696\n",
      "Epoch [1/1], Step [2991/3337], Loss: 5.8517\n",
      "Epoch [1/1], Step [2992/3337], Loss: 5.7495\n",
      "Epoch [1/1], Step [2993/3337], Loss: 5.8075\n",
      "Epoch [1/1], Step [2994/3337], Loss: 5.7647\n",
      "Epoch [1/1], Step [2995/3337], Loss: 5.8099\n",
      "Epoch [1/1], Step [2996/3337], Loss: 5.7371\n",
      "Epoch [1/1], Step [2997/3337], Loss: 5.8130\n",
      "Epoch [1/1], Step [2998/3337], Loss: 5.8310\n",
      "Epoch [1/1], Step [2999/3337], Loss: 5.7583\n",
      "Epoch [1/1], Step [3000/3337], Loss: 5.7157\n",
      "Epoch [1/1], Step [3001/3337], Loss: 5.8242\n",
      "Epoch [1/1], Step [3002/3337], Loss: 5.8470\n",
      "Epoch [1/1], Step [3003/3337], Loss: 5.8395\n",
      "Epoch [1/1], Step [3004/3337], Loss: 5.7815\n",
      "Epoch [1/1], Step [3005/3337], Loss: 5.8552\n",
      "Epoch [1/1], Step [3006/3337], Loss: 5.7166\n",
      "Epoch [1/1], Step [3007/3337], Loss: 5.7491\n",
      "Epoch [1/1], Step [3008/3337], Loss: 5.7898\n",
      "Epoch [1/1], Step [3009/3337], Loss: 5.7805\n",
      "Epoch [1/1], Step [3010/3337], Loss: 5.7690\n",
      "Epoch [1/1], Step [3011/3337], Loss: 5.9043\n",
      "Epoch [1/1], Step [3012/3337], Loss: 5.7116\n",
      "Epoch [1/1], Step [3013/3337], Loss: 5.8621\n",
      "Epoch [1/1], Step [3014/3337], Loss: 5.5851\n",
      "Epoch [1/1], Step [3015/3337], Loss: 5.8572\n",
      "Epoch [1/1], Step [3016/3337], Loss: 5.8362\n",
      "Epoch [1/1], Step [3017/3337], Loss: 5.6717\n",
      "Epoch [1/1], Step [3018/3337], Loss: 5.7331\n",
      "Epoch [1/1], Step [3019/3337], Loss: 5.7106\n",
      "Epoch [1/1], Step [3020/3337], Loss: 5.5978\n",
      "Epoch [1/1], Step [3021/3337], Loss: 5.6749\n",
      "Epoch [1/1], Step [3022/3337], Loss: 5.8130\n",
      "Epoch [1/1], Step [3023/3337], Loss: 5.7423\n",
      "Epoch [1/1], Step [3024/3337], Loss: 5.8099\n",
      "Epoch [1/1], Step [3025/3337], Loss: 5.8128\n",
      "Epoch [1/1], Step [3026/3337], Loss: 5.7604\n",
      "Epoch [1/1], Step [3027/3337], Loss: 5.7435\n",
      "Epoch [1/1], Step [3028/3337], Loss: 5.7058\n",
      "Epoch [1/1], Step [3029/3337], Loss: 5.8794\n",
      "Epoch [1/1], Step [3030/3337], Loss: 5.6769\n",
      "Epoch [1/1], Step [3031/3337], Loss: 5.7404\n",
      "Epoch [1/1], Step [3032/3337], Loss: 5.7130\n",
      "Epoch [1/1], Step [3033/3337], Loss: 5.7714\n",
      "Epoch [1/1], Step [3034/3337], Loss: 5.8061\n",
      "Epoch [1/1], Step [3035/3337], Loss: 5.8038\n",
      "Epoch [1/1], Step [3036/3337], Loss: 5.8546\n",
      "Epoch [1/1], Step [3037/3337], Loss: 5.7784\n",
      "Epoch [1/1], Step [3038/3337], Loss: 5.8318\n",
      "Epoch [1/1], Step [3039/3337], Loss: 5.7245\n",
      "Epoch [1/1], Step [3040/3337], Loss: 5.7792\n",
      "Epoch [1/1], Step [3041/3337], Loss: 5.7291\n",
      "Epoch [1/1], Step [3042/3337], Loss: 5.7219\n",
      "Epoch [1/1], Step [3043/3337], Loss: 5.7327\n",
      "Epoch [1/1], Step [3044/3337], Loss: 5.8433\n",
      "Epoch [1/1], Step [3045/3337], Loss: 5.7976\n",
      "Epoch [1/1], Step [3046/3337], Loss: 5.8993\n",
      "Epoch [1/1], Step [3047/3337], Loss: 5.8630\n",
      "Epoch [1/1], Step [3048/3337], Loss: 5.5719\n",
      "Epoch [1/1], Step [3049/3337], Loss: 5.6499\n",
      "Epoch [1/1], Step [3050/3337], Loss: 5.6958\n",
      "Epoch [1/1], Step [3051/3337], Loss: 5.8402\n",
      "Epoch [1/1], Step [3052/3337], Loss: 5.6272\n",
      "Epoch [1/1], Step [3053/3337], Loss: 5.9057\n",
      "Epoch [1/1], Step [3054/3337], Loss: 5.7093\n",
      "Epoch [1/1], Step [3055/3337], Loss: 5.8129\n",
      "Epoch [1/1], Step [3056/3337], Loss: 5.7822\n",
      "Epoch [1/1], Step [3057/3337], Loss: 5.8791\n",
      "Epoch [1/1], Step [3058/3337], Loss: 5.6908\n",
      "Epoch [1/1], Step [3059/3337], Loss: 5.6891\n",
      "Epoch [1/1], Step [3060/3337], Loss: 5.8230\n",
      "Epoch [1/1], Step [3061/3337], Loss: 5.7368\n",
      "Epoch [1/1], Step [3062/3337], Loss: 5.7421\n",
      "Epoch [1/1], Step [3063/3337], Loss: 5.8770\n",
      "Epoch [1/1], Step [3064/3337], Loss: 5.7940\n",
      "Epoch [1/1], Step [3065/3337], Loss: 5.5876\n",
      "Epoch [1/1], Step [3066/3337], Loss: 5.7137\n",
      "Epoch [1/1], Step [3067/3337], Loss: 5.7989\n",
      "Epoch [1/1], Step [3068/3337], Loss: 5.8691\n",
      "Epoch [1/1], Step [3069/3337], Loss: 5.7785\n",
      "Epoch [1/1], Step [3070/3337], Loss: 5.8501\n",
      "Epoch [1/1], Step [3071/3337], Loss: 5.8421\n",
      "Epoch [1/1], Step [3072/3337], Loss: 5.6909\n",
      "Epoch [1/1], Step [3073/3337], Loss: 5.7945\n",
      "Epoch [1/1], Step [3074/3337], Loss: 5.7988\n",
      "Epoch [1/1], Step [3075/3337], Loss: 5.7844\n",
      "Epoch [1/1], Step [3076/3337], Loss: 5.8486\n",
      "Epoch [1/1], Step [3077/3337], Loss: 5.7391\n",
      "Epoch [1/1], Step [3078/3337], Loss: 5.6841\n",
      "Epoch [1/1], Step [3079/3337], Loss: 5.7812\n",
      "Epoch [1/1], Step [3080/3337], Loss: 5.8770\n",
      "Epoch [1/1], Step [3081/3337], Loss: 5.8689\n",
      "Epoch [1/1], Step [3082/3337], Loss: 5.8299\n",
      "Epoch [1/1], Step [3083/3337], Loss: 5.8543\n",
      "Epoch [1/1], Step [3084/3337], Loss: 5.7409\n",
      "Epoch [1/1], Step [3085/3337], Loss: 5.7242\n",
      "Epoch [1/1], Step [3086/3337], Loss: 5.8993\n",
      "Epoch [1/1], Step [3087/3337], Loss: 5.6545\n",
      "Epoch [1/1], Step [3088/3337], Loss: 5.8003\n",
      "Epoch [1/1], Step [3089/3337], Loss: 5.7123\n",
      "Epoch [1/1], Step [3090/3337], Loss: 5.8413\n",
      "Epoch [1/1], Step [3091/3337], Loss: 5.7169\n",
      "Epoch [1/1], Step [3092/3337], Loss: 5.6832\n",
      "Epoch [1/1], Step [3093/3337], Loss: 5.8236\n",
      "Epoch [1/1], Step [3094/3337], Loss: 5.6712\n",
      "Epoch [1/1], Step [3095/3337], Loss: 5.6060\n",
      "Epoch [1/1], Step [3096/3337], Loss: 5.8061\n",
      "Epoch [1/1], Step [3097/3337], Loss: 5.7581\n",
      "Epoch [1/1], Step [3098/3337], Loss: 5.6981\n",
      "Epoch [1/1], Step [3099/3337], Loss: 5.6916\n",
      "Epoch [1/1], Step [3100/3337], Loss: 5.6142\n",
      "Epoch [1/1], Step [3101/3337], Loss: 5.7288\n",
      "Epoch [1/1], Step [3102/3337], Loss: 5.8879\n",
      "Epoch [1/1], Step [3103/3337], Loss: 5.7034\n",
      "Epoch [1/1], Step [3104/3337], Loss: 5.6271\n",
      "Epoch [1/1], Step [3105/3337], Loss: 5.6767\n",
      "Epoch [1/1], Step [3106/3337], Loss: 5.8078\n",
      "Epoch [1/1], Step [3107/3337], Loss: 5.7056\n",
      "Epoch [1/1], Step [3108/3337], Loss: 5.8028\n",
      "Epoch [1/1], Step [3109/3337], Loss: 5.8667\n",
      "Epoch [1/1], Step [3110/3337], Loss: 5.7979\n",
      "Epoch [1/1], Step [3111/3337], Loss: 5.7898\n",
      "Epoch [1/1], Step [3112/3337], Loss: 5.5926\n",
      "Epoch [1/1], Step [3113/3337], Loss: 5.7147\n",
      "Epoch [1/1], Step [3114/3337], Loss: 5.7951\n",
      "Epoch [1/1], Step [3115/3337], Loss: 5.9119\n",
      "Epoch [1/1], Step [3116/3337], Loss: 5.6242\n",
      "Epoch [1/1], Step [3117/3337], Loss: 5.7517\n",
      "Epoch [1/1], Step [3118/3337], Loss: 5.7727\n",
      "Epoch [1/1], Step [3119/3337], Loss: 5.6947\n",
      "Epoch [1/1], Step [3120/3337], Loss: 5.6289\n",
      "Epoch [1/1], Step [3121/3337], Loss: 5.7774\n",
      "Epoch [1/1], Step [3122/3337], Loss: 5.8562\n",
      "Epoch [1/1], Step [3123/3337], Loss: 5.9184\n",
      "Epoch [1/1], Step [3124/3337], Loss: 5.8471\n",
      "Epoch [1/1], Step [3125/3337], Loss: 5.8611\n",
      "Epoch [1/1], Step [3126/3337], Loss: 5.7957\n",
      "Epoch [1/1], Step [3127/3337], Loss: 5.7791\n",
      "Epoch [1/1], Step [3128/3337], Loss: 5.6052\n",
      "Epoch [1/1], Step [3129/3337], Loss: 5.7053\n",
      "Epoch [1/1], Step [3130/3337], Loss: 5.8808\n",
      "Epoch [1/1], Step [3131/3337], Loss: 5.7520\n",
      "Epoch [1/1], Step [3132/3337], Loss: 5.7508\n",
      "Epoch [1/1], Step [3133/3337], Loss: 5.7230\n",
      "Epoch [1/1], Step [3134/3337], Loss: 5.6865\n",
      "Epoch [1/1], Step [3135/3337], Loss: 5.9006\n",
      "Epoch [1/1], Step [3136/3337], Loss: 5.8035\n",
      "Epoch [1/1], Step [3137/3337], Loss: 5.7279\n",
      "Epoch [1/1], Step [3138/3337], Loss: 5.7507\n",
      "Epoch [1/1], Step [3139/3337], Loss: 5.8412\n",
      "Epoch [1/1], Step [3140/3337], Loss: 5.6733\n",
      "Epoch [1/1], Step [3141/3337], Loss: 5.8486\n",
      "Epoch [1/1], Step [3142/3337], Loss: 5.7708\n",
      "Epoch [1/1], Step [3143/3337], Loss: 5.6413\n",
      "Epoch [1/1], Step [3144/3337], Loss: 5.7920\n",
      "Epoch [1/1], Step [3145/3337], Loss: 5.8812\n",
      "Epoch [1/1], Step [3146/3337], Loss: 5.6930\n",
      "Epoch [1/1], Step [3147/3337], Loss: 5.6692\n",
      "Epoch [1/1], Step [3148/3337], Loss: 5.5313\n",
      "Epoch [1/1], Step [3149/3337], Loss: 5.6130\n",
      "Epoch [1/1], Step [3150/3337], Loss: 5.7399\n",
      "Epoch [1/1], Step [3151/3337], Loss: 5.6943\n",
      "Epoch [1/1], Step [3152/3337], Loss: 5.8197\n",
      "Epoch [1/1], Step [3153/3337], Loss: 5.6536\n",
      "Epoch [1/1], Step [3154/3337], Loss: 5.7745\n",
      "Epoch [1/1], Step [3155/3337], Loss: 5.7731\n",
      "Epoch [1/1], Step [3156/3337], Loss: 5.9140\n",
      "Epoch [1/1], Step [3157/3337], Loss: 5.6651\n",
      "Epoch [1/1], Step [3158/3337], Loss: 5.6258\n",
      "Epoch [1/1], Step [3159/3337], Loss: 5.8294\n",
      "Epoch [1/1], Step [3160/3337], Loss: 5.7949\n",
      "Epoch [1/1], Step [3161/3337], Loss: 5.7136\n",
      "Epoch [1/1], Step [3162/3337], Loss: 5.6549\n",
      "Epoch [1/1], Step [3163/3337], Loss: 5.8879\n",
      "Epoch [1/1], Step [3164/3337], Loss: 5.6144\n",
      "Epoch [1/1], Step [3165/3337], Loss: 5.7005\n",
      "Epoch [1/1], Step [3166/3337], Loss: 5.7710\n",
      "Epoch [1/1], Step [3167/3337], Loss: 5.7040\n",
      "Epoch [1/1], Step [3168/3337], Loss: 5.6598\n",
      "Epoch [1/1], Step [3169/3337], Loss: 5.7086\n",
      "Epoch [1/1], Step [3170/3337], Loss: 5.6866\n",
      "Epoch [1/1], Step [3171/3337], Loss: 5.7796\n",
      "Epoch [1/1], Step [3172/3337], Loss: 5.8768\n",
      "Epoch [1/1], Step [3173/3337], Loss: 5.7468\n",
      "Epoch [1/1], Step [3174/3337], Loss: 5.7397\n",
      "Epoch [1/1], Step [3175/3337], Loss: 5.8338\n",
      "Epoch [1/1], Step [3176/3337], Loss: 5.7265\n",
      "Epoch [1/1], Step [3177/3337], Loss: 5.6922\n",
      "Epoch [1/1], Step [3178/3337], Loss: 5.7426\n",
      "Epoch [1/1], Step [3179/3337], Loss: 5.6312\n",
      "Epoch [1/1], Step [3180/3337], Loss: 5.7368\n",
      "Epoch [1/1], Step [3181/3337], Loss: 5.8527\n",
      "Epoch [1/1], Step [3182/3337], Loss: 5.6940\n",
      "Epoch [1/1], Step [3183/3337], Loss: 5.8634\n",
      "Epoch [1/1], Step [3184/3337], Loss: 5.7428\n",
      "Epoch [1/1], Step [3185/3337], Loss: 5.7545\n",
      "Epoch [1/1], Step [3186/3337], Loss: 5.8447\n",
      "Epoch [1/1], Step [3187/3337], Loss: 5.6936\n",
      "Epoch [1/1], Step [3188/3337], Loss: 5.7302\n",
      "Epoch [1/1], Step [3189/3337], Loss: 5.5564\n",
      "Epoch [1/1], Step [3190/3337], Loss: 5.7637\n",
      "Epoch [1/1], Step [3191/3337], Loss: 5.7666\n",
      "Epoch [1/1], Step [3192/3337], Loss: 5.5754\n",
      "Epoch [1/1], Step [3193/3337], Loss: 5.7972\n",
      "Epoch [1/1], Step [3194/3337], Loss: 5.6913\n",
      "Epoch [1/1], Step [3195/3337], Loss: 5.8521\n",
      "Epoch [1/1], Step [3196/3337], Loss: 5.6859\n",
      "Epoch [1/1], Step [3197/3337], Loss: 5.8263\n",
      "Epoch [1/1], Step [3198/3337], Loss: 5.8629\n",
      "Epoch [1/1], Step [3199/3337], Loss: 5.8552\n",
      "Epoch [1/1], Step [3200/3337], Loss: 5.7674\n",
      "Epoch [1/1], Step [3201/3337], Loss: 5.8662\n",
      "Epoch [1/1], Step [3202/3337], Loss: 5.8641\n",
      "Epoch [1/1], Step [3203/3337], Loss: 5.5750\n",
      "Epoch [1/1], Step [3204/3337], Loss: 5.6841\n",
      "Epoch [1/1], Step [3205/3337], Loss: 5.8176\n",
      "Epoch [1/1], Step [3206/3337], Loss: 5.7805\n",
      "Epoch [1/1], Step [3207/3337], Loss: 5.8233\n",
      "Epoch [1/1], Step [3208/3337], Loss: 5.6592\n",
      "Epoch [1/1], Step [3209/3337], Loss: 5.7035\n",
      "Epoch [1/1], Step [3210/3337], Loss: 5.8919\n",
      "Epoch [1/1], Step [3211/3337], Loss: 5.7802\n",
      "Epoch [1/1], Step [3212/3337], Loss: 5.7385\n",
      "Epoch [1/1], Step [3213/3337], Loss: 5.7862\n",
      "Epoch [1/1], Step [3214/3337], Loss: 5.6861\n",
      "Epoch [1/1], Step [3215/3337], Loss: 5.5427\n",
      "Epoch [1/1], Step [3216/3337], Loss: 5.7113\n",
      "Epoch [1/1], Step [3217/3337], Loss: 5.6990\n",
      "Epoch [1/1], Step [3218/3337], Loss: 5.7500\n",
      "Epoch [1/1], Step [3219/3337], Loss: 5.6972\n",
      "Epoch [1/1], Step [3220/3337], Loss: 5.6624\n",
      "Epoch [1/1], Step [3221/3337], Loss: 5.7114\n",
      "Epoch [1/1], Step [3222/3337], Loss: 5.9564\n",
      "Epoch [1/1], Step [3223/3337], Loss: 5.8385\n",
      "Epoch [1/1], Step [3224/3337], Loss: 5.8017\n",
      "Epoch [1/1], Step [3225/3337], Loss: 5.9313\n",
      "Epoch [1/1], Step [3226/3337], Loss: 5.7230\n",
      "Epoch [1/1], Step [3227/3337], Loss: 5.8518\n",
      "Epoch [1/1], Step [3228/3337], Loss: 5.8008\n",
      "Epoch [1/1], Step [3229/3337], Loss: 5.5900\n",
      "Epoch [1/1], Step [3230/3337], Loss: 5.6783\n",
      "Epoch [1/1], Step [3231/3337], Loss: 5.8079\n",
      "Epoch [1/1], Step [3232/3337], Loss: 5.7078\n",
      "Epoch [1/1], Step [3233/3337], Loss: 5.7192\n",
      "Epoch [1/1], Step [3234/3337], Loss: 5.9177\n",
      "Epoch [1/1], Step [3235/3337], Loss: 5.6748\n",
      "Epoch [1/1], Step [3236/3337], Loss: 5.7796\n",
      "Epoch [1/1], Step [3237/3337], Loss: 5.7041\n",
      "Epoch [1/1], Step [3238/3337], Loss: 5.7368\n",
      "Epoch [1/1], Step [3239/3337], Loss: 5.7156\n",
      "Epoch [1/1], Step [3240/3337], Loss: 5.8185\n",
      "Epoch [1/1], Step [3241/3337], Loss: 5.9261\n",
      "Epoch [1/1], Step [3242/3337], Loss: 5.7272\n",
      "Epoch [1/1], Step [3243/3337], Loss: 5.7263\n",
      "Epoch [1/1], Step [3244/3337], Loss: 5.6930\n",
      "Epoch [1/1], Step [3245/3337], Loss: 5.8330\n",
      "Epoch [1/1], Step [3246/3337], Loss: 5.7329\n",
      "Epoch [1/1], Step [3247/3337], Loss: 5.8862\n",
      "Epoch [1/1], Step [3248/3337], Loss: 5.7636\n",
      "Epoch [1/1], Step [3249/3337], Loss: 5.7954\n",
      "Epoch [1/1], Step [3250/3337], Loss: 5.9093\n",
      "Epoch [1/1], Step [3251/3337], Loss: 5.8400\n",
      "Epoch [1/1], Step [3252/3337], Loss: 5.6484\n",
      "Epoch [1/1], Step [3253/3337], Loss: 5.7290\n",
      "Epoch [1/1], Step [3254/3337], Loss: 5.7404\n",
      "Epoch [1/1], Step [3255/3337], Loss: 5.6360\n",
      "Epoch [1/1], Step [3256/3337], Loss: 5.8291\n",
      "Epoch [1/1], Step [3257/3337], Loss: 5.9399\n",
      "Epoch [1/1], Step [3258/3337], Loss: 5.6659\n",
      "Epoch [1/1], Step [3259/3337], Loss: 5.8502\n",
      "Epoch [1/1], Step [3260/3337], Loss: 5.6974\n",
      "Epoch [1/1], Step [3261/3337], Loss: 5.9580\n",
      "Epoch [1/1], Step [3262/3337], Loss: 5.7468\n",
      "Epoch [1/1], Step [3263/3337], Loss: 5.8463\n",
      "Epoch [1/1], Step [3264/3337], Loss: 5.6819\n",
      "Epoch [1/1], Step [3265/3337], Loss: 5.7467\n",
      "Epoch [1/1], Step [3266/3337], Loss: 5.6573\n",
      "Epoch [1/1], Step [3267/3337], Loss: 5.7610\n",
      "Epoch [1/1], Step [3268/3337], Loss: 5.7302\n",
      "Epoch [1/1], Step [3269/3337], Loss: 5.6568\n",
      "Epoch [1/1], Step [3270/3337], Loss: 5.5694\n",
      "Epoch [1/1], Step [3271/3337], Loss: 5.8797\n",
      "Epoch [1/1], Step [3272/3337], Loss: 5.7521\n",
      "Epoch [1/1], Step [3273/3337], Loss: 5.6500\n",
      "Epoch [1/1], Step [3274/3337], Loss: 5.8593\n",
      "Epoch [1/1], Step [3275/3337], Loss: 5.8928\n",
      "Epoch [1/1], Step [3276/3337], Loss: 5.6831\n",
      "Epoch [1/1], Step [3277/3337], Loss: 5.6944\n",
      "Epoch [1/1], Step [3278/3337], Loss: 5.6845\n",
      "Epoch [1/1], Step [3279/3337], Loss: 5.7243\n",
      "Epoch [1/1], Step [3280/3337], Loss: 5.7092\n",
      "Epoch [1/1], Step [3281/3337], Loss: 5.7924\n",
      "Epoch [1/1], Step [3282/3337], Loss: 5.8778\n",
      "Epoch [1/1], Step [3283/3337], Loss: 5.7955\n",
      "Epoch [1/1], Step [3284/3337], Loss: 5.7453\n",
      "Epoch [1/1], Step [3285/3337], Loss: 5.9069\n",
      "Epoch [1/1], Step [3286/3337], Loss: 5.8713\n",
      "Epoch [1/1], Step [3287/3337], Loss: 5.7588\n",
      "Epoch [1/1], Step [3288/3337], Loss: 5.7864\n",
      "Epoch [1/1], Step [3289/3337], Loss: 5.7873\n",
      "Epoch [1/1], Step [3290/3337], Loss: 5.8449\n",
      "Epoch [1/1], Step [3291/3337], Loss: 5.7666\n",
      "Epoch [1/1], Step [3292/3337], Loss: 5.6598\n",
      "Epoch [1/1], Step [3293/3337], Loss: 5.7619\n",
      "Epoch [1/1], Step [3294/3337], Loss: 5.6530\n",
      "Epoch [1/1], Step [3295/3337], Loss: 5.7352\n",
      "Epoch [1/1], Step [3296/3337], Loss: 5.6558\n",
      "Epoch [1/1], Step [3297/3337], Loss: 5.6853\n",
      "Epoch [1/1], Step [3298/3337], Loss: 5.6669\n",
      "Epoch [1/1], Step [3299/3337], Loss: 5.6740\n",
      "Epoch [1/1], Step [3300/3337], Loss: 5.7235\n",
      "Epoch [1/1], Step [3301/3337], Loss: 5.7142\n",
      "Epoch [1/1], Step [3302/3337], Loss: 5.5702\n",
      "Epoch [1/1], Step [3303/3337], Loss: 5.7637\n",
      "Epoch [1/1], Step [3304/3337], Loss: 5.7903\n",
      "Epoch [1/1], Step [3305/3337], Loss: 5.6037\n",
      "Epoch [1/1], Step [3306/3337], Loss: 5.8896\n",
      "Epoch [1/1], Step [3307/3337], Loss: 5.6614\n",
      "Epoch [1/1], Step [3308/3337], Loss: 5.7214\n",
      "Epoch [1/1], Step [3309/3337], Loss: 5.6946\n",
      "Epoch [1/1], Step [3310/3337], Loss: 5.6765\n",
      "Epoch [1/1], Step [3311/3337], Loss: 5.6917\n",
      "Epoch [1/1], Step [3312/3337], Loss: 5.7908\n",
      "Epoch [1/1], Step [3313/3337], Loss: 5.8051\n",
      "Epoch [1/1], Step [3314/3337], Loss: 5.7762\n",
      "Epoch [1/1], Step [3315/3337], Loss: 5.7314\n",
      "Epoch [1/1], Step [3316/3337], Loss: 5.6802\n",
      "Epoch [1/1], Step [3317/3337], Loss: 5.6411\n",
      "Epoch [1/1], Step [3318/3337], Loss: 5.7662\n",
      "Epoch [1/1], Step [3319/3337], Loss: 5.7579\n",
      "Epoch [1/1], Step [3320/3337], Loss: 5.6372\n",
      "Epoch [1/1], Step [3321/3337], Loss: 5.7736\n",
      "Epoch [1/1], Step [3322/3337], Loss: 5.8094\n",
      "Epoch [1/1], Step [3323/3337], Loss: 5.7393\n",
      "Epoch [1/1], Step [3324/3337], Loss: 5.6646\n",
      "Epoch [1/1], Step [3325/3337], Loss: 5.5535\n",
      "Epoch [1/1], Step [3326/3337], Loss: 5.6629\n",
      "Epoch [1/1], Step [3327/3337], Loss: 5.7636\n",
      "Epoch [1/1], Step [3328/3337], Loss: 5.7777\n",
      "Epoch [1/1], Step [3329/3337], Loss: 5.7427\n",
      "Epoch [1/1], Step [3330/3337], Loss: 5.7196\n",
      "Epoch [1/1], Step [3331/3337], Loss: 5.6453\n",
      "Epoch [1/1], Step [3332/3337], Loss: 5.8264\n",
      "Epoch [1/1], Step [3333/3337], Loss: 5.7704\n",
      "Epoch [1/1], Step [3334/3337], Loss: 5.7494\n",
      "Epoch [1/1], Step [3335/3337], Loss: 5.6179\n",
      "Epoch [1/1], Step [3336/3337], Loss: 5.7831\n",
      "Epoch [1/1], Step [3337/3337], Loss: 5.5883\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "learning_rates = []\n",
    "\n",
    "# Training Loop\n",
    "for epoch_idx in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (train_images, train_labels) in enumerate(train_loader1k):\n",
    "        train_images, train_labels = train_images.to(device), train_labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        train_outputs = model(train_images)\n",
    "\n",
    "        # Compute the loss\n",
    "        train_loss = criterion(train_outputs, train_labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Gradient Clip\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Step optimizer and the scheduler\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss.item())\n",
    "        learning_rates.append(scheduler.get_last_lr()[0])  # Assumes optimizer has a single param group\n",
    "\n",
    "        print(f\"Epoch [{epoch_idx+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader1k)}], Loss: {train_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the validation images: 4.744%\n"
     ]
    }
   ],
   "source": [
    "# Validation Loop\n",
    "# NOTE: LOGITS TO MAX LOGIT FUNCTION MIGHT CHANGE DUE TO SPECIFIC NATURE OF VISION TRANSFORMER ALGORITHM\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for val_images, val_labels in val_loader1k:\n",
    "        val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "\n",
    "        # Logits\n",
    "        val_outputs = model(val_images)\n",
    "\n",
    "        # Let the index of the highest logit be the predicted class \n",
    "        _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "\n",
    "        # Update counts from this batch's values\n",
    "        total_count += val_labels.size(0)\n",
    "        correct_count += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "    # Print accuracy score\n",
    "    print(f'Accuracy of the model on the validation images: {100 * correct_count / total_count}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save(model, './models/vit-base-32p.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit21k_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
